{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CESNET TSZoo","text":"<p>This is documentation of the CESNET TSZoo project. </p> <p>The goal of <code>cesnet-tszoo</code> project is to provide time series datasets with useful tools for preprocessing and reproducibility. Such as:</p> <ul> <li>API for downloading, configuring and loading CESNET-TimeSeries24, CESNET-AGG23 datasets. Each with various sources and aggregations. Check dataset overview page for details about datasets.</li> <li>Example of configuration options:<ul> <li>Data can be split into train/val/test sets. Split can be done by time series, check <code>SeriesBasedCesnetDataset</code>, or by time periods, check <code>TimeBasedCesnetDataset</code>.</li> <li>Transforming of data with built-in scalers or with custom scalers. Check <code>scalers</code> for details.</li> <li>Handling missing values built-in fillers or with custom fillers. Check <code>fillers</code> for details.</li> </ul> </li> <li>Creation and import of benchmarks, for easy reproducibility of experiments.</li> <li>Creation and import of annotations. Can create annotations for specific time series, specific time or specific time in specific time series.</li> </ul>"},{"location":"annotations/","title":"Annotations","text":"<p>This tutorial will look at how to use annotations.</p> <p>Note</p> <p>For every option and more detailed examples refer to Jupyter notebook <code>annotations</code>.</p>"},{"location":"annotations/#basics","title":"Basics","text":"<ul> <li>You can get annotations for specific type with <code>get_annotations</code> method. </li> <li>Method <code>get_annotations</code> returns annotations as Pandas Dataframe.</li> </ul> <p>There are three annotation types:</p> <ol> <li>AnnotationType.TS_ID -&gt; Annotations for whole specific time series</li> <li>AnnotationType.ID_TIME -&gt; Annotations for specific time... independent on time series</li> <li>AnnotationType.BOTH -&gt; Annotations for specific time in specific time series</li> </ol> <pre><code>from cesnet_tszoo.utils.enums import AnnotationType                                                                          \n\ndataset.get_annotations(on=AnnotationType.TS_ID)\ndataset.get_annotations(on=AnnotationType.ID_TIME)\ndataset.get_annotations(on=AnnotationType.BOTH)\n</code></pre>"},{"location":"annotations/#annotation-groups","title":"Annotation groups","text":"<ul> <li>Annotation group could be understood as column names in Dataframe/CSV.</li> <li>You can add annotation groups or remove them.</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import AnnotationType                                                                          \n\n# Adding groups\ndataset.add_annotation_group(annotation_group=\"test1\", on=AnnotationType.TS_ID)\ndataset.add_annotation_group(annotation_group=\"test2\", on=AnnotationType.ID_TIME)\ndataset.add_annotation_group(annotation_group=\"test3\", on=AnnotationType.BOTH)\n\n# Removing groups\ndataset.remove_annotation_group(annotation_group=\"test1\", on=AnnotationType.TS_ID)\ndataset.remove_annotation_group(annotation_group=\"test2\", on=AnnotationType.ID_TIME)\ndataset.remove_annotation_group(annotation_group=\"test3\", on=AnnotationType.BOTH)\n</code></pre>"},{"location":"annotations/#annotation-values","title":"Annotation values","text":"<ul> <li>Annotations are specific values for selected annotation group and AnnotationType.</li> <li>You can add annotations or remove them.</li> <li>Adding annotation<ul> <li>When adding annotation to annotation group that does not exist, it will be created.</li> <li>To override existing annotation, you just need to specify same <code>annotation_group</code>, <code>ts_id</code>, <code>id_time</code> and new annotation.</li> <li>Setting <code>enforce_ids</code> to True, ensures that inputted <code>ts_id</code> and <code>id_time</code> must belong to used dataset.</li> </ul> </li> <li>Removing annotations<ul> <li>Removing annotation from every annotation group of a row, removes that row from Dataframe.</li> </ul> </li> </ul> <pre><code># Adding annotations\ndataset.add_annotation(annotation=\"test_annotation1_3\", annotation_group=\"test1\", ts_id=3, id_time=None, enforce_ids=True) # Adds to AnnotationType.TS_ID\ndataset.add_annotation(annotation=\"test_annotation2_0\", annotation_group=\"test2\", ts_id=None, id_time=0, enforce_ids=True) # Adds to AnnotationType.ID_TIME\ndataset.add_annotation(annotation=\"test_annotation3_3_0\", annotation_group=\"test3\", ts_id=3, id_time=0, enforce_ids=True) # Adds to AnnotationType.BOTH\n\n# Removing annotations\ndataset.remove_annotation(annotation_group=\"test1\", ts_id=3, id_time=None) # Removes from AnnotationType.TS_ID\ndataset.remove_annotation(annotation_group=\"test2\", ts_id=None, id_time=0 ) # Removes from AnnotationType.ID_TIME\ndataset.remove_annotation(annotation_group=\"test3\", ts_id=3, id_time=0 ) # Removes from AnnotationType.BOTH\n</code></pre>"},{"location":"annotations/#exporting-annotations","title":"Exporting annotations","text":"<ul> <li>You can export your created annotation with <code>save_annotations</code> method.</li> <li><code>save_annotations</code> creates CSV file at: <code>os.path.join(dataset.annotations_root, identifier)</code>.</li> <li>When parameter <code>force_write</code> is True, existing files with same name will be overwritten.</li> <li>You should not add \".csv\" to identifier, because it will be added automatically.</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import AnnotationType   \n\ndataset.save_annotations(identifier=\"test_name\", on=AnnotationType.BOTH, force_write=True)\n</code></pre>"},{"location":"annotations/#importing-annotations","title":"Importing annotations","text":"<ul> <li>You can import already existing annotations, be it your own or already built-in one.</li> <li>Setting <code>enforce_ids</code> to True, ensures that all <code>ts_id</code> or <code>id_time</code> from imported annotations must belong to used dataset.</li> <li>Method <code>import_annotations</code> automatically detects what AnnotationType imported annotations is, based on existing ts_id (expects name of ts_id for used dataset) or id_time columns.</li> <li>First, it attempts to load the built-in annotations, if no built-in annotations with such an identifier exists, it attempts to load a custom annotations from the <code>\"data_root\"/tszoo/annotations/</code> directory.</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import AnnotationType   \n\ndataset.import_annotations(identifier=\"test_name\", enforce_ids=True)\n</code></pre>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>CESNET-TS-Zoo enables easy sharing and reuse of configuration files to support open science, reproducibility, and transparent comparison of time series modeling approaches.</p> <p>We provide a collection of pre-defined configurations that serve as benchmarks, including use cases like network traffic forecasting and anomaly detection.</p> <p>The library includes tools for both importing and exporting configurations as benchmarks. This allows researchers to cite a specific benchmark via its unique hash or to share their own approach as a configuration file.</p> <p>To load and use a benchmark in your code, simply use the following snippet:</p> <pre><code>from cesnet_tszoo.benchmarks import load_benchmark\n\nbenchmark = load_benchmark(\"&lt;benchmark_hash&gt;\", \"&lt;path-to-datasets&gt;\")\ndataset = benchmark.get_initialized_dataset()\n</code></pre> <p>Note</p> <p>More detailed tutorial how to use benchmarks is available <code>here</code></p>"},{"location":"benchmarks/#available-benchmarks","title":"Available benchmarks","text":""},{"location":"benchmarks/#network-traffic-forecasting-benchmarks","title":"Network Traffic Forecasting Benchmarks","text":"<p>Network traffic forecasting plays a crucial role in network management and security. Therefore, we prepared several benchmarks for evaluation of network traffic forecasting methods for both management and security tasks. We split the <code>Network Traffic Forecasting Benchmarks</code> into these two groups:</p> <ul> <li>\"Univariate forecasting - Transmitted data size\": Benchmarks in this group are designed to support mostly used forecasting task for network management.</li> <li>\"Multivariate forecasting\": Benchmarks in this group are designed to multivariate forecasting of network traffic features which is more often usable in network security for anomaly/outlier detection.</li> </ul>"},{"location":"benchmarks/#network-device-type-classification-benchmarks","title":"Network Device Type Classification Benchmarks","text":"<p>Network device type classification focuses on evaluating the performance of models for classifying types of network devices. The goal of this benchmark is to allow comparison of various classification algorithms and methods in the context of network devices. This task is valuable in environments where it is essential to quickly and efficiently identify devices in a network for monitoring, security, and traffic optimization purposes. Analyzing the benchmarks helps determine which methods are most suitable for deployment in real-world scenarios.</p> <p>The network device type classification benchmarks are described in detail: here</p>"},{"location":"benchmarks/#anomaly-detection-benchmarks","title":"Anomaly Detection Benchmarks","text":"<p>This benchmarks are in process of making and they will be added soon.</p>"},{"location":"benchmarks/#similarity-search-benchmarks","title":"Similarity Search Benchmarks","text":"<p>This benchmarks are in process of making and they will be added soon.</p>"},{"location":"benchmarks/#available-dataset-configs-from-related-works","title":"Available dataset configs from related works","text":"<p>For supporting reproducibility of approaches, the CESNET-TS-Zoo allows to share ts-zoo configs with others using pull request from forked repository.</p> <p>Each related work contains configs and example of usage. Please follow authors instruction in example to ensure comparable results. Following configs are already included in the ts-zoo:</p> DOI Task Configs link https://doi.org/10.48550/arXiv.2503.17410 Univariate forecasting configs"},{"location":"benchmarks_tutorial/","title":"Benchmarks","text":"<p>This tutorial will look at how to use benchmarks.</p> <p>Only time-based will be used, because all methods work almost the same way for series-based.</p> <p>Note</p> <p>For every option and more detailed examples refer to Jupyter notebook <code>benchmarks</code></p> <p>Benchmarks can consist of various parts:</p> <ul> <li>identifier of used config</li> <li>identifier of used annotations (for each AnnotationType)</li> <li>identifier of related_results (only available for premade benchmarks)</li> <li>Used SourceType and AggregationType</li> <li>Database name (here it would be CESNET_TimeSeries24)</li> <li>Whether config or annotations are built-in</li> </ul>"},{"location":"benchmarks_tutorial/#importing-benchmarks","title":"Importing benchmarks","text":"<ul> <li>You can import your own or built-in benchmark with <code>load_benchmark</code> function.</li> <li>When importing benchmark with annotations that exist, but are not downloaded, they will be downloaded (only works for built-in annotations),</li> <li>First, it attempts to load the built-in benchmark, if no built-in benchmark with such an identifier exists, it attempts to load a custom benchmark from the <code>\"data_root\"/tszoo/benchmarks/</code> directory.</li> </ul> <pre><code>from cesnet_tszoo.benchmarks import load_benchmark                                                                       \n\n# Imports built-in benchmark\n# Can get related_results with `get_related_results` method.\n# Method `get_related_results` returns pandas Dataframe. \nbenchmark = load_benchmark(identifier=\"2e92831cb502\", data_root=\"/some_directory/\")\ndataset = benchmark.get_initialized_dataset(display_config_details=True, check_errors=False, workers=\"config\")\n\n# Imports custom benchmark\n# Looks for benchmark at: `os.path.join(\"/some_directory/\", \"tszoo\", \"benchmarks\", identifier)`\nbenchmark = load_benchmark(identifier=\"test2\", data_root=\"/some_directory/\")\ndataset = benchmark.get_initialized_dataset(display_config_details=True, check_errors=False, workers=\"config\")\n</code></pre>"},{"location":"benchmarks_tutorial/#exporting-benchmarks","title":"Exporting benchmarks","text":"<ul> <li>You can use method <code>save_benchmark</code> to save benchmark.</li> <li>Saving benchmark creates YAML file, which hold metadata, at: <code>os.path.join(dataset.benchmarks_root, identifier)</code>.</li> <li>Saving benchmark automatically creates files for config and annotations with identifiers matching benchmark identifier</li> <li>config will be saved at: <code>os.path.join(dataset.configs_root, identifier)</code></li> <li>annotations will be saved at: <code>os.path.join(dataset.annotations_root, identifier, str(AnnotationType))</code></li> <li>When parameter <code>force_write</code> is True, existing files with the same name will be overwritten.</li> <li>When using imported config or annotations, only their identifier will be passed to benchmark and no new files will get created</li> <li>if calling anything that changes annotations, it will no longer be taken as imported</li> <li>Only annotations with at least one value will be exported.</li> <li>You can export benchmarks with custom scalers or fillers, but should share their source code along with benchmark</li> </ul> <pre><code>from cesnet_tszoo.datasets import CESNET_TimeSeries24\nfrom cesnet_tszoo.configs import TimeBasedConfig                                                                            \n\ntime_based_dataset = CESNET_TimeSeries24.get_dataset(data_root=\"/some_directory/\", source_type=SourceType.IP_ADDRESSES_FULL, aggregation=AgreggationType.AGG_1_DAY, is_series_based=False, display_details=True)\nconfig = TimeBasedConfig([1548925, 443967], train_time_period=1.0, features_to_take=[\"n_flows\", \"n_packets\", \"n_bytes\"], scale_with=None)\n\n# Call on time-based dataset to use created config -&gt; must be done before saving exporting benchmark\ntime_based_dataset.set_dataset_config_and_initialize(config, workers=0, display_config_details=True)\n\ntime_based_dataset.save_benchmark(identifier=\"test1\", force_write=True)\n</code></pre>"},{"location":"benchmarks_tutorial/#other","title":"Other","text":"<p>Instead of exporting or importing whole benchmark you can do for specific config or annotations.</p>"},{"location":"benchmarks_tutorial/#config","title":"Config","text":"<ul> <li>Saving config<ul> <li>When parameter <code>force_write</code> is True, existing files with the same name will be overwritten.</li> <li>Config will be saved as pickle file at: <code>os.path.join(dataset.configs_root, identifier)</code></li> <li>When parameter <code>create_with_details_file</code> is True, text file with config details will be exported along pickle config.</li> </ul> </li> <li>Importing config<ul> <li> <ul> <li>First, it attempts to load the built-in config, if no built-in config with such an identifier exists, it attempts to load a custom config from the <code>\"data_root\"/tszoo/configs/</code> directory.</li> </ul> </li> </ul> </li> </ul> <pre><code>from cesnet_tszoo.configs import TimeBasedConfig                                                                      \n\nconfig = TimeBasedConfig([1548925, 443967], train_time_period=1.0, features_to_take=[\"n_flows\", \"n_packets\", \"n_bytes\"], scale_with=None)\n\ntime_based_dataset.set_dataset_config_and_initialize(config, workers=0, display_config_details=True)\n\n# Exports config\ntime_based_dataset.save_config(identifier=\"test_config1\", create_with_details_file=True, force_write=True)\n\n# Imports custom config\ntime_based_dataset.import_config(identifier=\"test_config1\", display_config_details=True, workers=\"config\")\n</code></pre>"},{"location":"benchmarks_tutorial/#annotations","title":"Annotations","text":"<ul> <li>Saving annotation<ul> <li>When parameter <code>force_write</code> is True, existing files with the same name will be overwritten.</li> <li>Annotations will be saved as CSV file at: <code>os.path.join(dataset.annotations_root, identifier)</code>.</li> </ul> </li> <li>Importing annotation<ul> <li>First, it attempts to load the built-in annotations, if no built-in annotations with such an identifier exists, it attempts to load a custom annotations from the <code>\"data_root\"/tszoo/annotations/</code> directory.</li> </ul> </li> </ul> <pre><code>from cesnet_tszoo.utils.enums import AnnotationType                                                                    \n\ndataset.add_annotation(annotation=\"test_annotation3_3_0\", annotation_group=\"test3\", ts_id=3, id_time=0, enforce_ids=True)\ndataset.add_annotation(annotation=\"test_annotation3_3_5\", annotation_group=\"test3_2\", ts_id=3, id_time=5, enforce_ids=True)\ndataset.add_annotation(annotation=\"test_annotation3_5_0\", annotation_group=\"test3\", ts_id=5, id_time=0, enforce_ids=True)\ndataset.add_annotation(annotation=\"test_annotation3_5_1\", annotation_group=\"test3_2\", ts_id=5, id_time=1, enforce_ids=True)\ndataset.get_annotations(on=AnnotationType.BOTH)\n\n# Exports annotation of type BOTH\ndataset.save_annotations(identifier=\"test_annotations1\", on=AnnotationType.BOTH, force_write=True)\n\n# Imports custom annotations\ndataset.import_annotations(identifier=\"test_annotations1\", enforce_ids=True)\n</code></pre>"},{"location":"cesnet_agg23/","title":"CESNET-AGG23","text":""},{"location":"cesnet_agg23/#data-capture","title":"Data capture","text":"<p>The data was captured in the flow monitoring infrastructure of the CESNET2 network. The capturing was done for two months between 25.2.2023 and 3.5.2023.</p>"},{"location":"cesnet_agg23/#data-description","title":"Data description","text":"<p>The dataset consists of rules set used by the Scalar aggregator to aggregate information from incoming flows gathered by ipfixprobe. Each row in the dataset represents a single time 1-minute window interval.</p>"},{"location":"cesnet_agg23/#ipfixprobe-parameters","title":"ipfixprobe parameters","text":"<pre><code>Active timeout: 10min\nInactive timeout: 1min\n</code></pre>"},{"location":"cesnet_agg23/#list-of-time-series-metrics","title":"List of time series metrics","text":"Time Series Metric Description id_time Unique identifier for each aggregation interval within the time series, used to segment the dataset into specific time periods for analysis. avr_duration The average duration of all flows. avr_duration_ipv4 The average duration of IPV4 flows. avr_duration_ipv6 The average duration of IPV6 flows. avr_duration_tcp The average duration of TCP flows. avr_duration_udp The average duration of UDP flows. byte_avg The average number of bytes of all flows. byte_avg_ipv4 The average number of bytes of IPV4 flows. byte_avg_ipv6 The average number of bytes of IPV6 flows. byte_avg_tcp The average number of bytes of TCP flows. byte_avg_udp The average number of bytes of UDP flows. byte_rate Byte rate estimation on the network of all flows. byte_rate_ipv4 Byte rate estimation on the network of IPV4 flows. byte_rate_ipv6 Byte rate estimation on the network of IPV6 flows. byte_rate_tcp Byte rate estimation on the network of TCP flows. byte_rate_udp Byte rate estimation on the network of UDP flows. bytes The sum of bytes of all flows. bytes_ipv4 The sum of bytes of IPV4 flows. bytes_ipv6 The sum of bytes of IPV6 flows. bytes_tcp The sum of bytes of TCP flows. bytes_udp The sum of bytes of UDP flows. no_flows The number of all active flows. no_flows_ipv4 The number of IPV4 active flows. no_flows_ipv6 The number of IPV6 active flows. no_flows_tcp The number of TCP active flows. no_flows_tcp_synonly The number of flows containing only SYN packets. no_flows_udp The number of UDP active flows. no_uniq_biflows The number of all unique biflows. no_uniq_flows The number of all unique flows. packet_avg The average packets of all flows. packet_avg_ipv4 The average packets of IPV4 flows. packet_avg_ipv6 The average packets of IPV6 flows. packet_avg_tcp The average packets of TCP flows. packet_avg_udp The average packets of UDP flows. packet_rate Packet rate estimation on the network of all flows. packet_rate_ipv4 Packet rate estimation on the network of IPV4 flows. packet_rate_ipv6 Packet rate estimation on the network of IPV6 flows. packet_rate_tcp Packet rate estimation on the network of TCP flows. packet_rate_udp Packet rate estimation on the network of UDP flows. packets The sum of packets of all flows. packets_ipv4 The sum of packets of IPV4 flows. packets_ipv6 The sum of packets of IPV6 flows. packets_tcp The sum of packets of TCP flows. packets_udp The sum of packets of UDP flows. <p>More detailed description is available in the paper or you can contact dataset author Jaroslav Pesek.</p>"},{"location":"cesnet_timeseries24/","title":"CESNET-TimeSeries24","text":""},{"location":"cesnet_timeseries24/#data-capture","title":"Data capture","text":"<p>The dataset called CESNET-TimeSeries24 was collected by long-term monitoring of selected statistical metrics for 40 weeks for each IP address on the ISP network CESNET3 (Czech Education and Science Network). The dataset encompasses network traffic from more than 275,000 active IP addresses, assigned to a wide variety of devices, including office computers, NATs, servers, WiFi routers, honeypots, and video-game consoles found in dormitories. Moreover, the dataset is also rich in network anomaly types since it contains all types of anomalies, ensuring a comprehensive evaluation of anomaly detection methods.</p> <p>Last but not least, the CESNET-TimeSeries24 dataset provides traffic time series on institutional and IP subnet levels to cover all possible anomaly detection or forecasting scopes. Overall, the time series dataset was created from the 66 billion IP flows that contain 4 trillion packets that carry approximately 3.7 petabytes of data. The CESNET-TimeSeries24 dataset is a complex real-world dataset that will finally bring insights into the evaluation of forecasting models in real-world environments.</p>"},{"location":"cesnet_timeseries24/#data-description","title":"Data description","text":"<p>We create evenly spaced time series for each IP address by aggregating IP flow records into time series datapoints. The created datapoints represent the behavior of IP addresses within a defined time window of 10 minutes. The time series are built from multivariate datapoints.  </p> <p>Datapoints created by the aggregation of IP flows contain the following time-series metrics:</p> <ul> <li>Simple volumetric metrics: the number of IP flows, the number of packets, and the transmitted data size (i.e. number of bytes)</li> <li>Unique volumetric metrics: the number of unique destination IP addresses, the number of unique destination Autonomous System Numbers (ASNs), and the number of unique destination transport layer ports. The aggregation of Unique volumetric metrics is memory intensive since all unique values must be stored in an array. We used a server with 41 GB of RAM, which was enough for 10-minute aggregation on the ISP network.</li> <li>Ratios metrics: the ratio of UDP/TCP packets, the ratio of UDP/TCP transmitted data size, the direction ratio of packets, and the direction ratio of transmitted data size</li> <li>Average metrics: the average flow duration, and the average Time To Live (TTL)</li> </ul>"},{"location":"cesnet_timeseries24/#multiple-time-aggregation","title":"Multiple time aggregation","text":"<p>The original datapoints in the dataset are aggregated by 10 minutes of network traffic. The size of the aggregation interval influences anomaly detection procedures, mainly the training speed of the detection model. However, the 10-minute intervals can be too short for longitudinal anomaly detection methods. Therefore, we added two more aggregation intervals to the datasets--1 hour and 1 day.</p>"},{"location":"cesnet_timeseries24/#time-series-of-institutions","title":"Time series of institutions","text":"<p>We identify 283 institutions inside the CESNET3 network. These time series aggregated per each institution ID provide a view of the institution's data.</p>"},{"location":"cesnet_timeseries24/#time-series-of-institutional-subnets","title":"Time series of institutional subnets","text":"<p>We identify 548 institution subnets inside the CESNET3 network. These time series aggregated per each institution ID provide a view of the institution subnet's data.</p>"},{"location":"cesnet_timeseries24/#ipfixprobe-parameters","title":"ipfixprobe parameters","text":"<pre><code>Active timeout: 5min\nInactive timeout: 65s\n</code></pre>"},{"location":"cesnet_timeseries24/#list-of-time-series-metrics","title":"List of time series metrics","text":"<p>The following list describes time series metrics in dataset:</p> Time Series Metric Description id_time Unique identifier for each aggregation interval within the time series, used to segment the dataset into specific time periods for analysis. n_flows Total number of flows observed in the aggregation interval, indicating the volume of distinct sessions or connections for the IP address. n_packets Total number of packets transmitted during the aggregation interval, reflecting the packet-level traffic volume for the IP address. n_bytes Total number of bytes transmitted during the aggregation interval, representing the data volume for the IP address. n_dest_ip Number of unique destination IP addresses contacted by the IP address during the aggregation interval, showing the diversity of endpoints reached. n_dest_asn Number of unique destination Autonomous System Numbers (ASNs) contacted by the IP address during the aggregation interval, indicating the diversity of networks reached. n_dest_port Number of unique destination transport layer ports contacted by the IP address during the aggregation interval, representing the variety of services accessed. tcp_udp_ratio_packets Ratio of packets sent using TCP versus UDP by the IP address during the aggregation interval, providing insight into the transport protocol usage pattern. This metric belongs to the interval &lt;0, 1&gt; where 1 is when all packets are sent over TCP, and 0 is when all packets are sent over UDP. tcp_udp_ratio_bytes Ratio of bytes sent using TCP versus UDP by the IP address during the aggregation interval, highlighting the data volume distribution between protocols. This metric belongs to the interval &lt;0, 1&gt;  with same rule as tcp_udp_ratio_packets. dir_ratio_packets Ratio of packet directions (inbound versus outbound) for the IP address during the aggregation interval, indicating the balance of traffic flow directions. This metric belongs to the interval &lt;0, 1&gt;, where 1 is when all packets are sent in the outgoing direction from the monitored IP address, and 0 is when all packets are sent in the incoming direction to the monitored IP address. dir_ratio_bytes Ratio of byte directions (inbound versus outbound) for the IP address during the aggregation interval, showing the data volume distribution in traffic flows. This metric belongs to the interval &lt;0, 1&gt; with the same rule as dir_ratio_packets. avg_duration Average duration of IP flows for the IP address during the aggregation interval, measuring the typical session length. avg_ttl Average Time To Live (TTL) of IP flows for the IP address during the aggregation interval, providing insight into the lifespan of packets. <p>Moreover, the time series created by re-aggregation contains following time series metrics instead of n_dest_ip, n_dest_asn, and n_dest_port:</p> Time Series Metric Description sum_n_dest_ip Sum of numbers of unique destination IP addresses. avg_n_dest_ip The average number of unique destination IP addresses. std_n_dest_ip Standard deviation of numbers of unique destination IP addresses. sum_n_dest_asn Sum of numbers of unique destination ASNs. avg_n_dest_asn The average number of unique destination ASNs. std_n_dest_asn Standard deviation of numbers of unique destination ASNs. sum_n_dest_port Sum of numbers of unique destination transport layer ports. avg_n_dest_port The average number of unique destination transport layer ports. std_n_dest_port Standard deviation of numbers of unique destination transport layer ports. <p>More detailed description is available in the dataset paper or you can contact dataset author Josef Koumar.</p>"},{"location":"choosing_data/","title":"Choosing data","text":"<p>This tutorial will look at some configuration options for choosing data you wish to load. </p> <p>Each dataset type will have its own part because of multiple differences of available configuration values.</p>"},{"location":"choosing_data/#timebasedcesnetdataset-dataset","title":"<code>TimeBasedCesnetDataset</code> dataset","text":"<p>Note</p> <p>For every configuration and more detailed examples refer to Jupyter notebook <code>time_based_choosing_data</code></p> <p>Relevant configuration values:</p> <ul> <li><code>ts_ids</code> - Defines which time series IDs are used for train/val/test/all.</li> <li><code>test_ts_ids</code> - Defines which time series IDs are used in the test_other set.</li> <li><code>train_time_period</code>/<code>val_time_period</code>/<code>test_time_period</code> - Defines time periods for train/val/test sets.</li> <li><code>features_to_take</code> - Defines which features are used.</li> <li><code>include_time</code> - If True, time data is included in the returned values.</li> <li><code>include_ts_id</code> - If True, time series IDs are included in the returned values.</li> <li><code>time_format</code> - Format for the returned time data.</li> <li><code>random_state</code> - Fixes randomness for reproducibility when setting <code>ts_ids</code> or <code>test_ts_ids</code></li> </ul>"},{"location":"choosing_data/#selecting-which-time-series-to-load","title":"Selecting which time series to load","text":"<ul> <li>Sets time series that will be used for train/val/test/all sets</li> </ul> <pre><code>from cesnet_tszoo.configs import TimeBasedConfig\n\n# Sets time series used in sets with count. Chosen randomly from available time series.\n# Affected by random_state.\nconfig = TimeBasedConfig(ts_ids=54, random_state = 111)\n\n# Sets time series used in sets with percentage of time series in dataset. Chosen randomly from available time series.\n# Affected by random_state.\nconfig = TimeBasedConfig(ts_ids=0.1, random_state = 111)\n\n# Sets ts_ids with specific time series\nconfig = TimeBasedConfig(ts_ids=[0,1,2,3,4,5])\n\n# Call on time-based dataset to use created config\ntime_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre> <p>You can also specify time series for <code>test_ts_ids</code>, but they will only be used when <code>test_time_period</code> is set. They can be set the same way as <code>ts_ids</code></p> <pre><code>from cesnet_tszoo.configs import TimeBasedConfig\n\n# Both ts_ids and test_ts_ids will contain unique time series.\nconfig = TimeBasedConfig(ts_ids=54, test_ts_ids=20, test_time_period=range(0, 1000), random_state = 111)\n\ntime_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre>"},{"location":"choosing_data/#creating-trainvaltest-sets","title":"Creating train/val/test sets","text":"<ul> <li>Sets time period in set for every time series in <code>ts_ids</code></li> <li>You can leave any set value set as None.</li> <li>Can use <code>nan_threshold</code> to set how many nan values will be tolerated.<ul> <li><code>nan_threshold</code> = 1.0, means that time series can be completely empty.</li> <li>is applied after sets.</li> <li>Is checked seperately for every set.</li> </ul> </li> <li>Sets must follow these rules:<ul> <li>Used time periods must be connected.</li> <li>Sets can share subset of times.</li> <li>start of <code>train_time_period</code> &lt; start of <code>val_time_period</code> &lt; start of <code>test_time_period</code>.</li> </ul> </li> </ul> <pre><code>from datetime import datetime\n\nfrom cesnet_tszoo.configs import TimeBasedConfig\n\n# Sets sets as range of time indices.\nconfig = TimeBasedConfig(ts_ids=54, train_time_period=range(0, 2000), val_time_period=range(2000, 4000), test_time_period=range(4000, 5000))\n\n# Sets sets with tuple of datetime objects.\n# Datetime objects are expected to be of UTC.\nconfig = TimeBasedConfig(ts_ids=54, train_time_period=(datetime(2023, 10, 9, 0), datetime(2023, 11, 9, 23)), val_time_period=(datetime(2023, 11, 9, 23), datetime(2023, 12, 9, 23)), test_time_period=(datetime(2023, 12, 9, 23), datetime(2023, 12, 25, 23)))\n\n# Sets sets a percentage of whole time period from dataset.\n# Always starts from first time.\nconfig = TimeBasedConfig(ts_ids=54, train_time_period=0.5, val_time_period=0.3, test_time_period=0.2)\n\ntime_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre>"},{"location":"choosing_data/#selecting-features","title":"Selecting features","text":"<ul> <li>Affects which features will be returned when loading data.</li> <li>Setting <code>include_time</code> as True will add time to features that return when loading data.</li> <li>Setting <code>include_ts_id</code> as True will add time series id to features that return when loading data.</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import TimeFormat\nfrom cesnet_tszoo.configs import TimeBasedConfig\n\nconfig = TimeBasedConfig(ts_ids=54, features_to_take=\"all\")\n\nconfig = TimeBasedConfig(ts_ids=54, features_to_take=[\"n_flows\", \"n_packets\"])\n\nconfig = TimeBasedConfig(ts_ids=54, features_to_take=[\"n_flows\", \"n_packets\"], include_time=True, include_ts_id=True, time_format=TimeFormat.ID_TIME)\n\ntime_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre>"},{"location":"choosing_data/#selecting-all-set","title":"Selecting all set","text":"<ul> <li>Contains time series from <code>ts_ids</code>.</li> </ul> <pre><code>from cesnet_tszoo.configs import TimeBasedConfig\n\n# All set will contain whole time period of dataset.\nconfig = TimeBasedConfig(ts_ids=54, train_time_period=None, val_time_period=None, test_time_period=None)\n\n# All set will contain total time period of train + val + test.\nconfig = TimeBasedConfig(ts_ids=54, train_time_period=0.5, val_time_period=0.3, test_time_period=0.2)\n\ntime_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre>"},{"location":"choosing_data/#seriesbasedcesnetdataset-dataset","title":"<code>SeriesBasedCesnetDataset</code> dataset","text":"<p>Note</p> <p>For every configuration and more detailed examples refer to Jupyter notebook <code>series_based_choosing_data</code></p> <p>Relevant configuration values:</p> <ul> <li><code>time_period</code> - Defines the time period for train/val/test/all sets.</li> <li><code>train_ts</code>/<code>val_ts</code>/<code>test_ts</code> - Defines time series for train/val/test</li> <li><code>features_to_take</code> - Defines which features are used.</li> <li><code>include_time</code> - If True, time data is included in the returned values.</li> <li><code>include_ts_id</code> - If True, time series IDs are included in the returned values.</li> <li><code>time_format</code> - Format for the returned time data.</li> <li><code>random_state</code> - Fixes randomness for reproducibility when setting <code>train_ts</code>, <code>val_ts</code>, <code>test_ts</code>.</li> </ul>"},{"location":"choosing_data/#selecting-time-period","title":"Selecting time period","text":"<ul> <li><code>time_period</code> sets time period for all sets (used time series).</li> </ul> <pre><code>from datetime import datetime\n\nfrom cesnet_tszoo.configs import SeriesBasedConfig\n\n# Sets time period for time series as a whole time period from dataset.\nconfig = SeriesBasedConfig(time_period=\"all\")\n\n# Sets time period for time series as range of time indices.\nconfig = SeriesBasedConfig(time_period=range(0, 2000))\n\n# Sets time period for time series with tuple of datetime objects.\n# Datetime objects are expected to be of UTC.\nconfig = SeriesBasedConfig(time_period=(datetime(2023, 10, 9, 0), datetime(2023, 11, 9, 23)))\n\n# Sets time period for time series as a percentage of whole time period from dataset.\n# Always starts from first time.\nconfig = SeriesBasedConfig(time_period=0.5)\n\n# Call on series-based dataset to use created config\nseries_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre>"},{"location":"choosing_data/#creating-trainvaltest-sets_1","title":"Creating train/val/test sets","text":"<ul> <li>Sets how many time series will be in each set.</li> <li>You can leave any set value set as None.</li> <li>Each set must have unique time series</li> <li>Can use <code>nan_threshold</code> to set how many nan values will be tolerated.<ul> <li><code>nan_threshold</code> = 1.0, means that time series can be completely empty.</li> <li>is applied after sets.</li> </ul> </li> </ul> <pre><code>from cesnet_tszoo.configs import SeriesBasedConfig\n\n# Sets time series in set with count. Chosen randomly from available time series.\n# Each set will contain unique time series.\n# Affected by random_state.\nconfig = SeriesBasedConfig(time_period=0.5, train_ts=54, val_ts=25, test_ts=10, random_state=None, nan_threshold=1.0)\n\n# Sets time series in set with percentage of time series in dataset. Chosen randomly from available time series.\n# Each set will contain unique time series.\n# Affected by random_state.\nconfig = SeriesBasedConfig(time_period=0.5, train_ts=0.5, val_ts=0.2, test_ts=0.1, random_state=None, nan_threshold=1.0)\n\n# Sets sets with specific time series\nconfig = SeriesBasedConfig(time_period=0.5, train_ts=[0,1,2,3,4], val_ts=[5,6,7,8,9], test_ts=[10,11,12,13,14], nan_threshold=1.0)\n\nseries_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre>"},{"location":"choosing_data/#selecting-features_1","title":"Selecting features","text":"<ul> <li>Affects which features will be returned when loading data.</li> <li>Setting <code>include_time</code> as True will add time to features that return when loading data.</li> <li>Setting <code>include_ts_id</code> as True will add time series id to features that return when loading data.</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import TimeFormat\nfrom cesnet_tszoo.configs import SeriesBasedConfig\n\nconfig = SeriesBasedConfig(time_period=0.5, features_to_take=\"all\")\n\nconfig = SeriesBasedConfig(time_period=0.5, features_to_take=[\"n_flows\", \"n_packets\"])\n\nconfig = SeriesBasedConfig(time_period=0.5, features_to_take=[\"n_flows\", \"n_packets\"], include_time=True, include_ts_id=True, time_format=TimeFormat.ID_TIME)\n\n# Call on series-based dataset to use created config\nseries_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre>"},{"location":"choosing_data/#selecting-all-set_1","title":"Selecting all set","text":"<pre><code>from cesnet_tszoo.configs import SeriesBasedConfig\n\n# All set will contain all time series from dataset.\nconfig = SeriesBasedConfig(time_period=0.5, train_ts=None, val_ts=None, test_ts=None)\n\n# All set will contain all time series that were set by other sets.\nconfig = SeriesBasedConfig(time_period=0.5, train_ts=54, val_ts=25, test_ts=10)\n\nseries_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre>"},{"location":"datasets_overview/","title":"Overview of datasets","text":""},{"location":"datasets_overview/#cesnet-timeseries24","title":"CESNET-TimeSeries24","text":"<p>CESNET-TimeSeries24</p> <ul> <li>Collected in 2023-2024</li> <li>Spans 40 weeks</li> <li>Contains multivariate time series for 283 institutions, 548 institution subnets, 275 124 IP addresses</li> <li>Three aggregation windows: 1 day, 1 hour, 10 minute</li> </ul> <p>This dataset was published in \"CESNET-TimeSeries24: Time Series Dataset for Network Traffic Anomaly Detection and Forecasting\" (DOI). It was built from live traffic collected using high-speed monitoring probes at the perimeter of the CESNET3 network.</p> <p>For detailed information about the dataset, please refer to the linked paper and the CESNET-TimeSeries24 page.</p>"},{"location":"datasets_overview/#cesnet-agg23","title":"CESNET-AGG23","text":"<p>CESNET-AGG23</p> <ul> <li>Collected in 2023</li> <li>Spans 10 weeks</li> <li>Contains overall multivariate time series for CESNET2 network</li> <li>One aggregation window: 1 minute</li> </ul> <p>This dataset was published in \"Look at my Network: An Insight into the ISP Backbone Traffic\" (DOI). It was built from live traffic collected using high-speed monitoring probes at the perimeter of the CESNET2 network.</p> <p>For detailed information about the dataset, please refer to the linked paper and the CESNET-AGG23 page.</p>"},{"location":"device_type_classification/","title":"Network Device Type Classification","text":"<p>Network device type classification focuses on evaluating the performance of models for classifying types of network devices. The goal of this benchmark is to allow comparison of various classification algorithms and methods in the context of network devices. This task is valuable in environments where it is essential to quickly and efficiently identify devices in a network for monitoring, security, and traffic optimization purposes. Analyzing the benchmarks helps determine which methods are most suitable for deployment in real-world scenarios.</p> <p>Available benchmarks for training unique model per each time series are here:</p> Benchmark hash Dataset Aggregation Source Original paper 69270dcc1819 CESNET-TimeSeries24 10 MINUTES IP_ADDRESSES_FULL None 941261e8c367 CESNET-TimeSeries24 1 HOUR IP_ADDRESSES_FULL None bf0aec939afe CESNET-TimeSeries24 1 DAY IP_ADDRESSES_FULL None <p>We encourage users to change default value for missing values, filler, scaler, sliding window step,  and batch sizes. However, users may not change the rest of the arguments. Usage of these benchmarks are following:</p> <pre><code>from cesnet_tszoo.benchmarks import load_benchmark\nfrom cesnet_tszoo.utils.enums import AnnotationType, FillerType, ScalerType, SplitType\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\n\nbenchmark = load_benchmark(\"bf0aec939afe\", \"../\")\ndataset = benchmark.get_initialized_dataset()\n\n# Get annotations\nannotations = benchmark.get_annotations(on=AnnotationType.TS_ID)\n\n# Prepare annotations\nencoder = LabelEncoder()\nannotations['group_encoded'] = encoder.fit_transform(annotations['group'])\n\ntrain_annotations = annotations[annotations['id_ip'].isin(dataset.get_data_about_set(about=SplitType.TRAIN)['ts_ids'])]\ntrain_target = train_annotations['group_encoded'].to_numpy()\n\ntest_annotations = annotations[annotations['id_ip'].isin(dataset.get_data_about_set(about=SplitType.TEST)['test_ts_ids'])]\ntest_target = test_annotations['group_encoded'].to_numpy()\n\n# (optional) Set default value for missing data \ndataset.set_default_values(0)\n\n# (optional) Set filler for filling missing data \ndataset.apply_filler(FillerType.MEAN_FILLER)\n\n# (optional) Set scaler for data\ndataset.apply_scaler(ScalerType.MIN_MAX_SCALER, create_scaler_per_time_series=False)\n\n# (optional) Change sliding window setting\ndataset.set_sliding_window(sliding_window_size=744, sliding_window_prediction_size=24, sliding_window_step=1, set_shared_size=744)\n\n# (optional) Change batch sizes\ndataset.set_batch_sizes(all_batch_size=32)\n\n# Process with your own defined model\nmodel = Model()\nmodel.fit(\n    dataset.get_train_dataloader(), \n    dataset.get_val_dataloader(),\n    train_target,\n)\n\n# Predict for time series which data are not in training\ny_pred = model.predict(\n    dataset.get_test_other_dataloader()\n)\n\n# Evaluate predictions, for example, with RMSE\naccuracy = accuracy_score(test_target, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n</code></pre>"},{"location":"fillers/","title":"Fillers","text":"<p>The <code>cesnet_tszoo</code> package supports various ways of dealing with missing data in dataset. Possible config parameters in <code>TimeBasedConfig</code> and <code>SeriesBasedConfig</code>:</p> <ul> <li><code>fill_missing_with</code>: Can pass enum <code>FillerType</code> for built-in filler or pass a type of custom filler that must derive from <code>Filler</code> base class.</li> <li><code>default_values</code>: Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature.</li> </ul> <p>Note</p> <p>Fillers can carry over values from the train set to the validation and test sets. For example, <code>ForwardFiller</code> can carry over values from previous sets. </p>"},{"location":"fillers/#built-in-fillers","title":"Built-in fillers","text":"<p>The <code>cesnet_tszoo</code> package comes with multiple built-in fillers. To check built-in fillers refer to <code>fillers</code>.</p>"},{"location":"fillers/#custom-fillers","title":"Custom fillers","text":"<p>It is possible to create and use own fillers. But custom filler must derive from <code>Filler</code> base class.</p>"},{"location":"forecasting_multivariate/","title":"Multivariate forecasting","text":"<p>We divided these benchmarks into two groups.</p>"},{"location":"forecasting_multivariate/#unique-model-for-each-time-series","title":"Unique model for each time series","text":"<p>First group target on training model for each time series. Available benchmarks for training unique model per each time series are here:</p> Benchmark hash Dataset Aggregation Source Original paper 930f0b401065 CESNET-TimeSeries24 10 MINUTES INSTITUTIONS None ca6999ea7e24 CESNET-TimeSeries24 10 MINUTES INSTITUTION_SUBNETS None 7495b16f5fe6 CESNET-TimeSeries24 10 MINUTES IP_ADDRESSES_FULL None 3687fb52c433 CESNET-TimeSeries24 10 MINUTES IP_ADDRESSES_SAMPLE None a6e56f99ab8a CESNET-TimeSeries24 1 HOUR INSTITUTION_SUBNETS None e44334732033 CESNET-TimeSeries24 1 HOUR INSTITUTIONS None 18d04cab63e4 CESNET-TimeSeries24 1 HOUR IP_ADDRESSES_FULL None b0ea46897cae CESNET-TimeSeries24 1 HOUR IP_ADDRESSES_SAMPLE None 63e1f696e7c5 CESNET-TimeSeries24 1 DAY INSTITUTION_SUBNETS None 71d17ad3550f CESNET-TimeSeries24 1 DAY INSTITUTIONS None 15737f3fceec CESNET-TimeSeries24 1 DAY IP_ADDRESSES_FULL None 084f368f4c82 CESNET-TimeSeries24 1 DAY IP_ADDRESSES_SAMPLE None <p>We encourage users to change default value for missing values, filler, scaler, sliding window step,  and batch sizes. However, users may not change the rest of the arguments. Usage of these benchmarks are following:</p> <pre><code>from cesnet_tszoo.benchmarks import load_benchmark\nfrom cesnet_tszoo.utils.enums import FillerType, ScalerType\nfrom sklearn.metrics import mean_squared_error\n\nbenchmark = load_benchmark(\"871f5972109e\", \"../\")\ndataset = benchmark.get_initialized_dataset()\n\n# (optional) Set default value for missing data \ndataset.set_default_values(0)\n\n# (optional) Set filler for filling missing data \ndataset.apply_filler(FillerType.MEAN_FILLER)\n\n# (optional) Set scaler for data\ndataset.apply_scaler(ScalerType.MIN_MAX_SCALER)\n\n# (optional) Change sliding window setting\ndataset.set_sliding_window(sliding_window_size=744, sliding_window_prediction_size=24, sliding_window_step=1, set_shared_size=744)\n\n# (optional) Change batch sizes\ndataset.set_batch_sizes(all_batch_size=32)\n\n# Process with model per each time series individualy \nresults = []\nfor ts_id in dataset.get_data_about_set(about='train')['ts_ids']:\n    # Define your own class Model uses dataloaders for perform training and prediction\n    model = Model()\n    model.fit(\n        dataset.get_train_dataloader(ts_id), \n        dataset.get_val_dataloader(ts_id),\n    )\n    y_pred, y_true = model.predict(\n        dataset.get_test_dataloader(ts_id), \n    )\n\n    # Evaluate predictions, for example, with RMSE\n    rmse = mean_squared_error(y_true, y_pred)\n\n    # Add individual result into all results\n    results.append(rmse)\n\nprint(f\"Mean RMSE: {np.mean(rmse):.4f}\")\nprint(f\"Std RMSE: {np.std(rmse):.4f}\")\n</code></pre>"},{"location":"forecasting_multivariate/#generic-model-for-multiple-time-series","title":"Generic model for multiple time series","text":"<p>Second group target on training one generic model which learns generic paterns in several time series and then it can forecast multiple other time series. Available benchmarks for training unique model per each time series are here:</p> Benchmark hash Dataset Aggregation Source Original paper 9ac2b87c9a7c CESNET-TimeSeries24 10 MINUTES INSTITUTIONS None 7cd4e41b05ec CESNET-TimeSeries24 10 MINUTES INSTITUTION_SUBNETS None 50eb509e1e77 CESNET-TimeSeries24 10 MINUTES IP_ADDRESSES_FULL None 681a7fb90948 CESNET-TimeSeries24 10 MINUTES IP_ADDRESSES_SAMPLE None ab8183ea80af CESNET-TimeSeries24 1 HOUR INSTITUTION_SUBNETS None f9bd005c7efe CESNET-TimeSeries24 1 HOUR INSTITUTIONS None 88fd173619b2 CESNET-TimeSeries24 1 HOUR IP_ADDRESSES_FULL None 4ae11863ee38 CESNET-TimeSeries24 1 HOUR IP_ADDRESSES_SAMPLE None cdb79dbf54ea CESNET-TimeSeries24 1 DAY INSTITUTION_SUBNETS None c95d66b0baf5 CESNET-TimeSeries24 1 DAY INSTITUTIONS None 16274e0b44af CESNET-TimeSeries24 1 DAY IP_ADDRESSES_FULL None 0197980a87c0 CESNET-TimeSeries24 1 DAY IP_ADDRESSES_SAMPLE None <p>We encourage users to change default value for missing values, filler, scaler, sliding window step,  and batch sizes. However, users may not change the rest of the arguments. Usage of these benchmarks are following:</p> <pre><code>from cesnet_tszoo.benchmarks import load_benchmark\nfrom cesnet_tszoo.utils.enums import FillerType, ScalerType\nfrom sklearn.metrics import mean_squared_error\n\nbenchmark = load_benchmark(\"09de83e89e42\", \"../\")\ndataset = benchmark.get_initialized_dataset()\n\n# (optional) Set default value for missing data \ndataset.set_default_values(0)\n\n# (optional) Set filler for filling missing data \ndataset.apply_filler(FillerType.MEAN_FILLER)\n\n# (optional) Set scaler for data\ndataset.apply_scaler(ScalerType.MIN_MAX_SCALER, create_scaler_per_time_series=False)\n\n# (optional) Change sliding window setting\ndataset.set_sliding_window(sliding_window_size=744, sliding_window_prediction_size=24, sliding_window_step=1, set_shared_size=744)\n\n# (optional) Change batch sizes\ndataset.set_batch_sizes(all_batch_size=32)\n\n# Process with your own defined model\nmodel = Model()\nmodel.fit(\n    dataset.get_train_dataloader(), \n    dataset.get_val_dataloader(),\n)\n\n# Predict for time series which data are not in training\ny_pred, y_true = model.predict(\n    dataset.get_test_other_dataloader(), \n)\n\n# Evaluate predictions, for example, with RMSE\nrmse = mean_squared_error(y_true, y_pred)\nprint(f\"RMSE: {rmse:.4f}\")\n</code></pre>"},{"location":"forecasting_univariate/","title":"Univariate forecasting","text":"<p>Benchmarks in this group are designed to support mostly used forecasting task for network management. We divided these benchmarks into two groups.</p>"},{"location":"forecasting_univariate/#unique-model-for-each-time-series","title":"Unique model for each time series","text":"<p>First group target on training model for each time series. Available benchmarks for training unique model per each time series are here:</p> Benchmark hash Dataset Aggregation Source Original paper 22c5a8e8ffd3 CESNET-TimeSeries24 10 MINUTES INSTITUTIONS None 095f847ca755 CESNET-TimeSeries24 10 MINUTES INSTITUTION_SUBNETS None c2970e89d824 CESNET-TimeSeries24 10 MINUTES IP_ADDRESSES_SAMPLE None ddb1f02dae43 CESNET-TimeSeries24 10 MINUTES IP_ADDRESSES_FULL None 871f5972109e CESNET-TimeSeries24 1 HOUR INSTITUTIONS None 080582bcd519 CESNET-TimeSeries24 1 HOUR INSTITUTION_SUBNETS None f3fc14310e2e CESNET-TimeSeries24 1 HOUR IP_ADDRESSES_SAMPLE None e268fa9957f2 CESNET-TimeSeries24 1 HOUR IP_ADDRESSES_FULL None 0d523e69c328 CESNET-TimeSeries24 1 DAY INSTITUTIONS None 8e2a07fb3177 CESNET-TimeSeries24 1 DAY INSTITUTION_SUBNETS None b5e5ea044b81 CESNET-TimeSeries24 1 DAY IP_ADDRESSES_SAMPLE None d19ba386743f CESNET-TimeSeries24 1 DAY IP_ADDRESSES_FULL None <p>We encourage users to change default value for missing values, filler, scaler, sliding window step,  and batch sizes. However, users may not change the rest of the arguments. Usage of these benchmarks are following:</p> <pre><code>from cesnet_tszoo.benchmarks import load_benchmark\nfrom cesnet_tszoo.utils.enums import FillerType, ScalerType\nfrom sklearn.metrics import mean_squared_error\n\nbenchmark = load_benchmark(\"871f5972109e\", \"../\")\ndataset = benchmark.get_initialized_dataset()\n\n# (optional) Set default value for missing data \ndataset.set_default_values(0)\n\n# (optional) Set filler for filling missing data \ndataset.apply_filler(FillerType.MEAN_FILLER)\n\n# (optional) Set scaler for data\ndataset.apply_scaler(ScalerType.MIN_MAX_SCALER)\n\n# (optional) Change sliding window setting\ndataset.set_sliding_window(sliding_window_size=744, sliding_window_prediction_size=24, sliding_window_step=1, set_shared_size=744)\n\n# (optional) Change batch sizes\ndataset.set_batch_sizes(all_batch_size=32)\n\n# Process with model per each time series individualy \nresults = []\nfor ts_id in dataset.get_data_about_set(about='train')['ts_ids']:\n    # Define your own class Model uses dataloaders for perform training and prediction\n    model = Model()\n    model.fit(\n        dataset.get_train_dataloader(ts_id), \n        dataset.get_val_dataloader(ts_id),\n    )\n    y_pred, y_true = model.predict(\n        dataset.get_test_dataloader(ts_id), \n    )\n\n    # Evaluate predictions, for example, with RMSE\n    rmse = mean_squared_error(y_true, y_pred)\n\n    # Add individual result into all results\n    results.append(rmse)\n\nprint(f\"Mean RMSE: {np.mean(rmse):.4f}\")\nprint(f\"Std RMSE: {np.std(rmse):.4f}\")\n</code></pre>"},{"location":"forecasting_univariate/#generic-model-for-multiple-time-series","title":"Generic model for multiple time series","text":"<p>Second group target on training one generic model which learns generic paterns in several time series and then it can forecast multiple other time series.  Available benchmarks for training unique model per each time series are here:</p> Benchmark hash Dataset Aggregation Source Original paper 7706f1087922 CESNET-TimeSeries24 10 MINUTES INSTITUTIONS None a642915953ad CESNET-TimeSeries24 10 MINUTES INSTITUTION_SUBNETS None e3de1fc0a44e CESNET-TimeSeries24 10 MINUTES IP_ADDRESSES_SAMPLE None 8b03d0d508ce CESNET-TimeSeries24 10 MINUTES IP_ADDRESSES_FULL None 09de83e89e42 CESNET-TimeSeries24 1 HOUR INSTITUTIONS None 73a9add2c4af CESNET-TimeSeries24 1 HOUR INSTITUTION_SUBNETS None 6249383544ef CESNET-TimeSeries24 1 HOUR IP_ADDRESSES_SAMPLE None b8098753b97b CESNET-TimeSeries24 1 HOUR IP_ADDRESSES_FULL None ef632e70c252 CESNET-TimeSeries24 1 DAY INSTITUTIONS None ce63551ffaab CESNET-TimeSeries24 1 DAY INSTITUTION_SUBNETS None 9f7047902d66 CESNET-TimeSeries24 1 DAY IP_ADDRESSES_SAMPLE None 570b215d790d CESNET-TimeSeries24 1 DAY IP_ADDRESSES_FULL None <p>We encourage users to change default value for missing values, filler, scaler, sliding window step,  and batch sizes. However, users may not change the rest of the arguments. Usage of these benchmarks are following:</p> <pre><code>from cesnet_tszoo.benchmarks import load_benchmark\nfrom cesnet_tszoo.utils.enums import FillerType, ScalerType\nfrom sklearn.metrics import mean_squared_error\n\nbenchmark = load_benchmark(\"09de83e89e42\", \"../\")\ndataset = benchmark.get_initialized_dataset()\n\n# (optional) Set default value for missing data \ndataset.set_default_values(0)\n\n# (optional) Set filler for filling missing data \ndataset.apply_filler(FillerType.MEAN_FILLER)\n\n# (optional) Set scaler for data\ndataset.apply_scaler(ScalerType.MIN_MAX_SCALER, create_scaler_per_time_series=False)\n\n# (optional) Change sliding window setting\ndataset.set_sliding_window(sliding_window_size=744, sliding_window_prediction_size=24, sliding_window_step=1, set_shared_size=744)\n\n# (optional) Change batch sizes\ndataset.set_batch_sizes(all_batch_size=32)\n\n# Process with your own defined model\nmodel = Model()\nmodel.fit(\n    dataset.get_train_dataloader(), \n    dataset.get_val_dataloader(),\n)\n\n# Predict for time series which data are not in training\ny_pred, y_true = model.predict(\n    dataset.get_test_other_dataloader(), \n)\n\n# Evaluate predictions, for example, with RMSE\nrmse = mean_squared_error(y_true, y_pred)\nprint(f\"RMSE: {rmse::4f}\")\n</code></pre>"},{"location":"getting_started/","title":"Getting started","text":"<p>Note</p> <p>For a demonstration of usage for simple forecasting refer to Jupyter notebook <code>simple_forecasting</code></p>"},{"location":"getting_started/#code-snippets","title":"Code snippets","text":""},{"location":"getting_started/#download-a-dataset","title":"Download a dataset","text":"<pre><code>from cesnet_tszoo.datasets import CESNET_TimeSeries24\nfrom cesnet_tszoo.utils.enums import SourceType, AgreggationType\n\ndataset = CESNET_TimeSeries24.get_dataset(\"/some_directory/\", source_type=SourceType.INSTITUTIONS, aggregation=AgreggationType.AGG_1_DAY, is_series_based=False)\n</code></pre> <p>Alternatively you can use <code>load_benchmark</code>.</p> <pre><code>from cesnet_tszoo.benchmarks import load_benchmark\n\nbenchmark = load_benchmark(\"SOME_BUILT_IN_IDENTIFIER\", \"/some_directory/\")\ndataset = benchmark.get_dataset()\n</code></pre> <p>This will create following directories:</p> <ul> <li>\"/some_directory/tszoo\"<ul> <li>\"/some_directory/tszoo/annotations\"</li> <li>\"/some_directory/tszoo/benchmarks\"</li> <li>\"/some_directory/tszoo/configs\"</li> <li>\"/some_directory/tszoo/databases\"</li> </ul> </li> </ul> <p>Dataset will be downloaded to \"/some_directory/tszoo/databases/CESNET_TimeSeries24/\".</p>"},{"location":"getting_started/#enable-logging","title":"Enable logging","text":"<p><pre><code>import logging\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"[%(asctime)s][%(name)s][%(levelname)s] - %(message)s\")\n</code></pre> Set up logging to get more information from the package.</p>"},{"location":"getting_started/#initialize-dataset-to-create-train-validation-and-test-sets","title":"Initialize dataset to create train, validation, and test sets","text":""},{"location":"getting_started/#using-timebasedcesnetdataset-dataset","title":"Using <code>TimeBasedCesnetDataset</code> dataset","text":"<p><pre><code>from cesnet_tszoo.datasets import CESNET_TimeSeries24\nfrom cesnet_tszoo.utils.enums import SourceType, AgreggationType\nfrom cesnet_tszoo.configs import TimeBasedConfig\n\ndataset = CESNET_TimeSeries24.get_dataset(\"/some_directory/\", source_type=SourceType.INSTITUTIONS, aggregation=AgreggationType.AGG_1_DAY, is_series_based=False)\nconfig = TimeBasedConfig(\n    ts_ids=50, # number of randomly selected time series from dataset\n    train_time_period=range(0, 100), \n    val_time_period=range(100, 150), \n    test_time_period=range(150, 250), \n    features_to_take=[\"n_flows\", \"n_packets\"])\ndataset.set_dataset_config_and_initialize(config)\n</code></pre> Time-based datasets are configured with <code>TimeBasedConfig</code>. Can load data using:</p> <ul> <li> <p><code>get_train_dataloader</code>, <code>get_val_dataloader</code>, <code>get_test_dataloader</code>, <code>get_test_other_dataloader</code>, <code>get_all_dataloader</code></p> </li> <li> <p><code>get_train_df</code>, <code>get_val_df</code>, <code>get_test_df</code>, <code>get_test_other_df</code>, <code>get_all_df</code></p> </li> <li> <p><code>get_train_numpy</code>, <code>get_val_numpy</code>, <code>get_test_numpy</code>, <code>get_test_other_numpy</code>, <code>get_all_numpy</code></p> </li> </ul>"},{"location":"getting_started/#using-seriesbasedcesnetdataset-dataset","title":"Using <code>SeriesBasedCesnetDataset</code> dataset","text":"<p><pre><code>from cesnet_tszoo.datasets import CESNET_TimeSeries24\nfrom cesnet_tszoo.utils.enums import SourceType, AgreggationType\nfrom cesnet_tszoo.configs import SeriesBasedConfig\n\ndataset = CESNET_TimeSeries24.get_dataset(\"/some_directory/\", source_type=SourceType.INSTITUTIONS, aggregation=AgreggationType.AGG_1_DAY, is_series_based=True)\nconfig = SeriesBasedConfig(\n    time_period=range(0, 250), \n    train_ts=100, # number of randomly selected time series from dataset\n    val_ts=30, # number of randomly selected time series from dataset\n    test_ts=20, # number of randomly selected time series from dataset\n    features_to_take=[\"n_flows\", \"n_packets\"])\ndataset.set_dataset_config_and_initialize(config)\n</code></pre> Series-based datasets are configured with <code>SeriesBasedConfig</code>. Can load data using:</p> <ul> <li> <p><code>get_train_dataloader</code>, <code>get_val_dataloader</code>, <code>get_test_dataloader</code>, <code>get_all_dataloader</code></p> </li> <li> <p><code>get_train_df</code>, <code>get_val_df</code>, <code>get_test_df</code>, <code>get_all_df</code></p> </li> <li> <p><code>get_train_numpy</code>, <code>get_val_numpy</code>, <code>get_test_numpy</code>, <code>get_all_numpy</code></p> </li> </ul>"},{"location":"getting_started/#using-load_benchmark","title":"Using <code>load_benchmark</code>","text":"<p><pre><code>from cesnet_tszoo.benchmarks import load_benchmark\n\nbenchmark = load_benchmark(\"SOME_BUILT_IN_IDENTIFIER\", \"/some_directory/\")\ndataset = benchmark.get_initialized_dataset()\n</code></pre> Whether loaded dataset is series-based or time-based depends on the benchmark. What can be loaded corresponds to previous datasets.</p>"},{"location":"handling_missing_data/","title":"Handling missing data","text":"<p>This tutorial will look at some configuration options used for handling missing data.</p> <p>Only time-based will be used, because all methods work almost the same way for series-based.</p> <p>Note</p> <p>For every configuration and more detailed examples refer to Jupyter notebook <code>handling_missing_data</code></p> <p>Relevant configuration values:</p> <ul> <li><code>default_values</code> - Default values for missing data, applied before fillers.</li> <li><code>fill_missing_with</code> - Defines how to fill missing values in the dataset. Can pass FillerType enum or custom Filler type.</li> </ul>"},{"location":"handling_missing_data/#default-values","title":"Default values","text":"<ul> <li>Default values are set to missing values before filler is used.</li> <li>You can change used default values later with <code>update_dataset_config_and_initialize</code> or <code>set_default_values</code>.</li> </ul> <pre><code>from cesnet_tszoo.configs import TimeBasedConfig\n\n# Default values are provided from used dataset.\nconfig = TimeBasedConfig(ts_ids=[1200], train_time_period=range(0, 30), test_time_period=range(30, 80), features_to_take=['n_flows', 'n_packets'],\n                         default_values=\"default\")\n\n# All missing values will be set as None\nconfig = TimeBasedConfig(ts_ids=[1200], train_time_period=range(0, 30), test_time_period=range(30, 80), features_to_take=['n_flows', 'n_packets'],\n                         default_values=None)     \n\n# All missing values will be set with 0\nconfig = TimeBasedConfig(ts_ids=[1200], train_time_period=range(0, 30), test_time_period=range(30, 80), features_to_take=['n_flows', 'n_packets'],\n                         default_values=0) \n\n# Using list to specify default values for each used feature\n# Position of values in list correspond to order of features in `features_to_take`.\nconfig = TimeBasedConfig(ts_ids=[1200], train_time_period=range(0, 30), test_time_period=range(30, 80), features_to_take=['n_flows', 'n_packets'],\n                         default_values=[1, None])       \n\n# Using dictionary with key as name for used feature and value as a default value for missing data\n# Dictionary must contain key and value for every feature in `features_to_take`.\nconfig = TimeBasedConfig(ts_ids=[1200], train_time_period=range(0, 30), test_time_period=range(30, 80), features_to_take=['n_flows', 'n_packets'],\n                         default_values={\"n_flows\" : 1, \"n_packets\": None})                                                                                       \n\n# Call on time-based dataset to use created config\ntime_based_dataset.set_dataset_config_and_initialize(config)\n\ntime_based_dataset.update_dataset_config_and_initialize(default_values=\"default\", workers=0)\n# Or\ntime_based_dataset.set_default_values(default_values=\"default\", workers=0)\n</code></pre>"},{"location":"handling_missing_data/#fillers","title":"Fillers","text":"<ul> <li>Fillers are implemented as classes.<ul> <li>You can create your own or use built-in one.</li> </ul> </li> <li>One filler per time series is created.</li> <li>Filler is applied after default values and usually overrides them.</li> <li>Fillers in time-based dataset can carry over values from train -&gt; val -&gt; test. Example is in Jupyter notebook.</li> <li>You can change used filler later with <code>update_dataset_config_and_initialize</code> or <code>apply_filler</code>.</li> </ul>"},{"location":"handling_missing_data/#built-in","title":"Built-in","text":"<p>To see all built-in fillers refer to <code>Fillers</code>.</p> <pre><code>from cesnet_tszoo.utils.enums import FillerType\nfrom cesnet_tszoo.configs import TimeBasedConfig\n\nconfig = TimeBasedConfig(ts_ids=[1200], train_time_period=range(0, 30), test_time_period=range(30, 80), features_to_take=['n_flows', 'n_packets'],\n                         default_values=None, fill_missing_with=FillerType.FORWARD_FILLER)                                                                                \n\n# Call on time-based dataset to use created config\ntime_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre> <p>Or later with:</p> <pre><code>time_based_dataset.update_dataset_config_and_initialize(fill_missing_with=FillerType.FORWARD_FILLER, workers=0)\n# Or\ntime_based_dataset.apply_filler(fill_missing_with=FillerType.FORWARD_FILLER, workers=0)\n</code></pre>"},{"location":"handling_missing_data/#custom","title":"Custom","text":"<p>You can create your own custom filler, which must derive from 'Filler' base class. </p> <p>To check Filler base class refer to <code>Filler</code></p> <pre><code>from cesnet_tszoo.utils.filler import Filler\nfrom cesnet_tszoo.configs import TimeBasedConfig\n\nclass CustomFiller(Filler):\n    def fill(self, batch_values: np.ndarray, existing_indices: np.ndarray, missing_indices: np.ndarray, **kwargs):\n        batch_values[missing_indices] = -1\n\nconfig = TimeBasedConfig(ts_ids=[1200], train_time_period=range(0, 30), test_time_period=range(30, 80), features_to_take=['n_flows', 'n_packets'],\n                         default_values=None, fill_missing_with=CustomFiller)                                                                            \n\n# Call on time-based dataset to use created config\ntime_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre> <p>Or later with:</p> <pre><code>time_based_dataset.update_dataset_config_and_initialize(fill_missing_with=CustomFiller, workers=0)\n# Or\ntime_based_dataset.apply_filler(CustomFiller, workers=0)\n</code></pre>"},{"location":"installation/","title":"Installation","text":"<p>Install the package from pip with:</p> <pre><code>pip install cesnet-tszoo\n</code></pre> <p>or for editable install with:</p> <pre><code>pip install -e git+https://github.com/CESNET/cesnet-tszoo\n</code></pre>"},{"location":"installation/#requirements","title":"Requirements","text":"<p>The <code>cesnet-tszoo</code> package requires Python &gt;=3.10.</p>"},{"location":"installation/#dependencies","title":"Dependencies","text":"Name Version matplotlib numpy &lt;2.0 pandas scikit-learn tables &gt;=3.8.0,&lt;=3.9.2 torch &gt;=1.10 tqdm plotly PyYAML requests"},{"location":"loading_data/","title":"Loading data","text":"<p>This tutorial will look at some configuration options used for loading data.</p> <p>Each dataset type will have its own part because of multiple differences of available configuration values.</p>"},{"location":"loading_data/#timebasedcesnetdataset-dataset","title":"<code>TimeBasedCesnetDataset</code> dataset","text":"<p>Note</p> <p>For every configuration and more detailed examples refer to Jupyter notebook <code>time_based_loading_data</code></p> <p>Relevant configuration values:</p> <ul> <li><code>sliding_window_size</code> - Number of times in one window.</li> <li><code>sliding_window_prediction_size</code> - Number of times to predict from <code>sliding_window_size</code>.</li> <li><code>sliding_window_step</code> - Number of times to move by after each window.</li> <li><code>set_shared_size</code> - How much times should time periods share. Order of sharing is training set &lt; validation set &lt; test set.</li> <li><code>train_batch_size</code>/<code>val_batch_size</code>/<code>test_batch_size</code>/<code>all_batch_size</code> - How many times for every time series will be in one batch (differs when sliding window is used).</li> <li><code>train_workers</code>/<code>val_workers</code>/<code>test_workers</code>/<code>all_workers</code> - Defines how many workers (processes) will be used for loading specific set.</li> </ul>"},{"location":"loading_data/#loading-data-with-dataloader","title":"Loading data with DataLoader","text":"<ul> <li>Load data using Pytorch Dataloader.</li> <li>Affected by workers and batch sizes.</li> <li>Last batch is never dropped (unless sliding window is used)</li> <li>Returned batch shape changes when used <code>time_format</code> is TimeFormat.DATETIME compare to other time formats. Check Jupyter notebook for details.</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import TimeFormat\nfrom cesnet_tszoo.configs import TimeBasedConfig\n\nconfig = TimeBasedConfig(ts_ids=54, train_time_period=0.5, val_time_period=0.3, test_time_period=0.2, test_ts_ids=22, features_to_take=\"all\", time_format=TimeFormat.ID_TIME,\n                         train_workers=0, val_workers=0, test_workers=0, all_workers=0, init_workers=0,\n                         train_batch_size=32, val_batch_size=64, test_batch_size=128, all_batch_size=128)\n\n# Call on time-based dataset to use created config -&gt; must be called before attempting to load data.\ntime_based_dataset.set_dataset_config_and_initialize(config)\n\n# train_time_period must be set\ndataloader = time_based_dataset.get_train_dataloader(workers=\"config\")\n\n# val_time_period must be set\ndataloader = time_based_dataset.get_val_dataloader(workers=\"config\")\n\n# test_time_period must be set\ndataloader = time_based_dataset.get_test_dataloader(workers=\"config\")\n\n# test_time_period and test_ts_ids must be set\ndataloader = time_based_dataset.get_test_other_dataloader(workers=\"config\")\n\n# Always usable\ndataloader = time_based_dataset.get_all_dataloader(workers=\"config\")\n\n# Example of usage -&gt; loads all batches into list\nbatches = []\n\nfor batch in tqdm(dataloader):\n    # batch is a Numpy array of shape (ts_ids/test_ts_ids, batch_size, features_to_take + used ids)\n    batches.append(batch)\n</code></pre> <p>You can also change set batch sizes later with <code>update_dataset_config_and_initialize</code> or <code>set_batch_sizes</code>.</p> <pre><code>time_based_dataset.update_dataset_config_and_initialize(train_batch_size=33, val_batch_size=65, test_batch_size=\"config\", all_batch_size=\"config\")\n# Or\ntime_based_dataset.set_batch_sizes(train_batch_size=33, val_batch_size=65, test_batch_size=\"config\", all_batch_size=\"config\")\n</code></pre> <p>You can also change set workers later with <code>update_dataset_config_and_initialize</code> or <code>set_workers</code>.</p> <pre><code>time_based_dataset.update_dataset_config_and_initialize(train_workers=0, val_workers=0, test_workers=0, all_workers=0, init_workers=0)\n# Or\ntime_based_dataset.set_workers(train_workers=0, val_workers=0, test_workers=0, all_workers=0, init_workers=0)\n</code></pre> <p>You can also specify which time series to load from set.</p> <pre><code>from cesnet_tszoo.utils.enums import TimeFormat\nfrom cesnet_tszoo.configs import TimeBasedConfig\n\nconfig = TimeBasedConfig(ts_ids=54, train_time_period=[0,1,2,3,4], features_to_take=\"all\", time_format=TimeFormat.ID_TIME,\n                         train_workers=0, train_batch_size=32,)\n\ntime_based_dataset.set_dataset_config_and_initialize(config)\n\n# train_time_period must be set; load time series with id == 1\ndataloader = time_based_dataset.get_train_dataloader(ts_id=1, workers=\"config\")\n\n# Example of usage -&gt; loads all batches into list\nbatches = []\n\nfor batch in tqdm(dataloader):\n    # batch is a Numpy array of shape (1, batch_size, features_to_take + used ids)\n    batches.append(batch)\n</code></pre>"},{"location":"loading_data/#sliding-window","title":"Sliding window","text":"<ul> <li>Both <code>sliding_window_size</code> and <code>sliding_window_prediction_size</code> must be set if you want to use sliding window.</li> <li>Batch sizes are used for background caching.</li> <li>You can modify sliding window step size with <code>sliding_window_step</code></li> <li>You can use <code>set_shared_size</code> to set how many times time periods should share.<ul> <li><code>val_time_period</code> takes from <code>train_time_period</code></li> <li><code>test_time_period</code> takes from <code>val_time_period</code> or <code>train_time_period</code></li> </ul> </li> </ul> <pre><code>from cesnet_tszoo.utils.enums import TimeFormat\nfrom cesnet_tszoo.configs import TimeBasedConfig\n\nconfig = TimeBasedConfig(ts_ids=54, train_time_period=range(0, 1000), val_time_period=range(1000, 1500), test_time_period=range(1500, 2000), test_ts_ids=22, features_to_take=[\"n_flows\"], time_format=TimeFormat.ID_TIME,\n                         train_workers=0, val_workers=0, test_workers=0, all_workers=0, init_workers=0,\n                         train_batch_size=32, val_batch_size=64, test_batch_size=128, all_batch_size=128,\n                         sliding_window_size=22, sliding_window_prediction_size=2, sliding_window_step=2, set_shared_size=0.05)\n\ntime_based_dataset.set_dataset_config_and_initialize(config)\n\ndataloader = time_based_dataset.get_train_dataloader(workers=\"config\")\n\n# Example of usage -&gt; loads all batches into list\nbatches = []\n\nfor sliding_window, sliding_window_prediction in tqdm(dataloader):\n    # sliding_window is a Numpy array of shape (ts_ids, sliding_window_size, features_to_take + used ids)\n    # sliding_window_prediction is a Numpy array of shape (ts_ids, sliding_window_prediction_size, features_to_take + used ids)\n    batches.append((sliding_window, sliding_window_prediction))    \n</code></pre> <p>You can also change sliding window parameters later with <code>update_dataset_config_and_initialize</code> or <code>set_sliding_window</code>.</p> <pre><code>time_based_dataset.update_dataset_config_and_initialize(sliding_window_size=22, sliding_window_prediction_size=3, sliding_window_step=\"config\", set_shared_size=\"config\", workers=0)\n# Or\ntime_based_dataset.set_sliding_window(sliding_window_size=22, sliding_window_prediction_size=3, sliding_window_step=\"config\", set_shared_size=\"config\", workers=0)\n</code></pre>"},{"location":"loading_data/#loading-data-as-dataframe","title":"Loading data as Dataframe","text":"<ul> <li>Batch size has no effect.</li> <li>Sliding window has no effect.</li> <li>Returns every time series in <code>ts_ids</code>/<code>test_ts_ids</code> with sets specified time period.</li> <li>Data is returned as Pandas Dataframe.</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import TimeFormat\nfrom cesnet_tszoo.configs import TimeBasedConfig\n\nconfig = TimeBasedConfig(ts_ids=54, train_time_period=0.5, val_time_period=0.3, test_time_period=0.2, test_ts_ids=22, features_to_take=\"all\", time_format=TimeFormat.ID_TIME,\n                         train_workers=0, val_workers=0, test_workers=0, all_workers=0, init_workers=0)\n\ntime_based_dataset.set_dataset_config_and_initialize(config)\n\n# train_time_period must be set\ndf = time_based_dataset.get_train_df(as_single_dataframe=True, workers=\"config\") # loads time series from ts_ids of train_time_period into one Pandas Dataframe\ndfs = time_based_dataset.get_train_df(as_single_dataframe=False, workers=\"config\") # loads time series from ts_ids of train_time_period into seperate Pandas Dataframes\n\n# val_time_period must be set\ndf = time_based_dataset.get_val_df(as_single_dataframe=True, workers=\"config\") # loads time series from ts_ids of val_time_period into one Pandas Dataframe\ndfs = time_based_dataset.get_val_df(as_single_dataframe=False, workers=\"config\") # loads time series from ts_ids of val_time_period into seperate Pandas Dataframes\n\n# test_time_period must be set\ndf = time_based_dataset.get_test_df(as_single_dataframe=True, workers=\"config\") # loads time series from ts_ids of test_time_period into one Pandas Dataframe\ndfs = time_based_dataset.get_test_df(as_single_dataframe=False, workers=\"config\") # loads time series from ts_ids of test_time_period into seperate Pandas Dataframes\n\n# test_time_period and test_ts_ids must be set\ndf = time_based_dataset.get_test_other_df(as_single_dataframe=True, workers=\"config\") # loads time series from test_ts_ids of test_time_period into one Pandas Dataframe\ndfs = time_based_dataset.get_test_other_df(as_single_dataframe=False, workers=\"config\") # loads time series from ts_ids of test_time_period into seperate Pandas Dataframes\n\n# Always usable\ndf = time_based_dataset.get_all_df(as_single_dataframe=True, workers=\"config\") # loads time series from ts_ids of all time period into one Pandas Dataframe\ndfs = time_based_dataset.get_all_df(as_single_dataframe=False, workers=\"config\") # loads time series from ts_ids of all time period into seperate Pandas Dataframes\n</code></pre>"},{"location":"loading_data/#loading-data-as-singular-numpy-array","title":"Loading data as singular Numpy array","text":"<ul> <li>Batch size has no effect.</li> <li>Sliding window has no effect.</li> <li>Returns every time series in <code>ts_ids</code>/<code>test_ts_ids</code> with sets specified time period.</li> <li>Data is returned as one Numpy array.</li> <li>Follows similar rules to Dataloader batches, regarding shape (excluding sliding window parameters).</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import TimeFormat\nfrom cesnet_tszoo.configs import TimeBasedConfig\n\nconfig = TimeBasedConfig(ts_ids=54, train_time_period=0.5, val_time_period=0.3, test_time_period=0.2, test_ts_ids=22, features_to_take=\"all\", time_format=TimeFormat.ID_TIME,\n                         train_workers=0, val_workers=0, test_workers=0, all_workers=0, init_workers=0)\n\ntime_based_dataset.set_dataset_config_and_initialize(config)\n\n# train_time_period must be set\nnumpy_array = time_based_dataset.get_train_numpy(workers=\"config\")\n\n# val_time_period must be set\nnumpy_array = time_based_dataset.get_val_numpy(workers=\"config\")\n\n# test_time_period must be set\nnumpy_array = time_based_dataset.get_test_numpy(workers=\"config\")\n\n# test_time_period and test_ts_ids must be set\nnumpy_array = time_based_dataset.get_test_other_numpy(workers=\"config\")\n\n# Always usable\nnumpy_array = time_based_dataset.get_all_numpy(workers=\"config\")\n</code></pre>"},{"location":"loading_data/#seriesbasedcesnetdataset-dataset","title":"<code>SeriesBasedCesnetDataset</code> dataset","text":"<p>Note</p> <p>For every configuration and more detailed examples refer to Jupyter notebook <code>series_based_loading_data</code></p> <p>Relevant configuration values:</p> <ul> <li><code>train_batch_size</code>/<code>val_batch_size</code>/<code>test_batch_size</code>/<code>all_batch_size</code> - How many time series will be in one batch.</li> <li><code>train_workers</code>/<code>val_workers</code>/<code>test_workers</code>/<code>all_workers</code> - Defines how many workers (processes) will be used for loading specific set.</li> <li><code>train_dataloader_order</code> - Affects order of time series in loaded batch.</li> <li><code>random_state</code> - When set, batches will be same when using random order type for <code>train_dataloader_order</code>. </li> </ul>"},{"location":"loading_data/#loading-data-with-dataloader_1","title":"Loading data with DataLoader","text":"<ul> <li>Load data using Pytorch Dataloader.</li> <li>Affected by workers and batch sizes.</li> <li>Last batch is never dropped.</li> <li>Returned batch shape changes when used <code>time_format</code> is TimeFormat.DATETIME compare to other time formats. Check Jupyter notebook for details.</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import TimeFormat\nfrom cesnet_tszoo.configs import SeriesBasedConfig\n\nconfig = SeriesBasedConfig(time_period=0.5, train_ts=54, val_ts=25, test_ts=10, features_to_take=[\"n_flows\"], time_format=TimeFormat.ID_TIME,\n                           train_workers=0, val_workers=0, test_workers=0, all_workers=0, init_workers=0,\n                           train_batch_size=32, val_batch_size=64, test_batch_size=128, all_batch_size=128)\n\n# Call on series-based dataset to use created config -&gt; must be called before attempting to load data.\nseries_based_dataset.set_dataset_config_and_initialize(config)\n\n# train_ts must be set\ndataloader = series_based_dataset.get_train_dataloader(workers=\"config\")\n\n# val_ts must be set\ndataloader = series_based_dataset.get_val_dataloader(workers=\"config\")\n\n# test_ts must be set\ndataloader = series_based_dataset.get_test_dataloader(workers=\"config\")\n\n# Always usable\ndataloader = series_based_dataset.get_all_dataloader(workers=\"config\")\n\n# Example of usage -&gt; loads all batches into list\nbatches = []\n\nfor batch in tqdm(dataloader):\n    # batch is a Numpy array of shape (batch_size, time_period, features_to_take + used ids)\n    batches.append(batch)\n</code></pre> <p>You can also change set batch sizes later with <code>update_dataset_config_and_initialize</code> or <code>set_batch_sizes</code>.</p> <pre><code>series_based_dataset.update_dataset_config_and_initialize(train_batch_size=33, val_batch_size=65, test_batch_size=\"config\", all_batch_size=\"config\")\n# Or\nseries_based_dataset.set_batch_sizes(train_batch_size=33, val_batch_size=65, test_batch_size=\"config\", all_batch_size=\"config\")\n</code></pre> <p>You can also change set workers later with <code>update_dataset_config_and_initialize</code> or <code>set_workers</code>.</p> <pre><code>series_based_dataset.update_dataset_config_and_initialize(train_workers=0, val_workers=0, test_workers=0, all_workers=0, init_workers=0)\n# Or\nseries_based_dataset.set_workers(train_workers=0, val_workers=0, test_workers=0, all_workers=0, init_workers=0)\n</code></pre> <p>You can also specify which time series to load from set.</p> <pre><code>from cesnet_tszoo.utils.enums import TimeFormat\nfrom cesnet_tszoo.configs import SeriesBasedConfig\n\nconfig = SeriesBasedConfig(time_period=0.5, train_ts=[177, 176, 319, 267], features_to_take=[\"n_flows\"], time_format=TimeFormat.ID_TIME,\n                           train_workers=0, train_batch_size=32)\n\nseries_based_dataset.set_dataset_config_and_initialize(config)\n\n# train_ts must be set; load time series with id == 176\ndataloader = series_based_dataset.get_train_dataloader(ts_id=176, workers=\"config\")\n\n# Example of usage -&gt; loads one whole time series\nbatches = []\n\nfor batch in tqdm(dataloader):\n    # batch is a Numpy array of shape (1, time_period, features_to_take + used ids)\n    batches.append(batch)\n</code></pre>"},{"location":"loading_data/#loading-data-as-dataframe_1","title":"Loading data as Dataframe","text":"<ul> <li>Batch size has no effect.</li> <li>Returns every time series in set with specified <code>time_period</code>.</li> <li>Data is returned as Pandas Dataframe.</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import TimeFormat\nfrom cesnet_tszoo.configs import SeriesBasedConfig\n\nconfig = SeriesBasedConfig(time_period=0.5, train_ts=54, val_ts=25, test_ts=10, features_to_take=\"all\", time_format=TimeFormat.ID_TIME,\n                           train_workers=0, val_workers=0, test_workers=0, all_workers=0, init_workers=0)\n\nseries_based_dataset.set_dataset_config_and_initialize(config)\n\n# train_ts must be set\ndf = series_based_dataset.get_train_df(as_single_dataframe=True, workers=\"config\") # loads every time series from train_ts with time_period into one Pandas Dataframe\ndfs = series_based_dataset.get_train_df(as_single_dataframe=False, workers=\"config\") # loads every time series from train_ts with time_period into seperate Pandas Dataframe\n\n# val_ts must be set\ndf = series_based_dataset.get_val_df(as_single_dataframe=True, workers=\"config\") # loads every time series with from val_ts time_period into one Pandas Dataframe\ndfs = series_based_dataset.get_val_df(as_single_dataframe=False, workers=\"config\") # loads every time series from val_ts with time_period into seperate Pandas Dataframe\n\n# test_ts must be set\ndf = series_based_dataset.get_test_df(as_single_dataframe=True, workers=\"config\") # loads every time series from test_ts with time_period into one Pandas Dataframe\ndfs = series_based_dataset.get_test_df(as_single_dataframe=False, workers=\"config\") # loads every time series from test_ts with time_period into seperate Pandas Dataframe\n\n# Always usable\ndf = series_based_dataset.get_all_df(as_single_dataframe=True, workers=\"config\") # loads every time series from all set with time_period into one Pandas Dataframe\ndfs = series_based_dataset.get_all_df(as_single_dataframe=False, workers=\"config\") # loads every time series from all set with time_period into seperate Pandas Dataframe\n</code></pre>"},{"location":"loading_data/#loading-data-as-singular-numpy-array_1","title":"Loading data as singular Numpy array","text":"<ul> <li>Batch size has no effect.</li> <li>Returns every time series in set with specified <code>time_period</code>.</li> <li>Data is returned as one Numpy array.</li> <li>Follows similar rules to Dataloader batches, regarding shape.</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import TimeFormat\nfrom cesnet_tszoo.configs import SeriesBasedConfig\n\nconfig = SeriesBasedConfig(time_period=0.5, train_ts=54, val_ts=25, test_ts=10, features_to_take=\"all\", time_format=TimeFormat.ID_TIME,\n                           train_workers=0, val_workers=0, test_workers=0, all_workers=0, init_workers=0)\n\nseries_based_dataset.set_dataset_config_and_initialize(config)\n\n# train_ts must be set\nnumpy_array = series_based_dataset.get_train_numpy(workers=\"config\")\n\n# val_ts must be set\nnumpy_array = series_based_dataset.get_val_numpy(workers=\"config\")\n\n# test_ts must be set\nnumpy_array = series_based_dataset.get_test_numpy(workers=\"config\")\n\n# Always usable\nnumpy_array = series_based_dataset.get_all_numpy(workers=\"config\")\n</code></pre>"},{"location":"reference_benchmarks/","title":"Benchmark","text":""},{"location":"reference_benchmarks/#cesnet_tszoo.benchmarks","title":"cesnet_tszoo.benchmarks","text":""},{"location":"reference_benchmarks/#cesnet_tszoo.benchmarks.Benchmark","title":"Benchmark","text":"<p>Used as wrapper for imported <code>dataset</code>, <code>config</code>, <code>annotations</code> and <code>related_results</code>.</p> <p>Intended usage:</p> <p>For time-based:</p> <ol> <li>Call <code>load_benchmark</code> with the desired benchmark identifier. You can use your own saved benchmark or you can use already built-in one. This will download the dataset and annotations (if available) if they have not been previously downloaded.</li> <li>Retrieve the initialized dataset using <code>get_initialized_dataset</code>. This will provide a dataset that is ready to use.</li> <li>Use <code>get_train_dataloader</code> or <code>get_train_df</code> to get training data for chosen model.</li> <li>Validate the model and perform the hyperparameter optimalization on <code>get_val_dataloader</code> or <code>get_val_df</code>.</li> <li>Evaluate the model on <code>get_test_dataloader</code> or <code>get_test_df</code>. </li> <li>(Optional) Evaluate the model on <code>get_test_other_dataloader</code> or <code>get_test_other_df</code>. </li> </ol> <p>For series-based: </p> <ol> <li>Call <code>load_benchmark</code> with the desired benchmark. You can use your own saved benchmark or you can use already built-in one. This will download the dataset and annotations (if available) if they have not been previously downloaded.</li> <li>Retrieve the initialized dataset using <code>get_initialized_dataset</code>. This will provide a dataset that is ready to use.</li> <li>Use <code>get_train_dataloader</code> or <code>get_train_df</code> to get training data for chosen model.</li> <li>Validate the model and perform the hyperparameter optimalization on <code>get_val_dataloader</code> or <code>get_val_df</code>.</li> <li>Evaluate the model on <code>get_test_dataloader</code> or <code>get_test_df</code>.     </li> </ol> <p>You can create custom time-based benchmarks with <code>save_benchmark</code> or series-based benchmarks with <code>save_benchmark</code>. They will be saved to <code>\"data_root\"/tszoo/benchmarks/</code> directory, where <code>data_root</code> was set when you created instance of dataset.</p> Source code in <code>cesnet_tszoo\\benchmarks.py</code> <pre><code>class Benchmark:\n    \"\"\"\n    Used as wrapper for imported `dataset`, `config`, `annotations` and `related_results`.\n\n    **Intended usage:**\n\n    For time-based:\n\n    1. Call [`load_benchmark`][cesnet_tszoo.benchmarks.load_benchmark] with the desired benchmark identifier. You can use your own saved benchmark or you can use already built-in one. This will download the dataset and annotations (if available) if they have not been previously downloaded.\n    2. Retrieve the initialized dataset using [`get_initialized_dataset`][cesnet_tszoo.benchmarks.Benchmark.get_initialized_dataset]. This will provide a dataset that is ready to use.\n    3. Use [`get_train_dataloader`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_train_dataloader] or [`get_train_df`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_train_df] to get training data for chosen model.\n    4. Validate the model and perform the hyperparameter optimalization on [`get_val_dataloader`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_val_dataloader] or [`get_val_df`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_val_df].\n    5. Evaluate the model on [`get_test_dataloader`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_dataloader] or [`get_test_df`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_df]. \n    6. (Optional) Evaluate the model on [`get_test_other_dataloader`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_other_dataloader] or [`get_test_other_df`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_other_df]. \n\n    For series-based: \n\n    1. Call [`load_benchmark`][cesnet_tszoo.benchmarks.load_benchmark] with the desired benchmark. You can use your own saved benchmark or you can use already built-in one. This will download the dataset and annotations (if available) if they have not been previously downloaded.\n    2. Retrieve the initialized dataset using [`get_initialized_dataset`][cesnet_tszoo.benchmarks.Benchmark.get_initialized_dataset]. This will provide a dataset that is ready to use.\n    3. Use [`get_train_dataloader`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_train_dataloader] or [`get_train_df`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_train_df] to get training data for chosen model.\n    4. Validate the model and perform the hyperparameter optimalization on [`get_val_dataloader`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_val_dataloader] or [`get_val_df`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_val_df].\n    5. Evaluate the model on [`get_test_dataloader`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_test_dataloader] or [`get_test_df`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_test_df].     \n\n    You can create custom time-based benchmarks with [`save_benchmark`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.save_benchmark] or series-based benchmarks with [`save_benchmark`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.save_benchmark].\n    They will be saved to `\"data_root\"/tszoo/benchmarks/` directory, where `data_root` was set when you created instance of dataset.\n    \"\"\"\n\n    def __init__(self, config: DatasetConfig, dataset: CesnetDataset, description: str = None):\n        self.config = config\n        self.dataset = dataset\n        self.description = description\n        self.related_results = None\n        self.logger = logging.getLogger(\"benchmark\")\n\n    def get_config(self) -&gt; SeriesBasedConfig | TimeBasedConfig:\n        \"\"\"Return config made for this benchmark. \"\"\"\n\n        return self.config\n\n    def get_initialized_dataset(self, display_config_details: bool = True, check_errors: bool = False, workers: Literal[\"config\"] | int = \"config\") -&gt; TimeBasedCesnetDataset | SeriesBasedCesnetDataset:\n        \"\"\"\n        Return dataset with intialized sets, scalers, fillers etc..\n\n        This method uses following config attributes:\n\n        | Dataset config                    | Description                                                                                    |\n        | --------------------------------- | ---------------------------------------------------------------------------------------------- |\n        | `init_workers`                    | Specifies the number of workers to use for initialization. Applied when `workers` = \"config\". |\n        | `partial_fit_initialized_scalers` | Determines whether initialized scalers should be partially fitted on the training data.        |\n        | `nan_threshold`                   | Filters out time series with missing values exceeding the specified threshold.                 |\n\n        Parameters:\n            display_config_details: Flag indicating whether to display the configuration values after initialization. `Default: True`   \n            check_errors: Whether to validate if dataset is not corrupted. `Default: False`\n            workers: The number of workers to use during initialization. `Default: \"config\"`        \n\n        Returns:\n            Return initialized dataset.\n        \"\"\"\n\n        if check_errors:\n            self.dataset.check_errors()\n\n        self.dataset.set_dataset_config_and_initialize(self.config, display_config_details, workers)\n\n        return self.dataset\n\n    def get_dataset(self, check_errors: bool = False) -&gt; TimeBasedCesnetDataset | SeriesBasedCesnetDataset:\n        \"\"\"Return dataset without initializing it.\n\n        Parameters:\n            check_errors: Whether to validate if dataset is not corrupted. `Default: False`\n\n        Returns:\n            Return dataset used for this benchmark.\n        \"\"\"\n\n        if check_errors:\n            self.dataset.check_errors()\n\n        return self.dataset\n\n    def get_annotations(self, on: AnnotationType | Literal[\"id_time\", \"ts_id\", \"both\"]) -&gt; pd.DataFrame:\n        \"\"\" \n        Return the annotations as a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html).\n\n        Parameters:\n            on: Specifies which annotations to return. If set to `\"both\"`, annotations will be applied as if `id_time` and `ts_id` were both set.         \n\n        Returns:\n            A Pandas DataFrame containing the selected annotations.      \n        \"\"\"\n\n        return self.dataset.get_annotations(on)\n\n    def get_related_results(self) -&gt; pd.DataFrame | None:\n        \"\"\"\n        Return the related results as a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), if they exist. \n\n        Returns:\n            A Pandas DataFrame containing related results or None if not related results exist. \n        \"\"\"\n\n        return self.related_results\n</code></pre>"},{"location":"reference_benchmarks/#cesnet_tszoo.benchmarks.Benchmark.get_annotations","title":"get_annotations","text":"<pre><code>get_annotations(on: AnnotationType | Literal['id_time', 'ts_id', 'both']) -&gt; pd.DataFrame\n</code></pre> <p>Return the annotations as a Pandas <code>DataFrame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>on</code> <code>AnnotationType | Literal['id_time', 'ts_id', 'both']</code> <p>Specifies which annotations to return. If set to <code>\"both\"</code>, annotations will be applied as if <code>id_time</code> and <code>ts_id</code> were both set.         </p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A Pandas DataFrame containing the selected annotations.</p> Source code in <code>cesnet_tszoo\\benchmarks.py</code> <pre><code>def get_annotations(self, on: AnnotationType | Literal[\"id_time\", \"ts_id\", \"both\"]) -&gt; pd.DataFrame:\n    \"\"\" \n    Return the annotations as a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html).\n\n    Parameters:\n        on: Specifies which annotations to return. If set to `\"both\"`, annotations will be applied as if `id_time` and `ts_id` were both set.         \n\n    Returns:\n        A Pandas DataFrame containing the selected annotations.      \n    \"\"\"\n\n    return self.dataset.get_annotations(on)\n</code></pre>"},{"location":"reference_benchmarks/#cesnet_tszoo.benchmarks.Benchmark.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; SeriesBasedConfig | TimeBasedConfig\n</code></pre> <p>Return config made for this benchmark.</p> Source code in <code>cesnet_tszoo\\benchmarks.py</code> <pre><code>def get_config(self) -&gt; SeriesBasedConfig | TimeBasedConfig:\n    \"\"\"Return config made for this benchmark. \"\"\"\n\n    return self.config\n</code></pre>"},{"location":"reference_benchmarks/#cesnet_tszoo.benchmarks.Benchmark.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(check_errors: bool = False) -&gt; TimeBasedCesnetDataset | SeriesBasedCesnetDataset\n</code></pre> <p>Return dataset without initializing it.</p> <p>Parameters:</p> Name Type Description Default <code>check_errors</code> <code>bool</code> <p>Whether to validate if dataset is not corrupted. <code>Default: False</code></p> <code>False</code> <p>Returns:</p> Type Description <code>TimeBasedCesnetDataset | SeriesBasedCesnetDataset</code> <p>Return dataset used for this benchmark.</p> Source code in <code>cesnet_tszoo\\benchmarks.py</code> <pre><code>def get_dataset(self, check_errors: bool = False) -&gt; TimeBasedCesnetDataset | SeriesBasedCesnetDataset:\n    \"\"\"Return dataset without initializing it.\n\n    Parameters:\n        check_errors: Whether to validate if dataset is not corrupted. `Default: False`\n\n    Returns:\n        Return dataset used for this benchmark.\n    \"\"\"\n\n    if check_errors:\n        self.dataset.check_errors()\n\n    return self.dataset\n</code></pre>"},{"location":"reference_benchmarks/#cesnet_tszoo.benchmarks.Benchmark.get_initialized_dataset","title":"get_initialized_dataset","text":"<pre><code>get_initialized_dataset(display_config_details: bool = True, check_errors: bool = False, workers: Literal['config'] | int = 'config') -&gt; TimeBasedCesnetDataset | SeriesBasedCesnetDataset\n</code></pre> <p>Return dataset with intialized sets, scalers, fillers etc..</p> <p>This method uses following config attributes:</p> Dataset config Description <code>init_workers</code> Specifies the number of workers to use for initialization. Applied when <code>workers</code> = \"config\". <code>partial_fit_initialized_scalers</code> Determines whether initialized scalers should be partially fitted on the training data. <code>nan_threshold</code> Filters out time series with missing values exceeding the specified threshold. <p>Parameters:</p> Name Type Description Default <code>display_config_details</code> <code>bool</code> <p>Flag indicating whether to display the configuration values after initialization. <code>Default: True</code> </p> <code>True</code> <code>check_errors</code> <code>bool</code> <p>Whether to validate if dataset is not corrupted. <code>Default: False</code></p> <code>False</code> <code>workers</code> <code>Literal['config'] | int</code> <p>The number of workers to use during initialization. <code>Default: \"config\"</code> </p> <code>'config'</code> <p>Returns:</p> Type Description <code>TimeBasedCesnetDataset | SeriesBasedCesnetDataset</code> <p>Return initialized dataset.</p> Source code in <code>cesnet_tszoo\\benchmarks.py</code> <pre><code>def get_initialized_dataset(self, display_config_details: bool = True, check_errors: bool = False, workers: Literal[\"config\"] | int = \"config\") -&gt; TimeBasedCesnetDataset | SeriesBasedCesnetDataset:\n    \"\"\"\n    Return dataset with intialized sets, scalers, fillers etc..\n\n    This method uses following config attributes:\n\n    | Dataset config                    | Description                                                                                    |\n    | --------------------------------- | ---------------------------------------------------------------------------------------------- |\n    | `init_workers`                    | Specifies the number of workers to use for initialization. Applied when `workers` = \"config\". |\n    | `partial_fit_initialized_scalers` | Determines whether initialized scalers should be partially fitted on the training data.        |\n    | `nan_threshold`                   | Filters out time series with missing values exceeding the specified threshold.                 |\n\n    Parameters:\n        display_config_details: Flag indicating whether to display the configuration values after initialization. `Default: True`   \n        check_errors: Whether to validate if dataset is not corrupted. `Default: False`\n        workers: The number of workers to use during initialization. `Default: \"config\"`        \n\n    Returns:\n        Return initialized dataset.\n    \"\"\"\n\n    if check_errors:\n        self.dataset.check_errors()\n\n    self.dataset.set_dataset_config_and_initialize(self.config, display_config_details, workers)\n\n    return self.dataset\n</code></pre>"},{"location":"reference_benchmarks/#cesnet_tszoo.benchmarks.Benchmark.get_related_results","title":"get_related_results","text":"<pre><code>get_related_results() -&gt; pd.DataFrame | None\n</code></pre> <p>Return the related results as a Pandas <code>DataFrame</code>, if they exist. </p> <p>Returns:</p> Type Description <code>DataFrame | None</code> <p>A Pandas DataFrame containing related results or None if not related results exist.</p> Source code in <code>cesnet_tszoo\\benchmarks.py</code> <pre><code>def get_related_results(self) -&gt; pd.DataFrame | None:\n    \"\"\"\n    Return the related results as a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), if they exist. \n\n    Returns:\n        A Pandas DataFrame containing related results or None if not related results exist. \n    \"\"\"\n\n    return self.related_results\n</code></pre>"},{"location":"reference_benchmarks/#cesnet_tszoo.benchmarks.load_benchmark","title":"load_benchmark","text":"<pre><code>load_benchmark(identifier: str, data_root: str) -&gt; Benchmark\n</code></pre> <p>Load a benchmark using the identifier.</p> <p>First, it attempts to load the built-in benchmark, if no built-in benchmark with such an identifier exists, it attempts to load a custom benchmark from the <code>\"data_root\"/tszoo/benchmarks/</code> directory.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The name of the benchmark YAML file.</p> required <code>data_root</code> <code>str</code> <p>Path to the folder where the dataset will be stored. Each database has its own subfolder <code>\"data_root\"/tszoo/databases/database_name/</code>.</p> required <p>Returns:</p> Type Description <code>Benchmark</code> <p>Return benchmark with <code>config</code>, <code>annotations</code>, <code>dataset</code> and <code>related_results</code>.</p> Source code in <code>cesnet_tszoo\\benchmarks.py</code> <pre><code>def load_benchmark(identifier: str, data_root: str) -&gt; Benchmark:\n    \"\"\"\n    Load a benchmark using the identifier.\n\n    First, it attempts to load the built-in benchmark, if no built-in benchmark with such an identifier exists, it attempts to load a custom benchmark from the `\"data_root\"/tszoo/benchmarks/` directory.\n\n    Parameters:\n        identifier: The name of the benchmark YAML file.\n        data_root: Path to the folder where the dataset will be stored. Each database has its own subfolder `\"data_root\"/tszoo/databases/database_name/`.\n\n    Returns:\n        Return benchmark with `config`, `annotations`, `dataset` and `related_results`.\n    \"\"\"\n\n    logger = logging.getLogger(\"benchmark\")\n\n    data_root = os.path.normpath(os.path.expanduser(data_root))\n\n    # For anything else\n    if isinstance(identifier, str):\n        _, is_built_in = get_benchmark_path_and_whether_it_is_built_in(identifier, data_root, logger)\n\n        if is_built_in:\n            logger.info(\"Built-in benchmark found: %s. Loading it.\", identifier)\n            return _get_built_in_benchmark(identifier, data_root)\n        else:\n            logger.info(\"Custom benchmark found: %s. Loading it.\", identifier)\n            return _get_custom_benchmark(identifier, data_root)\n\n    else:\n        logger.error(\"Invalid identifier.\")\n        raise ValueError(\"Invalid identifier.\")\n</code></pre>"},{"location":"reference_cesnet_database/","title":"CesnetDatabase","text":""},{"location":"reference_cesnet_database/#cesnet_tszoo.datasets.cesnet_database.CesnetDatabase","title":"cesnet_tszoo.datasets.cesnet_database.CesnetDatabase","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for cesnet databases. This class should not be used directly. Use it as base for adding new databases.</p> <p>Derived databases are used by calling class method <code>get_dataset</code> which will create a new dataset instance of <code>SeriesBasedCesnetDataset</code>  or <code>TimeBasedCesnetDataset</code>. Check them for more info about how to use them.</p> <p>Intended usage:</p> <p>When using <code>TimeBasedCesnetDataset</code> (<code>is_series_based</code> = <code>False</code>):</p> <ol> <li>Create an instance of the dataset with the desired data root by calling <code>get_dataset</code>. This will download the dataset if it has not been previously downloaded and return instance of dataset.</li> <li>Create an instance of <code>TimeBasedConfig</code> and set it using <code>set_dataset_config_and_initialize</code>.     This initializes the dataset, including data splitting (train/validation/test/test_other), fitting scalers (if needed), selecting features, and more. This is cached for later use.</li> <li>Use <code>get_train_dataloader</code>/<code>get_train_df</code>/<code>get_train_numpy</code> to get training data for chosen model.</li> <li>Validate the model and perform the hyperparameter optimalization on <code>get_val_dataloader</code>/<code>get_val_df</code>/<code>get_val_numpy</code>.</li> <li>Evaluate the model on <code>get_test_dataloader</code>/<code>get_test_df</code>/<code>get_test_numpy</code>. </li> <li>(Optional) Evaluate the model on <code>get_test_other_dataloader</code>/<code>get_test_other_df</code>/<code>get_test_other_numpy</code>.    </li> </ol> <p>When using <code>SeriesBasedCesnetDataset</code> (<code>is_series_based</code> = <code>True</code>):</p> <ol> <li>Create an instance of the dataset with the desired data root by calling <code>get_dataset</code>. This will download the dataset if it has not been previously downloaded and return instance of dataset.</li> <li>Create an instance of <code>SeriesBasedConfig</code> and set it using <code>set_dataset_config_and_initialize</code>.     This initializes the dataset, including data splitting (train/validation/test), fitting scalers (if needed), selecting features, and more. This is cached for later use.</li> <li>Use <code>get_train_dataloader</code>/<code>get_train_df</code>/<code>get_train_numpy</code> to get training data for chosen model.</li> <li>Validate the model and perform the hyperparameter optimalization on <code>get_val_dataloader</code>/<code>get_val_df</code>/<code>get_val_numpy</code>.</li> <li>Evaluate the model on <code>get_test_dataloader</code>/<code>get_test_df</code>/<code>get_test_numpy</code>.   </li> </ol> <p>Used class attributes:</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the database.</p> <code>bucket_url</code> <code>str</code> <p>URL of the bucket where the dataset is stored.</p> <code>tszoo_root</code> <code>str</code> <p>Path to folder where all databases are saved. Set after <code>get_dataset</code> was called at least once.</p> <code>database_root</code> <code>str</code> <p>Path to the folder where datasets belonging to the database are saved. Set after <code>get_dataset</code> was called at least once.</p> <code>configs_root</code> <code>str</code> <p>Path to the folder where configurations are saved. Set after <code>get_dataset</code> was called at least once.</p> <code>benchmarks_root</code> <code>str</code> <p>Path to the folder where benchmarks are saved. Set after <code>get_dataset</code> was called at least once.</p> <code>annotations_root</code> <code>str</code> <p>Path to the folder where annotations are saved. Set after <code>get_dataset</code> was called at least once.</p> <code>id_names</code> <code>dict</code> <p>Names for time series IDs for each <code>source_type</code>.</p> <code>default_values</code> <code>dict</code> <p>Default values for each available feature.</p> <code>source_types</code> <code>list[SourceType]</code> <p>Available source types for the database.</p> <code>aggregations</code> <code>list[AgreggationType]</code> <p>Available aggregations for the database.   </p> <code>additional_data</code> <code>dict[str, tuple]</code> <p>Available small datasets for each dataset.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_database.py</code> <pre><code>class CesnetDatabase(ABC):\n    \"\"\"\n    Base class for cesnet databases. This class should **not** be used directly. Use it as base for adding new databases.\n\n    Derived databases are used by calling class method [`get_dataset`][cesnet_tszoo.datasets.cesnet_database.CesnetDatabase.get_dataset] which will create a new dataset instance of [`SeriesBasedCesnetDataset`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset] \n    or [`TimeBasedCesnetDataset`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset]. Check them for more info about how to use them.\n\n    **Intended usage:**\n\n    When using [`TimeBasedCesnetDataset`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset] (`is_series_based` = `False`):\n\n    1. Create an instance of the dataset with the desired data root by calling [`get_dataset`][cesnet_tszoo.datasets.cesnet_database.CesnetDatabase.get_dataset]. This will download the dataset if it has not been previously downloaded and return instance of dataset.\n    2. Create an instance of [`TimeBasedConfig`][cesnet_tszoo.configs.time_based_config.TimeBasedConfig] and set it using [`set_dataset_config_and_initialize`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.set_dataset_config_and_initialize]. \n       This initializes the dataset, including data splitting (train/validation/test/test_other), fitting scalers (if needed), selecting features, and more. This is cached for later use.\n    3. Use [`get_train_dataloader`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_train_dataloader]/[`get_train_df`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_train_df]/[`get_train_numpy`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_train_numpy] to get training data for chosen model.\n    4. Validate the model and perform the hyperparameter optimalization on [`get_val_dataloader`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_val_dataloader]/[`get_val_df`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_val_df]/[`get_val_numpy`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_val_numpy].\n    5. Evaluate the model on [`get_test_dataloader`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_dataloader]/[`get_test_df`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_df]/[`get_test_numpy`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_numpy]. \n    6. (Optional) Evaluate the model on [`get_test_other_dataloader`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_other_dataloader]/[`get_test_other_df`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_other_df]/[`get_test_other_numpy`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_other_numpy].    \n\n    When using [`SeriesBasedCesnetDataset`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset] (`is_series_based` = `True`):\n\n    1. Create an instance of the dataset with the desired data root by calling [`get_dataset`][cesnet_tszoo.datasets.cesnet_database.CesnetDatabase.get_dataset]. This will download the dataset if it has not been previously downloaded and return instance of dataset.\n    2. Create an instance of [`SeriesBasedConfig`][cesnet_tszoo.configs.series_based_config.SeriesBasedConfig] and set it using [`set_dataset_config_and_initialize`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.set_dataset_config_and_initialize]. \n       This initializes the dataset, including data splitting (train/validation/test), fitting scalers (if needed), selecting features, and more. This is cached for later use.\n    3. Use [`get_train_dataloader`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_train_dataloader]/[`get_train_df`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_train_df]/[`get_train_numpy`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_train_numpy] to get training data for chosen model.\n    4. Validate the model and perform the hyperparameter optimalization on [`get_val_dataloader`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_val_dataloader]/[`get_val_df`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_val_df]/[`get_val_numpy`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_val_numpy].\n    5. Evaluate the model on [`get_test_dataloader`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_test_dataloader]/[`get_test_df`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_test_df]/[`get_test_numpy`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_test_numpy].   \n\n    Used class attributes:\n\n    Attributes:\n        name: Name of the database.\n        bucket_url: URL of the bucket where the dataset is stored.\n        tszoo_root: Path to folder where all databases are saved. Set after [`get_dataset`][cesnet_tszoo.datasets.cesnet_database.CesnetDatabase.get_dataset] was called at least once.\n        database_root: Path to the folder where datasets belonging to the database are saved. Set after [`get_dataset`][cesnet_tszoo.datasets.cesnet_database.CesnetDatabase.get_dataset] was called at least once.\n        configs_root: Path to the folder where configurations are saved. Set after [`get_dataset`][cesnet_tszoo.datasets.cesnet_database.CesnetDatabase.get_dataset] was called at least once.\n        benchmarks_root: Path to the folder where benchmarks are saved. Set after [`get_dataset`][cesnet_tszoo.datasets.cesnet_database.CesnetDatabase.get_dataset] was called at least once.\n        annotations_root: Path to the folder where annotations are saved. Set after [`get_dataset`][cesnet_tszoo.datasets.cesnet_database.CesnetDatabase.get_dataset] was called at least once.\n        id_names: Names for time series IDs for each `source_type`.\n        default_values: Default values for each available feature.\n        source_types: Available source types for the database.\n        aggregations: Available aggregations for the database.   \n        additional_data: Available small datasets for each dataset. \n    \"\"\"\n\n    name: str\n    bucket_url: str\n\n    tszoo_root: str\n    database_root: str\n    configs_root: str\n    benchmarks_root: str\n    annotations_root: str\n\n    id_names: dict = None\n    default_values: dict = None\n    source_types: list[SourceType] = []\n    aggregations: list[AgreggationType] = []\n    additional_data: dict[str, tuple] = {}\n\n    def __init__(self):\n        raise ValueError(\"To create dataset instance use class method 'get_dataset' instead.\")\n\n    @classmethod\n    def get_dataset(cls, data_root: str, source_type: SourceType | str, aggregation: AgreggationType | str, is_series_based: bool, check_errors: bool = False, display_details: bool = False) -&gt; TimeBasedCesnetDataset | SeriesBasedCesnetDataset:\n        \"\"\"\n        Create new dataset instance.\n\n        Parameters:\n            data_root: Path to the folder where the dataset will be stored. Each database has its own subfolder `data_root/tszoo/databases/database_name/`.\n            source_type: The source type of the desired dataset.\n            aggregation: The aggregation type for the selected source type.\n            is_series_based: Whether you want to create series-based dataset or time-based dataset.\n            check_errors: Whether to validate if the dataset is corrupted. `Default: False`\n            display_details: Whether to display details about the available data in chosen dataset. `Default: False`\n\n        Returns:\n            TimeBasedCesnetDataset or SeriesBasedCesnetDataset.\n        \"\"\"\n\n        logger = logging.getLogger(\"wrapper_dataset\")\n\n        source_type = SourceType(source_type)\n        aggregation = AgreggationType(aggregation)\n\n        if source_type not in cls.source_types:\n            raise ValueError(f\"Unsupported source type: {source_type}\")\n\n        if aggregation not in cls.aggregations:\n            raise ValueError(f\"Unsupported aggregation type: {aggregation}\")\n\n        # Dataset paths setup\n        cls.tszoo_root = os.path.normpath(os.path.expanduser(os.path.join(data_root, \"tszoo\")))\n        cls.database_root = os.path.join(cls.tszoo_root, \"databases\", cls.name)\n        cls.configs_root = os.path.join(cls.tszoo_root, \"configs\")\n        cls.benchmarks_root = os.path.join(cls.tszoo_root, \"benchmarks\")\n        cls.annotations_root = os.path.join(cls.tszoo_root, \"annotations\")\n        dataset_name = f\"{cls.name}-{source_type.value}-{AgreggationType._to_str_without_number(aggregation)}\"\n        dataset_path = os.path.join(cls.database_root, f\"{dataset_name}.h5\")\n\n        # Ensure necessary directories exist\n        for directory in [cls.database_root, cls.configs_root, cls.annotations_root, cls.benchmarks_root]:\n            if not os.path.exists(directory):\n                logger.info(\"Creating directory: %s\", directory)\n                os.makedirs(directory)\n\n        if not cls._is_downloaded(dataset_path):\n            cls._download(dataset_name, dataset_path)\n\n        if is_series_based:\n            dataset = SeriesBasedCesnetDataset(cls.name, dataset_path, cls.configs_root, cls.benchmarks_root, cls.annotations_root, source_type, aggregation, cls.id_names[source_type], cls.default_values, cls.additional_data)\n        else:\n            dataset = TimeBasedCesnetDataset(cls.name, dataset_path, cls.configs_root, cls.benchmarks_root, cls.annotations_root, source_type, aggregation, cls.id_names[source_type], cls.default_values, cls.additional_data)\n\n        if check_errors:\n            dataset.check_errors()\n\n        if display_details:\n            dataset.display_dataset_details()\n\n        if is_series_based:\n            logger.info(\"Dataset is series-based. Use cesnet_tszoo.configs.SeriesBasedConfig\")\n        else:\n            logger.info(\"Dataset is time-based. Use cesnet_tszoo.configs.TimeBasedConfig\")\n\n        return dataset\n\n    @classmethod\n    def _is_downloaded(cls, dataset_path: str) -&gt; bool:\n        \"\"\"Check whether the dataset at path has already been downloaded. \"\"\"\n\n        return os.path.exists(dataset_path)\n\n    @classmethod\n    def _download(cls, dataset_name: str, dataset_path: str) -&gt; None:\n        \"\"\"Download the dataset file. \"\"\"\n\n        logger = logging.getLogger(\"wrapper_dataset\")\n\n        logger.info(\"Downloading %s dataset.\", dataset_name)\n        database_url = f\"{cls.bucket_url}&amp;file={dataset_name}.h5\"\n        resumable_download(url=database_url, file_path=dataset_path, silent=False)\n</code></pre>"},{"location":"reference_cesnet_database/#cesnet_tszoo.datasets.cesnet_database.CesnetDatabase.get_dataset","title":"get_dataset  <code>classmethod</code>","text":"<pre><code>get_dataset(data_root: str, source_type: SourceType | str, aggregation: AgreggationType | str, is_series_based: bool, check_errors: bool = False, display_details: bool = False) -&gt; TimeBasedCesnetDataset | SeriesBasedCesnetDataset\n</code></pre> <p>Create new dataset instance.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the folder where the dataset will be stored. Each database has its own subfolder <code>data_root/tszoo/databases/database_name/</code>.</p> required <code>source_type</code> <code>SourceType | str</code> <p>The source type of the desired dataset.</p> required <code>aggregation</code> <code>AgreggationType | str</code> <p>The aggregation type for the selected source type.</p> required <code>is_series_based</code> <code>bool</code> <p>Whether you want to create series-based dataset or time-based dataset.</p> required <code>check_errors</code> <code>bool</code> <p>Whether to validate if the dataset is corrupted. <code>Default: False</code></p> <code>False</code> <code>display_details</code> <code>bool</code> <p>Whether to display details about the available data in chosen dataset. <code>Default: False</code></p> <code>False</code> <p>Returns:</p> Type Description <code>TimeBasedCesnetDataset | SeriesBasedCesnetDataset</code> <p>TimeBasedCesnetDataset or SeriesBasedCesnetDataset.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_database.py</code> <pre><code>@classmethod\ndef get_dataset(cls, data_root: str, source_type: SourceType | str, aggregation: AgreggationType | str, is_series_based: bool, check_errors: bool = False, display_details: bool = False) -&gt; TimeBasedCesnetDataset | SeriesBasedCesnetDataset:\n    \"\"\"\n    Create new dataset instance.\n\n    Parameters:\n        data_root: Path to the folder where the dataset will be stored. Each database has its own subfolder `data_root/tszoo/databases/database_name/`.\n        source_type: The source type of the desired dataset.\n        aggregation: The aggregation type for the selected source type.\n        is_series_based: Whether you want to create series-based dataset or time-based dataset.\n        check_errors: Whether to validate if the dataset is corrupted. `Default: False`\n        display_details: Whether to display details about the available data in chosen dataset. `Default: False`\n\n    Returns:\n        TimeBasedCesnetDataset or SeriesBasedCesnetDataset.\n    \"\"\"\n\n    logger = logging.getLogger(\"wrapper_dataset\")\n\n    source_type = SourceType(source_type)\n    aggregation = AgreggationType(aggregation)\n\n    if source_type not in cls.source_types:\n        raise ValueError(f\"Unsupported source type: {source_type}\")\n\n    if aggregation not in cls.aggregations:\n        raise ValueError(f\"Unsupported aggregation type: {aggregation}\")\n\n    # Dataset paths setup\n    cls.tszoo_root = os.path.normpath(os.path.expanduser(os.path.join(data_root, \"tszoo\")))\n    cls.database_root = os.path.join(cls.tszoo_root, \"databases\", cls.name)\n    cls.configs_root = os.path.join(cls.tszoo_root, \"configs\")\n    cls.benchmarks_root = os.path.join(cls.tszoo_root, \"benchmarks\")\n    cls.annotations_root = os.path.join(cls.tszoo_root, \"annotations\")\n    dataset_name = f\"{cls.name}-{source_type.value}-{AgreggationType._to_str_without_number(aggregation)}\"\n    dataset_path = os.path.join(cls.database_root, f\"{dataset_name}.h5\")\n\n    # Ensure necessary directories exist\n    for directory in [cls.database_root, cls.configs_root, cls.annotations_root, cls.benchmarks_root]:\n        if not os.path.exists(directory):\n            logger.info(\"Creating directory: %s\", directory)\n            os.makedirs(directory)\n\n    if not cls._is_downloaded(dataset_path):\n        cls._download(dataset_name, dataset_path)\n\n    if is_series_based:\n        dataset = SeriesBasedCesnetDataset(cls.name, dataset_path, cls.configs_root, cls.benchmarks_root, cls.annotations_root, source_type, aggregation, cls.id_names[source_type], cls.default_values, cls.additional_data)\n    else:\n        dataset = TimeBasedCesnetDataset(cls.name, dataset_path, cls.configs_root, cls.benchmarks_root, cls.annotations_root, source_type, aggregation, cls.id_names[source_type], cls.default_values, cls.additional_data)\n\n    if check_errors:\n        dataset.check_errors()\n\n    if display_details:\n        dataset.display_dataset_details()\n\n    if is_series_based:\n        logger.info(\"Dataset is series-based. Use cesnet_tszoo.configs.SeriesBasedConfig\")\n    else:\n        logger.info(\"Dataset is time-based. Use cesnet_tszoo.configs.TimeBasedConfig\")\n\n    return dataset\n</code></pre>"},{"location":"reference_enums/","title":"Enums","text":""},{"location":"reference_enums/#cesnet_tszoo.utils.enums","title":"cesnet_tszoo.utils.enums","text":""},{"location":"reference_enums/#cesnet_tszoo.utils.enums.AgreggationType","title":"AgreggationType","text":"<p>               Bases: <code>Enum</code></p> <p>Possible aggregations for database.</p> Source code in <code>cesnet_tszoo\\utils\\enums.py</code> <pre><code>class AgreggationType(Enum):\n    \"\"\"Possible aggregations for database. \"\"\"\n\n    AGG_1_DAY = \"1_day\"\n    \"\"\"1 day aggregation for source type. \"\"\"\n\n    AGG_1_HOUR = \"1_hour\"\n    \"\"\"1 hour aggregation for source type. \"\"\"\n\n    AGG_10_MINUTES = \"10_minutes\"\n    \"\"\"10 minutes aggregation for source type. \"\"\"\n\n    AGG_1_MINUTE = \"1_minute\"\n    \"\"\"1 minute aggregation for source type. \"\"\"\n\n    @staticmethod\n    def _to_str_with_agg(aggregation_type):\n        \"\"\"For paths. \"\"\"\n\n        return f\"agg_{aggregation_type.value}\"\n\n    @staticmethod\n    def _to_str_without_number(aggregation_type) -&gt; str:\n        \"\"\"For paths. \"\"\"\n\n        if aggregation_type == AgreggationType.AGG_10_MINUTES:\n            return \"minutes\"\n        elif aggregation_type == AgreggationType.AGG_1_HOUR:\n            return \"hour\"\n        elif aggregation_type == AgreggationType.AGG_1_DAY:\n            return \"day\"\n        elif aggregation_type == AgreggationType.AGG_1_MINUTE:\n            return \"minute\"\n        else:\n            raise NotImplementedError()\n</code></pre>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.AgreggationType.AGG_1_DAY","title":"AGG_1_DAY  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>AGG_1_DAY = '1_day'\n</code></pre> <p>1 day aggregation for source type.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.AgreggationType.AGG_1_HOUR","title":"AGG_1_HOUR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>AGG_1_HOUR = '1_hour'\n</code></pre> <p>1 hour aggregation for source type.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.AgreggationType.AGG_10_MINUTES","title":"AGG_10_MINUTES  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>AGG_10_MINUTES = '10_minutes'\n</code></pre> <p>10 minutes aggregation for source type.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.AgreggationType.AGG_1_MINUTE","title":"AGG_1_MINUTE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>AGG_1_MINUTE = '1_minute'\n</code></pre> <p>1 minute aggregation for source type.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.SourceType","title":"SourceType","text":"<p>               Bases: <code>Enum</code></p> <p>Possible source types for database.</p> Source code in <code>cesnet_tszoo\\utils\\enums.py</code> <pre><code>class SourceType(Enum):\n    \"\"\"Possible source types for database. \"\"\"\n\n    IP_ADDRESSES_FULL = \"ip_addresses_full\"\n    \"\"\"Traffic of ip addresses of specific devices. \"\"\"\n\n    IP_ADDRESSES_SAMPLE = \"ip_addresses_sample\"\n    \"\"\"Traffic of subset from `ip_addresses_full`. \"\"\"\n\n    INSTITUTION_SUBNETS = \"institution_subnets\"\n    \"\"\"Traffic of subnets in institutions`. \"\"\"\n\n    INSTITUTIONS = \"institutions\"\n    \"\"\"Traffic Institutions of CESNET3 network. \"\"\"\n\n    CESNET2 = \"CESNET2\"\n    \"\"\"Traffic of CESNET2 network. \"\"\"\n</code></pre>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.SourceType.IP_ADDRESSES_FULL","title":"IP_ADDRESSES_FULL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IP_ADDRESSES_FULL = 'ip_addresses_full'\n</code></pre> <p>Traffic of ip addresses of specific devices.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.SourceType.IP_ADDRESSES_SAMPLE","title":"IP_ADDRESSES_SAMPLE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IP_ADDRESSES_SAMPLE = 'ip_addresses_sample'\n</code></pre> <p>Traffic of subset from <code>ip_addresses_full</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.SourceType.INSTITUTION_SUBNETS","title":"INSTITUTION_SUBNETS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>INSTITUTION_SUBNETS = 'institution_subnets'\n</code></pre> <p>Traffic of subnets in institutions`.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.SourceType.INSTITUTIONS","title":"INSTITUTIONS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>INSTITUTIONS = 'institutions'\n</code></pre> <p>Traffic Institutions of CESNET3 network.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.SourceType.CESNET2","title":"CESNET2  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CESNET2 = 'CESNET2'\n</code></pre> <p>Traffic of CESNET2 network.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.FillerType","title":"FillerType","text":"<p>               Bases: <code>Enum</code></p> <p>Built-in filler types.</p> Source code in <code>cesnet_tszoo\\utils\\enums.py</code> <pre><code>class FillerType(Enum):\n    \"\"\"Built-in filler types. \"\"\"\n\n    MEAN_FILLER = \"mean_filler\"\n    \"\"\"Represents filler [`MeanFiller`][cesnet_tszoo.utils.filler.MeanFiller]. Equivalent to literal `mean_filler`. \"\"\"\n\n    FORWARD_FILLER = \"forward_filler\"\n    \"\"\"Represents filler [`ForwardFiller`][cesnet_tszoo.utils.filler.ForwardFiller]. Equivalent to literal `forward_filler`. \"\"\"\n\n    LINEAR_INTERPOLATION_FILLER = \"linear_interpolation_filler\"\n    \"\"\"Represents filler [`LinearInterpolationFiller`][cesnet_tszoo.utils.filler.LinearInterpolationFiller]. Equivalent to literal `linear_interpolation_filler`. \"\"\"\n</code></pre>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.FillerType.MEAN_FILLER","title":"MEAN_FILLER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MEAN_FILLER = 'mean_filler'\n</code></pre> <p>Represents filler <code>MeanFiller</code>. Equivalent to literal <code>mean_filler</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.FillerType.FORWARD_FILLER","title":"FORWARD_FILLER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FORWARD_FILLER = 'forward_filler'\n</code></pre> <p>Represents filler <code>ForwardFiller</code>. Equivalent to literal <code>forward_filler</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.FillerType.LINEAR_INTERPOLATION_FILLER","title":"LINEAR_INTERPOLATION_FILLER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>LINEAR_INTERPOLATION_FILLER = 'linear_interpolation_filler'\n</code></pre> <p>Represents filler <code>LinearInterpolationFiller</code>. Equivalent to literal <code>linear_interpolation_filler</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.ScalerType","title":"ScalerType","text":"<p>               Bases: <code>Enum</code></p> <p>Built-in scaler types.</p> Source code in <code>cesnet_tszoo\\utils\\enums.py</code> <pre><code>class ScalerType(Enum):\n    \"\"\"Built-in scaler types. \"\"\"\n\n    MIN_MAX_SCALER = \"min_max_scaler\"\n    \"\"\"Represents scaler [`MinMaxScaler`][cesnet_tszoo.utils.scaler.MinMaxScaler]. Equivalent to literal `min_max_scaler`. \"\"\"\n\n    STANDARD_SCALER = \"standard_scaler\"\n    \"\"\"Represents scaler [`StandardScaler`][cesnet_tszoo.utils.scaler.StandardScaler]. Equivalent to literal `standard_scaler`. \"\"\"\n\n    MAX_ABS_SCALER = \"max_abs_scaler\"\n    \"\"\"Represents scaler [`MaxAbsScaler`][cesnet_tszoo.utils.scaler.MaxAbsScaler]. Equivalent to literal `max_abs_scaler`. \"\"\"\n\n    LOG_SCALER = \"log_scaler\"\n    \"\"\"Represents scaler [`LogScaler`][cesnet_tszoo.utils.scaler.LogScaler]. Equivalent to literal `log_scaler`. \"\"\"\n\n    L2_NORMALIZER = \"l2_normalizer\"\n    \"\"\"Represents scaler [`L2Normalizer`][cesnet_tszoo.utils.scaler.L2Normalizer]. Equivalent to literal `l2_normalizer`. \"\"\"\n\n    ROBUST_SCALER = \"robust_scaler\"\n    \"\"\"Represents scaler [`RobustScaler`][cesnet_tszoo.utils.scaler.LogScaler]. Equivalent to literal `robust_scaler`. \"\"\"\n\n    POWER_TRANSFORMER = \"power_transformer\"\n    \"\"\"Represents scaler [`PowerTransformer`][cesnet_tszoo.utils.scaler.PowerTransformer]. Equivalent to literal `power_transformer`. \"\"\"\n\n    QUANTILE_TRANSFORMER = \"quantile_transformer\"\n    \"\"\"Represents scaler [`QuantileTransformer`][cesnet_tszoo.utils.scaler.QuantileTransformer]. Equivalent to literal `quantile_transformer`. \"\"\"\n</code></pre>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.ScalerType.MIN_MAX_SCALER","title":"MIN_MAX_SCALER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MIN_MAX_SCALER = 'min_max_scaler'\n</code></pre> <p>Represents scaler <code>MinMaxScaler</code>. Equivalent to literal <code>min_max_scaler</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.ScalerType.STANDARD_SCALER","title":"STANDARD_SCALER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>STANDARD_SCALER = 'standard_scaler'\n</code></pre> <p>Represents scaler <code>StandardScaler</code>. Equivalent to literal <code>standard_scaler</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.ScalerType.MAX_ABS_SCALER","title":"MAX_ABS_SCALER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MAX_ABS_SCALER = 'max_abs_scaler'\n</code></pre> <p>Represents scaler <code>MaxAbsScaler</code>. Equivalent to literal <code>max_abs_scaler</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.ScalerType.LOG_SCALER","title":"LOG_SCALER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>LOG_SCALER = 'log_scaler'\n</code></pre> <p>Represents scaler <code>LogScaler</code>. Equivalent to literal <code>log_scaler</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.ScalerType.L2_NORMALIZER","title":"L2_NORMALIZER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>L2_NORMALIZER = 'l2_normalizer'\n</code></pre> <p>Represents scaler <code>L2Normalizer</code>. Equivalent to literal <code>l2_normalizer</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.ScalerType.ROBUST_SCALER","title":"ROBUST_SCALER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ROBUST_SCALER = 'robust_scaler'\n</code></pre> <p>Represents scaler <code>RobustScaler</code>. Equivalent to literal <code>robust_scaler</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.ScalerType.POWER_TRANSFORMER","title":"POWER_TRANSFORMER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>POWER_TRANSFORMER = 'power_transformer'\n</code></pre> <p>Represents scaler <code>PowerTransformer</code>. Equivalent to literal <code>power_transformer</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.ScalerType.QUANTILE_TRANSFORMER","title":"QUANTILE_TRANSFORMER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>QUANTILE_TRANSFORMER = 'quantile_transformer'\n</code></pre> <p>Represents scaler <code>QuantileTransformer</code>. Equivalent to literal <code>quantile_transformer</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.TimeFormat","title":"TimeFormat","text":"<p>               Bases: <code>Enum</code></p> <p>Different supported time formats.</p> Source code in <code>cesnet_tszoo\\utils\\enums.py</code> <pre><code>class TimeFormat(Enum):\n    \"\"\"Different supported time formats. \"\"\"\n\n    ID_TIME = \"id_time\"\n    \"\"\"Time as indices, starting from 0. \"\"\"\n\n    DATETIME = \"datetime\"\n    \"\"\"Time as [`datetime`](https://docs.python.org/3/library/datetime.html) object. \"\"\"\n\n    UNIX_TIME = \"unix_time\"\n    \"\"\"Time in unix time format. \"\"\"\n\n    SHIFTED_UNIX_TIME = \"shifted_unix_time\"\n    \"\"\"Unix time format but offsetted so it starts from 0. \"\"\"\n</code></pre>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.TimeFormat.ID_TIME","title":"ID_TIME  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ID_TIME = 'id_time'\n</code></pre> <p>Time as indices, starting from 0.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.TimeFormat.DATETIME","title":"DATETIME  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DATETIME = 'datetime'\n</code></pre> <p>Time as <code>datetime</code> object.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.TimeFormat.UNIX_TIME","title":"UNIX_TIME  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>UNIX_TIME = 'unix_time'\n</code></pre> <p>Time in unix time format.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.TimeFormat.SHIFTED_UNIX_TIME","title":"SHIFTED_UNIX_TIME  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SHIFTED_UNIX_TIME = 'shifted_unix_time'\n</code></pre> <p>Unix time format but offsetted so it starts from 0.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.SplitType","title":"SplitType","text":"<p>               Bases: <code>Enum</code></p> <p>Different split variants.</p> Source code in <code>cesnet_tszoo\\utils\\enums.py</code> <pre><code>class SplitType(Enum):\n    \"\"\"Different split variants. \"\"\"\n\n    TRAIN = \"train\"\n    \"\"\"Represents training set of dataset. \"\"\"\n\n    VAL = \"val\"\n    \"\"\"Represents validation set of dataset. \"\"\"\n\n    TEST = \"test\"\n    \"\"\"Represents test set of dataset. \"\"\"\n\n    ALL = \"all\"\n    \"\"\"Represents train/val/test sets as one or just everything. \"\"\"\n</code></pre>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.SplitType.TRAIN","title":"TRAIN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TRAIN = 'train'\n</code></pre> <p>Represents training set of dataset.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.SplitType.VAL","title":"VAL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>VAL = 'val'\n</code></pre> <p>Represents validation set of dataset.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.SplitType.TEST","title":"TEST  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TEST = 'test'\n</code></pre> <p>Represents test set of dataset.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.SplitType.ALL","title":"ALL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ALL = 'all'\n</code></pre> <p>Represents train/val/test sets as one or just everything.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.AnnotationType","title":"AnnotationType","text":"<p>               Bases: <code>Enum</code></p> <p>Categories of Annotations.</p> Source code in <code>cesnet_tszoo\\utils\\enums.py</code> <pre><code>class AnnotationType(Enum):\n    \"\"\"Categories of Annotations. \"\"\"\n\n    ID_TIME = \"id_time\"\n    \"\"\"Represents annotations for time. \"\"\"\n\n    TS_ID = \"ts_id\"\n    \"\"\"Represents annotations for time series. \"\"\"\n\n    BOTH = \"both\"\n    \"\"\"Represents annotations for time in time series. \"\"\"\n</code></pre>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.AnnotationType.ID_TIME","title":"ID_TIME  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ID_TIME = 'id_time'\n</code></pre> <p>Represents annotations for time.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.AnnotationType.TS_ID","title":"TS_ID  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TS_ID = 'ts_id'\n</code></pre> <p>Represents annotations for time series.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.AnnotationType.BOTH","title":"BOTH  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>BOTH = 'both'\n</code></pre> <p>Represents annotations for time in time series.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.DataloaderOrder","title":"DataloaderOrder","text":"<p>               Bases: <code>Enum</code></p> <p>Order for loading data with PyTorch <code>DataLoader</code>.</p> Source code in <code>cesnet_tszoo\\utils\\enums.py</code> <pre><code>class DataloaderOrder(Enum):\n    \"\"\"Order for loading data with PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). \"\"\"\n\n    RANDOM = \"random\"\n    \"\"\"Loaded data will be randomly selected. \"\"\"\n\n    SEQUENTIAL = \"sequential\"\n    \"\"\"Loaded data will be in the selected order. \"\"\"\n</code></pre>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.DataloaderOrder.RANDOM","title":"RANDOM  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>RANDOM = 'random'\n</code></pre> <p>Loaded data will be randomly selected.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.DataloaderOrder.SEQUENTIAL","title":"SEQUENTIAL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SEQUENTIAL = 'sequential'\n</code></pre> <p>Loaded data will be in the selected order.</p>"},{"location":"reference_fillers/","title":"Fillers","text":""},{"location":"reference_fillers/#cesnet_tszoo.utils.filler","title":"cesnet_tszoo.utils.filler","text":""},{"location":"reference_fillers/#cesnet_tszoo.utils.filler.Filler","title":"Filler","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for data fillers.</p> <p>This class serves as the foundation for creating custom fillers. To implement a custom filler, this class must be subclassed and extended. Fillers are used to handle missing data in a dataset.</p> <p>Example:</p> <pre><code>import numpy as np\n\nclass ForwardFiller(Filler):\n\n    def __init__(self, features):\n        super().__init__(features)\n\n        self.last_values = None\n\n    def fill(self, batch_values: np.ndarray, existing_indices: np.ndarray, missing_indices: np.ndarray, **kwargs) -&gt; None:\n        if len(missing_indices) &gt; 0 and missing_indices[0] == 0 and self.last_values is not None:\n            batch_values[0] = self.last_values\n            missing_indices = missing_indices[1:]\n\n        mask = np.zeros_like(batch_values, dtype=bool)\n        mask[missing_indices] = True\n        mask = mask.T\n\n        idx = np.where(~mask, np.arange(mask.shape[1]), 0)\n        np.maximum.accumulate(idx, axis=1, out=idx)\n\n        batch_values = batch_values.T\n        batch_values[mask] = batch_values[np.nonzero(mask)[0], idx[mask]]\n        batch_values = batch_values.T\n\n        self.last_values = np.copy(batch_values[-1])\n</code></pre> Source code in <code>cesnet_tszoo\\utils\\filler.py</code> <pre><code>class Filler(ABC):\n    \"\"\"\n    Base class for data fillers.\n\n    This class serves as the foundation for creating custom fillers. To implement a custom filler, this class must be subclassed and extended.\n    Fillers are used to handle missing data in a dataset.\n\n    Example:\n\n        import numpy as np\n\n        class ForwardFiller(Filler):\n\n            def __init__(self, features):\n                super().__init__(features)\n\n                self.last_values = None\n\n            def fill(self, batch_values: np.ndarray, existing_indices: np.ndarray, missing_indices: np.ndarray, **kwargs) -&gt; None:\n                if len(missing_indices) &gt; 0 and missing_indices[0] == 0 and self.last_values is not None:\n                    batch_values[0] = self.last_values\n                    missing_indices = missing_indices[1:]\n\n                mask = np.zeros_like(batch_values, dtype=bool)\n                mask[missing_indices] = True\n                mask = mask.T\n\n                idx = np.where(~mask, np.arange(mask.shape[1]), 0)\n                np.maximum.accumulate(idx, axis=1, out=idx)\n\n                batch_values = batch_values.T\n                batch_values[mask] = batch_values[np.nonzero(mask)[0], idx[mask]]\n                batch_values = batch_values.T\n\n                self.last_values = np.copy(batch_values[-1])\n    \"\"\"\n\n    def __init__(self, features):\n        super().__init__()\n\n        self.features = features\n\n    @abstractmethod\n    def fill(self, batch_values: np.ndarray, existing_indices: np.ndarray, missing_indices: np.ndarray, **kwargs) -&gt; None:\n        \"\"\"Fills missing data in the `batch_values`.\n\n        This method is responsible for filling missing data within a single time series.\n\n        Parameters:\n            batch_values: Data of a single time series with shape `(times, features)` excluding IDs.\n            existing_indices: Indices in `batch_values` where data is not missing.\n            missing_indices: Indices in `batch_values` where data is missing.\n            kwargs: first_next_existing_values, first_next_existing_values_distance, default_values \n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference_fillers/#cesnet_tszoo.utils.filler.Filler.fill","title":"fill  <code>abstractmethod</code>","text":"<pre><code>fill(batch_values: ndarray, existing_indices: ndarray, missing_indices: ndarray, **kwargs) -&gt; None\n</code></pre> <p>Fills missing data in the <code>batch_values</code>.</p> <p>This method is responsible for filling missing data within a single time series.</p> <p>Parameters:</p> Name Type Description Default <code>batch_values</code> <code>ndarray</code> <p>Data of a single time series with shape <code>(times, features)</code> excluding IDs.</p> required <code>existing_indices</code> <code>ndarray</code> <p>Indices in <code>batch_values</code> where data is not missing.</p> required <code>missing_indices</code> <code>ndarray</code> <p>Indices in <code>batch_values</code> where data is missing.</p> required <code>kwargs</code> <p>first_next_existing_values, first_next_existing_values_distance, default_values</p> <code>{}</code> Source code in <code>cesnet_tszoo\\utils\\filler.py</code> <pre><code>@abstractmethod\ndef fill(self, batch_values: np.ndarray, existing_indices: np.ndarray, missing_indices: np.ndarray, **kwargs) -&gt; None:\n    \"\"\"Fills missing data in the `batch_values`.\n\n    This method is responsible for filling missing data within a single time series.\n\n    Parameters:\n        batch_values: Data of a single time series with shape `(times, features)` excluding IDs.\n        existing_indices: Indices in `batch_values` where data is not missing.\n        missing_indices: Indices in `batch_values` where data is missing.\n        kwargs: first_next_existing_values, first_next_existing_values_distance, default_values \n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference_fillers/#cesnet_tszoo.utils.filler.MeanFiller","title":"MeanFiller","text":"<p>               Bases: <code>Filler</code></p> <p>Fills values from total mean of all previous values.</p> <p>Corresponds to enum <code>FillerType.MEAN_FILLER</code> or literal <code>mean_filler</code>.</p> Source code in <code>cesnet_tszoo\\utils\\filler.py</code> <pre><code>class MeanFiller(Filler):\n    \"\"\"\n    Fills values from total mean of all previous values.\n\n    Corresponds to enum [`FillerType.MEAN_FILLER`][cesnet_tszoo.utils.enums.FillerType] or literal `mean_filler`.\n    \"\"\"\n\n    def __init__(self, features):\n        super().__init__(features)\n\n        self.averages = np.zeros(len(features), dtype=np.float64)\n        self.total_existing_values = 0\n\n    def fill(self, batch_values: np.ndarray, existing_indices: np.ndarray, missing_indices: np.ndarray, **kwargs) -&gt; None:\n        self.total_existing_values += len(existing_indices)\n\n        if len(existing_indices) == 0:\n            if self.total_existing_values &gt; 0:\n                batch_values[:, :][missing_indices] = self.averages\n            return\n\n        existing_values_until_now = self.total_existing_values - len(existing_indices)\n\n        total_divisors = np.arange(1 + existing_values_until_now, len(batch_values) + 1 + existing_values_until_now)\n\n        missing_mask = np.zeros_like(total_divisors)\n        missing_mask[missing_indices] = 1\n\n        total_divisors -= np.cumsum(missing_mask, axis=0)\n\n        if total_divisors[0] == 0:\n            missing_start_mask = np.logical_and(missing_mask, total_divisors &lt;= 0)\n            missing_start_offset = len(batch_values[missing_start_mask])\n            total_divisors = total_divisors[missing_start_offset:]\n            missing_indices = np.logical_and(missing_mask[missing_start_offset:], total_divisors &gt; 0)\n            batch_values[missing_start_offset:][missing_indices] = 0\n            new_sums = np.cumsum(batch_values[missing_start_offset:], axis=0)\n        else:\n            batch_values[missing_indices] = 0\n            missing_start_offset = 0\n            new_sums = np.cumsum(batch_values[:], axis=0)\n\n        for i in range(len(batch_values[0])):\n\n            updated_old_averages = existing_values_until_now * (self.averages[i] / total_divisors)\n\n            new_averages = new_sums[:, i] / total_divisors + updated_old_averages\n            batch_values[missing_start_offset:, i][missing_indices] = new_averages[missing_indices]\n\n            self.averages[i] = new_averages[-1]\n</code></pre>"},{"location":"reference_fillers/#cesnet_tszoo.utils.filler.ForwardFiller","title":"ForwardFiller","text":"<p>               Bases: <code>Filler</code></p> <p>Fills missing values based on last existing value. </p> <p>Corresponds to enum <code>FillerType.FORWARD_FILLER</code> or literal <code>forward_filler</code>.</p> Source code in <code>cesnet_tszoo\\utils\\filler.py</code> <pre><code>class ForwardFiller(Filler):\n    \"\"\"\n    Fills missing values based on last existing value. \n\n    Corresponds to enum [`FillerType.FORWARD_FILLER`][cesnet_tszoo.utils.enums.FillerType] or literal `forward_filler`.\n    \"\"\"\n\n    def __init__(self, features):\n        super().__init__(features)\n\n        self.last_values = None\n\n    def fill(self, batch_values: np.ndarray, existing_indices: np.ndarray, missing_indices: np.ndarray, **kwargs) -&gt; None:\n        if len(missing_indices) &gt; 0 and missing_indices[0] == 0 and self.last_values is not None:\n            batch_values[0] = self.last_values\n            missing_indices = missing_indices[1:]\n\n        mask = np.zeros_like(batch_values, dtype=bool)\n        mask[missing_indices] = True\n        mask = mask.T\n\n        idx = np.where(~mask, np.arange(mask.shape[1]), 0)\n        np.maximum.accumulate(idx, axis=1, out=idx)\n\n        batch_values = batch_values.T\n        batch_values[mask] = batch_values[np.nonzero(mask)[0], idx[mask]]\n        batch_values = batch_values.T\n\n        self.last_values = np.copy(batch_values[-1])\n</code></pre>"},{"location":"reference_fillers/#cesnet_tszoo.utils.filler.LinearInterpolationFiller","title":"LinearInterpolationFiller","text":"<p>               Bases: <code>Filler</code></p> <p>Fills values with linear interpolation. </p> <p>Corresponds to enum <code>FillerType.LINEAR_INTERPOLATION_FILLER</code> or literal <code>linear_interpolation_filler</code>.</p> Source code in <code>cesnet_tszoo\\utils\\filler.py</code> <pre><code>class LinearInterpolationFiller(Filler):\n    \"\"\"\n    Fills values with linear interpolation. \n\n    Corresponds to enum [`FillerType.LINEAR_INTERPOLATION_FILLER`][cesnet_tszoo.utils.enums.FillerType] or literal `linear_interpolation_filler`.\n    \"\"\"\n\n    def __init__(self, features):\n        super().__init__(features)\n\n        self.last_values = None\n        self.last_values_x_pos = None\n\n    def fill(self, batch_values: np.ndarray, existing_indices: np.ndarray, missing_indices: np.ndarray, **kwargs) -&gt; None:\n\n        if missing_indices is None:\n            self.last_values = np.copy(batch_values[-1, :])\n            return\n        elif len(existing_indices) == 0 and (self.last_values is None or kwargs['first_next_existing_values'] is None):\n            return\n\n        if self.last_values is not None and kwargs['first_next_existing_values'] is not None:\n            if len(existing_indices) == 0:\n                existing_values = np.vstack((self.last_values, kwargs['first_next_existing_values']))\n                existing_values_x = np.hstack((self.last_values_x_pos, kwargs['first_next_existing_values_distance']))\n            else:\n                existing_values = np.vstack((self.last_values, batch_values[existing_indices, :], kwargs['first_next_existing_values']))\n                existing_values_x = np.hstack((self.last_values_x_pos, existing_indices, kwargs['first_next_existing_values_distance']))\n        elif self.last_values is not None:\n            if len(existing_indices) == 0:\n                existing_values = np.reshape(self.last_values, (1, len(batch_values[0])))\n                existing_values_x = [self.last_values_x_pos]\n            else:\n                existing_values = np.vstack((self.last_values, batch_values[existing_indices, :]))\n                existing_values_x = np.hstack((self.last_values_x_pos, existing_indices))\n        elif kwargs['first_next_existing_values'] is not None:\n            existing_values = np.vstack((batch_values[existing_indices, :], kwargs['first_next_existing_values']))\n            existing_values_x = np.hstack((existing_indices, kwargs['first_next_existing_values_distance']))\n        else:\n            existing_values = batch_values[existing_indices].view()\n            existing_values_x = existing_indices\n\n        for i in range(len(batch_values[0])):\n            if len(existing_indices) == 0:\n                batch_values[:, i][missing_indices] = np.interp(missing_indices, existing_values_x, existing_values[:, i], left=kwargs[\"default_values\"][i], right=kwargs[\"default_values\"][i])\n            else:\n                batch_values[:, i][missing_indices] = np.interp(missing_indices, existing_values_x, existing_values[:, i], left=kwargs[\"default_values\"][i], right=kwargs[\"default_values\"][i])\n\n        self.last_values = np.copy(batch_values[-1, :])\n        self.last_values_x_pos = -1\n</code></pre>"},{"location":"reference_scalers/","title":"Scalers","text":""},{"location":"reference_scalers/#cesnet_tszoo.utils.scaler","title":"cesnet_tszoo.utils.scaler","text":""},{"location":"reference_scalers/#cesnet_tszoo.utils.scaler.Scaler","title":"Scaler","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for scalers, used for transforming data.</p> <p>This class serves as the foundation for creating custom scalers. To implement a custom scaler, this class is recommended to be subclassed and extended.</p> <p>Example:</p> <pre><code>import numpy as np\n\nclass LogScaler(Scaler):\n\n    def fit(self, data: np.ndarray):\n        ...\n\n    def partial_fit(self, data: np.ndarray) -&gt; None:\n        ...\n\n    def transform(self, data: np.ndarray):\n        log_data = np.ma.log(data)\n\n        return log_data.filled(np.nan)\n</code></pre> Source code in <code>cesnet_tszoo\\utils\\scaler.py</code> <pre><code>class Scaler(ABC):\n    \"\"\"\n    Base class for scalers, used for transforming data.\n\n    This class serves as the foundation for creating custom scalers. To implement a custom scaler, this class is recommended to be subclassed and extended.\n\n    Example:\n\n        import numpy as np\n\n        class LogScaler(Scaler):\n\n            def fit(self, data: np.ndarray):\n                ...\n\n            def partial_fit(self, data: np.ndarray) -&gt; None:\n                ...\n\n            def transform(self, data: np.ndarray):\n                log_data = np.ma.log(data)\n\n                return log_data.filled(np.nan)\n    \"\"\"\n\n    @abstractmethod\n    def fit(self, data: np.ndarray) -&gt; None:\n        \"\"\"\n        Sets the scaler values for a given time series part.\n\n        This method must be implemented if using multiple scalers that have not been pre-fitted.\n\n        Parameters:\n            data: A numpy array representing data for a single time series with shape `(times, features)` excluding any identifiers.  \n        \"\"\"\n        ...\n\n    @abstractmethod\n    def partial_fit(self, data: np.ndarray) -&gt; None:\n        \"\"\"\n        Partially sets the scaler values for a given time series part.\n\n        This method must be implemented if using a single scaler that is not pre-fitted for all time series, or when using pre-fitted scaler(s) with `partial_fit_initialized_scalers` set to `True`.\n\n        Parameters:\n            data: A numpy array representing data for a single time series with shape `(times, features)` excluding any identifiers.        \n        \"\"\"\n        ...\n\n    @abstractmethod\n    def transform(self, data: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Transforms the input data for a given time series part.\n\n        This method must always be implemented.\n\n        Parameters:\n            data: A numpy array representing data for a single time series with shape `(times, features)` excluding any identifiers.  \n\n        Returns:\n            The transformed data, with the same shape as the input `(times, features)`.            \n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference_scalers/#cesnet_tszoo.utils.scaler.Scaler.fit","title":"fit  <code>abstractmethod</code>","text":"<pre><code>fit(data: ndarray) -&gt; None\n</code></pre> <p>Sets the scaler values for a given time series part.</p> <p>This method must be implemented if using multiple scalers that have not been pre-fitted.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>A numpy array representing data for a single time series with shape <code>(times, features)</code> excluding any identifiers.</p> required Source code in <code>cesnet_tszoo\\utils\\scaler.py</code> <pre><code>@abstractmethod\ndef fit(self, data: np.ndarray) -&gt; None:\n    \"\"\"\n    Sets the scaler values for a given time series part.\n\n    This method must be implemented if using multiple scalers that have not been pre-fitted.\n\n    Parameters:\n        data: A numpy array representing data for a single time series with shape `(times, features)` excluding any identifiers.  \n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference_scalers/#cesnet_tszoo.utils.scaler.Scaler.partial_fit","title":"partial_fit  <code>abstractmethod</code>","text":"<pre><code>partial_fit(data: ndarray) -&gt; None\n</code></pre> <p>Partially sets the scaler values for a given time series part.</p> <p>This method must be implemented if using a single scaler that is not pre-fitted for all time series, or when using pre-fitted scaler(s) with <code>partial_fit_initialized_scalers</code> set to <code>True</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>A numpy array representing data for a single time series with shape <code>(times, features)</code> excluding any identifiers.</p> required Source code in <code>cesnet_tszoo\\utils\\scaler.py</code> <pre><code>@abstractmethod\ndef partial_fit(self, data: np.ndarray) -&gt; None:\n    \"\"\"\n    Partially sets the scaler values for a given time series part.\n\n    This method must be implemented if using a single scaler that is not pre-fitted for all time series, or when using pre-fitted scaler(s) with `partial_fit_initialized_scalers` set to `True`.\n\n    Parameters:\n        data: A numpy array representing data for a single time series with shape `(times, features)` excluding any identifiers.        \n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference_scalers/#cesnet_tszoo.utils.scaler.Scaler.transform","title":"transform  <code>abstractmethod</code>","text":"<pre><code>transform(data: ndarray) -&gt; np.ndarray\n</code></pre> <p>Transforms the input data for a given time series part.</p> <p>This method must always be implemented.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>A numpy array representing data for a single time series with shape <code>(times, features)</code> excluding any identifiers.  </p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The transformed data, with the same shape as the input <code>(times, features)</code>.</p> Source code in <code>cesnet_tszoo\\utils\\scaler.py</code> <pre><code>@abstractmethod\ndef transform(self, data: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Transforms the input data for a given time series part.\n\n    This method must always be implemented.\n\n    Parameters:\n        data: A numpy array representing data for a single time series with shape `(times, features)` excluding any identifiers.  \n\n    Returns:\n        The transformed data, with the same shape as the input `(times, features)`.            \n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference_scalers/#cesnet_tszoo.utils.scaler.LogScaler","title":"LogScaler","text":"<p>               Bases: <code>Scaler</code></p> <p>Tranforms data with natural logarithm. Zero or invalid values are set to <code>np.nan</code>.</p> <p>Corresponds to enum <code>ScalerType.LOG_SCALER</code> or literal <code>log_scaler</code>.</p> Source code in <code>cesnet_tszoo\\utils\\scaler.py</code> <pre><code>class LogScaler(Scaler):\n    \"\"\"\n    Tranforms data with natural logarithm. Zero or invalid values are set to `np.nan`.\n\n    Corresponds to enum [`ScalerType.LOG_SCALER`][cesnet_tszoo.utils.enums.ScalerType] or literal `log_scaler`.\n    \"\"\"\n\n    def fit(self, data: np.ndarray):\n        ...\n\n    def partial_fit(self, data: np.ndarray) -&gt; None:\n        ...\n\n    def transform(self, data: np.ndarray):\n        log_data = np.ma.log(data)\n\n        return log_data.filled(np.nan)\n</code></pre>"},{"location":"reference_scalers/#cesnet_tszoo.utils.scaler.L2Normalizer","title":"L2Normalizer","text":"<p>               Bases: <code>Scaler</code></p> <p>Tranforms data using Scikit <code>L2Normalizer</code>.</p> <p>Corresponds to enum <code>ScalerType.L2_NORMALIZER</code> or literal <code>l2_normalizer</code>.</p> Source code in <code>cesnet_tszoo\\utils\\scaler.py</code> <pre><code>class L2Normalizer(Scaler):\n    \"\"\"\n    Tranforms data using Scikit [`L2Normalizer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html).\n\n    Corresponds to enum [`ScalerType.L2_NORMALIZER`][cesnet_tszoo.utils.enums.ScalerType] or literal `l2_normalizer`.\n    \"\"\"\n\n    def __init__(self):\n        self.scaler = sk.Normalizer(norm=\"l2\")\n\n    def fit(self, data: np.ndarray):\n        ...\n\n    def partial_fit(self, data: np.ndarray) -&gt; None:\n        ...\n\n    def transform(self, data: np.ndarray):\n        return self.scaler.fit_transform(data)\n</code></pre>"},{"location":"reference_scalers/#cesnet_tszoo.utils.scaler.MinMaxScaler","title":"MinMaxScaler","text":"<p>               Bases: <code>Scaler</code></p> <p>Tranforms data using Scikit <code>MinMaxScaler</code>.</p> <p>Corresponds to enum <code>ScalerType.MIN_MAX_SCALER</code> or literal <code>min_max_scaler</code>.</p> Source code in <code>cesnet_tszoo\\utils\\scaler.py</code> <pre><code>class MinMaxScaler(Scaler):\n    \"\"\"\n    Tranforms data using Scikit [`MinMaxScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html).\n\n    Corresponds to enum [`ScalerType.MIN_MAX_SCALER`][cesnet_tszoo.utils.enums.ScalerType] or literal `min_max_scaler`.\n    \"\"\"\n\n    def __init__(self):\n        self.scaler = sk.MinMaxScaler()\n\n    def fit(self, data: np.ndarray):\n        self.scaler.fit(data)\n\n    def partial_fit(self, data: np.ndarray) -&gt; None:\n        self.scaler.partial_fit(data)\n\n    def transform(self, data: np.ndarray):\n        return self.scaler.transform(data)\n</code></pre>"},{"location":"reference_scalers/#cesnet_tszoo.utils.scaler.StandardScaler","title":"StandardScaler","text":"<p>               Bases: <code>Scaler</code></p> <p>Tranforms data using Scikit <code>StandardScaler</code>.</p> <p>Corresponds to enum <code>ScalerType.STANDARD_SCALER</code> or literal <code>standard_scaler</code>.</p> Source code in <code>cesnet_tszoo\\utils\\scaler.py</code> <pre><code>class StandardScaler(Scaler):\n    \"\"\"\n    Tranforms data using Scikit [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).\n\n    Corresponds to enum [`ScalerType.STANDARD_SCALER`][cesnet_tszoo.utils.enums.ScalerType] or literal `standard_scaler`.\n    \"\"\"\n\n    def __init__(self):\n        self.scaler = sk.StandardScaler()\n\n    def fit(self, data: np.ndarray):\n        self.scaler.fit(data)\n\n    def partial_fit(self, data: np.ndarray) -&gt; None:\n        self.scaler.partial_fit(data)\n\n    def transform(self, data: np.ndarray):\n        return self.scaler.transform(data)\n</code></pre>"},{"location":"reference_scalers/#cesnet_tszoo.utils.scaler.MaxAbsScaler","title":"MaxAbsScaler","text":"<p>               Bases: <code>Scaler</code></p> <p>Tranforms data using Scikit <code>MaxAbsScaler</code>.</p> <p>Corresponds to enum <code>ScalerType.MAX_ABS_SCALER</code> or literal <code>max_abs_scaler</code>.</p> Source code in <code>cesnet_tszoo\\utils\\scaler.py</code> <pre><code>class MaxAbsScaler(Scaler):\n    \"\"\"\n    Tranforms data using Scikit [`MaxAbsScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html).\n\n    Corresponds to enum [`ScalerType.MAX_ABS_SCALER`][cesnet_tszoo.utils.enums.ScalerType] or literal `max_abs_scaler`.\n    \"\"\"\n\n    def __init__(self):\n        self.scaler = sk.MaxAbsScaler()\n\n    def fit(self, data: np.ndarray):\n        self.scaler.fit(data)\n\n    def partial_fit(self, data: np.ndarray) -&gt; None:\n        self.scaler.partial_fit(data)\n\n    def transform(self, data: np.ndarray):\n        return self.scaler.transform(data)\n</code></pre>"},{"location":"reference_scalers/#cesnet_tszoo.utils.scaler.PowerTransformer","title":"PowerTransformer","text":"<p>               Bases: <code>Scaler</code></p> <p>Tranforms data using Scikit <code>PowerTransformer</code>.</p> <p>Corresponds to enum <code>ScalerType.POWER_TRANSFORMER</code> or literal <code>power_transformer</code>.</p> <p>partial_fit not supported</p> <p>Because this transformer does not support partial_fit it can't be used when using one scaler that needs to be fitted for multiple time series.</p> Source code in <code>cesnet_tszoo\\utils\\scaler.py</code> <pre><code>class PowerTransformer(Scaler):\n    \"\"\"\n    Tranforms data using Scikit [`PowerTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html).\n\n    Corresponds to enum [`ScalerType.POWER_TRANSFORMER`][cesnet_tszoo.utils.enums.ScalerType] or literal `power_transformer`.\n\n    !!! warning \"partial_fit not supported\"\n        Because this transformer does not support partial_fit it can't be used when using one scaler that needs to be fitted for multiple time series.\n    \"\"\"\n\n    def __init__(self):\n        self.scaler = sk.PowerTransformer()\n\n    def fit(self, data: np.ndarray):\n        self.scaler.fit(data)\n\n    def partial_fit(self, data: np.ndarray) -&gt; None:\n        raise NotImplementedError(\"PowerTransformer does not support partial_fit.\")\n\n    def transform(self, data: np.ndarray):\n        return self.scaler.transform(data)\n</code></pre>"},{"location":"reference_scalers/#cesnet_tszoo.utils.scaler.QuantileTransformer","title":"QuantileTransformer","text":"<p>               Bases: <code>Scaler</code></p> <p>Tranforms data using Scikit <code>QuantileTransformer</code>.</p> <p>Corresponds to enum <code>ScalerType.QUANTILE_TRANSFORMER</code> or literal <code>quantile_transformer</code>.</p> <p>partial_fit not supported</p> <p>Because this transformer does not support partial_fit it can't be used when using one scaler that needs to be fitted for multiple time series.</p> Source code in <code>cesnet_tszoo\\utils\\scaler.py</code> <pre><code>class QuantileTransformer(Scaler):\n    \"\"\"\n    Tranforms data using Scikit [`QuantileTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html).\n\n    Corresponds to enum [`ScalerType.QUANTILE_TRANSFORMER`][cesnet_tszoo.utils.enums.ScalerType] or literal `quantile_transformer`.\n\n    !!! warning \"partial_fit not supported\"\n        Because this transformer does not support partial_fit it can't be used when using one scaler that needs to be fitted for multiple time series.    \n    \"\"\"\n\n    def __init__(self):\n        self.scaler = sk.QuantileTransformer()\n\n    def fit(self, data: np.ndarray):\n        self.scaler.fit(data)\n\n    def partial_fit(self, data: np.ndarray) -&gt; None:\n        raise NotImplementedError(\"QuantileTransformer does not support partial_fit.\")\n\n    def transform(self, data: np.ndarray):\n        return self.scaler.transform(data)\n</code></pre>"},{"location":"reference_scalers/#cesnet_tszoo.utils.scaler.RobustScaler","title":"RobustScaler","text":"<p>               Bases: <code>Scaler</code></p> <p>Tranforms data using Scikit <code>RobustScaler</code>.</p> <p>Corresponds to enum <code>ScalerType.ROBUST_SCALER</code> or literal <code>robust_scaler</code>.</p> <p>partial_fit not supported</p> <p>Because this scaler does not support partial_fit it can't be used when using one scaler that needs to be fitted for multiple time series.</p> Source code in <code>cesnet_tszoo\\utils\\scaler.py</code> <pre><code>class RobustScaler(Scaler):\n    \"\"\"\n    Tranforms data using Scikit [`RobustScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html).\n\n    Corresponds to enum [`ScalerType.ROBUST_SCALER`][cesnet_tszoo.utils.enums.ScalerType] or literal `robust_scaler`.\n\n    !!! warning \"partial_fit not supported\"\n        Because this scaler does not support partial_fit it can't be used when using one scaler that needs to be fitted for multiple time series.    \n    \"\"\"\n\n    def __init__(self):\n        self.scaler = sk.RobustScaler()\n\n    def fit(self, data: np.ndarray):\n        self.scaler.fit(data)\n\n    def partial_fit(self, data: np.ndarray) -&gt; None:\n        raise NotImplementedError(\"RobustScaler does not support partial_fit.\")\n\n    def transform(self, data: np.ndarray):\n        return self.scaler.transform(data)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/","title":"Series-based dataset class","text":""},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset","title":"cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset  <code>dataclass</code>","text":"<p>               Bases: <code>CesnetDataset</code></p> <p>This class is used for series-based returning of data. Can be created by using <code>get_dataset</code> with parameter <code>is_series_based</code> = <code>True</code>.</p> <p>Series-based means batch size affects number of returned time series in one batch. Which times for each time series are returned does not change.</p> <p>The dataset provides multiple ways to access the data:</p> <ul> <li>Iterable PyTorch DataLoader: For batch processing.</li> <li>Pandas DataFrame: For loading the entire training, validation, test or all set at once.</li> <li>Numpy array: For loading the entire training, validation, test or all set at once.      </li> <li>See loading data for more details.</li> </ul> <p>The dataset is stored in a PyTables database. The internal <code>SeriesBasedDataset</code> and <code>SeriesBasedInitializerDataset</code> classes (used only when calling <code>set_dataset_config_and_initialize</code>) act as wrappers that implement the PyTorch <code>Dataset</code>  interface. These wrappers are compatible with PyTorch\u2019s <code>DataLoader</code>, providing efficient parallel data loading. </p> <p>The dataset configuration is done through the <code>SeriesBasedConfig</code> class.     </p> <p>Intended usage:</p> <ol> <li>Create an instance of the dataset with the desired data root by calling <code>get_dataset</code>. This will download the dataset if it has not been previously downloaded and return instance of dataset.</li> <li>Create an instance of <code>SeriesBasedConfig</code> and set it using <code>set_dataset_config_and_initialize</code>.     This initializes the dataset, including data splitting (train/validation/test), fitting scalers (if needed), selecting features, and more. This is cached for later use.</li> <li>Use <code>get_train_dataloader</code>/<code>get_train_df</code>/<code>get_train_numpy</code> to get training data for chosen model.</li> <li>Validate the model and perform the hyperparameter optimalization on <code>get_val_dataloader</code>/<code>get_val_df</code>/<code>get_val_numpy</code>.</li> <li>Evaluate the model on <code>get_test_dataloader</code>/<code>get_test_df</code>/<code>get_test_numpy</code>.     </li> </ol> <p>Alternatively you can use <code>load_benchmark</code></p> <ol> <li>Call <code>load_benchmark</code> with the desired benchmark. You can use your own saved benchmark or you can use already built-in one. This will download the dataset and annotations (if available) if they have not been previously downloaded.</li> <li>Retrieve the initialized dataset using <code>get_initialized_dataset</code>. This will provide a dataset that is ready to use.</li> <li>Use <code>get_train_dataloader</code>/<code>get_train_df</code>/<code>get_train_numpy</code> to get training data for chosen model.</li> <li>Validate the model and perform the hyperparameter optimalization on <code>get_val_dataloader</code>/<code>get_val_df</code>/<code>get_val_numpy</code>.</li> <li>Evaluate the model on <code>get_test_dataloader</code>/<code>get_test_df</code>/<code>get_test_numpy</code>.       </li> </ol> <p>Parameters:</p> Name Type Description Default <code>database_name</code> <code>str</code> <p>Name of the database.</p> required <code>dataset_path</code> <code>str</code> <p>Path to the dataset file.     </p> required <code>configs_root</code> <code>str</code> <p>Path to the folder where configurations are saved.</p> required <code>benchmarks_root</code> <code>str</code> <p>Path to the folder where benchmarks are saved.</p> required <code>annotations_root</code> <code>str</code> <p>Path to the folder where annotations are saved.</p> required <code>source_type</code> <code>SourceType</code> <p>The source type of the dataset.</p> required <code>aggregation</code> <code>AgreggationType</code> <p>The aggregation type for the selected source type.</p> required <code>ts_id_name</code> <code>str</code> <p>Name of the id used for time series.</p> required <code>default_values</code> <code>dict</code> <p>Default values for each available feature.</p> required <code>additional_data</code> <code>dict[str, tuple]</code> <p>Available small datasets. Can get them by calling <code>get_additional_data</code> with their name.</p> required <p>Attributes:</p> Name Type Description <code>time_indices</code> <p>Available time IDs for the dataset.</p> <code>ts_indices</code> <p>Available time series IDs for the dataset.</p> <code>annotations</code> <p>Annotations for the selected dataset.</p> <code>logger</code> <p>Logger for displaying information.  </p> <code>imported_annotations_ts_identifier</code> <p>Identifier for the imported annotations of type <code>AnnotationType.TS_ID</code>.</p> <code>imported_annotations_time_identifier</code> <p>Identifier for the imported annotations of type <code>AnnotationType.ID_TIME</code>.</p> <code>imported_annotations_both_identifier</code> <p>Identifier for the imported annotations of type <code>AnnotationType.BOTH</code>.  </p> <p>The following attributes are initialized when <code>set_dataset_config_and_initialize</code> is called.</p> <p>Attributes:</p> Name Type Description <code>dataset_config</code> <code>Optional[SeriesBasedConfig]</code> <p>Configuration of the dataset.</p> <code>train_dataset</code> <code>Optional[SeriesBasedDataset]</code> <p>Training set as a <code>SeriesBasedDataset</code> instance wrapping the PyTables database.</p> <code>val_dataset</code> <code>Optional[SeriesBasedDataset]</code> <p>Validation set as a <code>SeriesBasedDataset</code> instance wrapping the PyTables database.</p> <code>test_dataset</code> <code>Optional[SeriesBasedDataset]</code> <p>Test set as a <code>SeriesBasedDataset</code> instance wrapping the PyTables database.</p> <code>all_dataset</code> <code>Optional[SeriesBasedDataset]</code> <p>All set as a <code>SeriesBasedDataset</code> instance wrapping the PyTables database.        </p> <code>train_dataloader</code> <code>Optional[DataLoader]</code> <p>Iterable PyTorch <code>DataLoader</code> for training set.</p> <code>val_dataloader</code> <code>Optional[DataLoader]</code> <p>Iterable PyTorch <code>DataLoader</code> for validation set.</p> <code>test_dataloader</code> <code>Optional[DataLoader]</code> <p>Iterable PyTorch <code>DataLoader</code> for test set.</p> <code>all_dataloader</code> <code>Optional[DataLoader]</code> <p>Iterable PyTorch <code>DataLoader</code> for all set.</p> Source code in <code>cesnet_tszoo\\datasets\\series_based_cesnet_dataset.py</code> <pre><code>@dataclass\nclass SeriesBasedCesnetDataset(CesnetDataset):\n    \"\"\"\n    This class is used for series-based returning of data. Can be created by using [`get_dataset`][cesnet_tszoo.datasets.cesnet_database.CesnetDatabase.get_dataset] with parameter `is_series_based` = `True`.\n\n    Series-based means batch size affects number of returned time series in one batch. Which times for each time series are returned does not change.\n\n    The dataset provides multiple ways to access the data:\n\n    - **Iterable PyTorch DataLoader**: For batch processing.\n    - **Pandas DataFrame**: For loading the entire training, validation, test or all set at once.\n    - **Numpy array**: For loading the entire training, validation, test or all set at once.      \n    - See [loading data][loading-data] for more details.\n\n    The dataset is stored in a [PyTables](https://www.pytables.org/) database. The internal `SeriesBasedDataset` and `SeriesBasedInitializerDataset` classes (used only when calling [`set_dataset_config_and_initialize`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.set_dataset_config_and_initialize]) act as wrappers that implement the PyTorch [`Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) \n    interface. These wrappers are compatible with PyTorch\u2019s [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader), providing efficient parallel data loading. \n\n    The dataset configuration is done through the [`SeriesBasedConfig`][cesnet_tszoo.configs.series_based_config.SeriesBasedConfig] class.     \n\n    **Intended usage:**\n\n    1. Create an instance of the dataset with the desired data root by calling [`get_dataset`][cesnet_tszoo.datasets.cesnet_database.CesnetDatabase.get_dataset]. This will download the dataset if it has not been previously downloaded and return instance of dataset.\n    2. Create an instance of [`SeriesBasedConfig`][cesnet_tszoo.configs.series_based_config.SeriesBasedConfig] and set it using [`set_dataset_config_and_initialize`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.set_dataset_config_and_initialize]. \n       This initializes the dataset, including data splitting (train/validation/test), fitting scalers (if needed), selecting features, and more. This is cached for later use.\n    3. Use [`get_train_dataloader`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_train_dataloader]/[`get_train_df`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_train_df]/[`get_train_numpy`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_train_numpy] to get training data for chosen model.\n    4. Validate the model and perform the hyperparameter optimalization on [`get_val_dataloader`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_val_dataloader]/[`get_val_df`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_val_df]/[`get_val_numpy`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_val_numpy].\n    5. Evaluate the model on [`get_test_dataloader`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_test_dataloader]/[`get_test_df`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_test_df]/[`get_test_numpy`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_test_numpy].     \n\n    Alternatively you can use [`load_benchmark`][cesnet_tszoo.benchmarks.load_benchmark]\n\n    1. Call [`load_benchmark`][cesnet_tszoo.benchmarks.load_benchmark] with the desired benchmark. You can use your own saved benchmark or you can use already built-in one. This will download the dataset and annotations (if available) if they have not been previously downloaded.\n    2. Retrieve the initialized dataset using [`get_initialized_dataset`][cesnet_tszoo.benchmarks.Benchmark.get_initialized_dataset]. This will provide a dataset that is ready to use.\n    3. Use [`get_train_dataloader`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_train_dataloader]/[`get_train_df`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_train_df]/[`get_train_numpy`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_train_numpy] to get training data for chosen model.\n    4. Validate the model and perform the hyperparameter optimalization on [`get_val_dataloader`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_val_dataloader]/[`get_val_df`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_val_df]/[`get_val_numpy`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_val_numpy].\n    5. Evaluate the model on [`get_test_dataloader`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_test_dataloader]/[`get_test_df`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_test_df]/[`get_test_numpy`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_test_numpy].       \n\n    Parameters:\n        database_name: Name of the database.\n        dataset_path: Path to the dataset file.     \n        configs_root: Path to the folder where configurations are saved.\n        benchmarks_root: Path to the folder where benchmarks are saved.\n        annotations_root: Path to the folder where annotations are saved.\n        source_type: The source type of the dataset.\n        aggregation: The aggregation type for the selected source type.\n        ts_id_name: Name of the id used for time series.\n        default_values: Default values for each available feature.\n        additional_data: Available small datasets. Can get them by calling [`get_additional_data`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_additional_data] with their name.\n\n    Attributes:\n        time_indices: Available time IDs for the dataset.\n        ts_indices: Available time series IDs for the dataset.\n        annotations: Annotations for the selected dataset.\n        logger: Logger for displaying information.  \n        imported_annotations_ts_identifier: Identifier for the imported annotations of type `AnnotationType.TS_ID`.\n        imported_annotations_time_identifier: Identifier for the imported annotations of type `AnnotationType.ID_TIME`.\n        imported_annotations_both_identifier: Identifier for the imported annotations of type `AnnotationType.BOTH`.  \n\n    The following attributes are initialized when [`set_dataset_config_and_initialize`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.set_dataset_config_and_initialize] is called.\n\n    Attributes:\n        dataset_config: Configuration of the dataset.\n        train_dataset: Training set as a `SeriesBasedDataset` instance wrapping the PyTables database.\n        val_dataset: Validation set as a `SeriesBasedDataset` instance wrapping the PyTables database.\n        test_dataset: Test set as a `SeriesBasedDataset` instance wrapping the PyTables database.\n        all_dataset: All set as a `SeriesBasedDataset` instance wrapping the PyTables database.        \n        train_dataloader: Iterable PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for training set.\n        val_dataloader: Iterable PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for validation set.\n        test_dataloader: Iterable PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for test set.\n        all_dataloader: Iterable PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for all set.          \n    \"\"\"\n\n    dataset_config: Optional[SeriesBasedConfig] = field(default=None, init=False)\n\n    train_dataset: Optional[SeriesBasedDataset] = field(default=None, init=False)\n    val_dataset: Optional[SeriesBasedDataset] = field(default=None, init=False)\n    test_dataset: Optional[SeriesBasedDataset] = field(default=None, init=False)\n    all_dataset: Optional[SeriesBasedDataset] = field(default=None, init=False)\n\n    is_series_based: bool = field(default=True, init=False)\n\n    _export_config_copy: Optional[SeriesBasedConfig] = field(default=None, init=False)\n\n    def set_dataset_config_and_initialize(self, dataset_config: SeriesBasedConfig, display_config_details: bool = True, workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n        \"\"\"\n        Initialize training set, validation est, test set etc.. This method must be called before any data can be accessed. It is required for the final initialization of [`dataset_config`][cesnet_tszoo.configs.series_based_config.SeriesBasedConfig].\n\n        The following configuration attributes are used during initialization:\n\n        | Dataset config                    | Description                                                                                    |\n        | --------------------------------- | ---------------------------------------------------------------------------------------------- |\n        | `init_workers`                    | Specifies the number of workers to use for initialization. Applied when `workers` = \"config\".  |\n        | `partial_fit_initialized_scalers` | Determines whether initialized scalers should be partially fitted on the training data.        |\n        | `nan_threshold`                   | Filters out time series with missing values exceeding the specified threshold.                 |\n\n        Parameters:\n            dataset_config: Desired configuration of the dataset.\n            display_config_details: Flag indicating whether to display the configuration values after initialization. `Default: True`  \n            workers: The number of workers to use during initialization. `Default: \"config\"`  \n        \"\"\"\n\n        assert isinstance(dataset_config, SeriesBasedConfig), \"SeriesBasedCesnetDataset can only use SeriesBasedConfig.\"\n\n        super(SeriesBasedCesnetDataset, self).set_dataset_config_and_initialize(dataset_config, display_config_details, workers)\n\n    def update_dataset_config_and_initialize(self,\n                                             default_values: list[Number] | npt.NDArray[np.number] | dict[str, Number] | Number | Literal[\"default\"] | None | Literal[\"config\"] = \"config\",\n                                             train_batch_size: int | Literal[\"config\"] = \"config\",\n                                             val_batch_size: int | Literal[\"config\"] = \"config\",\n                                             test_batch_size: int | Literal[\"config\"] = \"config\",\n                                             all_batch_size: int | Literal[\"config\"] = \"config\",\n                                             fill_missing_with: type | FillerType | Literal[\"mean_filler\", \"forward_filler\", \"linear_interpolation_filler\"] | None | Literal[\"config\"] = \"config\",\n                                             scale_with: type | list[Scaler] | np.ndarray[Scaler] | ScalerType | Scaler | Literal[\"min_max_scaler\", \"standard_scaler\", \"max_abs_scaler\", \"log_scaler\", \"robust_scaler\", \"power_transformer\", \"quantile_transformer\", \"l2_normalizer\"] | None | Literal[\"config\"] = \"config\",\n                                             partial_fit_initialized_scalers: bool | Literal[\"config\"] = \"config\",\n                                             train_workers: int | Literal[\"config\"] = \"config\",\n                                             val_workers: int | Literal[\"config\"] = \"config\",\n                                             test_workers: int | Literal[\"config\"] = \"config\",\n                                             all_workers: int | Literal[\"config\"] = \"config\",\n                                             init_workers: int | Literal[\"config\"] = \"config\",\n                                             workers: int | Literal[\"config\"] = \"config\",\n                                             display_config_details: bool = False):\n        \"\"\"Used for updating selected configurations set in config.\n\n        Set parameter to `config` to keep it as it is config.\n\n        If exception is thrown during set, no changes are made.\n\n        Can affect following configuration. \n\n        | Dataset config                     | Description                                                                                                                                     |\n        | ---------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------- |\n        | `default_values`                   | Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature.                        |           \n        | `train_batch_size`                 | Number of samples per batch for train set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. |\n        | `val_batch_size`                   | Number of samples per batch for val set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.   |\n        | `test_batch_size`                  | Number of samples per batch for test set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.  |\n        | `all_batch_size`                   | Number of samples per batch for all set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.   |                   \n        | `fill_missing_with`                | Defines how to fill missing values in the dataset.                                                                                              |     \n        | `scale_with`                       | Defines the scaler to transform the dataset.                                                                                                    |      \n        | `partial_fit_initialized_scalers`  | If `True`, partial fitting on train set is performed when using initiliazed scalers.                                                            |   \n        | `train_workers`                    | Number of workers for loading training data.                                                                                                    |\n        | `val_workers`                      | Number of workers for loading validation data.                                                                                                  |\n        | `test_workers`                     | Number of workers for loading test data.                                                                                                        |\n        | `all_workers`                      | Number of workers for loading all data.                                                                                                         |     \n        | `init_workers`                     | Number of workers for dataset configuration.                                                                                                    |                        \n\n        Parameters:\n            default_values: Default values for missing data, applied before fillers. `Defaults: config`.            \n            train_batch_size: Number of samples per batch for train set. `Defaults: config`.\n            val_batch_size: Number of samples per batch for val set. `Defaults: config`.\n            test_batch_size: Number of samples per batch for test set. `Defaults: config`.\n            all_batch_size: Number of samples per batch for all set. `Defaults: config`.                    \n            fill_missing_with: Defines how to fill missing values in the dataset. `Defaults: config`. \n            scale_with: Defines the scaler to transform the dataset. `Defaults: config`.  \n            partial_fit_initialized_scalers: If `True`, partial fitting on train set is performed when using initiliazed scalers. `Defaults: config`.    \n            train_workers: Number of workers for loading training data. `Defaults: config`.\n            val_workers: Number of workers for loading validation data. `Defaults: config`.\n            test_workers: Number of workers for loading test data. `Defaults: config`.\n            all_workers: Number of workers for loading all data.  `Defaults: config`.\n            init_workers: Number of workers for dataset configuration. `Defaults: config`.                          \n            workers: How many workers to use when updating configuration. `Defaults: config`.  \n            display_config_details: Whether config details should be displayed after configuration. `Defaults: False`. \n        \"\"\"\n\n        return super(SeriesBasedCesnetDataset, self).update_dataset_config_and_initialize(default_values, \"config\", \"config\", \"config\", \"config\", train_batch_size, val_batch_size, test_batch_size, all_batch_size, fill_missing_with, scale_with, \"config\", partial_fit_initialized_scalers, train_workers, val_workers, test_workers, all_workers, init_workers, workers, display_config_details)\n\n    def get_data_about_set(self, about: SplitType | Literal[\"train\", \"val\", \"test\", \"all\"]) -&gt; dict:\n        \"\"\"\n        Retrieves data related to the specified set.\n\n        Parameters:\n            about: Specifies the set to retrieve data about.\n\n        Returned dictionary contains:\n\n        - **ts_ids:** Ids of time series in `about` set.\n        - **TimeFormat.ID_TIME:** Times in `about` set, where time format is `TimeFormat.ID_TIME`.\n        - **TimeFormat.DATETIME:** Times in `about` set, where time format is `TimeFormat.DATETIME`.\n        - **TimeFormat.UNIX_TIME:** Times in `about` set, where time format is `TimeFormat.UNIX_TIME`.\n        - **TimeFormat.SHIFTED_UNIX_TIME:** Times in `about` set, where time format is `TimeFormat.SHIFTED_UNIX_TIME`.        \n\n        Returns:\n            Returns dictionary with details about set.\n        \"\"\"\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before getting data about set.\")\n\n        about = SplitType(about)\n\n        time_period = self.dataset_config.time_period\n\n        result = {}\n\n        if about == SplitType.TRAIN:\n            if not self.dataset_config.has_train:\n                raise ValueError(\"Train set is not used.\")\n            ts_ids = self.dataset_config.train_ts\n        elif about == SplitType.VAL:\n            if not self.dataset_config.has_val:\n                raise ValueError(\"Val set is not used.\")\n            ts_ids = self.dataset_config.val_ts\n        elif about == SplitType.TEST:\n            if not self.dataset_config.has_test:\n                raise ValueError(\"Test set is not used.\")\n            ts_ids = self.dataset_config.test_ts\n        elif about == SplitType.ALL:\n            if not self.dataset_config.has_all:\n                raise ValueError(\"All set is not used.\")\n            ts_ids = self.dataset_config.all_ts\n        else:\n            raise NotImplementedError(\"Should not happen\")\n\n        datetime_temp = np.array([datetime.fromtimestamp(time, tz=timezone.utc) for time in self.time_indices[TIME_COLUMN_NAME][time_period[ID_TIME_COLUMN_NAME]]])\n\n        result[\"ts_ids\"] = ts_ids.copy()\n        result[TimeFormat.ID_TIME] = time_period[ID_TIME_COLUMN_NAME].copy()\n        result[TimeFormat.DATETIME] = datetime_temp.copy()\n        result[TimeFormat.UNIX_TIME] = self.time_indices[TIME_COLUMN_NAME][time_period[ID_TIME_COLUMN_NAME]].copy()\n        result[TimeFormat.SHIFTED_UNIX_TIME] = self.time_indices[TIME_COLUMN_NAME][time_period[ID_TIME_COLUMN_NAME]] - self.time_indices[TIME_COLUMN_NAME][0]\n\n        return result\n\n    def _initialize_datasets(self) -&gt; None:\n        \"\"\"Called in [`set_dataset_config_and_initialize`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.set_dataset_config_and_initialize], this method initializes the set datasets (train, validation, test and all). \"\"\"\n\n        if self.dataset_config.has_train:\n            self.train_dataset = SeriesBasedDataset(self.dataset_path,\n                                                    self.dataset_config._get_table_data_path(),\n                                                    self.dataset_config.ts_id_name,\n                                                    self.dataset_config.train_ts_row_ranges,\n                                                    self.dataset_config.time_period,\n                                                    self.dataset_config.features_to_take,\n                                                    self.dataset_config.indices_of_features_to_take_no_ids,\n                                                    self.dataset_config.default_values,\n                                                    self.dataset_config.train_fillers,\n                                                    self.dataset_config.include_time,\n                                                    self.dataset_config.include_ts_id,\n                                                    self.dataset_config.time_format,\n                                                    self.dataset_config.scalers)\n            self.logger.debug(\"train_dataset initiliazed.\")\n\n        if self.dataset_config.has_val:\n            self.val_dataset = SeriesBasedDataset(self.dataset_path,\n                                                  self.dataset_config._get_table_data_path(),\n                                                  self.dataset_config.ts_id_name,\n                                                  self.dataset_config.val_ts_row_ranges,\n                                                  self.dataset_config.time_period,\n                                                  self.dataset_config.features_to_take,\n                                                  self.dataset_config.indices_of_features_to_take_no_ids,\n                                                  self.dataset_config.default_values,\n                                                  self.dataset_config.val_fillers,\n                                                  self.dataset_config.include_time,\n                                                  self.dataset_config.include_ts_id,\n                                                  self.dataset_config.time_format,\n                                                  self.dataset_config.scalers)\n            self.logger.debug(\"val_dataset initiliazed.\")\n\n        if self.dataset_config.has_test:\n            self.test_dataset = SeriesBasedDataset(self.dataset_path,\n                                                   self.dataset_config._get_table_data_path(),\n                                                   self.dataset_config.ts_id_name,\n                                                   self.dataset_config.test_ts_row_ranges,\n                                                   self.dataset_config.time_period,\n                                                   self.dataset_config.features_to_take,\n                                                   self.dataset_config.indices_of_features_to_take_no_ids,\n                                                   self.dataset_config.default_values,\n                                                   self.dataset_config.test_fillers,\n                                                   self.dataset_config.include_time,\n                                                   self.dataset_config.include_ts_id,\n                                                   self.dataset_config.time_format,\n                                                   self.dataset_config.scalers)\n            self.logger.debug(\"test_dataset initiliazed.\")\n\n        if self.dataset_config.has_all:\n            self.all_dataset = SeriesBasedDataset(self.dataset_path,\n                                                  self.dataset_config._get_table_data_path(),\n                                                  self.dataset_config.ts_id_name,\n                                                  self.dataset_config.all_ts_row_ranges,\n                                                  self.dataset_config.time_period,\n                                                  self.dataset_config.features_to_take,\n                                                  self.dataset_config.indices_of_features_to_take_no_ids,\n                                                  self.dataset_config.default_values,\n                                                  self.dataset_config.all_fillers,\n                                                  self.dataset_config.include_time,\n                                                  self.dataset_config.include_ts_id,\n                                                  self.dataset_config.time_format,\n                                                  self.dataset_config.scalers)\n            self.logger.debug(\"all_dataset initiliazed.\")\n\n    def _initialize_scalers_and_details(self, workers: int) -&gt; None:\n        \"\"\"\n        Called in [`set_dataset_config_and_initialize`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.set_dataset_config_and_initialize]. \n\n        Goes through data to validate time series against `nan_threshold`, partial fit `scalers` and prepare `fillers`.\n        \"\"\"\n\n        init_dataset = SeriesBasedInitializerDataset(self.dataset_path,\n                                                     self.dataset_config._get_table_data_path(),\n                                                     self.dataset_config.ts_id_name,\n                                                     self.dataset_config.train_ts_row_ranges,\n                                                     self.dataset_config.val_ts_row_ranges,\n                                                     self.dataset_config.test_ts_row_ranges,\n                                                     self.dataset_config.all_ts_row_ranges,\n                                                     self.dataset_config.time_period,\n                                                     self.dataset_config.features_to_take,\n                                                     self.dataset_config.indices_of_features_to_take_no_ids,\n                                                     self.dataset_config.default_values,\n                                                     self.dataset_config.all_fillers)\n\n        sampler = SequentialSampler(init_dataset)\n        dataloader = DataLoader(init_dataset,\n                                num_workers=workers,\n                                collate_fn=self._collate_fn,\n                                worker_init_fn=SeriesBasedInitializerDataset.worker_init_fn,\n                                persistent_workers=False,\n                                sampler=sampler)\n\n        if workers == 0:\n            init_dataset.pytables_worker_init()\n\n        train_ts_ids_to_take = []\n        val_ts_ids_to_take = []\n        test_ts_ids_to_take = []\n        all_ts_ids_to_take = []\n\n        self.logger.info(\"Updating config on train/val/test/all and selected time period.\")\n        for i, data in enumerate(tqdm(dataloader)):\n            train_data, count_values, is_train, is_val, is_test, offsetted_idx = data[0]\n\n            missing_percentage = count_values[1] / (count_values[0] + count_values[1])\n\n            # Filter time series based on missing data threshold\n            if missing_percentage &lt;= self.dataset_config.nan_threshold:\n                if is_train:\n                    train_ts_ids_to_take.append(offsetted_idx)\n                elif is_val:\n                    val_ts_ids_to_take.append(offsetted_idx)\n                elif is_test:\n                    test_ts_ids_to_take.append(offsetted_idx)\n\n                all_ts_ids_to_take.append(i)\n\n                # Partial fit scaler on train data if applicable\n                if self.dataset_config.scale_with is not None and is_train and (not self.dataset_config.are_scalers_premade or self.dataset_config.partial_fit_initialized_scalers):\n                    self.dataset_config.scalers.partial_fit(train_data)\n\n        if workers == 0:\n            init_dataset.cleanup()\n\n        # Update sets based on filtered time series\n        if self.dataset_config.has_train:\n            if len(train_ts_ids_to_take) == 0:\n                raise ValueError(\"No time series left in training set after applying nan_threshold.\")\n            self.dataset_config.train_ts_row_ranges = self.dataset_config.train_ts_row_ranges[train_ts_ids_to_take]\n            self.dataset_config.train_ts = self.dataset_config.train_ts[train_ts_ids_to_take]\n\n            if self.dataset_config.fill_missing_with is not None:\n                self.dataset_config.train_fillers = self.dataset_config.train_fillers[train_ts_ids_to_take]\n\n            self.logger.debug(\"Train set updated: %s time series left.\", len(train_ts_ids_to_take))\n\n        if self.dataset_config.has_val:\n            if len(val_ts_ids_to_take) == 0:\n                raise ValueError(\"No time series left in validation set after applying nan_threshold.\")\n            self.dataset_config.val_ts_row_ranges = self.dataset_config.val_ts_row_ranges[val_ts_ids_to_take]\n            self.dataset_config.val_ts = self.dataset_config.val_ts[val_ts_ids_to_take]\n\n            if self.dataset_config.fill_missing_with is not None:\n                self.dataset_config.val_fillers = self.dataset_config.val_fillers[val_ts_ids_to_take]\n\n            self.logger.debug(\"Validation set updated: %s time series selected.\", len(val_ts_ids_to_take))\n\n        if self.dataset_config.has_test:\n            if len(test_ts_ids_to_take) == 0:\n                raise ValueError(\"No time series left in test set after applying nan_threshold.\")\n            self.dataset_config.test_ts_row_ranges = self.dataset_config.test_ts_row_ranges[test_ts_ids_to_take]\n            self.dataset_config.test_ts = self.dataset_config.test_ts[test_ts_ids_to_take]\n\n            if self.dataset_config.fill_missing_with is not None:\n                self.dataset_config.test_fillers = self.dataset_config.test_fillers[test_ts_ids_to_take]\n\n            self.logger.debug(\"Test set updated: %s time series selected.\", len(test_ts_ids_to_take))\n\n        if self.dataset_config.has_all:\n            if len(all_ts_ids_to_take) == 0:\n                raise ValueError(\"No series left in all set after applying nan_threshold.\")\n            self.dataset_config.all_ts = self.dataset_config.all_ts[all_ts_ids_to_take]\n            self.dataset_config.all_ts_row_ranges = self.dataset_config.all_ts_row_ranges[all_ts_ids_to_take]\n\n            if self.dataset_config.fill_missing_with is not None:\n                self.dataset_config.all_fillers = self.dataset_config.all_fillers[all_ts_ids_to_take]\n\n            self.logger.debug(\"All set updated: %s time series selected.\", len(all_ts_ids_to_take))\n\n        self.dataset_config.used_ts_ids = self.dataset_config.all_ts\n        self.dataset_config.used_ts_row_ranges = self.dataset_config.all_ts_row_ranges\n        self.dataset_config.used_fillers = self.dataset_config.all_fillers\n        self.dataset_config.used_times = self.time_indices\n\n        self.logger.info(\"Dataset initialization complete. Configuration updated.\")\n\n    def _update_export_config_copy(self) -&gt; None:\n        \"\"\"\n        Called at the end of [`set_dataset_config_and_initialize`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.set_dataset_config_and_initialize]. \n\n        Updates values of config used for saving config.\n        \"\"\"\n\n        self._export_config_copy.database_name = self.database_name\n\n        if self.dataset_config.has_train:\n            self._export_config_copy.train_ts = self.dataset_config.train_ts.copy()\n            self.logger.debug(\"Updated train_ts of _export_config_copy.\")\n\n        if self.dataset_config.has_val:\n            self._export_config_copy.val_ts = self.dataset_config.val_ts.copy()\n            self.logger.debug(\"Updated val_ts of _export_config_copy.\")\n\n        if self.dataset_config.has_test:\n            self._export_config_copy.test_ts = self.dataset_config.test_ts.copy()\n            self.logger.debug(\"Updated test_ts of _export_config_copy.\")\n\n        super(SeriesBasedCesnetDataset, self)._update_export_config_copy()\n\n    def apply_scaler(self, scale_with: type | list[Scaler] | np.ndarray[Scaler] | ScalerType | Scaler | Literal[\"min_max_scaler\", \"standard_scaler\", \"max_abs_scaler\", \"log_scaler\", \"l2_normalizer\"] | None | Literal[\"config\"] = \"config\",\n                     partial_fit_initialized_scalers: bool | Literal[\"config\"] = \"config\", workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n        \"\"\"Used for updating scaler and relevenat configurations set in config.\n\n        Set parameter to `config` to keep it as it is config.\n\n        If exception is thrown during set, no changes are made.\n\n        Affects following configuration. \n\n        | Dataset config                     | Description                                                                                                    |\n        | ---------------------------------- | -------------------------------------------------------------------------------------------------------------- |\n        | `scale_with`                       | Defines the scaler to transform the dataset.                                                                   |     \n        | `partial_fit_initialized_scalers`  | If `True`, partial fitting on train set is performed when using initiliazed scalers.                           |    \n\n        Parameters:\n            scale_with: Defines the scaler to transform the dataset. `Defaults: config`.  \n            partial_fit_initialized_scalers: If `True`, partial fitting on train set is performed when using initiliazed scalers. `Defaults: config`.  \n            workers: How many workers to use when setting new scaler. `Defaults: config`.      \n        \"\"\"\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating scaler values.\")\n\n        self.update_dataset_config_and_initialize(scale_with=scale_with, partial_fit_initialized_scalers=partial_fit_initialized_scalers, workers=workers)\n\n    def _get_singular_time_series_dataset(self, parent_dataset: SeriesBasedDataset, ts_id: int) -&gt; SeriesBasedDataset:\n        \"\"\"Returns dataset for single time series \"\"\"\n\n        temp = np.where(np.isin(parent_dataset.ts_row_ranges[self.ts_id_name], [ts_id]))[0]\n\n        if len(temp) == 0:\n            raise ValueError(f\"ts_id {ts_id} was not found in valid time series for this set. Available time series are: {parent_dataset.ts_row_ranges[self.ts_id_name]}\")\n\n        time_series_position = temp[0]\n\n        filler = None if parent_dataset.fillers is None else parent_dataset.fillers[time_series_position:time_series_position + 1]\n        scaler = None if parent_dataset.scalers is None else parent_dataset.scalers\n\n        dataset = SeriesBasedDataset(self.dataset_path,\n                                     self.dataset_config._get_table_data_path(),\n                                     self.dataset_config.ts_id_name,\n                                     parent_dataset.ts_row_ranges[time_series_position: time_series_position + 1],\n                                     parent_dataset.time_period,\n                                     self.dataset_config.features_to_take,\n                                     self.dataset_config.indices_of_features_to_take_no_ids,\n                                     self.dataset_config.default_values,\n                                     filler,\n                                     self.dataset_config.include_time,\n                                     self.dataset_config.include_ts_id,\n                                     self.dataset_config.time_format,\n                                     scaler)\n        self.logger.debug(\"Singular time series dataset initiliazed.\")\n\n        return dataset\n\n    def _get_dataloader(self, dataset: SeriesBasedDataset, workers: int | Literal[\"config\"], take_all: bool, batch_size: int, **kwargs) -&gt; DataLoader:\n        \"\"\"Set series based dataloader for this dataset. \"\"\"\n\n        defaultKwargs = {'order': DataloaderOrder.SEQUENTIAL}\n        kwargs = {**defaultKwargs, **kwargs}\n\n        return self._get_series_based_dataloader(dataset, workers, take_all, batch_size, kwargs[\"order\"])\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.set_dataset_config_and_initialize","title":"set_dataset_config_and_initialize","text":"<pre><code>set_dataset_config_and_initialize(dataset_config: SeriesBasedConfig, display_config_details: bool = True, workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Initialize training set, validation est, test set etc.. This method must be called before any data can be accessed. It is required for the final initialization of <code>dataset_config</code>.</p> <p>The following configuration attributes are used during initialization:</p> Dataset config Description <code>init_workers</code> Specifies the number of workers to use for initialization. Applied when <code>workers</code> = \"config\". <code>partial_fit_initialized_scalers</code> Determines whether initialized scalers should be partially fitted on the training data. <code>nan_threshold</code> Filters out time series with missing values exceeding the specified threshold. <p>Parameters:</p> Name Type Description Default <code>dataset_config</code> <code>SeriesBasedConfig</code> <p>Desired configuration of the dataset.</p> required <code>display_config_details</code> <code>bool</code> <p>Flag indicating whether to display the configuration values after initialization. <code>Default: True</code> </p> <code>True</code> <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use during initialization. <code>Default: \"config\"</code></p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\series_based_cesnet_dataset.py</code> <pre><code>def set_dataset_config_and_initialize(self, dataset_config: SeriesBasedConfig, display_config_details: bool = True, workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"\n    Initialize training set, validation est, test set etc.. This method must be called before any data can be accessed. It is required for the final initialization of [`dataset_config`][cesnet_tszoo.configs.series_based_config.SeriesBasedConfig].\n\n    The following configuration attributes are used during initialization:\n\n    | Dataset config                    | Description                                                                                    |\n    | --------------------------------- | ---------------------------------------------------------------------------------------------- |\n    | `init_workers`                    | Specifies the number of workers to use for initialization. Applied when `workers` = \"config\".  |\n    | `partial_fit_initialized_scalers` | Determines whether initialized scalers should be partially fitted on the training data.        |\n    | `nan_threshold`                   | Filters out time series with missing values exceeding the specified threshold.                 |\n\n    Parameters:\n        dataset_config: Desired configuration of the dataset.\n        display_config_details: Flag indicating whether to display the configuration values after initialization. `Default: True`  \n        workers: The number of workers to use during initialization. `Default: \"config\"`  \n    \"\"\"\n\n    assert isinstance(dataset_config, SeriesBasedConfig), \"SeriesBasedCesnetDataset can only use SeriesBasedConfig.\"\n\n    super(SeriesBasedCesnetDataset, self).set_dataset_config_and_initialize(dataset_config, display_config_details, workers)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_train_dataloader","title":"get_train_dataloader","text":"<pre><code>get_train_dataloader(ts_id: int | None = None, workers: int | Literal['config'] = 'config', **kwargs) -&gt; DataLoader\n</code></pre> <p>Returns a PyTorch <code>DataLoader</code> for training set.</p> <p>The <code>DataLoader</code> is created on the first call and cached for subsequent use.  The cached dataloader is cleared when either <code>get_train_df</code> or <code>get_train_numpy</code> is called.</p> <p>The structure of the returned batch depends on the <code>time_format</code> and whether <code>sliding_window_size</code> is used:</p> <ul> <li>When <code>sliding_window_size</code> is used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> <li><code>np.ndarray</code> of times with shape <code>(times - 1)</code></li> <li><code>np.ndarray</code> of time with shape <code>(1)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> </ul> </li> </ul> </li> <li>When <code>sliding_window_size</code> is not used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> <li><code>np.ndarray</code> of time with shape <code>(times)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> </ul> </li> </ul> </li> </ul> <p>The <code>DataLoader</code> is configured with the following config attributes:</p> Dataset config Description <code>train_batch_size</code> Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>sliding_window_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_prediction_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_step</code> Available only for time-based datasets. Number of times to move by after each window. <code>train_workers</code> Specifies the number of workers to use for loading train data. Applied when <code>workers</code> = \"config\". <code>train_dataloader_order</code> Available only for series-based datasets. Whether to load train data in sequential or random order. See cesnet_tszoo.utils.enums.DataloaderOrder. <code>random_state</code> Seed for loading train data in random order. <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading train data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>ts_id</code> <code>int | None</code> <p>Specifies time series to take. If None returns all time series as normal. <code>Default: \"None\"</code></p> <code>None</code> <p>Returns:</p> Type Description <code>DataLoader</code> <p>An iterable <code>DataLoader</code> containing data from training set.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_train_dataloader(self, ts_id: int | None = None, workers: int | Literal[\"config\"] = \"config\", **kwargs) -&gt; DataLoader:\n    \"\"\"\n    Returns a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for training set.\n\n    The `DataLoader` is created on the first call and cached for subsequent use. &lt;br/&gt;\n    The cached dataloader is cleared when either [`get_train_df`][cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_train_df] or [`get_train_numpy`][cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_train_numpy] is called.\n\n    The structure of the returned batch depends on the `time_format` and whether `sliding_window_size` is used:\n\n    - When `sliding_window_size` is used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n            - `np.ndarray` of times with shape `(times - 1)`\n            - `np.ndarray` of time with shape `(1)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n    - When `sliding_window_size` is not used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n            - `np.ndarray` of time with shape `(times)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n\n    The `DataLoader` is configured with the following config attributes:\n\n    | Dataset config                    | Description                                                                                                                                            |\n    | --------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ |\n    | `train_batch_size`                | Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.                      |\n    | `sliding_window_size`             | Available only for time-based datasets. Modifies the shape of the returned data.                                                                       |\n    | `sliding_window_prediction_size`  | Available only for time-based datasets. Modifies the shape of the returned data.                                                                       |\n    | `sliding_window_step`             | Available only for time-based datasets. Number of times to move by after each window.                                                     |\n    | `train_workers`                   | Specifies the number of workers to use for loading train data. Applied when `workers` = \"config\".                                                      |\n    | `train_dataloader_order`          | Available only for series-based datasets. Whether to load train data in sequential or random order. See [cesnet_tszoo.utils.enums.DataloaderOrder][].  |\n    | `random_state`                    | Seed for loading train data in random order.                                                                                                           |                 \n\n    Parameters:\n        workers: The number of workers to use for loading train data. `Default: \"config\"` \n        ts_id: Specifies time series to take. If None returns all time series as normal. `Default: \"None\"`\n\n    Returns:\n        An iterable `DataLoader` containing data from training set.          \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access train_dataloader.\")\n\n    if not self.dataset_config.has_train:\n        raise ValueError(\"Dataloader for training set is not available in the dataset configuration.\")\n\n    assert self.train_dataset is not None, \"The train_dataset must be initialized before accessing data from training set.\"\n\n    defaultKwargs = {'take_all': False, \"cache_loader\": True}\n    kwargs = {**defaultKwargs, **kwargs}\n\n    if ts_id is not None:\n\n        if ts_id == self.dataset_config.used_singular_train_time_series and self.train_dataloader is not None:\n            self.logger.debug(\"Returning cached train_dataloader.\")\n            return self.train_dataloader\n\n        dataset = self._get_singular_time_series_dataset(self.train_dataset, ts_id)\n        self.dataset_config.used_singular_train_time_series = ts_id\n        if self.train_dataloader:\n            del self.train_dataloader\n            self.train_dataloader = None\n            self.logger.info(\"Destroyed previous cached train_dataloader.\")\n\n        self.dataset_config.used_train_workers = 0\n        self.train_dataloader = self._get_dataloader(dataset, 0, False, self.dataset_config.train_batch_size)\n        self.logger.info(\"Created new cached train_dataloader.\")\n        return self.train_dataloader\n    elif self.dataset_config.used_singular_train_time_series is not None and self.train_dataloader is not None:\n        del self.train_dataloader\n        self.train_dataloader = None\n        self.dataset_config.used_singular_train_time_series = None\n        self.logger.info(\"Destroyed previous cached train_dataloader.\")\n\n    if workers == \"config\":\n        workers = self.dataset_config.train_workers\n\n    # If the dataloader is cached and number of used workers did not change, return the cached dataloader\n    if self.train_dataloader and kwargs[\"cache_loader\"] and workers == self.dataset_config.used_train_workers:\n        self.logger.debug(\"Returning cached train_dataloader.\")\n        return self.train_dataloader\n\n    # Update the used workers count\n    self.dataset_config.used_train_workers = workers\n\n    # If there's a previously cached dataloader, destroy it\n    if self.train_dataloader:\n        del self.train_dataloader\n        self.train_dataloader = None\n        self.logger.info(\"Destroyed previous cached train_dataloader.\")\n\n    # If caching is enabled, create a new cached dataloader\n    if kwargs[\"cache_loader\"]:\n        self.train_dataloader = self._get_dataloader(self.train_dataset, workers, kwargs['take_all'], self.dataset_config.train_batch_size, order=self.dataset_config.train_dataloader_order)\n        self.logger.info(\"Created new cached train_dataloader.\")\n        return self.train_dataloader\n\n    # If caching is disabled, create a new uncached dataloader\n    self.logger.debug(\"Created new uncached train_dataloader.\")\n    return self._get_dataloader(self.train_dataset, workers, kwargs['take_all'], self.dataset_config.train_batch_size, order=self.dataset_config.train_dataloader_order)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_val_dataloader","title":"get_val_dataloader","text":"<pre><code>get_val_dataloader(ts_id: int | None = None, workers: int | Literal['config'] = 'config', **kwargs) -&gt; DataLoader\n</code></pre> <p>Returns a PyTorch <code>DataLoader</code> for validation set.</p> <p>The <code>DataLoader</code> is created on the first call and cached for subsequent use.  The cached dataloader is cleared when either <code>get_val_df</code> or <code>get_val_numpy</code> is called.</p> <p>The structure of the returned batch depends on the <code>time_format</code> and whether <code>sliding_window_size</code> is used:</p> <ul> <li>When <code>sliding_window_size</code> is used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> <li><code>np.ndarray</code> of times with shape <code>(times - 1)</code></li> <li><code>np.ndarray</code> of time with shape <code>(1)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> </ul> </li> </ul> </li> <li>When <code>sliding_window_size</code> is not used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> <li><code>np.ndarray</code> of time with shape <code>(times)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> </ul> </li> </ul> </li> </ul> <p>The <code>DataLoader</code> is configured with the following config attributes:</p> Dataset config Description <code>val_batch_size</code> Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>sliding_window_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_prediction_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_step</code> Available only for time-based datasets. Number of times to move by after each window. <code>val_workers</code> Specifies the number of workers to use for loading validation data. Applied when <code>workers</code> = \"config\". <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading validation data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>ts_id</code> <code>int | None</code> <p>Specifies time series to take. If None returns all time series as normal. <code>Default: \"None\"</code></p> <code>None</code> <p>Returns:</p> Type Description <code>DataLoader</code> <p>An iterable <code>DataLoader</code> containing data from validation set.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_val_dataloader(self, ts_id: int | None = None, workers: int | Literal[\"config\"] = \"config\", **kwargs) -&gt; DataLoader:\n    \"\"\"\n    Returns a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for validation set.\n\n    The `DataLoader` is created on the first call and cached for subsequent use. &lt;br/&gt;\n    The cached dataloader is cleared when either [`get_val_df`][cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_val_df] or [`get_val_numpy`][cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_val_numpy] is called.\n\n    The structure of the returned batch depends on the `time_format` and whether `sliding_window_size` is used:\n\n    - When `sliding_window_size` is used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n            - `np.ndarray` of times with shape `(times - 1)`\n            - `np.ndarray` of time with shape `(1)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n    - When `sliding_window_size` is not used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n            - `np.ndarray` of time with shape `(times)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n\n    The `DataLoader` is configured with the following config attributes:\n\n    | Dataset config                    | Description                                                                                                                               |\n    | --------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------- |\n    | `val_batch_size`                  | Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.         |\n    | `sliding_window_size`             | Available only for time-based datasets. Modifies the shape of the returned data.                                                          |\n    | `sliding_window_prediction_size`  | Available only for time-based datasets. Modifies the shape of the returned data.                                                          |\n    | `sliding_window_step`             | Available only for time-based datasets. Number of times to move by after each window.                                                     |\n    | `val_workers`                     | Specifies the number of workers to use for loading validation data. Applied when `workers` = \"config\".                                    |\n\n    Parameters:\n        workers: The number of workers to use for loading validation data. `Default: \"config\"`  \n        ts_id: Specifies time series to take. If None returns all time series as normal. `Default: \"None\"`\n\n    Returns:\n        An iterable `DataLoader` containing data from validation set.        \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access val_dataloader.\")\n\n    if not self.dataset_config.has_val:\n        raise ValueError(\"Dataloader for validation set is not available in the dataset configuration.\")\n\n    assert self.val_dataset is not None, \"The val_dataset must be initialized before accessing data from validation set.\"\n\n    defaultKwargs = {'take_all': False, \"cache_loader\": True}\n    kwargs = {**defaultKwargs, **kwargs}\n\n    if ts_id is not None:\n\n        if ts_id == self.dataset_config.used_singular_val_time_series and self.val_dataloader is not None:\n            self.logger.debug(\"Returning cached val_dataloader.\")\n            return self.val_dataloader\n\n        dataset = self._get_singular_time_series_dataset(self.val_dataset, ts_id)\n        self.dataset_config.used_singular_val_time_series = ts_id\n        if self.val_dataloader:\n            del self.val_dataloader\n            self.val_dataloader = None\n            self.logger.info(\"Destroyed previous cached val_dataloader.\")\n\n        self.dataset_config.used_val_workers = 0\n        self.val_dataloader = self._get_dataloader(dataset, 0, False, self.dataset_config.val_batch_size)\n        self.logger.info(\"Created new cached val_dataloader.\")\n        return self.val_dataloader\n    elif self.dataset_config.used_singular_val_time_series is not None and self.val_dataloader is not None:\n        del self.val_dataloader\n        self.val_dataloader = None\n        self.dataset_config.used_singular_val_time_series = None\n        self.logger.info(\"Destroyed previous cached val_dataloader.\")\n\n    if workers == \"config\":\n        workers = self.dataset_config.val_workers\n\n    # If the dataloader is cached and number of used workers did not change, return the cached dataloader\n    if self.val_dataloader and kwargs[\"cache_loader\"] and workers == self.dataset_config.used_val_workers:\n        self.logger.debug(\"Returning cached val_dataloader.\")\n        return self.val_dataloader\n\n    # Update the used workers count\n    self.dataset_config.used_val_workers = workers\n\n    # If there's a previously cached dataloader, destroy it\n    if self.val_dataloader:\n        del self.val_dataloader\n        self.val_dataloader = None\n        self.logger.info(\"Destroyed previous cached val_dataloader.\")\n\n    # If caching is enabled, create a new cached dataloader\n    if kwargs[\"cache_loader\"]:\n        self.val_dataloader = self._get_dataloader(self.val_dataset, workers, kwargs['take_all'], self.dataset_config.val_batch_size)\n        self.logger.info(\"Created new cached val_dataloader.\")\n        return self.val_dataloader\n\n    # If caching is disabled, create a new uncached dataloader\n    self.logger.debug(\"Created new uncached val_dataloader.\")\n    return self._get_dataloader(self.val_dataset, workers, kwargs['take_all'], self.dataset_config.val_batch_size)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_test_dataloader","title":"get_test_dataloader","text":"<pre><code>get_test_dataloader(ts_id: int | None = None, workers: int | Literal['config'] = 'config', **kwargs) -&gt; DataLoader\n</code></pre> <p>Returns a PyTorch <code>DataLoader</code> for test set.</p> <p>The <code>DataLoader</code> is created on the first call and cached for subsequent use.  The cached dataloader is cleared when either <code>get_test_df</code> or <code>get_test_numpy</code> is called.</p> <p>The structure of the returned batch depends on the <code>time_format</code> and whether <code>sliding_window_size</code> is used:</p> <ul> <li>When <code>sliding_window_size</code> is used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> <li><code>np.ndarray</code> of times with shape <code>(times - 1)</code></li> <li><code>np.ndarray</code> of time with shape <code>(1)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> </ul> </li> </ul> </li> <li>When <code>sliding_window_size</code> is not used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> <li><code>np.ndarray</code> of time with shape <code>(times)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> </ul> </li> </ul> </li> </ul> <p>The <code>DataLoader</code> is configured with the following config attributes:</p> Dataset config Description <code>test_batch_size</code> Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>sliding_window_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_prediction_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_step</code> Available only for time-based datasets. Number of times to move by after each window. <code>test_workers</code> Specifies the number of workers to use for loading test data. Applied when <code>workers</code> = \"config\". <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading test data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>ts_id</code> <code>int | None</code> <p>Specifies time series to take. If None returns all time series as normal. <code>Default: \"None\"</code></p> <code>None</code> <p>Returns:</p> Type Description <code>DataLoader</code> <p>An iterable <code>DataLoader</code> containing data from test set.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_test_dataloader(self, ts_id: int | None = None, workers: int | Literal[\"config\"] = \"config\", **kwargs) -&gt; DataLoader:\n    \"\"\"\n    Returns a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for test set.\n\n    The `DataLoader` is created on the first call and cached for subsequent use. &lt;br/&gt;\n    The cached dataloader is cleared when either [`get_test_df`][cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_test_df] or [`get_test_numpy`][cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_test_numpy] is called.\n\n    The structure of the returned batch depends on the `time_format` and whether `sliding_window_size` is used:\n\n    - When `sliding_window_size` is used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n            - `np.ndarray` of times with shape `(times - 1)`\n            - `np.ndarray` of time with shape `(1)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n    - When `sliding_window_size` is not used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n            - `np.ndarray` of time with shape `(times)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n\n    The `DataLoader` is configured with the following config attributes:\n\n    | Dataset config                     | Description                                                                                                                               |\n    | ---------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------- |\n    | `test_batch_size`                  | Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.         |\n    | `sliding_window_size`              | Available only for time-based datasets. Modifies the shape of the returned data.                                                          |\n    | `sliding_window_prediction_size`   | Available only for time-based datasets. Modifies the shape of the returned data.                                                          |\n    | `sliding_window_step`              | Available only for time-based datasets. Number of times to move by after each window.                                                     |\n    | `test_workers`                     | Specifies the number of workers to use for loading test data. Applied when `workers` = \"config\".                                          |\n\n    Parameters:\n        workers: The number of workers to use for loading test data. `Default: \"config\"`  \n        ts_id: Specifies time series to take. If None returns all time series as normal. `Default: \"None\"`\n\n    Returns:\n        An iterable `DataLoader` containing data from test set.        \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access test_dataloader.\")\n\n    if not self.dataset_config.has_all:\n        raise ValueError(\"Dataloader for test set is not available in the dataset configuration.\")\n\n    assert self.test_dataset is not None, \"The test_dataset must be initialized before accessing data from test set.\"\n\n    defaultKwargs = {'take_all': False, \"cache_loader\": True}\n    kwargs = {**defaultKwargs, **kwargs}\n\n    if ts_id is not None:\n\n        if ts_id == self.dataset_config.used_singular_test_time_series and self.test_dataloader is not None:\n            self.logger.debug(\"Returning cached test_dataloader.\")\n            return self.test_dataloader\n\n        dataset = self._get_singular_time_series_dataset(self.test_dataset, ts_id)\n        self.dataset_config.used_singular_test_time_series = ts_id\n        if self.test_dataloader:\n            del self.test_dataloader\n            self.test_dataloader = None\n            self.logger.info(\"Destroyed previous cached test_dataloader.\")\n\n        self.dataset_config.used_test_workers = 0\n        self.test_dataloader = self._get_dataloader(dataset, 0, False, self.dataset_config.test_batch_size)\n        self.logger.info(\"Created new cached test_dataloader.\")\n        return self.test_dataloader\n    elif self.dataset_config.used_singular_test_time_series is not None and self.test_dataloader is not None:\n        del self.test_dataloader\n        self.test_dataloader = None\n        self.dataset_config.used_singular_test_time_series = None\n        self.logger.info(\"Destroyed previous cached test_dataloader.\")\n\n    if workers == \"config\":\n        workers = self.dataset_config.test_workers\n\n    # If the dataloader is cached and number of used workers did not change, return the cached dataloader\n    if self.test_dataloader and kwargs[\"cache_loader\"] and workers == self.dataset_config.used_test_workers:\n        self.logger.debug(\"Returning cached test_dataloader.\")\n        return self.test_dataloader\n\n    # Update the used workers count\n    self.dataset_config.used_test_workers = workers\n\n    # If there's a previously cached dataloader, destroy it\n    if self.test_dataloader:\n        del self.test_dataloader\n        self.test_dataloader = None\n        self.logger.info(\"Destroyed previous cached test_dataloader.\")\n\n    # If caching is enabled, create a new cached dataloader\n    if kwargs[\"cache_loader\"]:\n        self.test_dataloader = self._get_dataloader(self.test_dataset, workers, kwargs['take_all'], self.dataset_config.test_batch_size)\n        self.logger.info(\"Created new cached test_dataloader.\")\n        return self.test_dataloader\n\n    # If caching is disabled, create a new uncached dataloader\n    self.logger.debug(\"Created new uncached test_dataloader.\")\n    return self._get_dataloader(self.test_dataset, workers, kwargs['take_all'], self.dataset_config.test_batch_size)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_all_dataloader","title":"get_all_dataloader","text":"<pre><code>get_all_dataloader(ts_id: int | None = None, workers: int | Literal['config'] = 'config', **kwargs) -&gt; DataLoader\n</code></pre> <p>Returns a PyTorch <code>DataLoader</code> for all set.</p> <p>The <code>DataLoader</code> is created on the first call and cached for subsequent use.  The cached dataloader is cleared when either <code>get_all_df</code> or <code>get_all_numpy</code> is called.</p> <p>The structure of the returned batch depends on the <code>time_format</code> and whether <code>sliding_window_size</code> is used:</p> <ul> <li>When <code>sliding_window_size</code> is used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> <li><code>np.ndarray</code> of times with shape <code>(times - 1)</code></li> <li><code>np.ndarray</code> of time with shape <code>(1)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> </ul> </li> </ul> </li> <li>When <code>sliding_window_size</code> is not used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> <li><code>np.ndarray</code> of time with shape <code>(times)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> </ul> </li> </ul> </li> </ul> <p>The <code>DataLoader</code> is configured with the following config attributes:</p> Dataset config Description <code>all_batch_size</code> Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>sliding_window_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_prediction_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_step</code> Available only for time-based datasets. Number of times to move by after each window. <code>all_workers</code> Specifies the number of workers to use for loading all data. Applied when <code>workers</code> = \"config\". <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading all data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>ts_id</code> <code>int | None</code> <p>Specifies time series to take. If None returns all time series as normal. <code>Default: \"None\"</code></p> <code>None</code> <p>Returns:</p> Type Description <code>DataLoader</code> <p>An iterable <code>DataLoader</code> containing data from all set.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_all_dataloader(self, ts_id: int | None = None, workers: int | Literal[\"config\"] = \"config\", **kwargs) -&gt; DataLoader:\n    \"\"\"\n    Returns a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for all set.\n\n    The `DataLoader` is created on the first call and cached for subsequent use. &lt;br/&gt;\n    The cached dataloader is cleared when either [`get_all_df`][cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_all_df] or [`get_all_numpy`][cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_all_numpy] is called.\n\n    The structure of the returned batch depends on the `time_format` and whether `sliding_window_size` is used:\n\n    - When `sliding_window_size` is used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n            - `np.ndarray` of times with shape `(times - 1)`\n            - `np.ndarray` of time with shape `(1)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n    - When `sliding_window_size` is not used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n            - `np.ndarray` of time with shape `(times)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n\n    The `DataLoader` is configured with the following config attributes:\n\n    | Dataset config                    | Description                                                                                                                               |\n    | --------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------- |\n    | `all_batch_size`                  | Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.         |\n    | `sliding_window_size`             | Available only for time-based datasets. Modifies the shape of the returned data.                                                          |\n    | `sliding_window_prediction_size`  | Available only for time-based datasets. Modifies the shape of the returned data.                                                          |\n    | `sliding_window_step`             | Available only for time-based datasets. Number of times to move by after each window.                                                     |\n    | `all_workers`                     | Specifies the number of workers to use for loading all data. Applied when `workers` = \"config\".                                           |\n\n    Parameters:\n        workers: The number of workers to use for loading all data. `Default: \"config\"`  \n        ts_id: Specifies time series to take. If None returns all time series as normal. `Default: \"None\"`\n\n    Returns:\n        An iterable `DataLoader` containing data from all set.       \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access all_dataloader.\")\n\n    if not self.dataset_config.has_all:\n        raise ValueError(\"Dataloader for all set is not available in the dataset configuration.\")\n\n    assert self.all_dataset is not None, \"The all_dataset must be initialized before accessing data from all set.\"\n\n    defaultKwargs = {'take_all': False, \"cache_loader\": True}\n    kwargs = {**defaultKwargs, **kwargs}\n\n    if ts_id is not None:\n\n        if ts_id == self.dataset_config.used_singular_all_time_series and self.all_dataloader is not None:\n            self.logger.debug(\"Returning cached all_dataloader.\")\n            return self.all_dataloader\n\n        dataset = self._get_singular_time_series_dataset(self.all_dataset, ts_id)\n        self.dataset_config.used_singular_all_time_series = ts_id\n        if self.all_dataloader:\n            del self.all_dataloader\n            self.all_dataloader = None\n            self.logger.info(\"Destroyed previous cached all_dataloader.\")\n\n        self.dataset_config.used_all_workers = 0\n        self.all_dataloader = self._get_dataloader(dataset, 0, False, self.dataset_config.all_batch_size)\n        self.logger.info(\"Created new cached all_dataloader.\")\n        return self.all_dataloader\n    elif self.dataset_config.used_singular_all_time_series is not None and self.all_dataloader is not None:\n        del self.all_dataloader\n        self.all_dataloader = None\n        self.dataset_config.used_singular_all_time_series = None\n        self.logger.info(\"Destroyed previous cached all_dataloader.\")\n\n    if workers == \"config\":\n        workers = self.dataset_config.all_workers\n\n    # If the dataloader is cached and number of used workers did not change, return the cached dataloader\n    if self.all_dataloader and kwargs[\"cache_loader\"] and workers == self.dataset_config.used_all_workers:\n        self.logger.debug(\"Returning cached all_dataloader.\")\n        return self.all_dataloader\n\n    # Update the used workers count\n    self.dataset_config.used_all_workers = workers\n\n    # If there's a previously cached dataloader, destroy it\n    if self.all_dataloader:\n        del self.all_dataloader\n        self.all_dataloader = None\n        self.logger.info(\"Destroyed previous cached all_dataloader.\")\n\n    # If caching is enabled, create a new cached dataloader\n    if kwargs[\"cache_loader\"]:\n        self.all_dataloader = self._get_dataloader(self.all_dataset, workers, kwargs['take_all'], self.dataset_config.all_batch_size)\n        self.logger.info(\"Created new cached all_dataloader.\")\n        return self.all_dataloader\n\n    # If caching is disabled, create a new uncached dataloader\n    self.logger.debug(\"Creating new uncached all_dataloader.\")\n    return self._get_dataloader(self.all_dataset, workers, kwargs['take_all'], self.dataset_config.all_batch_size)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_train_df","title":"get_train_df","text":"<pre><code>get_train_df(workers: int | Literal['config'] = 'config', as_single_dataframe: bool = True) -&gt; pd.DataFrame\n</code></pre> <p>Creates a Pandas <code>DataFrame</code> containing all the data from training set grouped by time series.</p> <p>This method uses the <code>train_dataloader</code> with a batch size set to the total number of data in the training set. The cached <code>train_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire training set is loaded into memory, which may lead to high memory usage. If working with large training set, consider using <code>get_train_dataloader</code> instead to handle data in batches.</p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading train data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>as_single_dataframe</code> <code>bool</code> <p>Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. <code>Default: True</code> </p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A single Pandas DataFrame containing all data from training set, or a list of DataFrames (one per time series).</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_train_df(self, workers: int | Literal[\"config\"] = \"config\", as_single_dataframe: bool = True) -&gt; pd.DataFrame:\n    \"\"\"\n    Creates a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) containing all the data from training set grouped by time series.\n\n    This method uses the `train_dataloader` with a batch size set to the total number of data in the training set. The cached `train_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire training set is loaded into memory, which may lead to high memory usage. If working with large training set, consider using `get_train_dataloader` instead to handle data in batches.\n\n    Parameters:\n        workers: The number of workers to use for loading train data. `Default: \"config\"`  \n        as_single_dataframe: Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. `Default: True` \n\n    Returns:\n        A single Pandas DataFrame containing all data from training set, or a list of DataFrames (one per time series).\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access train_dataloader.\")\n\n    if not self.dataset_config.has_train:\n        raise ValueError(\"Dataloader for training set is not available in the dataset configuration.\")\n\n    assert self.train_dataset is not None, \"The train_dataset must be initialized before accessing data from training set.\"\n\n    ts_ids, time_period = self.dataset_config._get_train()\n\n    dataloader = self.get_train_dataloader(workers=workers, take_all=not self.dataset_config.is_series_based, cache_loader=False)\n    return self._get_df(dataloader, as_single_dataframe, ts_ids, time_period)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_val_df","title":"get_val_df","text":"<pre><code>get_val_df(workers: int | Literal['config'] = 'config', as_single_dataframe: bool = True) -&gt; pd.DataFrame\n</code></pre> <p>Create a Pandas <code>DataFrame</code> containing all the data from validation set grouped by time series.</p> <p>This method uses the <code>val_dataloader</code> with a batch size set to the total number of data in the validation set. The cached <code>val_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire validation set is loaded into memory, which may lead to high memory usage. If working with large validation set, consider using <code>get_val_dataloader</code> instead to handle data in batches.</p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading validation data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>as_single_dataframe</code> <code>bool</code> <p>Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. <code>Default: True</code> </p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A single Pandas DataFrame containing all data from validation set, or a list of DataFrames (one per time series).</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_val_df(self, workers: int | Literal[\"config\"] = \"config\", as_single_dataframe: bool = True) -&gt; pd.DataFrame:\n    \"\"\"\n    Create a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) containing all the data from validation set grouped by time series.\n\n    This method uses the `val_dataloader` with a batch size set to the total number of data in the validation set. The cached `val_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire validation set is loaded into memory, which may lead to high memory usage. If working with large validation set, consider using `get_val_dataloader` instead to handle data in batches.\n\n    Parameters:\n        workers: The number of workers to use for loading validation data. `Default: \"config\"`  \n        as_single_dataframe: Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. `Default: True` \n\n    Returns:\n        A single Pandas DataFrame containing all data from validation set, or a list of DataFrames (one per time series).\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access val_dataloader.\")\n\n    if not self.dataset_config.has_val:\n        raise ValueError(\"Dataloader for validation set is not available in the dataset configuration.\")\n\n    assert self.val_dataset is not None, \"The val_dataset must be initialized before accessing data from validation set.\"\n\n    ts_ids, time_period = self.dataset_config._get_val()\n\n    dataloader = self.get_val_dataloader(workers=workers, take_all=not self.dataset_config.is_series_based, cache_loader=False)\n    return self._get_df(dataloader, as_single_dataframe, ts_ids, time_period)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_test_df","title":"get_test_df","text":"<pre><code>get_test_df(workers: int | Literal['config'] = 'config', as_single_dataframe: bool = True) -&gt; pd.DataFrame\n</code></pre> <p>Creates a Pandas <code>DataFrame</code> containing all the data from test set grouped by time series.</p> <p>This method uses the <code>test_dataloader</code> with a batch size set to the total number of data in the test set. The cached <code>test_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire test set is loaded into memory, which may lead to high memory usage. If working with large test set, consider using <code>get_test_dataloader</code> instead to handle data in batches.</p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading test data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>as_single_dataframe</code> <code>bool</code> <p>Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. <code>Default: True</code> </p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A single Pandas DataFrame containing all data from test set, or a list of DataFrames (one per time series).</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_test_df(self, workers: int | Literal[\"config\"] = \"config\", as_single_dataframe: bool = True) -&gt; pd.DataFrame:\n    \"\"\"\n    Creates a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) containing all the data from test set grouped by time series.\n\n    This method uses the `test_dataloader` with a batch size set to the total number of data in the test set. The cached `test_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire test set is loaded into memory, which may lead to high memory usage. If working with large test set, consider using `get_test_dataloader` instead to handle data in batches.\n\n    Parameters:\n        workers: The number of workers to use for loading test data. `Default: \"config\"`  \n        as_single_dataframe: Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. `Default: True` \n\n    Returns:\n        A single Pandas DataFrame containing all data from test set, or a list of DataFrames (one per time series).\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access test_dataloader.\")\n\n    if not self.dataset_config.has_all:\n        raise ValueError(\"Dataloader for test set is not available in the dataset configuration.\")\n\n    assert self.test_dataset is not None, \"The test_dataset must be initialized before accessing data from test set.\"\n\n    ts_ids, time_period = self.dataset_config._get_test()\n\n    dataloader = self.get_test_dataloader(workers=workers, take_all=not self.dataset_config.is_series_based, cache_loader=False)\n    return self._get_df(dataloader, as_single_dataframe, ts_ids, time_period)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_all_df","title":"get_all_df","text":"<pre><code>get_all_df(workers: int | Literal['config'] = 'config', as_single_dataframe: bool = True) -&gt; pd.DataFrame\n</code></pre> <p>Creates a Pandas <code>DataFrame</code> containing all the data from all set grouped by time series.</p> <p>This method uses the <code>all_dataloader</code> with a batch size set to the total number of data in the all set. The cached <code>all_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire all set is loaded into memory, which may lead to high memory usage. If working with large all set, consider using <code>get_all_dataloader</code> instead to handle data in batches.</p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading all data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>as_single_dataframe</code> <code>bool</code> <p>Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. <code>Default: True</code> </p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A single Pandas DataFrame containing all data from all set, or a list of DataFrames (one per time series).</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_all_df(self, workers: int | Literal[\"config\"] = \"config\", as_single_dataframe: bool = True) -&gt; pd.DataFrame:\n    \"\"\"\n    Creates a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) containing all the data from all set grouped by time series.\n\n    This method uses the `all_dataloader` with a batch size set to the total number of data in the all set. The cached `all_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire all set is loaded into memory, which may lead to high memory usage. If working with large all set, consider using `get_all_dataloader` instead to handle data in batches.\n\n    Parameters:\n        workers: The number of workers to use for loading all data. `Default: \"config\"`  \n        as_single_dataframe: Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. `Default: True` \n\n    Returns:\n        A single Pandas DataFrame containing all data from all set, or a list of DataFrames (one per time series).\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access all_dataloader.\")\n\n    if not self.dataset_config.has_all:\n        raise ValueError(\"Dataloader for all set is not available in the dataset configuration.\")\n\n    assert self.all_dataset is not None, \"The all_dataset must be initialized before accessing data from all set.\"\n\n    ts_ids, time_period = self.dataset_config._get_all()\n\n    dataloader = self.get_all_dataloader(workers=workers, take_all=not self.dataset_config.is_series_based, cache_loader=False)\n\n    return self._get_df(dataloader, as_single_dataframe, ts_ids, time_period)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_train_numpy","title":"get_train_numpy","text":"<pre><code>get_train_numpy(workers: int | Literal['config'] = 'config') -&gt; np.ndarray\n</code></pre> <p>Creates a NumPy array containing all the data from training set grouped by time series, with the shape <code>(num_time_series, num_times, num_features)</code>.</p> <p>This method uses the <code>train_dataloader</code> with a batch size set to the total number of data in the training set. The cached <code>train_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire training set is loaded into memory, which may lead to high memory usage. If working with large training set, consider using <code>get_train_dataloader</code> instead to handle data in batches.        </p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading train data. <code>Default: \"config\"</code> </p> <code>'config'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A NumPy array containing all the data in training set with the shape <code>(num_time_series, num_times, num_features)</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_train_numpy(self, workers: int | Literal[\"config\"] = \"config\") -&gt; np.ndarray:\n    \"\"\"\n    Creates a NumPy array containing all the data from training set grouped by time series, with the shape `(num_time_series, num_times, num_features)`.\n\n    This method uses the `train_dataloader` with a batch size set to the total number of data in the training set. The cached `train_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire training set is loaded into memory, which may lead to high memory usage. If working with large training set, consider using `get_train_dataloader` instead to handle data in batches.        \n\n    Parameters:\n        workers: The number of workers to use for loading train data. `Default: \"config\"`  \n\n    Returns:\n        A NumPy array containing all the data in training set with the shape `(num_time_series, num_times, num_features)`.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access train_dataloader.\")\n\n    if not self.dataset_config.has_train:\n        raise ValueError(\"Dataloader for training set is not available in the dataset configuration.\")\n\n    assert self.train_dataset is not None, \"The train_dataset must be initialized before accessing data from training set.\"\n\n    ts_ids, time_period = self.dataset_config._get_train()\n\n    dataloader = self.get_train_dataloader(workers=workers, take_all=not self.dataset_config.is_series_based, cache_loader=False)\n    return self._get_numpy(dataloader, ts_ids, time_period)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_val_numpy","title":"get_val_numpy","text":"<pre><code>get_val_numpy(workers: int | Literal['config'] = 'config') -&gt; np.ndarray\n</code></pre> <p>Creates a NumPy array containing all the data from validation set grouped by time series, with the shape <code>(num_time_series, num_times, num_features)</code>.</p> <p>This method uses the <code>val_dataloader</code> with a batch size set to the total number of data in the validation set. The cached <code>val_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire validation set is loaded into memory, which may lead to high memory usage. If working with large validation set, consider using <code>get_val_dataloader</code> instead to handle data in batches.        </p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading validation data. <code>Default: \"config\"</code> </p> <code>'config'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A NumPy array containing all the data in validation set with the shape <code>(num_time_series, num_times, num_features)</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_val_numpy(self, workers: int | Literal[\"config\"] = \"config\") -&gt; np.ndarray:\n    \"\"\"\n    Creates a NumPy array containing all the data from validation set grouped by time series, with the shape `(num_time_series, num_times, num_features)`.\n\n    This method uses the `val_dataloader` with a batch size set to the total number of data in the validation set. The cached `val_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire validation set is loaded into memory, which may lead to high memory usage. If working with large validation set, consider using `get_val_dataloader` instead to handle data in batches.        \n\n    Parameters:\n        workers: The number of workers to use for loading validation data. `Default: \"config\"`  \n\n    Returns:\n        A NumPy array containing all the data in validation set with the shape `(num_time_series, num_times, num_features)`.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access val_dataloader.\")\n\n    if not self.dataset_config.has_val:\n        raise ValueError(\"Dataloader for validation set is not available in the dataset configuration.\")\n\n    assert self.val_dataset is not None, \"The val_dataset must be initialized before accessing data from validation set.\"\n\n    ts_ids, time_period = self.dataset_config._get_val()\n\n    dataloader = self.get_val_dataloader(workers=workers, take_all=not self.dataset_config.is_series_based, cache_loader=False)\n    return self._get_numpy(dataloader, ts_ids, time_period)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_test_numpy","title":"get_test_numpy","text":"<pre><code>get_test_numpy(workers: int | Literal['config'] = 'config') -&gt; np.ndarray\n</code></pre> <p>Creates a NumPy array containing all the data from test set grouped by time series, with the shape <code>(num_time_series, num_times, num_features)</code>.</p> <p>This method uses the <code>test_dataloader</code> with a batch size set to the total number of data in the test set. The cached <code>test_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire test set is loaded into memory, which may lead to high memory usage. If working with large test set, consider using <code>get_test_dataloader</code> instead to handle data in batches.        </p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading test data. <code>Default: \"config\"</code> </p> <code>'config'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A NumPy array containing all the data in test set with the shape <code>(num_time_series, num_times, num_features)</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_test_numpy(self, workers: int | Literal[\"config\"] = \"config\") -&gt; np.ndarray:\n    \"\"\"\n    Creates a NumPy array containing all the data from test set grouped by time series, with the shape `(num_time_series, num_times, num_features)`.\n\n    This method uses the `test_dataloader` with a batch size set to the total number of data in the test set. The cached `test_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire test set is loaded into memory, which may lead to high memory usage. If working with large test set, consider using `get_test_dataloader` instead to handle data in batches.        \n\n    Parameters:\n        workers: The number of workers to use for loading test data. `Default: \"config\"`  \n\n    Returns:\n        A NumPy array containing all the data in test set with the shape `(num_time_series, num_times, num_features)`.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access test_dataloader.\")\n\n    if not self.dataset_config.has_all:\n        raise ValueError(\"Dataloader for test set is not available in the dataset configuration.\")\n\n    assert self.test_dataset is not None, \"The test_dataset must be initialized before accessing data from test set.\"\n\n    ts_ids, time_period = self.dataset_config._get_test()\n\n    dataloader = self.get_test_dataloader(workers=workers, take_all=not self.dataset_config.is_series_based, cache_loader=False)\n    return self._get_numpy(dataloader, ts_ids, time_period)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_all_numpy","title":"get_all_numpy","text":"<pre><code>get_all_numpy(workers: int | Literal['config'] = 'config') -&gt; np.ndarray\n</code></pre> <p>Creates a NumPy array containing all the data from all set grouped by time series, with the shape <code>(num_time_series, num_times, num_features)</code>.</p> <p>This method uses the <code>all_dataloader</code> with a batch size set to the total number of data in the all set. The cached <code>all_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire all set is loaded into memory, which may lead to high memory usage. If working with large all set, consider using <code>get_all_dataloader</code> instead to handle data in batches.        </p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading all data. <code>Default: \"config\"</code> </p> <code>'config'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A NumPy array containing all the data in all set with the shape <code>(num_time_series, num_times, num_features)</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_all_numpy(self, workers: int | Literal[\"config\"] = \"config\") -&gt; np.ndarray:\n    \"\"\"\n    Creates a NumPy array containing all the data from all set grouped by time series, with the shape `(num_time_series, num_times, num_features)`.\n\n    This method uses the `all_dataloader` with a batch size set to the total number of data in the all set. The cached `all_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire all set is loaded into memory, which may lead to high memory usage. If working with large all set, consider using `get_all_dataloader` instead to handle data in batches.        \n\n    Parameters:\n        workers: The number of workers to use for loading all data. `Default: \"config\"`  \n\n    Returns:\n        A NumPy array containing all the data in all set with the shape `(num_time_series, num_times, num_features)`.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access all_dataloader.\")\n\n    if not self.dataset_config.has_all:\n        raise ValueError(\"Dataloader for all set is not available in the dataset configuration.\")\n\n    assert self.all_dataset is not None, \"The all_dataset must be initialized before accessing data from all set.\"\n\n    ts_ids, time_period = self.dataset_config._get_all()\n\n    dataloader = self.get_all_dataloader(workers=workers, take_all=not self.dataset_config.is_series_based, cache_loader=False)\n\n    return self._get_numpy(dataloader, ts_ids, time_period)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.display_dataset_details","title":"display_dataset_details","text":"<pre><code>display_dataset_details() -&gt; None\n</code></pre> <p>Display information about the contents of the dataset.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>    def display_dataset_details(self) -&gt; None:\n        \"\"\"Display information about the contents of the dataset.  \"\"\"\n\n        to_display = f'''\nDataset details:\n\n    {self.aggregation}\n        Time indices: {range(self.time_indices[ID_TIME_COLUMN_NAME][0], self.time_indices[ID_TIME_COLUMN_NAME][-1])}\n        Datetime: {(datetime.fromtimestamp(self.time_indices['time'][0], tz=timezone.utc), datetime.fromtimestamp(self.time_indices['time'][-1], timezone.utc))}\n\n    {self.source_type}\n        Time series indices: {get_abbreviated_list_string(self.ts_indices[self.ts_id_name])}; use 'get_available_ts_indices' for full list\n        Features with default values: {self.default_values}\n\n        Additional data: {list(self.additional_data.keys())}\n        '''\n\n        print(to_display)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.display_config","title":"display_config","text":"<pre><code>display_config() -&gt; None\n</code></pre> <p>Displays the values of the initialized configuration.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def display_config(self) -&gt; None:\n    \"\"\"Displays the values of the initialized configuration. \"\"\"\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before displaying config.\")\n\n    print(self.dataset_config)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_feature_names","title":"get_feature_names","text":"<pre><code>get_feature_names() -&gt; list[str]\n</code></pre> <p>Returns a list of all available feature names in the dataset.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_feature_names(self) -&gt; list[str]:\n    \"\"\"Returns a list of all available feature names in the dataset. \"\"\"\n\n    return get_column_names(self.dataset_path, self.source_type, self.aggregation)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_data_about_set","title":"get_data_about_set","text":"<pre><code>get_data_about_set(about: SplitType | Literal['train', 'val', 'test', 'all']) -&gt; dict\n</code></pre> <p>Retrieves data related to the specified set.</p> <p>Parameters:</p> Name Type Description Default <code>about</code> <code>SplitType | Literal['train', 'val', 'test', 'all']</code> <p>Specifies the set to retrieve data about.</p> required <p>Returned dictionary contains:</p> <ul> <li>ts_ids: Ids of time series in <code>about</code> set.</li> <li>TimeFormat.ID_TIME: Times in <code>about</code> set, where time format is <code>TimeFormat.ID_TIME</code>.</li> <li>TimeFormat.DATETIME: Times in <code>about</code> set, where time format is <code>TimeFormat.DATETIME</code>.</li> <li>TimeFormat.UNIX_TIME: Times in <code>about</code> set, where time format is <code>TimeFormat.UNIX_TIME</code>.</li> <li>TimeFormat.SHIFTED_UNIX_TIME: Times in <code>about</code> set, where time format is <code>TimeFormat.SHIFTED_UNIX_TIME</code>.        </li> </ul> <p>Returns:</p> Type Description <code>dict</code> <p>Returns dictionary with details about set.</p> Source code in <code>cesnet_tszoo\\datasets\\series_based_cesnet_dataset.py</code> <pre><code>def get_data_about_set(self, about: SplitType | Literal[\"train\", \"val\", \"test\", \"all\"]) -&gt; dict:\n    \"\"\"\n    Retrieves data related to the specified set.\n\n    Parameters:\n        about: Specifies the set to retrieve data about.\n\n    Returned dictionary contains:\n\n    - **ts_ids:** Ids of time series in `about` set.\n    - **TimeFormat.ID_TIME:** Times in `about` set, where time format is `TimeFormat.ID_TIME`.\n    - **TimeFormat.DATETIME:** Times in `about` set, where time format is `TimeFormat.DATETIME`.\n    - **TimeFormat.UNIX_TIME:** Times in `about` set, where time format is `TimeFormat.UNIX_TIME`.\n    - **TimeFormat.SHIFTED_UNIX_TIME:** Times in `about` set, where time format is `TimeFormat.SHIFTED_UNIX_TIME`.        \n\n    Returns:\n        Returns dictionary with details about set.\n    \"\"\"\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before getting data about set.\")\n\n    about = SplitType(about)\n\n    time_period = self.dataset_config.time_period\n\n    result = {}\n\n    if about == SplitType.TRAIN:\n        if not self.dataset_config.has_train:\n            raise ValueError(\"Train set is not used.\")\n        ts_ids = self.dataset_config.train_ts\n    elif about == SplitType.VAL:\n        if not self.dataset_config.has_val:\n            raise ValueError(\"Val set is not used.\")\n        ts_ids = self.dataset_config.val_ts\n    elif about == SplitType.TEST:\n        if not self.dataset_config.has_test:\n            raise ValueError(\"Test set is not used.\")\n        ts_ids = self.dataset_config.test_ts\n    elif about == SplitType.ALL:\n        if not self.dataset_config.has_all:\n            raise ValueError(\"All set is not used.\")\n        ts_ids = self.dataset_config.all_ts\n    else:\n        raise NotImplementedError(\"Should not happen\")\n\n    datetime_temp = np.array([datetime.fromtimestamp(time, tz=timezone.utc) for time in self.time_indices[TIME_COLUMN_NAME][time_period[ID_TIME_COLUMN_NAME]]])\n\n    result[\"ts_ids\"] = ts_ids.copy()\n    result[TimeFormat.ID_TIME] = time_period[ID_TIME_COLUMN_NAME].copy()\n    result[TimeFormat.DATETIME] = datetime_temp.copy()\n    result[TimeFormat.UNIX_TIME] = self.time_indices[TIME_COLUMN_NAME][time_period[ID_TIME_COLUMN_NAME]].copy()\n    result[TimeFormat.SHIFTED_UNIX_TIME] = self.time_indices[TIME_COLUMN_NAME][time_period[ID_TIME_COLUMN_NAME]] - self.time_indices[TIME_COLUMN_NAME][0]\n\n    return result\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_available_ts_indices","title":"get_available_ts_indices","text":"<pre><code>get_available_ts_indices()\n</code></pre> <p>Returns the available time series indices in this dataset.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_available_ts_indices(self):\n    \"\"\"Returns the available time series indices in this dataset. \"\"\"\n    return self.ts_indices\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_additional_data","title":"get_additional_data","text":"<pre><code>get_additional_data(data_name: str) -&gt; pd.DataFrame\n</code></pre> <p>Create a Pandas <code>DataFrame</code> of additional data of <code>data_name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data_name</code> <code>str</code> <p>Name of additional data to return.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Dataframe of additional data of <code>data_name</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_additional_data(self, data_name: str) -&gt; pd.DataFrame:\n    \"\"\"Create a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) of additional data of `data_name`.\n\n    Parameters:\n        data_name: Name of additional data to return.\n\n    Returns:\n        Dataframe of additional data of `data_name`.\n    \"\"\"\n\n    if data_name not in self.additional_data:\n        self.logger.error(\"%s is not available for this dataset.\", data_name)\n        raise ValueError(f\"{data_name} is not available for this dataset.\", f\"Possible options are: {self.additional_data}\")\n\n    data = get_additional_data(self.dataset_path, data_name)\n    data_df = pd.DataFrame(data)\n\n    for column, column_type in self.additional_data[data_name]:\n        if column_type == datetime:\n            data_df[column] = data_df[column].apply(lambda x: datetime.fromtimestamp(x, tz=timezone.utc))\n        else:\n            data_df[column] = data_df[column].astype(column_type)\n\n    return data_df\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.plot","title":"plot","text":"<pre><code>plot(ts_id: int, plot_type: Literal['scatter', 'line'], features: list[str] | str | Literal['config'] = 'config', feature_per_plot: bool = True, time_format: TimeFormat | Literal['config', 'id_time', 'datetime', 'unix_time', 'shifted_unix_time'] = 'config', use_scalers: bool = True, is_interactive: bool = True) -&gt; None\n</code></pre> <p>Displays a graph for the selected <code>ts_id</code> and its <code>features</code>.</p> <p>The plotting is done using the <code>Plotly</code> library, which provides interactive graphs.</p> <p>Parameters:</p> Name Type Description Default <code>ts_id</code> <code>int</code> <p>The ID of the time series to display.</p> required <code>plot_type</code> <code>Literal['scatter', 'line']</code> <p>The type of graph to plot.</p> required <code>features</code> <code>list[str] | str | Literal['config']</code> <p>The features to display in the plot. <code>Defaults: \"config\"</code>.</p> <code>'config'</code> <code>feature_per_plot</code> <code>bool</code> <p>Whether each feature should be displayed in a separate plot or combined into one. <code>Defaults: True</code>.</p> <code>True</code> <code>time_format</code> <code>TimeFormat | Literal['config', 'id_time', 'datetime', 'unix_time', 'shifted_unix_time']</code> <p>The time format to use for the x-axis. <code>Defaults: \"config\"</code>.</p> <code>'config'</code> <code>use_scalers</code> <code>bool</code> <p>Whether the data should be scaled. If <code>True</code>, scaling will be applied using the available scaler for the selected <code>ts_id</code>. <code>Defaults: True</code>.</p> <code>True</code> <code>is_interactive</code> <code>bool</code> <p>Whether the plot should be interactive (e.g., zoom, hover). <code>Defaults: True</code>.</p> <code>True</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def plot(self, ts_id: int, plot_type: Literal[\"scatter\", \"line\"], features: list[str] | str | Literal[\"config\"] = \"config\", feature_per_plot: bool = True,\n         time_format: TimeFormat | Literal[\"config\", \"id_time\", \"datetime\", \"unix_time\", \"shifted_unix_time\"] = \"config\", use_scalers: bool = True, is_interactive: bool = True) -&gt; None:\n    \"\"\"\n    Displays a graph for the selected `ts_id` and its `features`.\n\n    The plotting is done using the [`Plotly`](https://plotly.com/python/) library, which provides interactive graphs.\n\n    Parameters:\n        ts_id: The ID of the time series to display.\n        plot_type: The type of graph to plot.\n        features: The features to display in the plot. `Defaults: \"config\"`.\n        feature_per_plot: Whether each feature should be displayed in a separate plot or combined into one. `Defaults: True`.\n        time_format: The time format to use for the x-axis. `Defaults: \"config\"`.\n        use_scalers: Whether the data should be scaled. If `True`, scaling will be applied using the available scaler for the selected `ts_id`. `Defaults: True`.\n        is_interactive: Whether the plot should be interactive (e.g., zoom, hover). `Defaults: True`.\n    \"\"\"\n\n    if time_format == \"config\":\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to plot.\")\n\n        time_format = self.dataset_config.time_format\n        self.logger.debug(\"Using time format from dataset configuration: %s\", time_format)\n    else:\n        time_format = TimeFormat(time_format)\n        self.logger.debug(\"Using specified time format: %s\", time_format)\n\n    time_series, times, features = self._get_data_for_plot(ts_id, features, time_format, use_scalers)\n    self.logger.debug(\"Received data for plotting. Time series, times, and features are ready.\")\n\n    plots = []\n\n    if feature_per_plot:\n        self.logger.debug(\"Creating individual plots for each feature.\")\n        fig = make_subplots(rows=len(features), cols=1, shared_xaxes=False, x_title=time_format.value)\n\n        for i, feature in enumerate(features):\n            if plot_type == \"scatter\":\n                plot = go.Scatter(x=times, y=time_series[:, i], mode=\"markers\", name=feature, legendgroup=feature)\n                self.logger.debug(\"Creating scatter plot for feature: %s\", feature)\n            elif plot_type == \"line\":\n                plot = go.Scatter(x=times, y=time_series[:, i], mode=\"lines\", name=feature)\n                self.logger.debug(\"Creating line plot for feature: %s\", feature)\n            else:\n                raise ValueError(\"Invalid plot type.\")\n\n            fig.add_traces(plot, rows=i + 1, cols=1)\n\n        fig.update_layout(height=200 + 120 * len(features), width=2000, autosize=len(features) == 1, showlegend=True)\n        self.logger.debug(\"Created subplots for features: %s.\", features)\n    else:\n        self.logger.debug(\"Creating a combined plot for all features.\")\n        for i, feature in enumerate(features):\n            if plot_type == \"scatter\":\n                plot = go.Scatter(x=times, y=time_series[:, i], mode=\"markers\", name=feature)\n                self.logger.debug(\"Creating scatter plot for feature: %s\", feature)\n            elif plot_type == \"line\":\n                plot = go.Scatter(x=times, y=time_series[:, i], mode=\"lines\", name=feature)\n                self.logger.debug(\"Creating line plot for feature: %s\", feature)\n            else:\n                raise ValueError(\"Invalid plot type.\")\n            plots.append(plot)\n\n        fig = go.Figure(data=plots)\n        fig.update_layout(xaxis_title=time_format.value, showlegend=True, height=200 + 120 * 2)\n        self.logger.debug(\"Created combined plot for features: %s.\", features)\n\n    if not is_interactive:\n        self.logger.debug(\"Disabling interactivity for the plot.\")\n        fig.update_layout(updatemenus=[], dragmode=False, hovermode=False)\n\n    self.logger.debug(\"Displaying the plot.\")\n    fig.show()\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.add_annotation","title":"add_annotation","text":"<pre><code>add_annotation(annotation: str, annotation_group: str, ts_id: int | None, id_time: int | None, enforce_ids: bool = True) -&gt; None\n</code></pre> <p>Adds an annotation to the specified <code>annotation_group</code>.</p> <ul> <li>If the provided <code>annotation_group</code> does not exist, it will be created.</li> <li>At least one of <code>ts_id</code> or <code>id_time</code> must be provided to associate the annotation with time series or/and time point.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>annotation</code> <code>str</code> <p>The annotation to be added.</p> required <code>annotation_group</code> <code>str</code> <p>The group to which the annotation should be added.</p> required <code>ts_id</code> <code>int | None</code> <p>The time series ID to which the annotation should be added.</p> required <code>id_time</code> <code>int | None</code> <p>The time ID to which the annotation should be added.</p> required <code>enforce_ids</code> <code>bool</code> <p>Flag indicating whether the <code>ts_id</code> and <code>id_time</code> must belong to this dataset. <code>Default: True</code></p> <code>True</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def add_annotation(self, annotation: str, annotation_group: str, ts_id: int | None, id_time: int | None, enforce_ids: bool = True) -&gt; None:\n    \"\"\" \n    Adds an annotation to the specified `annotation_group`.\n\n    - If the provided `annotation_group` does not exist, it will be created.\n    - At least one of `ts_id` or `id_time` must be provided to associate the annotation with time series or/and time point.\n\n    Parameters:\n        annotation: The annotation to be added.\n        annotation_group: The group to which the annotation should be added.\n        ts_id: The time series ID to which the annotation should be added.\n        id_time: The time ID to which the annotation should be added.\n        enforce_ids: Flag indicating whether the `ts_id` and `id_time` must belong to this dataset. `Default: True`  \n    \"\"\"\n\n    if enforce_ids:\n        self._validate_annotation_ids(ts_id, id_time)\n    self.annotations.add_annotation(annotation, annotation_group, ts_id, id_time)\n\n    if ts_id is not None and id_time is not None:\n        self._update_annotations_imported_status(AnnotationType.BOTH, None)\n    elif ts_id is not None and id_time is None:\n        self._update_annotations_imported_status(AnnotationType.TS_ID, None)\n    elif ts_id is None and id_time is not None:\n        self._update_annotations_imported_status(AnnotationType.ID_TIME, None)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.remove_annotation","title":"remove_annotation","text":"<pre><code>remove_annotation(annotation_group: str, ts_id: int | None, id_time: int | None) -&gt; None\n</code></pre> <p>Removes an annotation from the specified <code>annotation_group</code>.</p> <ul> <li>At least one of <code>ts_id</code> or <code>id_time</code> must be provided to associate the annotation with time series or/and time point.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>annotation_group</code> <code>str</code> <p>The annotation group from which the annotation should be removed.</p> required <code>ts_id</code> <code>int | None</code> <p>The time series ID from which the annotation should be removed.</p> required <code>id_time</code> <code>int | None</code> <p>The time ID from which the annotation should be removed.</p> required Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def remove_annotation(self, annotation_group: str, ts_id: int | None, id_time: int | None) -&gt; None:\n    \"\"\"  \n    Removes an annotation from the specified `annotation_group`.\n\n    - At least one of `ts_id` or `id_time` must be provided to associate the annotation with time series or/and time point.\n\n    Parameters:\n        annotation_group: The annotation group from which the annotation should be removed.\n        ts_id: The time series ID from which the annotation should be removed.\n        id_time: The time ID from which the annotation should be removed. \n    \"\"\"\n\n    self.annotations.remove_annotation(annotation_group, ts_id, id_time, False)\n\n    if ts_id is not None and id_time is not None:\n        self._update_annotations_imported_status(AnnotationType.BOTH, None)\n    elif ts_id is not None and id_time is None:\n        self._update_annotations_imported_status(AnnotationType.TS_ID, None)\n    elif ts_id is None and id_time is not None:\n        self._update_annotations_imported_status(AnnotationType.ID_TIME, None)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.add_annotation_group","title":"add_annotation_group","text":"<pre><code>add_annotation_group(annotation_group: str, on: AnnotationType | Literal['id_time', 'ts_id', 'both'])\n</code></pre> <p>Adds a new <code>annotation_group</code>.</p> <p>Parameters:</p> Name Type Description Default <code>annotation_group</code> <code>str</code> <p>The name of the annotation group to be added.</p> required <code>on</code> <code>AnnotationType | Literal['id_time', 'ts_id', 'both']</code> <p>Specifies which part of the data should be annotated. If set to <code>\"both\"</code>, annotations will be applied as if <code>id_time</code> and <code>ts_id</code> were both set.</p> required Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def add_annotation_group(self, annotation_group: str, on: AnnotationType | Literal[\"id_time\", \"ts_id\", \"both\"]):\n    \"\"\" \n    Adds a new `annotation_group`.\n\n    Parameters:\n        annotation_group: The name of the annotation group to be added.\n        on: Specifies which part of the data should be annotated. If set to `\"both\"`, annotations will be applied as if `id_time` and `ts_id` were both set.\n    \"\"\"\n    on = AnnotationType(on)\n\n    self.annotations.add_annotation_group(annotation_group, on, False)\n\n    self._update_annotations_imported_status(on, None)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.remove_annotation_group","title":"remove_annotation_group","text":"<pre><code>remove_annotation_group(annotation_group: str, on: AnnotationType | Literal['id_time', 'ts_id', 'both'])\n</code></pre> <p>Removes the specified <code>annotation_group</code>.</p> <p>Parameters:</p> Name Type Description Default <code>annotation_group</code> <code>str</code> <p>The name of the annotation group to be removed.</p> required <code>on</code> <code>AnnotationType | Literal['id_time', 'ts_id', 'both']</code> <p>Specifies which part of the data the <code>annotation_group</code> should be removed from. If set to <code>\"both\"</code>, annotations will be applied as if <code>id_time</code> and <code>ts_id</code> were both set.</p> required Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def remove_annotation_group(self, annotation_group: str, on: AnnotationType | Literal[\"id_time\", \"ts_id\", \"both\"]):\n    \"\"\" \n    Removes the specified `annotation_group`.\n\n    Parameters:\n        annotation_group: The name of the annotation group to be removed.\n        on: Specifies which part of the data the `annotation_group` should be removed from. If set to `\"both\"`, annotations will be applied as if `id_time` and `ts_id` were both set.        \n    \"\"\"\n    on = AnnotationType(on)\n\n    self.annotations.remove_annotation_group(annotation_group, on, False)\n\n    self._update_annotations_imported_status(on, None)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_annotations","title":"get_annotations","text":"<pre><code>get_annotations(on: AnnotationType | Literal['id_time', 'ts_id', 'both']) -&gt; pd.DataFrame\n</code></pre> <p>Returns the annotations as a Pandas <code>DataFrame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>on</code> <code>AnnotationType | Literal['id_time', 'ts_id', 'both']</code> <p>Specifies which annotations to return. If set to <code>\"both\"</code>, annotations will be applied as if <code>id_time</code> and <code>ts_id</code> were both set.         </p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A Pandas DataFrame containing the selected annotations.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_annotations(self, on: AnnotationType | Literal[\"id_time\", \"ts_id\", \"both\"]) -&gt; pd.DataFrame:\n    \"\"\" \n    Returns the annotations as a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html).\n\n    Parameters:\n        on: Specifies which annotations to return. If set to `\"both\"`, annotations will be applied as if `id_time` and `ts_id` were both set.         \n\n    Returns:\n        A Pandas DataFrame containing the selected annotations.      \n    \"\"\"\n    on = AnnotationType(on)\n\n    return self.annotations.get_annotations(on, self.ts_id_name)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.import_annotations","title":"import_annotations","text":"<pre><code>import_annotations(identifier: str, enforce_ids: bool = True) -&gt; None\n</code></pre> <p>Imports annotations from a CSV file.</p> <p>First, it attempts to load the built-in annotations, if no built-in annotations with such an identifier exists, it attempts to load a custom annotations from the <code>\"data_root\"/tszoo/annotations/</code> directory.</p> <p><code>data_root</code> is specified when the dataset is created.     </p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The name of the CSV file.     </p> required <code>enforce_ids</code> <code>bool</code> <p>Flag indicating whether the <code>ts_id</code> and <code>id_time</code> must belong to this dataset. <code>Default: True</code></p> <code>True</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def import_annotations(self, identifier: str, enforce_ids: bool = True) -&gt; None:\n    \"\"\" \n    Imports annotations from a CSV file.\n\n    First, it attempts to load the built-in annotations, if no built-in annotations with such an identifier exists, it attempts to load a custom annotations from the `\"data_root\"/tszoo/annotations/` directory.\n\n    `data_root` is specified when the dataset is created.     \n\n    Parameters:\n        identifier: The name of the CSV file.     \n        enforce_ids: Flag indicating whether the `ts_id` and `id_time` must belong to this dataset. `Default: True`                \n    \"\"\"\n\n    annotations_file_path, is_built_in = get_annotations_path_and_whether_it_is_built_in(identifier, self.annotations_root, self.logger)\n\n    if is_built_in:\n        self.logger.info(\"Built-in annotations found: %s.\", identifier)\n        if not os.path.exists(annotations_file_path):\n            self.logger.info(\"Downloading annotations with identifier: %s\", identifier)\n            annotations_url = f\"{ANNOTATIONS_DOWNLOAD_BUCKET}&amp;file={identifier}\"  # probably will change annotations bucket... placeholder\n            resumable_download(url=annotations_url, file_path=annotations_file_path, silent=False)\n\n        self.logger.debug(\"Loading annotations from %s\", annotations_file_path)\n        temp_df = pd.read_csv(annotations_file_path)\n        self.logger.debug(\"Created DataFrame from file: %s\", annotations_file_path)\n    else:\n        self.logger.info(\"Custom annotations found: %s.\", identifier)\n        self.logger.debug(\"Loading annotations from %s\", annotations_file_path)\n        temp_df = pd.read_csv(annotations_file_path)\n        self.logger.debug(\"Created DataFrame from file: %s\", annotations_file_path)\n\n    ts_id_index = None\n    time_id_index = None\n    on = None\n\n    # Check the columns of the DataFrame to identify the type of annotation\n    if self.ts_id_name in temp_df.columns and ID_TIME_COLUMN_NAME in temp_df.columns:\n        self.annotations.clear_time_in_time_series()\n        time_id_index = temp_df.columns.tolist().index(ID_TIME_COLUMN_NAME)\n        ts_id_index = temp_df.columns.tolist().index(self.ts_id_name)\n        on = AnnotationType.BOTH\n        self.logger.info(\"Annotations detected as %s (both %s and id_time)\", AnnotationType.BOTH, self.ts_id_name)\n\n    elif self.ts_id_name in temp_df.columns:\n        self.annotations.clear_time_series()\n        ts_id_index = temp_df.columns.tolist().index(self.ts_id_name)\n        on = AnnotationType.TS_ID\n        self.logger.info(\"Annotations detected as %s (%s only)\", AnnotationType.TS_ID, self.ts_id_name)\n\n    elif ID_TIME_COLUMN_NAME in temp_df.columns:\n        self.annotations.clear_time()\n        time_id_index = temp_df.columns.tolist().index(ID_TIME_COLUMN_NAME)\n        on = AnnotationType.ID_TIME\n        self.logger.info(\"Annotations detected as %s (%s only)\", AnnotationType.ID_TIME, ID_TIME_COLUMN_NAME)\n\n    else:\n        raise ValueError(f\"Could not find {self.ts_id_name} and {ID_TIME_COLUMN_NAME} in the imported CSV.\")\n\n    # Process each row in the DataFrame and add annotations\n    for row in temp_df.itertuples(False):\n        for i, _ in enumerate(temp_df.columns):\n            if i == time_id_index or i == ts_id_index:\n                continue\n\n            ts_id = None\n            if ts_id_index is not None:\n                ts_id = row[ts_id_index]\n\n            id_time = None\n            if time_id_index is not None:\n                id_time = row[time_id_index]\n\n            self.add_annotation(row[i], temp_df.columns[i], ts_id, id_time, enforce_ids)\n\n    self._update_annotations_imported_status(on, identifier)\n    self.logger.info(\"Successfully imported annotations from %s\", annotations_file_path)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.import_config","title":"import_config","text":"<pre><code>import_config(identifier: str, display_config_details: bool = True, workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Import the dataset_config from a pickle file and initializes the dataset. Config type must correspond to dataset type.</p> <p>First, it attempts to load the built-in config, if no built-in config with such an identifier exists, it attempts to load a custom config from the <code>\"data_root\"/tszoo/configs/</code> directory.</p> <p><code>data_root</code> is specified when the dataset is created.       </p> <p>The following configuration attributes are used during initialization:</p> Dataset config Description <code>init_workers</code> Specifies the number of workers to use for initialization. Applied when <code>workers</code> = \"config\". <code>partial_fit_initialized_scalers</code> Determines whether initialized scalers should be partially fitted on the training data. <code>nan_threshold</code> Filters out time series with missing values exceeding the specified threshold. <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>Name of the pickle file.</p> required <code>display_config_details</code> <code>bool</code> <p>Flag indicating whether to display the configuration values after initialization. <code>Default: True</code> </p> <code>True</code> <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use during initialization. <code>Default: \"config\"</code></p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def import_config(self, identifier: str, display_config_details: bool = True, workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\" \n    Import the dataset_config from a pickle file and initializes the dataset. Config type must correspond to dataset type.\n\n    First, it attempts to load the built-in config, if no built-in config with such an identifier exists, it attempts to load a custom config from the `\"data_root\"/tszoo/configs/` directory.\n\n    `data_root` is specified when the dataset is created.       \n\n    The following configuration attributes are used during initialization:\n\n    | Dataset config                    | Description                                                                                    |\n    | --------------------------------- | ---------------------------------------------------------------------------------------------- |\n    | `init_workers`                    | Specifies the number of workers to use for initialization. Applied when `workers` = \"config\". |\n    | `partial_fit_initialized_scalers` | Determines whether initialized scalers should be partially fitted on the training data.        |\n    | `nan_threshold`                   | Filters out time series with missing values exceeding the specified threshold.                 |  \n\n    Parameters:\n        identifier: Name of the pickle file.\n        display_config_details: Flag indicating whether to display the configuration values after initialization. `Default: True` \n        workers: The number of workers to use during initialization. `Default: \"config\"`  \n    \"\"\"\n\n    # Load config\n    config_file_path, is_built_in = get_config_path_and_whether_it_is_built_in(identifier, self.configs_root, self.database_name, self.source_type, self.aggregation, self.logger)\n\n    if is_built_in:\n        self.logger.info(\"Built-in config found: %s. Loading it.\", identifier)\n        config = pickle_load(config_file_path)\n    else:\n        self.logger.info(\"Custom config found: %s. Loading it.\", identifier)\n        config = pickle_load(config_file_path)\n\n    self.logger.info(\"Initializing dataset configuration with the imported config.\")\n    self.set_dataset_config_and_initialize(config, display_config_details, workers)\n\n    self._update_config_imported_status(identifier)\n    self.logger.info(\"Successfully imported config from %s\", config_file_path)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.save_annotations","title":"save_annotations","text":"<pre><code>save_annotations(identifier: str, on: AnnotationType | Literal['id_time', 'ts_id', 'both'], force_write: bool = False) -&gt; None\n</code></pre> <p>Saves the annotations as a CSV file.</p> <p>The file will be saved to a path determined by the <code>data_root</code> specified when the dataset was created.</p> <p>The annotations will be saved under the directory <code>data_root/tszoo/annotations/</code>.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The name of the CSV file.</p> required <code>on</code> <code>AnnotationType | Literal['id_time', 'ts_id', 'both']</code> <p>What annotation type should be saved. If set to <code>\"both\"</code>, annotations will be applied as if <code>id_time</code> and <code>ts_id</code> were both set.   </p> required <code>force_write</code> <code>bool</code> <p>If set to <code>True</code>, will overwrite any existing files with the same name. <code>Default: False</code></p> <code>False</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def save_annotations(self, identifier: str, on: AnnotationType | Literal[\"id_time\", \"ts_id\", \"both\"], force_write: bool = False) -&gt; None:\n    \"\"\" \n    Saves the annotations as a CSV file.\n\n    The file will be saved to a path determined by the `data_root` specified when the dataset was created.\n\n    The annotations will be saved under the directory `data_root/tszoo/annotations/`.\n\n    Parameters:\n        identifier: The name of the CSV file.\n        on: What annotation type should be saved. If set to `\"both\"`, annotations will be applied as if `id_time` and `ts_id` were both set.   \n        force_write: If set to `True`, will overwrite any existing files with the same name. `Default: False`               \n    \"\"\"\n\n    if exists_built_in_annotations(identifier):\n        raise ValueError(\"Built-in annotations with this identifier already exists. Choose another identifier.\")\n\n    on = AnnotationType(on)\n\n    temp_df = self.get_annotations(on)\n\n    # Ensure the annotations root directory exists, creating it if necessary\n    if not os.path.exists(self.annotations_root):\n        os.makedirs(self.annotations_root)\n        self.logger.info(\"Created annotations directory at %s\", self.annotations_root)\n\n    path = os.path.join(self.annotations_root, f\"{identifier}.csv\")\n\n    if os.path.exists(path) and not force_write:\n        raise ValueError(f\"Annotations already exist at {path}. Set force_write=True to overwrite.\")\n    self.logger.debug(\"Annotations CSV file path: %s\", path)\n\n    temp_df.to_csv(path, index=False)\n\n    self._update_annotations_imported_status(on, identifier)\n    self.logger.info(\"Annotations successfully saved to %s\", path)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.save_config","title":"save_config","text":"<pre><code>save_config(identifier: str, create_with_details_file: bool = True, force_write: bool = False) -&gt; None\n</code></pre> <p>Saves the config as a pickle file.</p> <p>The file will be saved to a path determined by the <code>data_root</code> specified when the dataset was created.  The config will be saved under the directory <code>data_root/tszoo/configs/</code>.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The name of the pickle file.</p> required <code>create_with_details_file</code> <code>bool</code> <p>Whether to export the config along with a readable text file that provides details. <code>Defaults: True</code>. </p> <code>True</code> <code>force_write</code> <code>bool</code> <p>If set to <code>True</code>, will overwrite any existing files with the same name. <code>Default: False</code></p> <code>False</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def save_config(self, identifier: str, create_with_details_file: bool = True, force_write: bool = False) -&gt; None:\n    \"\"\" \n    Saves the config as a pickle file.\n\n    The file will be saved to a path determined by the `data_root` specified when the dataset was created. \n    The config will be saved under the directory `data_root/tszoo/configs/`.\n\n    Parameters:\n        identifier: The name of the pickle file.\n        create_with_details_file: Whether to export the config along with a readable text file that provides details. `Defaults: True`. \n        force_write: If set to `True`, will overwrite any existing files with the same name. `Default: False`            \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to save config.\")\n\n    if exists_built_in_config(identifier):\n        raise ValueError(\"Built-in config with this identifier already exists. Choose another identifier.\")\n\n    # Ensure the config directory exists\n    if not os.path.exists(self.configs_root):\n        os.makedirs(self.configs_root)\n        self.logger.info(\"Created config directory at %s\", self.configs_root)\n\n    path_pickle = os.path.join(self.configs_root, f\"{identifier}.pickle\")\n    path_details = os.path.join(self.configs_root, f\"{identifier}.txt\")\n\n    if os.path.exists(path_pickle) and not force_write:\n        raise ValueError(f\"Config at path {path_pickle} already exists. Set force_write=True to overwrite.\")\n    self.logger.debug(\"Config pickle path: %s\", path_pickle)\n\n    if create_with_details_file:\n        if os.path.exists(path_details) and not force_write:\n            raise ValueError(f\"Config details at path {path_details} already exists. Set force_write=True to overwrite.\")\n        self.logger.debug(\"Config details path: %s\", path_details)\n\n    if self.dataset_config.is_filler_custom:\n        self.logger.warning(\"You are using a custom filler. Ensure the config is distributed with the source code of the filler.\")\n\n    if self.dataset_config.is_scaler_custom:\n        self.logger.warning(\"You are using a custom scaler. Ensure the config is distributed with the source code of the scaler.\")\n\n    pickle_dump(self._export_config_copy, path_pickle)\n    self.logger.info(\"Config pickle saved to %s\", path_pickle)\n\n    if create_with_details_file:\n        with open(path_details, \"w\", encoding=\"utf-8\") as file:\n            file.write(str(self.dataset_config))\n        self.logger.info(\"Config details saved to %s\", path_details)\n\n    self._update_config_imported_status(identifier)\n    self.logger.info(\"Config successfully saved\")\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.save_benchmark","title":"save_benchmark","text":"<pre><code>save_benchmark(identifier: str, force_write: bool = False) -&gt; None\n</code></pre> <p>Saves the benchmark as a YAML file.</p> <p>The benchmark, along with any associated annotations and config files, will be saved in a path determined by the <code>data_root</code> specified when creating the dataset.  The default save path for benchmark is <code>\"data_root/tszoo/benchmarks/\"</code>.</p> <p>If you are using imported <code>annotations</code> or <code>config</code> (whether custom or built-in), their file names will be set in the <code>benchmark</code> file.  If new <code>annotations</code> or <code>config</code> are created during the process, their filenames will be derived from the provided <code>identifier</code> and set in the <code>benchmark</code> file.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The name of the YAML file.</p> required <code>force_write</code> <code>bool</code> <p>If set to <code>True</code>, will overwrite any existing files with the same name. <code>Default: False</code></p> <code>False</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def save_benchmark(self, identifier: str, force_write: bool = False) -&gt; None:\n    \"\"\" \n    Saves the benchmark as a YAML file.\n\n    The benchmark, along with any associated annotations and config files, will be saved in a path determined by the `data_root` specified when creating the dataset. \n    The default save path for benchmark is `\"data_root/tszoo/benchmarks/\"`.\n\n    If you are using imported `annotations` or `config` (whether custom or built-in), their file names will be set in the `benchmark` file. \n    If new `annotations` or `config` are created during the process, their filenames will be derived from the provided `identifier` and set in the `benchmark` file.\n\n    Parameters:\n        identifier: The name of the YAML file.\n        force_write: If set to `True`, will overwrite any existing files with the same name. `Default: False`            \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to save benchmark.\")\n\n    if exists_built_in_benchmark(identifier):\n        raise ValueError(\"Built-in benchmark with this identifier already exists. Choose another identifier.\")\n\n    # Determine annotation names based on the available annotations and whether the annotations were imported\n    if len(self.annotations.time_series_annotations) &gt; 0:\n        annotations_ts_name = self.imported_annotations_ts_identifier if self.imported_annotations_ts_identifier is not None else f\"{identifier}_{AnnotationType.TS_ID.value}\"\n    else:\n        annotations_ts_name = None\n\n    if len(self.annotations.time_annotations) &gt; 0:\n        annotations_time_name = self.imported_annotations_time_identifier if self.imported_annotations_time_identifier is not None else f\"{identifier}_{AnnotationType.ID_TIME.value}\"\n    else:\n        annotations_time_name = None\n\n    if len(self.annotations.time_in_series_annotations) &gt; 0:\n        annotations_both_name = self.imported_annotations_both_identifier if self.imported_annotations_both_identifier is not None else f\"{identifier}_{AnnotationType.BOTH.value}\"\n    else:\n        annotations_both_name = None\n\n    # Use the imported identifier if available, otherwise default to the current identifier\n    config_name = self.dataset_config.import_identifier if self.dataset_config.import_identifier is not None else identifier\n\n    export_benchmark = ExportBenchmark(self.database_name,\n                                       self.is_series_based,\n                                       self.source_type.value,\n                                       self.aggregation.value,\n                                       config_name,\n                                       annotations_ts_name,\n                                       annotations_time_name,\n                                       annotations_both_name)\n\n    # If the config was not imported, save it\n    if self.dataset_config.import_identifier is None:\n        self.save_config(export_benchmark.config_identifier, force_write=force_write)\n    else:\n        self.logger.info(\"Using already existing config with identifier: %s\", self.dataset_config.import_identifier)\n\n    # Save ts_id annotations if available and not previously imported\n    if self.imported_annotations_ts_identifier is None and len(self.annotations.time_series_annotations) &gt; 0:\n        self.save_annotations(export_benchmark.annotations_ts_identifier, AnnotationType.TS_ID, force_write=force_write)\n    elif self.imported_annotations_ts_identifier is not None:\n        self.logger.info(\"Using already existing annotations with identifier: %s; type: %s\", self.imported_annotations_ts_identifier, AnnotationType.TS_ID)\n\n    # Save id_time annotations if available and not previously imported\n    if self.imported_annotations_time_identifier is None and len(self.annotations.time_annotations) &gt; 0:\n        self.save_annotations(export_benchmark.annotations_time_identifier, AnnotationType.ID_TIME, force_write=force_write)\n    elif self.imported_annotations_time_identifier is not None:\n        self.logger.info(\"Using already existing annotations with identifier: %s; type: %s\", self.imported_annotations_time_identifier, AnnotationType.ID_TIME)\n\n    # Save both annotations if available and not previously imported\n    if self.imported_annotations_both_identifier is None and len(self.annotations.time_in_series_annotations) &gt; 0:\n        self.save_annotations(export_benchmark.annotations_both_identifier, AnnotationType.BOTH, force_write=force_write)\n    elif self.imported_annotations_both_identifier is not None:\n        self.logger.info(\"Using already existing annotations with identifier: %s; type: %s\", self.imported_annotations_both_identifier, AnnotationType.BOTH)\n\n    # Ensure the benchmark directory exists\n    if not os.path.exists(self.benchmarks_root):\n        os.makedirs(self.benchmarks_root)\n        self.logger.info(\"Created benchmarks directory at %s\", self.benchmarks_root)\n\n    benchmark_path = os.path.join(self.benchmarks_root, f\"{identifier}.yaml\")\n\n    if os.path.exists(benchmark_path) and not force_write:\n        self.logger.error(\"Benchmark file already exists at %s\", benchmark_path)\n        raise ValueError(f\"Benchmark at path {benchmark_path} already exists. Set force_write=True to overwrite.\")\n    self.logger.debug(\"Benchmark YAML file path: %s\", benchmark_path)\n\n    yaml_dump(export_benchmark.to_dict(), benchmark_path)\n    self.logger.info(\"Benchmark successfully saved to %s\", benchmark_path)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_scalers","title":"get_scalers","text":"<pre><code>get_scalers() -&gt; np.ndarray[Scaler] | Scaler | None\n</code></pre> <p>Return used scalers from config.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_scalers(self) -&gt; np.ndarray[Scaler] | Scaler | None:\n    \"\"\"Return used scalers from config. \"\"\"\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting get scalers.\")\n\n    return self.dataset_config.scalers\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.check_errors","title":"check_errors","text":"<pre><code>check_errors() -&gt; None\n</code></pre> <p>Validates whether the dataset is corrupted. </p> <p>Raises an exception if corrupted.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def check_errors(self) -&gt; None:\n    \"\"\"\n    Validates whether the dataset is corrupted. \n\n    Raises an exception if corrupted.\n    \"\"\"\n\n    dataset, _ = load_database(self.dataset_path)\n\n    try:\n        node_iter = dataset.walk_nodes()\n\n        # Process each node in the dataset\n        for node in node_iter:\n            if isinstance(node, tb.Table):\n\n                iter_by = min(LOADING_WARNING_THRESHOLD, len(node))\n                iters_done = 0\n\n                # Process the node in chunks to avoid memory issues\n                while iters_done &lt; len(node):\n                    iter_by = min(LOADING_WARNING_THRESHOLD, len(node) - iters_done)\n                    _ = node[iters_done: iters_done + iter_by]  # Fetch the data in chunks\n                    iters_done += iter_by\n\n                self.logger.info(\"Table '%s' checked successfully. (%d rows processed)\", node._v_pathname, len(node))\n\n        self.logger.info(\"Dataset check completed with no errors found.\")\n\n    except Exception as e:\n        self.logger.error(\"Error encountered during dataset check: %s\", str(e))\n\n    finally:\n        dataset.close()\n        self.logger.debug(\"Dataset connection closed.\")\n</code></pre>"},{"location":"reference_series_based_config/","title":"Series-based config class","text":""},{"location":"reference_series_based_config/#cesnet_tszoo.configs.series_based_config.SeriesBasedConfig","title":"cesnet_tszoo.configs.series_based_config.SeriesBasedConfig","text":"<p>               Bases: <code>DatasetConfig</code></p> <p>This class is used for configuring the <code>SeriesBasedCesnetDataset</code>.</p> <p>Used to configure the following:</p> <ul> <li>Train, validation, test, all sets (time period, sizes, features)</li> <li>Handling missing values (default values, <code>fillers</code>)</li> <li>Data transformation using <code>scalers</code></li> <li>Dataloader options (train/val/test/all/init workers, batch size, train loading order)</li> <li>Plotting</li> </ul> <p>Important Notes:</p> <ul> <li>Custom fillers must inherit from the <code>fillers</code> base class.</li> <li>Fillers can carry over values from the train set to the validation and test sets. For example, <code>ForwardFiller</code> can carry over values from previous sets.</li> <li>It is recommended to use the <code>scalers</code> base class, though this is not mandatory as long as it meets the required methods.<ul> <li>If a scaler is already initialized and <code>partial_fit_initialized_scalers</code> is <code>False</code>, the scaler does not require <code>partial_fit</code>.</li> <li>Otherwise, the scaler must support <code>partial_fit</code>.</li> <li>Scalers must implement <code>transform</code> method.</li> <li>Both <code>partial_fit</code> and <code>transform</code> methods must accept an input of type <code>np.ndarray</code> with shape <code>(times, features)</code>.</li> </ul> </li> <li><code>train_ts</code>, <code>val_ts</code>, and <code>test_ts</code> must not contain any overlapping time series IDs.</li> </ul> <p>For available configuration options, refer to here.</p> <p>Attributes:</p> Name Type Description <code>used_train_workers</code> <p>Tracks the number of train workers in use. Helps determine if the train dataloader should be recreated based on worker changes.</p> <code>used_val_workers</code> <p>Tracks the number of validation workers in use. Helps determine if the validation dataloader should be recreated based on worker changes.</p> <code>used_test_workers</code> <p>Tracks the number of test workers in use. Helps determine if the test dataloader should be recreated based on worker changes.</p> <code>used_all_workers</code> <p>Tracks the total number of all workers in use. Helps determine if the all dataloader should be recreated based on worker changes.</p> <code>import_identifier</code> <p>Tracks the name of the config upon import. None if not imported.</p> <code>logger</code> <p>Logger for displaying information.   </p> <p>The following attributes are initialized when <code>set_dataset_config_and_initialize</code> is called:</p> <p>Attributes:</p> Name Type Description <code>all_ts</code> <p>If no specific sets (train/val/test) are provided, all time series IDs are used. When any set is defined, only the time series IDs in defined sets are used.</p> <code>train_ts_row_ranges</code> <p>Initialized when <code>train_ts_id</code> is set. Contains time series IDs in train set with their respective time ID ranges.</p> <code>val_ts_row_ranges</code> <p>Initialized when <code>val_ts_id</code> is set. Contains time series IDs in validation set with their respective time ID ranges.</p> <code>test_ts_row_ranges</code> <p>Initialized when <code>test_ts</code> is set. Contains time series IDs in test set with their respective time ID ranges.</p> <code>all_ts_row_ranges</code> <p>Initialized when <code>all_ts</code> is set. Contains time series IDs in all set with their respective time ID ranges.</p> <code>display_time_period</code> <p>Used to display the configured value of <code>time_period</code>.</p> <code>aggregation</code> <p>The aggregation period used for the data.</p> <code>source_type</code> <p>The source type of the data.</p> <code>database_name</code> <p>Specifies which database this config applies to.</p> <code>scale_with_display</code> <p>Used to display the configured type of <code>scale_with</code>.</p> <code>fill_missing_with_display</code> <p>Used to display the configured type of <code>fill_missing_with</code>.</p> <code>features_to_take_without_ids</code> <p>Features to be returned, excluding time or time series IDs.</p> <code>indices_of_features_to_take_no_ids</code> <p>Indices of non-ID features in <code>features_to_take</code>.</p> <code>is_scaler_custom</code> <p>Flag indicating whether the scaler is custom.</p> <code>is_filler_custom</code> <p>Flag indicating whether the filler is custom.</p> <code>ts_id_name</code> <p>Name of the time series ID, dependent on <code>source_type</code>.</p> <code>used_times</code> <p>List of all times used in the configuration.</p> <code>used_ts_ids</code> <p>List of all time series IDs used in the configuration.</p> <code>used_ts_row_ranges</code> <p>List of time series IDs with their respective time ID ranges.</p> <code>used_fillers</code> <p>List of all fillers used in the configuration.</p> <code>used_singular_train_time_series</code> <p>Currently used singular train set time series for dataloader.</p> <code>used_singular_val_time_series</code> <p>Currently used singular validation set time series for dataloader.</p> <code>used_singular_test_time_series</code> <p>Currently used singular test set time series for dataloader.</p> <code>used_singular_all_time_series</code> <p>Currently used singular all set time series for dataloader.             </p> <code>scalers</code> <p>Prepared scalers for fitting/transforming. Can be one scaler, array of scalers or <code>None</code>.</p> <code>are_scalers_premade</code> <p>Indicates whether the scalers are premade.</p> <code>has_train</code> <p>Flag indicating whether the training set is in use.</p> <code>has_val</code> <p>Flag indicating whether the validation set is in use.</p> <code>has_test</code> <p>Flag indicating whether the test set is in use.</p> <code>has_all</code> <p>Flag indicating whether the all set is in use.</p> <code>train_fillers</code> <p>Fillers used in the train set. <code>None</code> if no filler is used or train set is not used.</p> <code>val_fillers</code> <p>Fillers used in the validation set. <code>None</code> if no filler is used or validation set is not used.</p> <code>test_fillers</code> <p>Fillers used in the test set. <code>None</code> if no filler is used or test set is not used.</p> <code>all_fillers</code> <p>Fillers used for the all set. <code>None</code> if no filler is used or all set is not used.</p> <code>is_initialized</code> <p>Flag indicating if the configuration has already been initialized. If true, config initialization will be skipped.          </p>"},{"location":"reference_series_based_config/#cesnet_tszoo.configs.series_based_config.SeriesBasedConfig--configuration-options","title":"Configuration options","text":"<p>Attributes:</p> Name Type Description <code>time_period</code> <p>Defines the time period for returning data from <code>train/val/test/all</code>. Can be a range of time IDs, a tuple of datetime objects or a float. Float value is equivalent to percentage of available times from start.</p> <code>train_ts</code> <p>Defines which time series IDs are used in the training set. Can be a list of IDs, or an integer/float to specify a random selection. An <code>int</code> specifies the number of random time series, and a <code>float</code> specifies the proportion of available time series.        <code>int</code> and <code>float</code> must be greater than 0, and a float should be smaller or equal to 1.0. Using <code>int</code> or <code>float</code> guarantees that no time series from other sets will be used. <code>Default: None</code></p> <code>val_ts</code> <p>Defines which time series IDs are used in the validation set. Same as <code>train_ts</code> but for the validation set. <code>Default: None</code></p> <code>test_ts</code> <p>Defines which time series IDs are used in the test set. Same as <code>train_ts</code> but for the test set. <code>Default: None</code> </p> <code>features_to_take</code> <p>Defines which features are used. <code>Default: \"all\"</code></p> <code>default_values</code> <p>Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature. <code>Default: \"default\"</code></p> <code>train_batch_size</code> <p>Batch size for the train dataloader. Affects number of returned time series in one batch. <code>Default: 32</code></p> <code>val_batch_size</code> <p>Batch size for the validation dataloader. Affects number of returned time series in one batch. <code>Default: 64</code></p> <code>test_batch_size</code> <p>Batch size for the test dataloader. Affects number of returned time series in one batch. <code>Default: 128</code></p> <code>all_batch_size</code> <p>Batch size for the all dataloader. Affects number of returned time series in one batch. <code>Default: 128</code> </p> <code>fill_missing_with</code> <p>Defines how to fill missing values in the dataset. Can pass enum <code>FillerType</code> for built-in filler or pass a type of custom filler that must derive from <code>Filler</code> base class. <code>Default: None</code></p> <code>scale_with</code> <p>Defines the scaler used to transform the dataset. Can pass enum <code>ScalerType</code> for built-in scaler, pass a type of custom scaler or instance of already fitted scaler. <code>Default: None</code></p> <code>partial_fit_initialized_scaler</code> <p>If <code>True</code>, partial fitting on train set is performed when using initiliazed scaler. <code>Default: False</code></p> <code>include_time</code> <p>If <code>True</code>, time data is included in the returned values. <code>Default: True</code></p> <code>include_ts_id</code> <p>If <code>True</code>, time series IDs are included in the returned values. <code>Default: True</code></p> <code>time_format</code> <p>Format for the returned time data. When using TimeFormat.DATETIME, time will be returned as separate list along rest of the values. <code>Default: TimeFormat.ID_TIME</code></p> <code>train_workers</code> <p>Number of workers for loading training data. <code>0</code> means that the data will be loaded in the main process. <code>Default: 4</code></p> <code>val_workers</code> <p>Number of workers for loading validation data. <code>0</code> means that the data will be loaded in the main process. <code>Default: 3</code></p> <code>test_workers</code> <p>Number of workers for loading test data. <code>0</code> means that the data will be loaded in the main process. <code>Default: 2</code></p> <code>all_workers</code> <p>Number of workers for loading all data. <code>0</code> means that the data will be loaded in the main process. <code>Default: 4</code></p> <code>init_workers</code> <p>Number of workers for initial dataset processing during configuration. <code>0</code> means that the data will be loaded in the main process. <code>Default: 4</code></p> <code>nan_threshold</code> <p>Maximum allowable percentage of missing data. Time series exceeding this threshold are excluded. Time series over the threshold will not be used. Used for <code>train/val/test/all</code> separately. <code>Default: 1.0</code></p> <code>train_dataloader_order</code> <p>Defines the order of data returned by the training dataloader. <code>Default: DataloaderOrder.SEQUENTIAL</code></p> <code>random_state</code> <p>Fixes randomness for reproducibility during configuration and dataset initialization. <code>Default: None</code></p> Source code in <code>cesnet_tszoo\\configs\\series_based_config.py</code> <pre><code>class SeriesBasedConfig(DatasetConfig):\n    \"\"\"\n    This class is used for configuring the [`SeriesBasedCesnetDataset`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset].\n\n    Used to configure the following:\n\n    - Train, validation, test, all sets (time period, sizes, features)\n    - Handling missing values (default values, [`fillers`][cesnet_tszoo.utils.filler])\n    - Data transformation using [`scalers`][cesnet_tszoo.utils.scaler]\n    - Dataloader options (train/val/test/all/init workers, batch size, train loading order)\n    - Plotting\n\n    **Important Notes:**\n\n    - Custom fillers must inherit from the [`fillers`][cesnet_tszoo.utils.filler.Filler] base class.\n    - Fillers can carry over values from the train set to the validation and test sets. For example, [`ForwardFiller`][cesnet_tszoo.utils.filler.ForwardFiller] can carry over values from previous sets.\n    - It is recommended to use the [`scalers`][cesnet_tszoo.utils.scaler.Scaler] base class, though this is not mandatory as long as it meets the required methods.\n        - If a scaler is already initialized and `partial_fit_initialized_scalers` is `False`, the scaler does not require `partial_fit`.\n        - Otherwise, the scaler must support `partial_fit`.\n        - Scalers must implement `transform` method.\n        - Both `partial_fit` and `transform` methods must accept an input of type `np.ndarray` with shape `(times, features)`.\n    - `train_ts`, `val_ts`, and `test_ts` must not contain any overlapping time series IDs.\n\n    For available configuration options, refer to [here][cesnet_tszoo.configs.series_based_config.SeriesBasedConfig--configuration-options].\n\n    Attributes:\n        used_train_workers: Tracks the number of train workers in use. Helps determine if the train dataloader should be recreated based on worker changes.\n        used_val_workers: Tracks the number of validation workers in use. Helps determine if the validation dataloader should be recreated based on worker changes.\n        used_test_workers: Tracks the number of test workers in use. Helps determine if the test dataloader should be recreated based on worker changes.\n        used_all_workers: Tracks the total number of all workers in use. Helps determine if the all dataloader should be recreated based on worker changes.\n        import_identifier: Tracks the name of the config upon import. None if not imported.\n        logger: Logger for displaying information.   \n\n    The following attributes are initialized when [`set_dataset_config_and_initialize`][cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.set_dataset_config_and_initialize] is called:\n\n    Attributes:\n        all_ts: If no specific sets (train/val/test) are provided, all time series IDs are used. When any set is defined, only the time series IDs in defined sets are used.\n        train_ts_row_ranges: Initialized when `train_ts_id` is set. Contains time series IDs in train set with their respective time ID ranges.\n        val_ts_row_ranges: Initialized when `val_ts_id` is set. Contains time series IDs in validation set with their respective time ID ranges.\n        test_ts_row_ranges: Initialized when `test_ts` is set. Contains time series IDs in test set with their respective time ID ranges.\n        all_ts_row_ranges: Initialized when `all_ts` is set. Contains time series IDs in all set with their respective time ID ranges.\n        display_time_period: Used to display the configured value of `time_period`.\n\n        aggregation: The aggregation period used for the data.\n        source_type: The source type of the data.\n        database_name: Specifies which database this config applies to.\n        scale_with_display: Used to display the configured type of `scale_with`.\n        fill_missing_with_display: Used to display the configured type of `fill_missing_with`.\n        features_to_take_without_ids: Features to be returned, excluding time or time series IDs.\n        indices_of_features_to_take_no_ids: Indices of non-ID features in `features_to_take`.\n        is_scaler_custom: Flag indicating whether the scaler is custom.\n        is_filler_custom: Flag indicating whether the filler is custom.\n        ts_id_name: Name of the time series ID, dependent on `source_type`.\n        used_times: List of all times used in the configuration.\n        used_ts_ids: List of all time series IDs used in the configuration.\n        used_ts_row_ranges: List of time series IDs with their respective time ID ranges.\n        used_fillers: List of all fillers used in the configuration.\n        used_singular_train_time_series: Currently used singular train set time series for dataloader.\n        used_singular_val_time_series: Currently used singular validation set time series for dataloader.\n        used_singular_test_time_series: Currently used singular test set time series for dataloader.\n        used_singular_all_time_series: Currently used singular all set time series for dataloader.             \n        scalers: Prepared scalers for fitting/transforming. Can be one scaler, array of scalers or `None`.\n        are_scalers_premade: Indicates whether the scalers are premade.\n        has_train: Flag indicating whether the training set is in use.\n        has_val: Flag indicating whether the validation set is in use.\n        has_test: Flag indicating whether the test set is in use.\n        has_all: Flag indicating whether the all set is in use.\n        train_fillers: Fillers used in the train set. `None` if no filler is used or train set is not used.\n        val_fillers: Fillers used in the validation set. `None` if no filler is used or validation set is not used.\n        test_fillers: Fillers used in the test set. `None` if no filler is used or test set is not used.\n        all_fillers: Fillers used for the all set. `None` if no filler is used or all set is not used.\n        is_initialized: Flag indicating if the configuration has already been initialized. If true, config initialization will be skipped.          \n\n    # Configuration options\n\n    Attributes:\n        time_period: Defines the time period for returning data from `train/val/test/all`. Can be a range of time IDs, a tuple of datetime objects or a float. Float value is equivalent to percentage of available times from start.\n        train_ts: Defines which time series IDs are used in the training set. Can be a list of IDs, or an integer/float to specify a random selection. An `int` specifies the number of random time series, and a `float` specifies the proportion of available time series. \n                  `int` and `float` must be greater than 0, and a float should be smaller or equal to 1.0. Using `int` or `float` guarantees that no time series from other sets will be used. `Default: None`\n        val_ts: Defines which time series IDs are used in the validation set. Same as `train_ts` but for the validation set. `Default: None`\n        test_ts: Defines which time series IDs are used in the test set. Same as `train_ts` but for the test set. `Default: None`           \n        features_to_take: Defines which features are used. `Default: \"all\"`\n        default_values: Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature. `Default: \"default\"`\n        train_batch_size: Batch size for the train dataloader. Affects number of returned time series in one batch. `Default: 32`\n        val_batch_size: Batch size for the validation dataloader. Affects number of returned time series in one batch. `Default: 64`\n        test_batch_size: Batch size for the test dataloader. Affects number of returned time series in one batch. `Default: 128`\n        all_batch_size: Batch size for the all dataloader. Affects number of returned time series in one batch. `Default: 128`         \n        fill_missing_with: Defines how to fill missing values in the dataset. Can pass enum [`FillerType`][cesnet_tszoo.utils.enums.FillerType] for built-in filler or pass a type of custom filler that must derive from [`Filler`][cesnet_tszoo.utils.filler.Filler] base class. `Default: None`\n        scale_with: Defines the scaler used to transform the dataset. Can pass enum [`ScalerType`][cesnet_tszoo.utils.enums.ScalerType] for built-in scaler, pass a type of custom scaler or instance of already fitted scaler. `Default: None`\n        partial_fit_initialized_scaler: If `True`, partial fitting on train set is performed when using initiliazed scaler. `Default: False`\n        include_time: If `True`, time data is included in the returned values. `Default: True`\n        include_ts_id: If `True`, time series IDs are included in the returned values. `Default: True`\n        time_format: Format for the returned time data. When using TimeFormat.DATETIME, time will be returned as separate list along rest of the values. `Default: TimeFormat.ID_TIME`\n        train_workers: Number of workers for loading training data. `0` means that the data will be loaded in the main process. `Default: 4`\n        val_workers: Number of workers for loading validation data. `0` means that the data will be loaded in the main process. `Default: 3`\n        test_workers: Number of workers for loading test data. `0` means that the data will be loaded in the main process. `Default: 2`\n        all_workers: Number of workers for loading all data. `0` means that the data will be loaded in the main process. `Default: 4`\n        init_workers: Number of workers for initial dataset processing during configuration. `0` means that the data will be loaded in the main process. `Default: 4`\n        nan_threshold: Maximum allowable percentage of missing data. Time series exceeding this threshold are excluded. Time series over the threshold will not be used. Used for `train/val/test/all` separately. `Default: 1.0`\n        train_dataloader_order: Defines the order of data returned by the training dataloader. `Default: DataloaderOrder.SEQUENTIAL`\n        random_state: Fixes randomness for reproducibility during configuration and dataset initialization. `Default: None`                  \n    \"\"\"\n\n    def __init__(self,\n                 time_period: tuple[datetime, datetime] | range | float | Literal[\"all\"],\n                 train_ts: list[int] | npt.NDArray[np.int_] | float | int | None = None,\n                 val_ts: list[int] | npt.NDArray[np.int_] | float | int | None = None,\n                 test_ts: list[int] | npt.NDArray[np.int_] | float | int | None = None,\n                 features_to_take: list[str] | Literal[\"all\"] = \"all\",\n                 default_values: list[Number] | npt.NDArray[np.number] | dict[str, Number] | Number | Literal[\"default\"] | None = \"default\",\n                 train_batch_size: int = 32,\n                 val_batch_size: int = 64,\n                 test_batch_size: int = 128,\n                 all_batch_size: int = 128,\n                 fill_missing_with: type | FillerType | Literal[\"mean_filler\", \"forward_filler\", \"linear_interpolation_filler\"] | None = None,\n                 scale_with: type | ScalerType | Scaler | Literal[\"min_max_scaler\", \"standard_scaler\", \"max_abs_scaler\", \"log_scaler\", \"l2_normalizer\"] | None = None,\n                 partial_fit_initialized_scaler: bool = False,\n                 include_time: bool = True,\n                 include_ts_id: bool = True,\n                 time_format: TimeFormat | Literal[\"id_time\", \"datetime\", \"unix_time\", \"shifted_unix_time\"] = TimeFormat.ID_TIME,\n                 train_workers: int = 4,\n                 val_workers: int = 3,\n                 test_workers: int = 2,\n                 all_workers: int = 4,\n                 init_workers: int = 4,\n                 nan_threshold: float = 1.0,\n                 train_dataloader_order: DataloaderOrder | Literal[\"random\", \"sequential\"] = DataloaderOrder.SEQUENTIAL,\n                 random_state: int | None = None):\n\n        self.time_period = time_period\n        self.train_ts = train_ts\n        self.val_ts = val_ts\n        self.test_ts = test_ts\n\n        self.all_ts = None\n        self.train_ts_row_ranges = None\n        self.val_ts_row_ranges = None\n        self.test_ts_row_ranges = None\n        self.all_ts_row_ranges = None\n        self.display_time_period = None\n\n        super(SeriesBasedConfig, self).__init__(features_to_take, default_values, None, None, 1, 0, train_batch_size, val_batch_size, test_batch_size, all_batch_size, fill_missing_with, scale_with, partial_fit_initialized_scaler, include_time, include_ts_id, time_format,\n                                                train_workers, val_workers, test_workers, all_workers, init_workers, nan_threshold, False, True, train_dataloader_order, random_state)\n\n    def _validate_construction(self) -&gt; None:\n        \"\"\"Performs basic parameter validation to ensure correct configuration. More comprehensive validation, which requires dataset-specific data, is handled in [`_dataset_init`][cesnet_tszoo.configs.series_based_config.SeriesBasedConfig._dataset_init]. \"\"\"\n\n        super(SeriesBasedConfig, self)._validate_construction()\n\n        if isinstance(self.time_period, (float, int)):\n            self.time_period = float(self.time_period)\n            assert self.time_period &gt; 0.0, \"time_period must be greater than 0\"\n            assert self.time_period &lt;= 1.0, \"time_period must be lower or equal to 1.0\"\n\n        split_float_total = 0\n\n        if isinstance(self.train_ts, (float, int)):\n            assert self.train_ts &gt; 0, \"train_ts must be greater than 0.\"\n            if isinstance(self.train_ts, float):\n                split_float_total += self.train_ts\n\n        if isinstance(self.val_ts, (float, int)):\n            assert self.val_ts &gt; 0, \"val_ts must be greater than 0\"\n            if isinstance(self.val_ts, float):\n                split_float_total += self.val_ts\n\n        if isinstance(self.test_ts, (float, int)):\n            assert self.test_ts &gt; 0, \"test_ts must be greater than 0\"\n            if isinstance(self.test_ts, float):\n                split_float_total += self.test_ts\n\n        # Check if the total of float splits exceeds 1.0\n        if split_float_total &gt; 1.0:\n            self.logger.error(\"The total of the float split sizes is greater than 1.0. Current total: %s\", split_float_total)\n            raise ValueError(\"Total value of used float split sizes can't be larger than 1.0.\")\n\n        self.logger.debug(\"Series-based configuration validated successfully.\")\n\n    def _get_train(self) -&gt; tuple[np.ndarray, np.ndarray] | tuple[None, None]:\n        \"\"\"Returns the indices corresponding to the training set. \"\"\"\n        return self.train_ts, self.time_period\n\n    def _get_val(self) -&gt; tuple[np.ndarray, np.ndarray] | tuple[None, None]:\n        \"\"\"Returns the indices corresponding to the validation set. \"\"\"\n        return self.val_ts, self.time_period\n\n    def _get_test(self) -&gt; tuple[np.ndarray, np.ndarray] | tuple[None, None]:\n        \"\"\"Returns the indices corresponding to the test set. \"\"\"\n        return self.test_ts, self.time_period\n\n    def _get_all(self) -&gt; tuple[np.ndarray, np.ndarray] | tuple[None, None]:\n        \"\"\"Returns the indices corresponding to the all set. \"\"\"\n        return self.all_ts, self.time_period\n\n    def _set_time_period(self, all_time_ids: np.ndarray) -&gt; None:\n        \"\"\"Validates and filters the input time period based on the dataset and aggregation. \"\"\"\n\n        if self.time_period == \"all\":\n            self.time_period = range(len(all_time_ids))\n            self.logger.debug(\"Time period set to 'all'. Using all available time IDs, range: %s\", self.time_period)\n        elif isinstance(self.time_period, float):\n            self.time_period = range(int(self.time_period * len(all_time_ids)))\n            self.logger.debug(\"Time period set with float value. Using range: %s\", self.time_period)\n\n        self.time_period, self.display_time_period = self._process_time_period(self.time_period, all_time_ids)\n        self.logger.debug(\"Processed time_period: %s, display_time_period: %s\", self.time_period, self.display_time_period)\n\n    def _set_ts(self, all_ts_ids: np.ndarray, all_ts_row_ranges: np.ndarray) -&gt; None:\n        \"\"\"Validates and filters the input time series IDs based on the `dataset` and `source_type`. Handles random split.\"\"\"\n\n        random_ts_ids = all_ts_ids[self.ts_id_name]\n        random_indices = np.arange(len(all_ts_ids))\n\n        # Process train_ts if it was specified with times series ids\n        if self.train_ts is not None and not isinstance(self.train_ts, (float, int)):\n            self.train_ts, self.train_ts_row_ranges, _ = self._process_ts_ids(self.train_ts, all_ts_ids, all_ts_row_ranges, None, None)\n            self.has_train = True\n\n            mask = np.isin(random_ts_ids, self.train_ts, invert=True)\n            random_ts_ids = random_ts_ids[mask]\n            random_indices = random_indices[mask]\n\n            self.logger.debug(\"train_ts set: %s\", self.train_ts)\n\n        # Process val_ts if it was specified with times series ids\n        if self.val_ts is not None and not isinstance(self.val_ts, (float, int)):\n            self.val_ts, self.val_ts_row_ranges, _ = self._process_ts_ids(self.val_ts, all_ts_ids, all_ts_row_ranges, None, None)\n            self.has_val = True\n\n            mask = np.isin(random_ts_ids, self.val_ts, invert=True)\n            random_ts_ids = random_ts_ids[mask]\n            random_indices = random_indices[mask]\n\n            self.logger.debug(\"val_ts set: %s\", self.val_ts)\n\n        # Process time_ts if it was specified with times series ids\n        if self.test_ts is not None and not isinstance(self.test_ts, (float, int)):\n            self.test_ts, self.test_ts_row_ranges, _ = self._process_ts_ids(self.test_ts, all_ts_ids, all_ts_row_ranges, None, None)\n            self.has_test = True\n\n            mask = np.isin(random_ts_ids, self.test_ts, invert=True)\n            random_ts_ids = random_ts_ids[mask]\n            random_indices = random_indices[mask]\n\n            self.logger.debug(\"test_ts set: %s\", self.test_ts)\n\n        # Convert proportions to total values\n        if isinstance(self.train_ts, float):\n            self.train_ts = int(self.train_ts * len(random_ts_ids))\n            self.logger.debug(\"train_ts converted to total values: %s\", self.train_ts)\n        if isinstance(self.val_ts, float):\n            self.val_ts = int(self.val_ts * len(random_ts_ids))\n            self.logger.debug(\"val_ts converted to total values: %s\", self.val_ts)\n        if isinstance(self.test_ts, float):\n            self.test_ts = int(self.test_ts * len(random_ts_ids))\n            self.logger.debug(\"test_ts converted to total values: %s\", self.test_ts)\n\n        # Process random train_ts if it is to be randomly made\n        if isinstance(self.train_ts, int):\n            self.train_ts, self.train_ts_row_ranges, random_indices = self._process_ts_ids(None, all_ts_ids, all_ts_row_ranges, self.train_ts, random_indices)\n            self.has_train = True\n            self.logger.debug(\"Random train_ts set with %s time series.\", self.train_ts)\n\n        # Process random val_ts if it is to be randomly made\n        if isinstance(self.val_ts, int):\n            self.val_ts, self.val_ts_row_ranges, random_indices = self._process_ts_ids(None, all_ts_ids, all_ts_row_ranges, self.val_ts, random_indices)\n            self.has_val = True\n            self.logger.debug(\"Random val_ts set with %s time series.\", self.val_ts)\n\n        # Process random test_ts if it is to be randomly made\n        if isinstance(self.test_ts, int):\n            self.test_ts, self.test_ts_row_ranges, random_indices = self._process_ts_ids(None, all_ts_ids, all_ts_row_ranges, self.test_ts, random_indices)\n            self.has_test = True\n            self.logger.debug(\"Random test_ts set with %s time series.\", self.test_ts)\n\n        if not self.has_train and not self.has_val and not self.has_test:\n            self.all_ts = all_ts_ids[self.ts_id_name]\n            self.all_ts, self.all_ts_row_ranges, _ = self._process_ts_ids(self.all_ts, all_ts_ids, all_ts_row_ranges, None, None)\n            self.logger.info(\"Using all time series for all_ts because train_ts, val_ts, and test_ts are all set to None.\")\n        else:\n            for temp_ts_ids in [self.train_ts, self.val_ts, self.test_ts]:\n                if temp_ts_ids is None:\n                    continue\n                elif self.all_ts is None:\n                    self.all_ts = temp_ts_ids.copy()\n                else:\n                    self.all_ts = np.concatenate((self.all_ts, temp_ts_ids))\n\n            if self.has_train:\n                self.logger.debug(\"all_ts includes ids from train_ts.\")\n            if self.has_val:\n                self.logger.debug(\"all_ts includes ids from val_ts.\")\n            if self.has_test:\n                self.logger.debug(\"all_ts includes ids from test_ts.\")\n\n            self.all_ts, self.all_ts_row_ranges, _ = self._process_ts_ids(self.all_ts, all_ts_ids, all_ts_row_ranges, None, None)\n\n        self.has_all = self.all_ts is not None\n\n        if self.has_all:\n            self.logger.debug(\"all_ts set with %s time series.\", self.all_ts)\n\n    def _set_feature_scalers(self) -&gt; None:\n        \"\"\"Creates and/or validates scalers based on the `scale_with` parameter. \"\"\"\n\n        if self.scale_with is None:\n            self.scale_with_display = None\n            self.are_scalers_premade = False\n            self.scalers = None\n            self.is_scaler_custom = None\n\n            self.logger.debug(\"No scaler will be used because scale_with is not set.\")\n            return\n\n        # Treat scale_with as already initialized scaler\n        if not isinstance(self.scale_with, (type, ScalerType)):\n            self.scalers = self.scale_with\n\n            if not self.has_train:\n                if self.partial_fit_initialized_scalers:\n                    self.logger.warning(\"partial_fit_initialized_scalers will be ignored because train set is not used.\")\n                self.partial_fit_initialized_scalers = False\n\n            self.scale_with, self.scale_with_display = scaler_from_input_to_scaler_type(type(self.scale_with), check_for_fit=False, check_for_partial_fit=self.partial_fit_initialized_scalers)\n\n            self.are_scalers_premade = True\n\n            self.is_scaler_custom = \"Custom\" in self.scale_with_display\n            self.logger.debug(\"Using initialized scaler of type: %s\", self.scale_with_display)\n\n        # Treat scale_with as uninitialized scaler\n        else:\n            if not self.has_train:\n                self.scale_with = None\n                self.scale_with_display = None\n                self.are_scalers_premade = False\n                self.scalers = None\n                self.is_scaler_custom = None\n\n                self.logger.warning(\"No scaler will be used because train set is not used.\")\n                return\n\n            self.scale_with, self.scale_with_display = scaler_from_input_to_scaler_type(self.scale_with, check_for_fit=False, check_for_partial_fit=True)\n            self.scalers = self.scale_with()\n\n            self.are_scalers_premade = False\n\n            self.is_scaler_custom = \"Custom\" in self.scale_with_display\n            self.logger.debug(\"Using uninitialized scaler of type: %s\", self.scale_with_display)\n\n    def _set_fillers(self) -&gt; None:\n        \"\"\"Creates and/or validates fillers based on the `fill_missing_with` parameter. \"\"\"\n\n        self.fill_missing_with, self.fill_missing_with_display = filler_from_input_to_type(self.fill_missing_with)\n        self.is_filler_custom = \"Custom\" in self.fill_missing_with_display if self.fill_missing_with is not None else None\n\n        if self.fill_missing_with is None:\n            self.logger.debug(\"No filler is used because fill_missing_with is set to None.\")\n            return\n\n        # Set the fillers for the training set\n        if self.has_train:\n            self.train_fillers = np.array([self.fill_missing_with(self.features_to_take_without_ids) for _ in self.train_ts])\n            self.logger.debug(\"Fillers for training set are set.\")\n\n        # Set the fillers for the validation set\n        if self.has_val:\n            self.val_fillers = np.array([self.fill_missing_with(self.features_to_take_without_ids) for _ in self.val_ts])\n            self.logger.debug(\"Fillers for validation set are set.\")\n\n        # Set the fillers for the test set\n        if self.has_test:\n            self.test_fillers = np.array([self.fill_missing_with(self.features_to_take_without_ids) for _ in self.test_ts])\n            self.logger.debug(\"Fillers for test set are set.\")\n\n        # Set the fillers for the all set\n        if self.has_all:\n            self.all_fillers = np.array([self.fill_missing_with(self.features_to_take_without_ids) for _ in self.all_ts])\n            self.logger.debug(\"Fillers for all set are set.\")\n\n        self.logger.debug(\"Using filler: %s.\", self.fill_missing_with_display)\n\n    def _validate_finalization(self) -&gt; None:\n        \"\"\"Performs final validation of the configuration. \"\"\"\n\n        train_size = 0\n        if self.has_train:\n            train_size += len(self.train_ts)\n\n        val_size = 0\n        if self.has_val:\n            val_size += len(self.val_ts)\n\n        test_size = 0\n        if self.has_test:\n            test_size += len(self.test_ts)\n\n        # Check for overlap between train, val, and test sets\n        if train_size + val_size + test_size &gt; 0 and train_size + val_size + test_size != len(np.unique(self.all_ts)):\n            self.logger.error(\"Overlap detected! Train, Val, and Test sets can't have the same IDs.\")\n            raise ValueError(\"Train, Val, and Test can't have the same IDs.\")\n\n    def __str__(self) -&gt; str:\n\n        if self.scale_with is None:\n            scaler_part = f\"Scaler type: {str(self.scale_with_display)}\"\n        else:\n            scaler_part = f'''Scaler type: {str(self.scale_with_display)}\n        Are scalers premade: {self.are_scalers_premade}\n        Are premade scalers partial_fitted: {self.partial_fit_initialized_scalers}'''\n\n        if self.include_time:\n            time_part = f'''Time included: {str(self.include_time)}    \n        Time format: {str(self.time_format)}'''\n        else:\n            time_part = f\"Time included: {str(self.include_time)}\"\n\n        return f'''\nConfig Details:\n    Used for database: {self.database_name}\n    Aggregation: {str(self.aggregation)}\n    Source: {str(self.source_type)}\n\n    Time series\n        Train time series IDS: {get_abbreviated_list_string(self.train_ts)}\n        Val time series IDS: {get_abbreviated_list_string(self.val_ts)}\n        Test time series IDS {get_abbreviated_list_string(self.test_ts)}\n        All time series IDS {get_abbreviated_list_string(self.all_ts)}\n    Time periods\n        Time period: {str(self.display_time_period)}\n    Features\n        Taken features: {str(self.features_to_take_without_ids)}\n        Default values: {self.default_values}\n        Time series ID included: {str(self.include_ts_id)}\n        {time_part}\n    Fillers         \n        Filler type: {str(self.fill_missing_with_display)}\n    Scalers\n        {scaler_part}\n    Batch sizes\n        Train batch size: {self.train_batch_size}\n        Val batch size: {self.val_batch_size}\n        Test batch size: {self.test_batch_size}\n        All batch size: {self.all_batch_size}\n    Default workers\n        Train worker count: {str(self.train_workers)}\n        Val worker count: {str(self.val_workers)}\n        Test worker count: {str(self.test_workers)}\n        All worker count: {str(self.all_workers)}\n        Init worker count: {str(self.init_workers)}\n    Other\n        Nan threshold: {str(self.nan_threshold)}\n        Random state: {self.random_state}\n        Train dataloader order {str(self.train_dataloader_order)}\n                '''\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/","title":"Time-based dataset class","text":""},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset","title":"cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset  <code>dataclass</code>","text":"<p>               Bases: <code>CesnetDataset</code></p> <p>This class is used for time-based returning of data. Can be created by using <code>get_dataset</code> with parameter <code>is_series_based</code> = <code>False</code>.</p> <p>Time-based means batch size affects number of returned times in one batch. Which time series are returned does not change.</p> <p>The dataset provides multiple ways to access the data:</p> <ul> <li>Iterable PyTorch DataLoader: For batch processing.</li> <li>Pandas DataFrame: For loading the entire training, validation, test or all set at once.</li> <li>Numpy array: For loading the entire training, validation, test or all set at once. </li> <li>See loading data for more details.</li> </ul> <p>The dataset is stored in a PyTables database. The internal <code>TimeBasedDataset</code>, <code>SplittedDataset</code>, <code>TimeBasedInitializerDataset</code> classes (used only when calling <code>set_dataset_config_and_initialize</code>) act as wrappers that implement the PyTorch <code>Dataset</code>  interface. These wrappers are compatible with PyTorch\u2019s <code>DataLoader</code>, providing efficient parallel data loading. </p> <p>The dataset configuration is done through the <code>TimeBasedConfig</code> class.       </p> <p>Intended usage:</p> <ol> <li>Create an instance of the dataset with the desired data root by calling <code>get_dataset</code>. This will download the dataset if it has not been previously downloaded and return instance of dataset.</li> <li>Create an instance of <code>TimeBasedConfig</code> and set it using <code>set_dataset_config_and_initialize</code>.     This initializes the dataset, including data splitting (train/validation/test/test_other), fitting scalers (if needed), selecting features, and more. This is cached for later use.</li> <li>Use <code>get_train_dataloader</code>/<code>get_train_df</code>/<code>get_train_numpy</code> to get training data for chosen model.</li> <li>Validate the model and perform the hyperparameter optimalization on <code>get_val_dataloader</code>/<code>get_val_df</code>/<code>get_val_numpy</code>.</li> <li>Evaluate the model on <code>get_test_dataloader</code>/<code>get_test_df</code>/<code>get_test_numpy</code>. </li> <li>(Optional) Evaluate the model on <code>get_test_other_dataloader</code>/<code>get_test_other_df</code>/<code>get_test_other_numpy</code>.    </li> </ol> <p>Alternatively you can use <code>load_benchmark</code></p> <ol> <li>Call <code>load_benchmark</code> with the desired benchmark. You can use your own saved benchmark or you can use already built-in one. This will download the dataset and annotations (if available) if they have not been previously downloaded.</li> <li>Retrieve the initialized dataset using <code>get_initialized_dataset</code>. This will provide a dataset that is ready to use.</li> <li>Use <code>get_train_dataloader</code>/<code>get_train_df</code>/<code>get_train_numpy</code> to get training data for chosen model.</li> <li>Validate the model and perform the hyperparameter optimalization on <code>get_val_dataloader</code>/<code>get_val_df</code>/<code>get_val_numpy</code>.</li> <li>Evaluate the model on <code>get_test_dataloader</code>/<code>get_test_df</code>/<code>get_test_numpy</code>. </li> <li>(Optional) Evaluate the model on <code>get_test_other_dataloader</code>/<code>get_test_other_df</code>/<code>get_test_other_numpy</code>.    </li> </ol> <p>Parameters:</p> Name Type Description Default <code>database_name</code> <code>str</code> <p>Name of the database.</p> required <code>dataset_path</code> <code>str</code> <p>Path to the dataset file.     </p> required <code>configs_root</code> <code>str</code> <p>Path to the folder where configurations are saved.</p> required <code>benchmarks_root</code> <code>str</code> <p>Path to the folder where benchmarks are saved.</p> required <code>annotations_root</code> <code>str</code> <p>Path to the folder where annotations are saved.</p> required <code>source_type</code> <code>SourceType</code> <p>The source type of the dataset.</p> required <code>aggregation</code> <code>AgreggationType</code> <p>The aggregation type for the selected source type.</p> required <code>ts_id_name</code> <code>str</code> <p>Name of the id used for time series.</p> required <code>default_values</code> <code>dict</code> <p>Default values for each available feature.</p> required <code>additional_data</code> <code>dict[str, tuple]</code> <p>Available small datasets. Can get them by calling <code>get_additional_data</code> with their name.</p> required <p>Attributes:</p> Name Type Description <code>time_indices</code> <p>Available time IDs for the dataset.</p> <code>ts_indices</code> <p>Available time series IDs for the dataset.</p> <code>annotations</code> <p>Annotations for the selected dataset.</p> <code>logger</code> <p>Logger for displaying information.  </p> <code>imported_annotations_ts_identifier</code> <p>Identifier for the imported annotations of type <code>AnnotationType.TS_ID</code>.</p> <code>imported_annotations_time_identifier</code> <p>Identifier for the imported annotations of type <code>AnnotationType.ID_TIME</code>.</p> <code>imported_annotations_both_identifier</code> <p>Identifier for the imported annotations of type <code>AnnotationType.BOTH</code>.  </p> <p>The following attributes are initialized when <code>set_dataset_config_and_initialize</code> is called.</p> <p>Attributes:</p> Name Type Description <code>dataset_config</code> <code>Optional[TimeBasedConfig]</code> <p>Configuration of the dataset.</p> <code>train_dataset</code> <code>Optional[SplittedDataset]</code> <p>Training set as a <code>SplittedDataset</code> instance wrapping multiple <code>TimeBasedDataset</code> that wrap the PyTables database.</p> <code>val_dataset</code> <code>Optional[SplittedDataset]</code> <p>Validation set as a <code>SplittedDataset</code> instance wrapping multiple <code>TimeBasedDataset</code> that wrap the PyTables database.</p> <code>test_dataset</code> <code>Optional[SplittedDataset]</code> <p>Test set as a <code>SplittedDataset</code> instance wrapping multiple <code>TimeBasedDataset</code> that wrap the PyTables database.  </p> <code>test_other_dataset</code> <code>Optional[SplittedDataset]</code> <p>Test_other set as a <code>SplittedDataset</code> instance wrapping multiple <code>TimeBasedDataset</code> that wrap the PyTables database.</p> <code>all_dataset</code> <code>Optional[SplittedDataset]</code> <p>All set as a <code>SplittedDataset</code> instance wrapping multiple <code>TimeBasedDataset</code> that wrap the PyTables database.</p> <code>train_dataloader</code> <code>Optional[DataLoader]</code> <p>Iterable PyTorch <code>DataLoader</code> for training set.</p> <code>val_dataloader</code> <code>Optional[DataLoader]</code> <p>Iterable PyTorch <code>DataLoader</code> for validation set.</p> <code>test_dataloader</code> <code>Optional[DataLoader]</code> <p>Iterable PyTorch <code>DataLoader</code> for test set.</p> <code>test_other_dataloader</code> <code>Optional[DataLoader]</code> <p>Iterable PyTorch <code>DataLoader</code> for test_other set.            </p> <code>all_dataloader</code> <code>Optional[DataLoader]</code> <p>Iterable PyTorch <code>DataLoader</code> for all set.</p> Source code in <code>cesnet_tszoo\\datasets\\time_based_cesnet_dataset.py</code> <pre><code>@dataclass\nclass TimeBasedCesnetDataset(CesnetDataset):\n    \"\"\"This class is used for time-based returning of data. Can be created by using [`get_dataset`][cesnet_tszoo.datasets.cesnet_database.CesnetDatabase.get_dataset] with parameter `is_series_based` = `False`.\n\n    Time-based means batch size affects number of returned times in one batch. Which time series are returned does not change.\n\n    The dataset provides multiple ways to access the data:\n\n    - **Iterable PyTorch DataLoader**: For batch processing.\n    - **Pandas DataFrame**: For loading the entire training, validation, test or all set at once.\n    - **Numpy array**: For loading the entire training, validation, test or all set at once. \n    - See [loading data][loading-data] for more details.\n\n    The dataset is stored in a [PyTables](https://www.pytables.org/) database. The internal `TimeBasedDataset`, `SplittedDataset`, `TimeBasedInitializerDataset` classes (used only when calling [`set_dataset_config_and_initialize`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.set_dataset_config_and_initialize]) act as wrappers that implement the PyTorch [`Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) \n    interface. These wrappers are compatible with PyTorch\u2019s [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader), providing efficient parallel data loading. \n\n    The dataset configuration is done through the [`TimeBasedConfig`][cesnet_tszoo.configs.time_based_config.TimeBasedConfig] class.       \n\n    **Intended usage:**\n\n    1. Create an instance of the dataset with the desired data root by calling [`get_dataset`][cesnet_tszoo.datasets.cesnet_database.CesnetDatabase.get_dataset]. This will download the dataset if it has not been previously downloaded and return instance of dataset.\n    2. Create an instance of [`TimeBasedConfig`][cesnet_tszoo.configs.time_based_config.TimeBasedConfig] and set it using [`set_dataset_config_and_initialize`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.set_dataset_config_and_initialize]. \n       This initializes the dataset, including data splitting (train/validation/test/test_other), fitting scalers (if needed), selecting features, and more. This is cached for later use.\n    3. Use [`get_train_dataloader`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_train_dataloader]/[`get_train_df`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_train_df]/[`get_train_numpy`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_train_numpy] to get training data for chosen model.\n    4. Validate the model and perform the hyperparameter optimalization on [`get_val_dataloader`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_val_dataloader]/[`get_val_df`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_val_df]/[`get_val_numpy`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_val_numpy].\n    5. Evaluate the model on [`get_test_dataloader`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_dataloader]/[`get_test_df`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_df]/[`get_test_numpy`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_numpy]. \n    6. (Optional) Evaluate the model on [`get_test_other_dataloader`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_other_dataloader]/[`get_test_other_df`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_other_df]/[`get_test_other_numpy`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_other_numpy].    \n\n    Alternatively you can use [`load_benchmark`][cesnet_tszoo.benchmarks.load_benchmark]\n\n    1. Call [`load_benchmark`][cesnet_tszoo.benchmarks.load_benchmark] with the desired benchmark. You can use your own saved benchmark or you can use already built-in one. This will download the dataset and annotations (if available) if they have not been previously downloaded.\n    2. Retrieve the initialized dataset using [`get_initialized_dataset`][cesnet_tszoo.benchmarks.Benchmark.get_initialized_dataset]. This will provide a dataset that is ready to use.\n    3. Use [`get_train_dataloader`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_train_dataloader]/[`get_train_df`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_train_df]/[`get_train_numpy`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_train_numpy] to get training data for chosen model.\n    4. Validate the model and perform the hyperparameter optimalization on [`get_val_dataloader`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_val_dataloader]/[`get_val_df`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_val_df]/[`get_val_numpy`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_val_numpy].\n    5. Evaluate the model on [`get_test_dataloader`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_dataloader]/[`get_test_df`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_df]/[`get_test_numpy`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_numpy]. \n    6. (Optional) Evaluate the model on [`get_test_other_dataloader`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_other_dataloader]/[`get_test_other_df`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_other_df]/[`get_test_other_numpy`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_other_numpy].    \n\n    Parameters:\n        database_name: Name of the database.\n        dataset_path: Path to the dataset file.     \n        configs_root: Path to the folder where configurations are saved.\n        benchmarks_root: Path to the folder where benchmarks are saved.\n        annotations_root: Path to the folder where annotations are saved.\n        source_type: The source type of the dataset.\n        aggregation: The aggregation type for the selected source type.\n        ts_id_name: Name of the id used for time series.\n        default_values: Default values for each available feature.\n        additional_data: Available small datasets. Can get them by calling [`get_additional_data`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_additional_data] with their name.\n\n    Attributes:\n        time_indices: Available time IDs for the dataset.\n        ts_indices: Available time series IDs for the dataset.\n        annotations: Annotations for the selected dataset.\n        logger: Logger for displaying information.  \n        imported_annotations_ts_identifier: Identifier for the imported annotations of type `AnnotationType.TS_ID`.\n        imported_annotations_time_identifier: Identifier for the imported annotations of type `AnnotationType.ID_TIME`.\n        imported_annotations_both_identifier: Identifier for the imported annotations of type `AnnotationType.BOTH`.  \n\n    The following attributes are initialized when [`set_dataset_config_and_initialize`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.set_dataset_config_and_initialize] is called.\n\n    Attributes:\n        dataset_config: Configuration of the dataset.\n        train_dataset: Training set as a `SplittedDataset` instance wrapping multiple `TimeBasedDataset` that wrap the PyTables database.\n        val_dataset: Validation set as a `SplittedDataset` instance wrapping multiple `TimeBasedDataset` that wrap the PyTables database.\n        test_dataset: Test set as a `SplittedDataset` instance wrapping multiple `TimeBasedDataset` that wrap the PyTables database.  \n        test_other_dataset: Test_other set as a `SplittedDataset` instance wrapping multiple `TimeBasedDataset` that wrap the PyTables database.\n        all_dataset: All set as a `SplittedDataset` instance wrapping multiple `TimeBasedDataset` that wrap the PyTables database.\n        train_dataloader: Iterable PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for training set.\n        val_dataloader: Iterable PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for validation set.\n        test_dataloader: Iterable PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for test set.\n        test_other_dataloader: Iterable PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for test_other set.            \n        all_dataloader: Iterable PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for all set.        \n    \"\"\"\n\n    dataset_config: Optional[TimeBasedConfig] = field(default=None, init=False)\n\n    train_dataset: Optional[SplittedDataset] = field(default=None, init=False)\n    val_dataset: Optional[SplittedDataset] = field(default=None, init=False)\n    test_dataset: Optional[SplittedDataset] = field(default=None, init=False)\n    test_other_dataset: Optional[SplittedDataset] = field(default=None, init=False)\n    all_dataset: Optional[SplittedDataset] = field(default=None, init=False)\n\n    train_dataloader: Optional[DataLoader] = field(default=None, init=False)\n    val_dataloader: Optional[DataLoader] = field(default=None, init=False)\n    test_dataloader: Optional[DataLoader] = field(default=None, init=False)\n    test_other_dataloader: Optional[DataLoader] = field(default=None, init=False)\n    all_dataloader: Optional[DataLoader] = field(default=None, init=False)\n\n    is_series_based: bool = field(default=False, init=False)\n\n    _export_config_copy: Optional[TimeBasedConfig] = field(default=None, init=False)\n\n    def set_dataset_config_and_initialize(self, dataset_config: TimeBasedConfig, display_config_details: bool = True, workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n        \"\"\"\n        Initialize training set, validation est, test set etc.. This method must be called before any data can be accessed. It is required for the final initialization of [`dataset_config`][cesnet_tszoo.configs.time_based_config.TimeBasedConfig].\n\n        The following configuration attributes are used during initialization:\n\n        | Dataset config                    | Description                                                                                    |\n        | --------------------------------- | ---------------------------------------------------------------------------------------------- |\n        | `init_workers`                    | Specifies the number of workers to use for initialization. Applied when `workers` = \"config\".  |\n        | `partial_fit_initialized_scalers` | Determines whether initialized scalers should be partially fitted on the training data.        |\n        | `nan_threshold`                   | Filters out time series with missing values exceeding the specified threshold.                 |\n\n        Parameters:\n            dataset_config: Desired configuration of the dataset.\n            display_config_details: Flag indicating whether to display the configuration values after initialization. `Default: True`  \n            workers: The number of workers to use during initialization. `Default: \"config\"`  \n        \"\"\"\n\n        assert isinstance(dataset_config, TimeBasedConfig), \"TimeBasedCesnetDataset can only use TimeBasedConfig.\"\n\n        super(TimeBasedCesnetDataset, self).set_dataset_config_and_initialize(dataset_config, display_config_details, workers)\n\n    def get_data_about_set(self, about: SplitType | Literal[\"train\", \"val\", \"test\", \"all\"]) -&gt; dict:\n        \"\"\"\n        Retrieve data related to the specified set.\n\n        Parameters:\n            about: Specifies the set to retrieve data about.\n\n        Returned dictionary contains:\n\n        - **ts_ids:** Ids of time series in `about` set.\n        - **test_ts_ids:** Ids of  time series in `test_ts_ids`. Only for `about` == SplitType.TEST and when `test_ts_id` is set in used config.\n        - **TimeFormat.ID_TIME:** Times in `about` set, where time format is `TimeFormat.ID_TIME`.\n        - **TimeFormat.DATETIME:** Times in `about` set, where time format is `TimeFormat.DATETIME`.\n        - **TimeFormat.UNIX_TIME:** Times in `about` set, where time format is `TimeFormat.UNIX_TIME`.\n        - **TimeFormat.SHIFTED_UNIX_TIME:** Times in `about` set, where time format is `TimeFormat.SHIFTED_UNIX_TIME`.\n\n        Returns:\n            Returns dictionary with details about set.\n        \"\"\"\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before getting data about set.\")\n\n        about = SplitType(about)\n\n        time_period = None\n\n        result = {}\n\n        if about == SplitType.TRAIN:\n            if not self.dataset_config.has_train:\n                raise ValueError(\"Train split is not used.\")\n            time_period = self.dataset_config.train_time_period\n        elif about == SplitType.VAL:\n            if not self.dataset_config.has_val:\n                raise ValueError(\"Val split is not used.\")\n            time_period = self.dataset_config.val_time_period\n        elif about == SplitType.TEST:\n            if not self.dataset_config.has_test:\n                raise ValueError(\"Test split is not used.\")\n            time_period = self.dataset_config.test_time_period\n            result[\"test_ts_ids\"] = self.dataset_config.test_ts_ids.copy() if self.dataset_config.test_ts_ids is not None else None\n        elif about == SplitType.ALL:\n            if not self.dataset_config.has_all:\n                raise ValueError(\"All split is not used.\")\n\n            time_period = self.dataset_config.all_time_period\n        else:\n            raise ValueError(\"Invalid split type!\")\n\n        datetime_temp = np.array([datetime.fromtimestamp(time, timezone.utc) for time in self.time_indices[TIME_COLUMN_NAME][time_period[ID_TIME_COLUMN_NAME]]])\n\n        result[\"ts_ids\"] = self.dataset_config.ts_ids.copy()\n        result[TimeFormat.ID_TIME] = time_period[ID_TIME_COLUMN_NAME].copy()\n        result[TimeFormat.DATETIME] = datetime_temp.copy()\n        result[TimeFormat.UNIX_TIME] = self.time_indices[TIME_COLUMN_NAME][time_period[ID_TIME_COLUMN_NAME]].copy()\n        result[TimeFormat.SHIFTED_UNIX_TIME] = self.time_indices[TIME_COLUMN_NAME][time_period[ID_TIME_COLUMN_NAME]] - self.time_indices[TIME_COLUMN_NAME][0]\n\n        return result\n\n    def get_test_other_dataloader(self, ts_id: int | None = None, workers: int | Literal[\"config\"] = \"config\", **kwargs) -&gt; DataLoader:\n        \"\"\"\n        Returns a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for test_other set.\n\n        The `DataLoader` is created on the first call and cached for subsequent use. &lt;br/&gt;\n        The cached dataloader is cleared when either [`get_test_other_df`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_other_df] or [`get_test_other_numpy`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_other_numpy] is called.\n\n        The structure of the returned batch depends on the `time_format` and whether `sliding_window_size` is used:\n\n        - When `sliding_window_size` is used:\n            - With `time_format` == TimeFormat.DATETIME and included time:\n                - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n                - `np.ndarray` of shape `(num_time_series, 1, features)`\n                - `np.ndarray` of times with shape `(times - 1)`\n                - `np.ndarray` of time with shape `(1)`\n            - When `time_format` != TimeFormat.DATETIME or time is not included:\n                - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n                - `np.ndarray` of shape `(num_time_series, 1, features)`\n        - When `sliding_window_size` is not used:\n            - With `time_format` == TimeFormat.DATETIME and included time:\n                - `np.ndarray` of shape `(num_time_series, times, features)`\n                - `np.ndarray` of time with shape `(times)`\n            - When `time_format` != TimeFormat.DATETIME or time is not included:\n                - `np.ndarray` of shape `(num_time_series, times, features)`\n\n        The `DataLoader` is configured with the following config attributes:\n\n        | Dataset config                     | Description                                                                                                                               |\n        | ---------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------- |\n        | `test_batch_size`                  | Number of times for time series per batch.                                                                                                |\n        | `sliding_window_size`              | Modifies the shape of the returned data.                                                                                                  |\n        | `sliding_window_prediction_size`   | Modifies the shape of the returned data.                                                                                                  |\n        | `sliding_window_step`              | Available only for time-based datasets. Number of times to move by after each window.                                                     |\n        | `test_workers`                     | Specifies the number of workers to use for loading test_other data. Applied when `workers` = \"config\".                                    |\n\n        Parameters:\n            workers: The number of workers to use for loading test_other data. `Default: \"config\"`  \n            ts_id: Specifies time series to take. If None returns all time series as normal. `Default: \"None\"`\n\n        Returns:\n            An iterable `DataLoader` containing data from test_other set.         \n        \"\"\"\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access test_other_dataloader.\")\n\n        if not self.dataset_config.has_all:\n            raise ValueError(\"Dataloader for test_other set is not available in the dataset configuration.\")\n\n        assert self.test_dataset is not None, \"The test_other_dataset must be initialized before accessing data from test_other set.\"\n\n        defaultKwargs = {'take_all': False, \"cache_loader\": True}\n        kwargs = {**defaultKwargs, **kwargs}\n\n        if ts_id is not None:\n\n            if ts_id == self.dataset_config.used_singular_test_other_time_series and self.test_other_dataloader is not None:\n                self.logger.debug(\"Returning cached test_other_dataloader.\")\n                return self.test_other_dataloader\n\n            dataset = self._get_singular_time_series_dataset(self.test_other_dataset, ts_id)\n            self.dataset_config.used_singular_test_other_time_series = ts_id\n            if self.test_other_dataloader:\n                del self.test_other_dataloader\n                self.test_other_dataloader = None\n                self.logger.info(\"Destroyed previous cached test_other_dataloader.\")\n\n            self.dataset_config.used_test_other_workers = 0\n            self.test_other_dataloader = self._get_dataloader(dataset, 0, False, self.dataset_config.test_batch_size)\n            self.logger.info(\"Created new cached test_other_dataloader.\")\n            return self.test_other_dataloader\n        elif self.dataset_config.used_singular_test_other_time_series is not None and self.test_other_dataloader is not None:\n            del self.test_other_dataloader\n            self.test_other_dataloader = None\n            self.dataset_config.used_singular_test_other_time_series = None\n            self.logger.info(\"Destroyed previous cached test_other_dataloader.\")\n\n        if workers == \"config\":\n            workers = self.dataset_config.test_workers\n\n        # If the dataloader is cached and number of used workers did not change, return the cached dataloader\n        if self.test_other_dataloader and kwargs[\"cache_loader\"] and workers == self.dataset_config.used_test_other_workers:\n            self.logger.debug(\"Returning cached test_other_dataloader.\")\n            return self.test_other_dataloader\n\n        # Update the used workers count\n        self.dataset_config.used_test_other_workers = workers\n\n        # If there's a previously cached dataloader, destroy it\n        if self.test_other_dataloader:\n            del self.test_other_dataloader\n            self.test_other_dataloader = None\n            self.logger.info(\"Destroyed previous cached test_other_dataloader.\")\n\n        # If caching is enabled, create a new cached dataloader\n        if kwargs[\"cache_loader\"]:\n            self.test_other_dataloader = self._get_dataloader(self.test_other_dataset, workers, kwargs['take_all'], self.dataset_config.test_batch_size)\n            self.logger.info(\"Created new cached test_other_dataloader.\")\n            return self.test_other_dataloader\n\n        # If caching is disabled, create a new uncached dataloader\n        self.logger.debug(\"Created new uncached test_other_dataloader.\")\n        return self._get_dataloader(self.test_other_dataset, workers, kwargs['take_all'], self.dataset_config.test_batch_size)\n\n    def get_test_other_df(self, workers: int | Literal[\"config\"] = \"config\", as_single_dataframe: bool = True) -&gt; pd.DataFrame:\n        \"\"\"\n        Creates a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) containing all the data from test_other set grouped by time series.\n\n        This method uses the `test_other_dataloader` with a batch size set to the total number of data in the test_other set. The cached `test_other_dataloader` is cleared during this operation.\n\n        !!! warning \"Memory usage\"\n            The entire test_other set is loaded into memory, which may lead to high memory usage. If working with large test_other set, consider using `get_test_other_dataloader` instead to handle data in batches.\n\n        Parameters:\n            workers: The number of workers to use for loading test_other data. `Default: \"config\"`  \n            as_single_dataframe: Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. `Default: True` \n\n        Returns:\n            A single Pandas DataFrame containing all data from test_other set, or a list of DataFrames (one per time series).\n        \"\"\"\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access test_other_dataloader.\")\n\n        if not self.dataset_config.has_all:\n            raise ValueError(\"Dataloader for test_other set is not available in the dataset configuration.\")\n\n        assert self.test_dataset is not None, \"The test_other_dataset must be initialized before accessing data from test_other set.\"\n\n        ts_ids, time_period = self.dataset_config._get_test_other()\n\n        dataloader = self.get_test_other_dataloader(workers=workers, take_all=True, cache_loader=False)\n        return self._get_df(dataloader, as_single_dataframe, ts_ids, time_period)\n\n    def get_test_other_numpy(self, workers: int | Literal[\"config\"] = \"config\",) -&gt; np.ndarray:\n        \"\"\"\n        Creates a NumPy array containing all the data from test_other set grouped by time series, with the shape `(num_time_series, num_times, num_features)`.\n\n        This method uses the `test_other_dataloader` with a batch size set to the total number of data in the test_other set. The cached `test_other_dataloader` is cleared during this operation.\n\n        !!! warning \"Memory usage\"\n            The entire test_other set is loaded into memory, which may lead to high memory usage. If working with large test_other set, consider using `get_test_other_dataloader` instead to handle data in batches.        \n\n        Parameters:\n            workers: The number of workers to use for loading test_other data. `Default: \"config\"`  \n\n        Returns:\n            A NumPy array containing all the data in test_other set with the shape `(num_time_series, num_times, num_features)`.\n        \"\"\"\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access test_other_dataloader.\")\n\n        if not self.dataset_config.has_all:\n            raise ValueError(\"Dataloader for test_other set is not available in the dataset configuration.\")\n\n        assert self.test_dataset is not None, \"The test_other_dataset must be initialized before accessing data from test_other set.\"\n\n        ts_ids, time_period = self.dataset_config._get_test_other()\n\n        dataloader = self.get_test_other_dataloader(workers=workers, take_all=True, cache_loader=False)\n        return self._get_numpy(dataloader, ts_ids, time_period)\n\n    def set_sliding_window(self, sliding_window_size: int | None | Literal[\"config\"] = \"config\", sliding_window_prediction_size: int | None | Literal[\"config\"] = \"config\",\n                           sliding_window_step: int | None | Literal[\"config\"] = \"config\", set_shared_size: float | int | Literal[\"config\"] = \"config\", workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n        \"\"\"Used for updating sliding window related values set in config.\n\n        Set parameter to `config` to keep it as it is config.\n\n        If exception is thrown during set, no changes are made.\n\n        Affects following configuration. \n\n        | Dataset config                     | Description                                                                                                                                     |\n        | ---------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------- |\n        | `sliding_window_size`              | Number of times in one window. Impacts dataloader behavior. Refer to relevant config for details.                                               |\n        | `sliding_window_prediction_size`   | Number of times to predict from sliding_window_size. Refer to relevant config for details.                                                      |\n        | `sliding_window_step`              | Number of times to move by after each window. Refer to relevant config for details.                                                             |\n        | `set_shared_size`                  | How much times should time periods share. Order of sharing is training set &lt; validation set &lt; test set. Refer to relevant config for details.   |        \n\n        Parameters:\n            sliding_window_size: Number of times in one window. `Defaults: config`.\n            sliding_window_prediction_size: Number of times to predict from sliding_window_size. `Defaults: config`.\n            sliding_window_step: Number of times to move by after each window. `Defaults: config`.\n            set_shared_size: How much times should time periods share. `Defaults: config`.\n            workers: How many workers to use when setting new sliding window values. `Defaults: config`.  \n        \"\"\"\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating sliding window values.\")\n\n        self.update_dataset_config_and_initialize(sliding_window_size=sliding_window_size, sliding_window_prediction_size=sliding_window_prediction_size, sliding_window_step=sliding_window_step, set_shared_size=set_shared_size, workers=workers)\n        self.logger.info(\"Sliding window values has been changed successfuly.\")\n\n    def _initialize_datasets(self) -&gt; None:\n        \"\"\"Called in [`set_dataset_config_and_initialize`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.set_dataset_config_and_initialize], this method initializes the set datasets (train, validation, test, test_other and all). \"\"\"\n\n        if self.dataset_config.has_train:\n            self.train_dataset = SplittedDataset(self.dataset_path,\n                                                 self.dataset_config._get_table_data_path(),\n                                                 self.dataset_config.ts_id_name,\n                                                 self.dataset_config.ts_row_ranges,\n                                                 self.dataset_config.train_time_period,\n                                                 self.dataset_config.features_to_take,\n                                                 self.dataset_config.indices_of_features_to_take_no_ids,\n                                                 self.dataset_config.default_values,\n                                                 self.dataset_config.train_fillers,\n                                                 self.dataset_config.create_scaler_per_time_series,\n                                                 self.dataset_config.include_time,\n                                                 self.dataset_config.include_ts_id,\n                                                 self.dataset_config.time_format,\n                                                 self.dataset_config.train_workers,\n                                                 self.dataset_config.scalers)\n            self.logger.debug(\"train_dataset initiliazed.\")\n\n        if self.dataset_config.has_val:\n            self.val_dataset = SplittedDataset(self.dataset_path,\n                                               self.dataset_config._get_table_data_path(),\n                                               self.dataset_config.ts_id_name,\n                                               self.dataset_config.ts_row_ranges,\n                                               self.dataset_config.val_time_period,\n                                               self.dataset_config.features_to_take,\n                                               self.dataset_config.indices_of_features_to_take_no_ids,\n                                               self.dataset_config.default_values,\n                                               self.dataset_config.val_fillers,\n                                               self.dataset_config.create_scaler_per_time_series,\n                                               self.dataset_config.include_time,\n                                               self.dataset_config.include_ts_id,\n                                               self.dataset_config.time_format,\n                                               self.dataset_config.val_workers,\n                                               self.dataset_config.scalers)\n            self.logger.debug(\"val_dataset initiliazed.\")\n\n        if self.dataset_config.has_test:\n            self.test_dataset = SplittedDataset(self.dataset_path,\n                                                self.dataset_config._get_table_data_path(),\n                                                self.dataset_config.ts_id_name,\n                                                self.dataset_config.ts_row_ranges,\n                                                self.dataset_config.test_time_period,\n                                                self.dataset_config.features_to_take,\n                                                self.dataset_config.indices_of_features_to_take_no_ids,\n                                                self.dataset_config.default_values,\n                                                self.dataset_config.test_fillers,\n                                                self.dataset_config.create_scaler_per_time_series,\n                                                self.dataset_config.include_time,\n                                                self.dataset_config.include_ts_id,\n                                                self.dataset_config.time_format,\n                                                self.dataset_config.test_workers,\n                                                self.dataset_config.scalers)\n            self.logger.debug(\"test_dataset initiliazed.\")\n\n        if self.dataset_config.has_all:\n            self.all_dataset = SplittedDataset(self.dataset_path,\n                                               self.dataset_config._get_table_data_path(),\n                                               self.dataset_config.ts_id_name,\n                                               self.dataset_config.ts_row_ranges,\n                                               self.dataset_config.all_time_period,\n                                               self.dataset_config.features_to_take,\n                                               self.dataset_config.indices_of_features_to_take_no_ids,\n                                               self.dataset_config.default_values,\n                                               self.dataset_config.all_fillers,\n                                               self.dataset_config.create_scaler_per_time_series,\n                                               self.dataset_config.include_time,\n                                               self.dataset_config.include_ts_id,\n                                               self.dataset_config.time_format,\n                                               self.dataset_config.all_workers,\n                                               self.dataset_config.scalers)\n            self.logger.debug(\"all_dataset initiliazed.\")\n\n        if self.dataset_config.has_test_ts_ids:\n            test_other_scaler = None if self.dataset_config.create_scaler_per_time_series else self.dataset_config.scalers\n            self.test_other_dataset = SplittedDataset(self.dataset_path,\n                                                      self.dataset_config._get_table_data_path(),\n                                                      self.dataset_config.ts_id_name,\n                                                      self.dataset_config.test_ts_row_ranges,\n                                                      self.dataset_config.test_time_period,\n                                                      self.dataset_config.features_to_take,\n                                                      self.dataset_config.indices_of_features_to_take_no_ids,\n                                                      self.dataset_config.default_values,\n                                                      self.dataset_config.other_test_fillers,\n                                                      self.dataset_config.create_scaler_per_time_series,\n                                                      self.dataset_config.include_time,\n                                                      self.dataset_config.include_ts_id,\n                                                      self.dataset_config.time_format,\n                                                      self.dataset_config.test_workers,\n                                                      test_other_scaler)\n            self.logger.debug(\"test_other_dataset initiliazed.\")\n\n    def _initialize_scalers_and_details(self, workers: int) -&gt; None:\n        \"\"\"\n        Called in [`set_dataset_config_and_initialize`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.set_dataset_config_and_initialize]. \n\n        Goes through data to validate time series against `nan_threshold`, fit/partial fit `scalers` and prepare `fillers`.\n        \"\"\"\n\n        init_dataset = TimeBasedInitializerDataset(self.dataset_path,\n                                                   self.dataset_config._get_table_data_path(),\n                                                   self.dataset_config.ts_id_name,\n                                                   self.dataset_config.ts_row_ranges,\n                                                   self.dataset_config.all_time_period,\n                                                   self.dataset_config.train_time_period,\n                                                   self.dataset_config.val_time_period,\n                                                   self.dataset_config.test_time_period,\n                                                   self.dataset_config.features_to_take,\n                                                   self.dataset_config.indices_of_features_to_take_no_ids,\n                                                   self.dataset_config.default_values,\n                                                   self.dataset_config.train_fillers,\n                                                   self.dataset_config.val_fillers,\n                                                   self.dataset_config.test_fillers)\n\n        sampler = SequentialSampler(init_dataset)\n        dataloader = DataLoader(init_dataset, num_workers=workers, collate_fn=self._collate_fn, worker_init_fn=TimeBasedInitializerDataset.worker_init_fn, persistent_workers=False, sampler=sampler)\n\n        if workers == 0:\n            init_dataset.pytables_worker_init()\n\n        ts_ids_to_take = []\n\n        self.logger.info(\"Updating config on train/val/test/all and selected time series.\")\n        for i, data in enumerate(tqdm(dataloader, total=len(self.dataset_config.ts_row_ranges))):\n            train_data, train_count_values, val_count_values, test_count_values, all_count_values, val_filler, test_filler = data[0]\n\n            missing_train_percentage = 0\n            missing_val_percentage = 0\n            missing_test_percentage = 0\n            missing_all_percentage = 0\n\n            # Filter time series based on missing data threshold\n            if self.dataset_config.has_train:\n                missing_train_percentage = train_count_values[1] / (train_count_values[0] + train_count_values[1])\n            if self.dataset_config.has_val:\n                missing_val_percentage = val_count_values[1] / (val_count_values[0] + val_count_values[1])\n            if self.dataset_config.has_test:\n                missing_test_percentage = test_count_values[1] / (test_count_values[0] + test_count_values[1])\n            if self.dataset_config.has_all:\n                missing_all_percentage = all_count_values[1] / (all_count_values[0] + all_count_values[1])\n\n            if max(missing_train_percentage, missing_val_percentage, missing_test_percentage, missing_all_percentage) &lt;= self.dataset_config.nan_threshold:\n                ts_ids_to_take.append(i)\n\n                # Fit scalers if required\n                if self.dataset_config.scale_with is not None and train_data is not None and (not self.dataset_config.are_scalers_premade or self.dataset_config.partial_fit_initialized_scalers):\n\n                    if self.dataset_config.are_scalers_premade and self.dataset_config.partial_fit_initialized_scalers:\n                        if self.dataset_config.create_scaler_per_time_series:\n                            self.dataset_config.scalers[i].partial_fit(train_data)\n                        else:\n                            self.dataset_config.scalers.partial_fit(train_data)\n                    else:\n                        if self.dataset_config.create_scaler_per_time_series:\n                            self.dataset_config.scalers[i].fit(train_data)\n                        else:\n                            self.dataset_config.scalers.partial_fit(train_data)\n\n                # Only update fillers for val/test because train doesnt need to know about previous data\n                if self.dataset_config.fill_missing_with is not None:\n                    if self.dataset_config.has_val:\n                        self.dataset_config.val_fillers[i] = val_filler\n                    if self.dataset_config.has_test:\n                        self.dataset_config.test_fillers[i] = test_filler\n\n        if workers == 0:\n            init_dataset.cleanup()\n\n        if len(ts_ids_to_take) == 0:\n            raise ValueError(\"No valid time series left in ts_ids after applying nan_threshold.\")\n\n        # Update config based on filtered time series\n        self.dataset_config.ts_row_ranges = self.dataset_config.ts_row_ranges[ts_ids_to_take]\n        self.dataset_config.ts_ids = self.dataset_config.ts_ids[ts_ids_to_take]\n\n        if self.dataset_config.scale_with is not None:\n            if self.dataset_config.create_scaler_per_time_series:\n                self.dataset_config.scalers = self.dataset_config.scalers[ts_ids_to_take]\n\n        if self.dataset_config.fill_missing_with is not None:\n            if self.dataset_config.has_train:\n                self.dataset_config.train_fillers = self.dataset_config.train_fillers[ts_ids_to_take]\n            if self.dataset_config.has_val:\n                self.dataset_config.val_fillers = self.dataset_config.val_fillers[ts_ids_to_take]\n            if self.dataset_config.has_test:\n                self.dataset_config.test_fillers = self.dataset_config.test_fillers[ts_ids_to_take]\n            if self.dataset_config.has_all:\n                self.dataset_config.all_fillers = self.dataset_config.all_fillers[ts_ids_to_take]\n\n        self.dataset_config.used_ts_row_ranges = self.dataset_config.ts_row_ranges\n        self.dataset_config.used_ts_ids = self.dataset_config.ts_ids\n        self.dataset_config.used_times = self.dataset_config.all_time_period\n        self.dataset_config.used_fillers = self.dataset_config.all_fillers\n\n        self.logger.debug(\"ts_ids updated: %s time series left.\", len(ts_ids_to_take))\n\n        # Check if going through test_other data is needed\n        if self.dataset_config.test_ts_row_ranges is not None:\n            test_init_dataset = TimeBasedInitializerDataset(self.dataset_path,\n                                                            self.dataset_config._get_table_data_path(),\n                                                            self.dataset_config.ts_id_name,\n                                                            self.dataset_config.test_ts_row_ranges,\n                                                            None,\n                                                            None,\n                                                            None,\n                                                            self.dataset_config.test_time_period,\n                                                            self.dataset_config.features_to_take,\n                                                            self.dataset_config.indices_of_features_to_take_no_ids,\n                                                            self.dataset_config.default_values,\n                                                            None,\n                                                            None,\n                                                            self.dataset_config.other_test_fillers)\n\n            sampler = SequentialSampler(test_init_dataset)\n            dataloader = DataLoader(test_init_dataset, num_workers=workers, collate_fn=self._collate_fn, worker_init_fn=TimeBasedInitializerDataset.worker_init_fn, persistent_workers=False, sampler=sampler)\n\n            if workers == 0:\n                test_init_dataset.pytables_worker_init()\n\n            test_ts_ids_to_take = []\n\n            self.logger.info(\"Updating config on test_other and selected time series.\")\n            for i, data in enumerate(tqdm(dataloader, total=len(self.dataset_config.test_ts_row_ranges))):\n                _, _, _, test_count_values, _, _, test_filler = data[0]\n\n                missing_test_percentage = test_count_values[1] / (test_count_values[0] + test_count_values[1])\n\n                if missing_test_percentage &lt;= self.dataset_config.nan_threshold:\n                    test_ts_ids_to_take.append(i)\n\n                    if self.dataset_config.fill_missing_with is not None:\n                        self.dataset_config.other_test_fillers[i] = test_filler\n\n            if workers == 0:\n                test_init_dataset.cleanup()\n\n            if len(test_ts_ids_to_take) == 0:\n                raise ValueError(\"No valid time series left in test_ts_ids after applying nan_threshold.\")\n\n            # Update config based on filtered time series\n            self.dataset_config.test_ts_row_ranges = self.dataset_config.test_ts_row_ranges[test_ts_ids_to_take]\n            self.dataset_config.test_ts_ids = self.dataset_config.test_ts_ids[test_ts_ids_to_take]\n\n            if self.dataset_config.fill_missing_with is not None:\n                if self.dataset_config.other_test_fillers is not None:\n                    self.dataset_config.other_test_fillers = self.dataset_config.other_test_fillers[test_ts_ids_to_take]\n\n            self.dataset_config.used_ts_row_ranges = np.concatenate((self.dataset_config.used_ts_row_ranges, self.dataset_config.test_ts_row_ranges))\n            self.dataset_config.used_ts_ids = np.concatenate((self.dataset_config.used_ts_ids, self.dataset_config.test_ts_ids))\n\n            if self.dataset_config.fill_missing_with is not None:\n                self.dataset_config.used_fillers = np.concatenate((self.dataset_config.used_fillers, self.dataset_config.other_test_fillers))\n\n            self.logger.debug(\"test_ts_ids updated: %s time series left.\", len(test_ts_ids_to_take))\n\n    def _update_export_config_copy(self) -&gt; None:\n        \"\"\"\n        Called at the end of [`set_dataset_config_and_initialize`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.set_dataset_config_and_initialize] or when changing config values. \n\n        Updates values of config used for saving config.\n        \"\"\"\n        self._export_config_copy.database_name = self.database_name\n\n        if self.dataset_config.has_train:\n            self._export_config_copy.ts_ids = self.dataset_config.ts_ids.copy()\n            self.logger.debug(\"Updated ts_ids of _export_config_copy.\")\n\n        if self.dataset_config.has_test_ts_ids:\n            self._export_config_copy.test_ts_ids = self.dataset_config.test_ts_ids.copy()\n            self.logger.debug(\"Updated test_ts_ids of _export_config_copy\")\n\n        super(TimeBasedCesnetDataset, self)._update_export_config_copy()\n\n    def _get_singular_time_series_dataset(self, parent_dataset: SplittedDataset, ts_id: int) -&gt; SplittedDataset:\n        \"\"\"Returns dataset for single time series \"\"\"\n\n        temp = np.where(np.isin(parent_dataset.ts_row_ranges[self.ts_id_name], [ts_id]))[0]\n\n        if len(temp) == 0:\n            raise ValueError(f\"ts_id {ts_id} was not found in valid time series for this set. Available time series are: {parent_dataset.ts_row_ranges[self.ts_id_name]}\")\n\n        time_series_position = temp[0]\n\n        filler = None if parent_dataset.fillers is None else parent_dataset.fillers[time_series_position:time_series_position + 1]\n\n        scaler = None\n        if parent_dataset.feature_scalers is not None:\n            scaler = parent_dataset.feature_scalers[time_series_position:time_series_position + 1] if parent_dataset.is_scaler_per_time_series else parent_dataset.feature_scalers\n\n        dataset = SplittedDataset(self.dataset_path,\n                                  self.dataset_config._get_table_data_path(),\n                                  self.dataset_config.ts_id_name,\n                                  parent_dataset.ts_row_ranges[time_series_position: time_series_position + 1],\n                                  parent_dataset.time_period,\n                                  self.dataset_config.features_to_take,\n                                  self.dataset_config.indices_of_features_to_take_no_ids,\n                                  self.dataset_config.default_values,\n                                  filler,\n                                  self.dataset_config.create_scaler_per_time_series,\n                                  self.dataset_config.include_time,\n                                  self.dataset_config.include_ts_id,\n                                  self.dataset_config.time_format,\n                                  0,\n                                  scaler)\n        self.logger.debug(\"Singular time series dataset initiliazed.\")\n\n        return dataset\n\n    def _get_dataloader(self, dataset: SplittedDataset, workers: int | Literal[\"config\"], take_all: bool, batch_size: int, **kwargs) -&gt; DataLoader:\n        \"\"\" Set time based dataloader for this dataset. \"\"\"\n\n        return self._get_time_based_dataloader(dataset, workers, take_all, batch_size)\n\n    def _clear(self) -&gt; None:\n        \"\"\" Clears set data. Mainly called when initializing new config. \"\"\"\n\n        self.test_other_dataloader = None\n        self.test_other_dataset = None\n        super(TimeBasedCesnetDataset, self)._clear()\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.set_dataset_config_and_initialize","title":"set_dataset_config_and_initialize","text":"<pre><code>set_dataset_config_and_initialize(dataset_config: TimeBasedConfig, display_config_details: bool = True, workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Initialize training set, validation est, test set etc.. This method must be called before any data can be accessed. It is required for the final initialization of <code>dataset_config</code>.</p> <p>The following configuration attributes are used during initialization:</p> Dataset config Description <code>init_workers</code> Specifies the number of workers to use for initialization. Applied when <code>workers</code> = \"config\". <code>partial_fit_initialized_scalers</code> Determines whether initialized scalers should be partially fitted on the training data. <code>nan_threshold</code> Filters out time series with missing values exceeding the specified threshold. <p>Parameters:</p> Name Type Description Default <code>dataset_config</code> <code>TimeBasedConfig</code> <p>Desired configuration of the dataset.</p> required <code>display_config_details</code> <code>bool</code> <p>Flag indicating whether to display the configuration values after initialization. <code>Default: True</code> </p> <code>True</code> <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use during initialization. <code>Default: \"config\"</code></p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\time_based_cesnet_dataset.py</code> <pre><code>def set_dataset_config_and_initialize(self, dataset_config: TimeBasedConfig, display_config_details: bool = True, workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"\n    Initialize training set, validation est, test set etc.. This method must be called before any data can be accessed. It is required for the final initialization of [`dataset_config`][cesnet_tszoo.configs.time_based_config.TimeBasedConfig].\n\n    The following configuration attributes are used during initialization:\n\n    | Dataset config                    | Description                                                                                    |\n    | --------------------------------- | ---------------------------------------------------------------------------------------------- |\n    | `init_workers`                    | Specifies the number of workers to use for initialization. Applied when `workers` = \"config\".  |\n    | `partial_fit_initialized_scalers` | Determines whether initialized scalers should be partially fitted on the training data.        |\n    | `nan_threshold`                   | Filters out time series with missing values exceeding the specified threshold.                 |\n\n    Parameters:\n        dataset_config: Desired configuration of the dataset.\n        display_config_details: Flag indicating whether to display the configuration values after initialization. `Default: True`  \n        workers: The number of workers to use during initialization. `Default: \"config\"`  \n    \"\"\"\n\n    assert isinstance(dataset_config, TimeBasedConfig), \"TimeBasedCesnetDataset can only use TimeBasedConfig.\"\n\n    super(TimeBasedCesnetDataset, self).set_dataset_config_and_initialize(dataset_config, display_config_details, workers)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_train_dataloader","title":"get_train_dataloader","text":"<pre><code>get_train_dataloader(ts_id: int | None = None, workers: int | Literal['config'] = 'config', **kwargs) -&gt; DataLoader\n</code></pre> <p>Returns a PyTorch <code>DataLoader</code> for training set.</p> <p>The <code>DataLoader</code> is created on the first call and cached for subsequent use.  The cached dataloader is cleared when either <code>get_train_df</code> or <code>get_train_numpy</code> is called.</p> <p>The structure of the returned batch depends on the <code>time_format</code> and whether <code>sliding_window_size</code> is used:</p> <ul> <li>When <code>sliding_window_size</code> is used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> <li><code>np.ndarray</code> of times with shape <code>(times - 1)</code></li> <li><code>np.ndarray</code> of time with shape <code>(1)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> </ul> </li> </ul> </li> <li>When <code>sliding_window_size</code> is not used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> <li><code>np.ndarray</code> of time with shape <code>(times)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> </ul> </li> </ul> </li> </ul> <p>The <code>DataLoader</code> is configured with the following config attributes:</p> Dataset config Description <code>train_batch_size</code> Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>sliding_window_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_prediction_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_step</code> Available only for time-based datasets. Number of times to move by after each window. <code>train_workers</code> Specifies the number of workers to use for loading train data. Applied when <code>workers</code> = \"config\". <code>train_dataloader_order</code> Available only for series-based datasets. Whether to load train data in sequential or random order. See cesnet_tszoo.utils.enums.DataloaderOrder. <code>random_state</code> Seed for loading train data in random order. <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading train data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>ts_id</code> <code>int | None</code> <p>Specifies time series to take. If None returns all time series as normal. <code>Default: \"None\"</code></p> <code>None</code> <p>Returns:</p> Type Description <code>DataLoader</code> <p>An iterable <code>DataLoader</code> containing data from training set.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_train_dataloader(self, ts_id: int | None = None, workers: int | Literal[\"config\"] = \"config\", **kwargs) -&gt; DataLoader:\n    \"\"\"\n    Returns a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for training set.\n\n    The `DataLoader` is created on the first call and cached for subsequent use. &lt;br/&gt;\n    The cached dataloader is cleared when either [`get_train_df`][cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_train_df] or [`get_train_numpy`][cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_train_numpy] is called.\n\n    The structure of the returned batch depends on the `time_format` and whether `sliding_window_size` is used:\n\n    - When `sliding_window_size` is used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n            - `np.ndarray` of times with shape `(times - 1)`\n            - `np.ndarray` of time with shape `(1)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n    - When `sliding_window_size` is not used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n            - `np.ndarray` of time with shape `(times)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n\n    The `DataLoader` is configured with the following config attributes:\n\n    | Dataset config                    | Description                                                                                                                                            |\n    | --------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ |\n    | `train_batch_size`                | Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.                      |\n    | `sliding_window_size`             | Available only for time-based datasets. Modifies the shape of the returned data.                                                                       |\n    | `sliding_window_prediction_size`  | Available only for time-based datasets. Modifies the shape of the returned data.                                                                       |\n    | `sliding_window_step`             | Available only for time-based datasets. Number of times to move by after each window.                                                     |\n    | `train_workers`                   | Specifies the number of workers to use for loading train data. Applied when `workers` = \"config\".                                                      |\n    | `train_dataloader_order`          | Available only for series-based datasets. Whether to load train data in sequential or random order. See [cesnet_tszoo.utils.enums.DataloaderOrder][].  |\n    | `random_state`                    | Seed for loading train data in random order.                                                                                                           |                 \n\n    Parameters:\n        workers: The number of workers to use for loading train data. `Default: \"config\"` \n        ts_id: Specifies time series to take. If None returns all time series as normal. `Default: \"None\"`\n\n    Returns:\n        An iterable `DataLoader` containing data from training set.          \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access train_dataloader.\")\n\n    if not self.dataset_config.has_train:\n        raise ValueError(\"Dataloader for training set is not available in the dataset configuration.\")\n\n    assert self.train_dataset is not None, \"The train_dataset must be initialized before accessing data from training set.\"\n\n    defaultKwargs = {'take_all': False, \"cache_loader\": True}\n    kwargs = {**defaultKwargs, **kwargs}\n\n    if ts_id is not None:\n\n        if ts_id == self.dataset_config.used_singular_train_time_series and self.train_dataloader is not None:\n            self.logger.debug(\"Returning cached train_dataloader.\")\n            return self.train_dataloader\n\n        dataset = self._get_singular_time_series_dataset(self.train_dataset, ts_id)\n        self.dataset_config.used_singular_train_time_series = ts_id\n        if self.train_dataloader:\n            del self.train_dataloader\n            self.train_dataloader = None\n            self.logger.info(\"Destroyed previous cached train_dataloader.\")\n\n        self.dataset_config.used_train_workers = 0\n        self.train_dataloader = self._get_dataloader(dataset, 0, False, self.dataset_config.train_batch_size)\n        self.logger.info(\"Created new cached train_dataloader.\")\n        return self.train_dataloader\n    elif self.dataset_config.used_singular_train_time_series is not None and self.train_dataloader is not None:\n        del self.train_dataloader\n        self.train_dataloader = None\n        self.dataset_config.used_singular_train_time_series = None\n        self.logger.info(\"Destroyed previous cached train_dataloader.\")\n\n    if workers == \"config\":\n        workers = self.dataset_config.train_workers\n\n    # If the dataloader is cached and number of used workers did not change, return the cached dataloader\n    if self.train_dataloader and kwargs[\"cache_loader\"] and workers == self.dataset_config.used_train_workers:\n        self.logger.debug(\"Returning cached train_dataloader.\")\n        return self.train_dataloader\n\n    # Update the used workers count\n    self.dataset_config.used_train_workers = workers\n\n    # If there's a previously cached dataloader, destroy it\n    if self.train_dataloader:\n        del self.train_dataloader\n        self.train_dataloader = None\n        self.logger.info(\"Destroyed previous cached train_dataloader.\")\n\n    # If caching is enabled, create a new cached dataloader\n    if kwargs[\"cache_loader\"]:\n        self.train_dataloader = self._get_dataloader(self.train_dataset, workers, kwargs['take_all'], self.dataset_config.train_batch_size, order=self.dataset_config.train_dataloader_order)\n        self.logger.info(\"Created new cached train_dataloader.\")\n        return self.train_dataloader\n\n    # If caching is disabled, create a new uncached dataloader\n    self.logger.debug(\"Created new uncached train_dataloader.\")\n    return self._get_dataloader(self.train_dataset, workers, kwargs['take_all'], self.dataset_config.train_batch_size, order=self.dataset_config.train_dataloader_order)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_val_dataloader","title":"get_val_dataloader","text":"<pre><code>get_val_dataloader(ts_id: int | None = None, workers: int | Literal['config'] = 'config', **kwargs) -&gt; DataLoader\n</code></pre> <p>Returns a PyTorch <code>DataLoader</code> for validation set.</p> <p>The <code>DataLoader</code> is created on the first call and cached for subsequent use.  The cached dataloader is cleared when either <code>get_val_df</code> or <code>get_val_numpy</code> is called.</p> <p>The structure of the returned batch depends on the <code>time_format</code> and whether <code>sliding_window_size</code> is used:</p> <ul> <li>When <code>sliding_window_size</code> is used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> <li><code>np.ndarray</code> of times with shape <code>(times - 1)</code></li> <li><code>np.ndarray</code> of time with shape <code>(1)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> </ul> </li> </ul> </li> <li>When <code>sliding_window_size</code> is not used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> <li><code>np.ndarray</code> of time with shape <code>(times)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> </ul> </li> </ul> </li> </ul> <p>The <code>DataLoader</code> is configured with the following config attributes:</p> Dataset config Description <code>val_batch_size</code> Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>sliding_window_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_prediction_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_step</code> Available only for time-based datasets. Number of times to move by after each window. <code>val_workers</code> Specifies the number of workers to use for loading validation data. Applied when <code>workers</code> = \"config\". <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading validation data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>ts_id</code> <code>int | None</code> <p>Specifies time series to take. If None returns all time series as normal. <code>Default: \"None\"</code></p> <code>None</code> <p>Returns:</p> Type Description <code>DataLoader</code> <p>An iterable <code>DataLoader</code> containing data from validation set.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_val_dataloader(self, ts_id: int | None = None, workers: int | Literal[\"config\"] = \"config\", **kwargs) -&gt; DataLoader:\n    \"\"\"\n    Returns a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for validation set.\n\n    The `DataLoader` is created on the first call and cached for subsequent use. &lt;br/&gt;\n    The cached dataloader is cleared when either [`get_val_df`][cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_val_df] or [`get_val_numpy`][cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_val_numpy] is called.\n\n    The structure of the returned batch depends on the `time_format` and whether `sliding_window_size` is used:\n\n    - When `sliding_window_size` is used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n            - `np.ndarray` of times with shape `(times - 1)`\n            - `np.ndarray` of time with shape `(1)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n    - When `sliding_window_size` is not used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n            - `np.ndarray` of time with shape `(times)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n\n    The `DataLoader` is configured with the following config attributes:\n\n    | Dataset config                    | Description                                                                                                                               |\n    | --------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------- |\n    | `val_batch_size`                  | Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.         |\n    | `sliding_window_size`             | Available only for time-based datasets. Modifies the shape of the returned data.                                                          |\n    | `sliding_window_prediction_size`  | Available only for time-based datasets. Modifies the shape of the returned data.                                                          |\n    | `sliding_window_step`             | Available only for time-based datasets. Number of times to move by after each window.                                                     |\n    | `val_workers`                     | Specifies the number of workers to use for loading validation data. Applied when `workers` = \"config\".                                    |\n\n    Parameters:\n        workers: The number of workers to use for loading validation data. `Default: \"config\"`  \n        ts_id: Specifies time series to take. If None returns all time series as normal. `Default: \"None\"`\n\n    Returns:\n        An iterable `DataLoader` containing data from validation set.        \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access val_dataloader.\")\n\n    if not self.dataset_config.has_val:\n        raise ValueError(\"Dataloader for validation set is not available in the dataset configuration.\")\n\n    assert self.val_dataset is not None, \"The val_dataset must be initialized before accessing data from validation set.\"\n\n    defaultKwargs = {'take_all': False, \"cache_loader\": True}\n    kwargs = {**defaultKwargs, **kwargs}\n\n    if ts_id is not None:\n\n        if ts_id == self.dataset_config.used_singular_val_time_series and self.val_dataloader is not None:\n            self.logger.debug(\"Returning cached val_dataloader.\")\n            return self.val_dataloader\n\n        dataset = self._get_singular_time_series_dataset(self.val_dataset, ts_id)\n        self.dataset_config.used_singular_val_time_series = ts_id\n        if self.val_dataloader:\n            del self.val_dataloader\n            self.val_dataloader = None\n            self.logger.info(\"Destroyed previous cached val_dataloader.\")\n\n        self.dataset_config.used_val_workers = 0\n        self.val_dataloader = self._get_dataloader(dataset, 0, False, self.dataset_config.val_batch_size)\n        self.logger.info(\"Created new cached val_dataloader.\")\n        return self.val_dataloader\n    elif self.dataset_config.used_singular_val_time_series is not None and self.val_dataloader is not None:\n        del self.val_dataloader\n        self.val_dataloader = None\n        self.dataset_config.used_singular_val_time_series = None\n        self.logger.info(\"Destroyed previous cached val_dataloader.\")\n\n    if workers == \"config\":\n        workers = self.dataset_config.val_workers\n\n    # If the dataloader is cached and number of used workers did not change, return the cached dataloader\n    if self.val_dataloader and kwargs[\"cache_loader\"] and workers == self.dataset_config.used_val_workers:\n        self.logger.debug(\"Returning cached val_dataloader.\")\n        return self.val_dataloader\n\n    # Update the used workers count\n    self.dataset_config.used_val_workers = workers\n\n    # If there's a previously cached dataloader, destroy it\n    if self.val_dataloader:\n        del self.val_dataloader\n        self.val_dataloader = None\n        self.logger.info(\"Destroyed previous cached val_dataloader.\")\n\n    # If caching is enabled, create a new cached dataloader\n    if kwargs[\"cache_loader\"]:\n        self.val_dataloader = self._get_dataloader(self.val_dataset, workers, kwargs['take_all'], self.dataset_config.val_batch_size)\n        self.logger.info(\"Created new cached val_dataloader.\")\n        return self.val_dataloader\n\n    # If caching is disabled, create a new uncached dataloader\n    self.logger.debug(\"Created new uncached val_dataloader.\")\n    return self._get_dataloader(self.val_dataset, workers, kwargs['take_all'], self.dataset_config.val_batch_size)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_dataloader","title":"get_test_dataloader","text":"<pre><code>get_test_dataloader(ts_id: int | None = None, workers: int | Literal['config'] = 'config', **kwargs) -&gt; DataLoader\n</code></pre> <p>Returns a PyTorch <code>DataLoader</code> for test set.</p> <p>The <code>DataLoader</code> is created on the first call and cached for subsequent use.  The cached dataloader is cleared when either <code>get_test_df</code> or <code>get_test_numpy</code> is called.</p> <p>The structure of the returned batch depends on the <code>time_format</code> and whether <code>sliding_window_size</code> is used:</p> <ul> <li>When <code>sliding_window_size</code> is used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> <li><code>np.ndarray</code> of times with shape <code>(times - 1)</code></li> <li><code>np.ndarray</code> of time with shape <code>(1)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> </ul> </li> </ul> </li> <li>When <code>sliding_window_size</code> is not used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> <li><code>np.ndarray</code> of time with shape <code>(times)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> </ul> </li> </ul> </li> </ul> <p>The <code>DataLoader</code> is configured with the following config attributes:</p> Dataset config Description <code>test_batch_size</code> Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>sliding_window_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_prediction_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_step</code> Available only for time-based datasets. Number of times to move by after each window. <code>test_workers</code> Specifies the number of workers to use for loading test data. Applied when <code>workers</code> = \"config\". <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading test data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>ts_id</code> <code>int | None</code> <p>Specifies time series to take. If None returns all time series as normal. <code>Default: \"None\"</code></p> <code>None</code> <p>Returns:</p> Type Description <code>DataLoader</code> <p>An iterable <code>DataLoader</code> containing data from test set.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_test_dataloader(self, ts_id: int | None = None, workers: int | Literal[\"config\"] = \"config\", **kwargs) -&gt; DataLoader:\n    \"\"\"\n    Returns a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for test set.\n\n    The `DataLoader` is created on the first call and cached for subsequent use. &lt;br/&gt;\n    The cached dataloader is cleared when either [`get_test_df`][cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_test_df] or [`get_test_numpy`][cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_test_numpy] is called.\n\n    The structure of the returned batch depends on the `time_format` and whether `sliding_window_size` is used:\n\n    - When `sliding_window_size` is used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n            - `np.ndarray` of times with shape `(times - 1)`\n            - `np.ndarray` of time with shape `(1)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n    - When `sliding_window_size` is not used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n            - `np.ndarray` of time with shape `(times)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n\n    The `DataLoader` is configured with the following config attributes:\n\n    | Dataset config                     | Description                                                                                                                               |\n    | ---------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------- |\n    | `test_batch_size`                  | Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.         |\n    | `sliding_window_size`              | Available only for time-based datasets. Modifies the shape of the returned data.                                                          |\n    | `sliding_window_prediction_size`   | Available only for time-based datasets. Modifies the shape of the returned data.                                                          |\n    | `sliding_window_step`              | Available only for time-based datasets. Number of times to move by after each window.                                                     |\n    | `test_workers`                     | Specifies the number of workers to use for loading test data. Applied when `workers` = \"config\".                                          |\n\n    Parameters:\n        workers: The number of workers to use for loading test data. `Default: \"config\"`  \n        ts_id: Specifies time series to take. If None returns all time series as normal. `Default: \"None\"`\n\n    Returns:\n        An iterable `DataLoader` containing data from test set.        \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access test_dataloader.\")\n\n    if not self.dataset_config.has_all:\n        raise ValueError(\"Dataloader for test set is not available in the dataset configuration.\")\n\n    assert self.test_dataset is not None, \"The test_dataset must be initialized before accessing data from test set.\"\n\n    defaultKwargs = {'take_all': False, \"cache_loader\": True}\n    kwargs = {**defaultKwargs, **kwargs}\n\n    if ts_id is not None:\n\n        if ts_id == self.dataset_config.used_singular_test_time_series and self.test_dataloader is not None:\n            self.logger.debug(\"Returning cached test_dataloader.\")\n            return self.test_dataloader\n\n        dataset = self._get_singular_time_series_dataset(self.test_dataset, ts_id)\n        self.dataset_config.used_singular_test_time_series = ts_id\n        if self.test_dataloader:\n            del self.test_dataloader\n            self.test_dataloader = None\n            self.logger.info(\"Destroyed previous cached test_dataloader.\")\n\n        self.dataset_config.used_test_workers = 0\n        self.test_dataloader = self._get_dataloader(dataset, 0, False, self.dataset_config.test_batch_size)\n        self.logger.info(\"Created new cached test_dataloader.\")\n        return self.test_dataloader\n    elif self.dataset_config.used_singular_test_time_series is not None and self.test_dataloader is not None:\n        del self.test_dataloader\n        self.test_dataloader = None\n        self.dataset_config.used_singular_test_time_series = None\n        self.logger.info(\"Destroyed previous cached test_dataloader.\")\n\n    if workers == \"config\":\n        workers = self.dataset_config.test_workers\n\n    # If the dataloader is cached and number of used workers did not change, return the cached dataloader\n    if self.test_dataloader and kwargs[\"cache_loader\"] and workers == self.dataset_config.used_test_workers:\n        self.logger.debug(\"Returning cached test_dataloader.\")\n        return self.test_dataloader\n\n    # Update the used workers count\n    self.dataset_config.used_test_workers = workers\n\n    # If there's a previously cached dataloader, destroy it\n    if self.test_dataloader:\n        del self.test_dataloader\n        self.test_dataloader = None\n        self.logger.info(\"Destroyed previous cached test_dataloader.\")\n\n    # If caching is enabled, create a new cached dataloader\n    if kwargs[\"cache_loader\"]:\n        self.test_dataloader = self._get_dataloader(self.test_dataset, workers, kwargs['take_all'], self.dataset_config.test_batch_size)\n        self.logger.info(\"Created new cached test_dataloader.\")\n        return self.test_dataloader\n\n    # If caching is disabled, create a new uncached dataloader\n    self.logger.debug(\"Created new uncached test_dataloader.\")\n    return self._get_dataloader(self.test_dataset, workers, kwargs['take_all'], self.dataset_config.test_batch_size)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_other_dataloader","title":"get_test_other_dataloader","text":"<pre><code>get_test_other_dataloader(ts_id: int | None = None, workers: int | Literal['config'] = 'config', **kwargs) -&gt; DataLoader\n</code></pre> <p>Returns a PyTorch <code>DataLoader</code> for test_other set.</p> <p>The <code>DataLoader</code> is created on the first call and cached for subsequent use.  The cached dataloader is cleared when either <code>get_test_other_df</code> or <code>get_test_other_numpy</code> is called.</p> <p>The structure of the returned batch depends on the <code>time_format</code> and whether <code>sliding_window_size</code> is used:</p> <ul> <li>When <code>sliding_window_size</code> is used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> <li><code>np.ndarray</code> of times with shape <code>(times - 1)</code></li> <li><code>np.ndarray</code> of time with shape <code>(1)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> </ul> </li> </ul> </li> <li>When <code>sliding_window_size</code> is not used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> <li><code>np.ndarray</code> of time with shape <code>(times)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> </ul> </li> </ul> </li> </ul> <p>The <code>DataLoader</code> is configured with the following config attributes:</p> Dataset config Description <code>test_batch_size</code> Number of times for time series per batch. <code>sliding_window_size</code> Modifies the shape of the returned data. <code>sliding_window_prediction_size</code> Modifies the shape of the returned data. <code>sliding_window_step</code> Available only for time-based datasets. Number of times to move by after each window. <code>test_workers</code> Specifies the number of workers to use for loading test_other data. Applied when <code>workers</code> = \"config\". <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading test_other data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>ts_id</code> <code>int | None</code> <p>Specifies time series to take. If None returns all time series as normal. <code>Default: \"None\"</code></p> <code>None</code> <p>Returns:</p> Type Description <code>DataLoader</code> <p>An iterable <code>DataLoader</code> containing data from test_other set.</p> Source code in <code>cesnet_tszoo\\datasets\\time_based_cesnet_dataset.py</code> <pre><code>def get_test_other_dataloader(self, ts_id: int | None = None, workers: int | Literal[\"config\"] = \"config\", **kwargs) -&gt; DataLoader:\n    \"\"\"\n    Returns a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for test_other set.\n\n    The `DataLoader` is created on the first call and cached for subsequent use. &lt;br/&gt;\n    The cached dataloader is cleared when either [`get_test_other_df`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_other_df] or [`get_test_other_numpy`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_other_numpy] is called.\n\n    The structure of the returned batch depends on the `time_format` and whether `sliding_window_size` is used:\n\n    - When `sliding_window_size` is used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n            - `np.ndarray` of times with shape `(times - 1)`\n            - `np.ndarray` of time with shape `(1)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n    - When `sliding_window_size` is not used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n            - `np.ndarray` of time with shape `(times)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n\n    The `DataLoader` is configured with the following config attributes:\n\n    | Dataset config                     | Description                                                                                                                               |\n    | ---------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------- |\n    | `test_batch_size`                  | Number of times for time series per batch.                                                                                                |\n    | `sliding_window_size`              | Modifies the shape of the returned data.                                                                                                  |\n    | `sliding_window_prediction_size`   | Modifies the shape of the returned data.                                                                                                  |\n    | `sliding_window_step`              | Available only for time-based datasets. Number of times to move by after each window.                                                     |\n    | `test_workers`                     | Specifies the number of workers to use for loading test_other data. Applied when `workers` = \"config\".                                    |\n\n    Parameters:\n        workers: The number of workers to use for loading test_other data. `Default: \"config\"`  \n        ts_id: Specifies time series to take. If None returns all time series as normal. `Default: \"None\"`\n\n    Returns:\n        An iterable `DataLoader` containing data from test_other set.         \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access test_other_dataloader.\")\n\n    if not self.dataset_config.has_all:\n        raise ValueError(\"Dataloader for test_other set is not available in the dataset configuration.\")\n\n    assert self.test_dataset is not None, \"The test_other_dataset must be initialized before accessing data from test_other set.\"\n\n    defaultKwargs = {'take_all': False, \"cache_loader\": True}\n    kwargs = {**defaultKwargs, **kwargs}\n\n    if ts_id is not None:\n\n        if ts_id == self.dataset_config.used_singular_test_other_time_series and self.test_other_dataloader is not None:\n            self.logger.debug(\"Returning cached test_other_dataloader.\")\n            return self.test_other_dataloader\n\n        dataset = self._get_singular_time_series_dataset(self.test_other_dataset, ts_id)\n        self.dataset_config.used_singular_test_other_time_series = ts_id\n        if self.test_other_dataloader:\n            del self.test_other_dataloader\n            self.test_other_dataloader = None\n            self.logger.info(\"Destroyed previous cached test_other_dataloader.\")\n\n        self.dataset_config.used_test_other_workers = 0\n        self.test_other_dataloader = self._get_dataloader(dataset, 0, False, self.dataset_config.test_batch_size)\n        self.logger.info(\"Created new cached test_other_dataloader.\")\n        return self.test_other_dataloader\n    elif self.dataset_config.used_singular_test_other_time_series is not None and self.test_other_dataloader is not None:\n        del self.test_other_dataloader\n        self.test_other_dataloader = None\n        self.dataset_config.used_singular_test_other_time_series = None\n        self.logger.info(\"Destroyed previous cached test_other_dataloader.\")\n\n    if workers == \"config\":\n        workers = self.dataset_config.test_workers\n\n    # If the dataloader is cached and number of used workers did not change, return the cached dataloader\n    if self.test_other_dataloader and kwargs[\"cache_loader\"] and workers == self.dataset_config.used_test_other_workers:\n        self.logger.debug(\"Returning cached test_other_dataloader.\")\n        return self.test_other_dataloader\n\n    # Update the used workers count\n    self.dataset_config.used_test_other_workers = workers\n\n    # If there's a previously cached dataloader, destroy it\n    if self.test_other_dataloader:\n        del self.test_other_dataloader\n        self.test_other_dataloader = None\n        self.logger.info(\"Destroyed previous cached test_other_dataloader.\")\n\n    # If caching is enabled, create a new cached dataloader\n    if kwargs[\"cache_loader\"]:\n        self.test_other_dataloader = self._get_dataloader(self.test_other_dataset, workers, kwargs['take_all'], self.dataset_config.test_batch_size)\n        self.logger.info(\"Created new cached test_other_dataloader.\")\n        return self.test_other_dataloader\n\n    # If caching is disabled, create a new uncached dataloader\n    self.logger.debug(\"Created new uncached test_other_dataloader.\")\n    return self._get_dataloader(self.test_other_dataset, workers, kwargs['take_all'], self.dataset_config.test_batch_size)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_all_dataloader","title":"get_all_dataloader","text":"<pre><code>get_all_dataloader(ts_id: int | None = None, workers: int | Literal['config'] = 'config', **kwargs) -&gt; DataLoader\n</code></pre> <p>Returns a PyTorch <code>DataLoader</code> for all set.</p> <p>The <code>DataLoader</code> is created on the first call and cached for subsequent use.  The cached dataloader is cleared when either <code>get_all_df</code> or <code>get_all_numpy</code> is called.</p> <p>The structure of the returned batch depends on the <code>time_format</code> and whether <code>sliding_window_size</code> is used:</p> <ul> <li>When <code>sliding_window_size</code> is used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> <li><code>np.ndarray</code> of times with shape <code>(times - 1)</code></li> <li><code>np.ndarray</code> of time with shape <code>(1)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> </ul> </li> </ul> </li> <li>When <code>sliding_window_size</code> is not used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> <li><code>np.ndarray</code> of time with shape <code>(times)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> </ul> </li> </ul> </li> </ul> <p>The <code>DataLoader</code> is configured with the following config attributes:</p> Dataset config Description <code>all_batch_size</code> Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>sliding_window_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_prediction_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_step</code> Available only for time-based datasets. Number of times to move by after each window. <code>all_workers</code> Specifies the number of workers to use for loading all data. Applied when <code>workers</code> = \"config\". <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading all data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>ts_id</code> <code>int | None</code> <p>Specifies time series to take. If None returns all time series as normal. <code>Default: \"None\"</code></p> <code>None</code> <p>Returns:</p> Type Description <code>DataLoader</code> <p>An iterable <code>DataLoader</code> containing data from all set.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_all_dataloader(self, ts_id: int | None = None, workers: int | Literal[\"config\"] = \"config\", **kwargs) -&gt; DataLoader:\n    \"\"\"\n    Returns a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for all set.\n\n    The `DataLoader` is created on the first call and cached for subsequent use. &lt;br/&gt;\n    The cached dataloader is cleared when either [`get_all_df`][cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_all_df] or [`get_all_numpy`][cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_all_numpy] is called.\n\n    The structure of the returned batch depends on the `time_format` and whether `sliding_window_size` is used:\n\n    - When `sliding_window_size` is used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n            - `np.ndarray` of times with shape `(times - 1)`\n            - `np.ndarray` of time with shape `(1)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n    - When `sliding_window_size` is not used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n            - `np.ndarray` of time with shape `(times)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n\n    The `DataLoader` is configured with the following config attributes:\n\n    | Dataset config                    | Description                                                                                                                               |\n    | --------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------- |\n    | `all_batch_size`                  | Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.         |\n    | `sliding_window_size`             | Available only for time-based datasets. Modifies the shape of the returned data.                                                          |\n    | `sliding_window_prediction_size`  | Available only for time-based datasets. Modifies the shape of the returned data.                                                          |\n    | `sliding_window_step`             | Available only for time-based datasets. Number of times to move by after each window.                                                     |\n    | `all_workers`                     | Specifies the number of workers to use for loading all data. Applied when `workers` = \"config\".                                           |\n\n    Parameters:\n        workers: The number of workers to use for loading all data. `Default: \"config\"`  \n        ts_id: Specifies time series to take. If None returns all time series as normal. `Default: \"None\"`\n\n    Returns:\n        An iterable `DataLoader` containing data from all set.       \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access all_dataloader.\")\n\n    if not self.dataset_config.has_all:\n        raise ValueError(\"Dataloader for all set is not available in the dataset configuration.\")\n\n    assert self.all_dataset is not None, \"The all_dataset must be initialized before accessing data from all set.\"\n\n    defaultKwargs = {'take_all': False, \"cache_loader\": True}\n    kwargs = {**defaultKwargs, **kwargs}\n\n    if ts_id is not None:\n\n        if ts_id == self.dataset_config.used_singular_all_time_series and self.all_dataloader is not None:\n            self.logger.debug(\"Returning cached all_dataloader.\")\n            return self.all_dataloader\n\n        dataset = self._get_singular_time_series_dataset(self.all_dataset, ts_id)\n        self.dataset_config.used_singular_all_time_series = ts_id\n        if self.all_dataloader:\n            del self.all_dataloader\n            self.all_dataloader = None\n            self.logger.info(\"Destroyed previous cached all_dataloader.\")\n\n        self.dataset_config.used_all_workers = 0\n        self.all_dataloader = self._get_dataloader(dataset, 0, False, self.dataset_config.all_batch_size)\n        self.logger.info(\"Created new cached all_dataloader.\")\n        return self.all_dataloader\n    elif self.dataset_config.used_singular_all_time_series is not None and self.all_dataloader is not None:\n        del self.all_dataloader\n        self.all_dataloader = None\n        self.dataset_config.used_singular_all_time_series = None\n        self.logger.info(\"Destroyed previous cached all_dataloader.\")\n\n    if workers == \"config\":\n        workers = self.dataset_config.all_workers\n\n    # If the dataloader is cached and number of used workers did not change, return the cached dataloader\n    if self.all_dataloader and kwargs[\"cache_loader\"] and workers == self.dataset_config.used_all_workers:\n        self.logger.debug(\"Returning cached all_dataloader.\")\n        return self.all_dataloader\n\n    # Update the used workers count\n    self.dataset_config.used_all_workers = workers\n\n    # If there's a previously cached dataloader, destroy it\n    if self.all_dataloader:\n        del self.all_dataloader\n        self.all_dataloader = None\n        self.logger.info(\"Destroyed previous cached all_dataloader.\")\n\n    # If caching is enabled, create a new cached dataloader\n    if kwargs[\"cache_loader\"]:\n        self.all_dataloader = self._get_dataloader(self.all_dataset, workers, kwargs['take_all'], self.dataset_config.all_batch_size)\n        self.logger.info(\"Created new cached all_dataloader.\")\n        return self.all_dataloader\n\n    # If caching is disabled, create a new uncached dataloader\n    self.logger.debug(\"Creating new uncached all_dataloader.\")\n    return self._get_dataloader(self.all_dataset, workers, kwargs['take_all'], self.dataset_config.all_batch_size)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_train_df","title":"get_train_df","text":"<pre><code>get_train_df(workers: int | Literal['config'] = 'config', as_single_dataframe: bool = True) -&gt; pd.DataFrame\n</code></pre> <p>Creates a Pandas <code>DataFrame</code> containing all the data from training set grouped by time series.</p> <p>This method uses the <code>train_dataloader</code> with a batch size set to the total number of data in the training set. The cached <code>train_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire training set is loaded into memory, which may lead to high memory usage. If working with large training set, consider using <code>get_train_dataloader</code> instead to handle data in batches.</p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading train data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>as_single_dataframe</code> <code>bool</code> <p>Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. <code>Default: True</code> </p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A single Pandas DataFrame containing all data from training set, or a list of DataFrames (one per time series).</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_train_df(self, workers: int | Literal[\"config\"] = \"config\", as_single_dataframe: bool = True) -&gt; pd.DataFrame:\n    \"\"\"\n    Creates a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) containing all the data from training set grouped by time series.\n\n    This method uses the `train_dataloader` with a batch size set to the total number of data in the training set. The cached `train_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire training set is loaded into memory, which may lead to high memory usage. If working with large training set, consider using `get_train_dataloader` instead to handle data in batches.\n\n    Parameters:\n        workers: The number of workers to use for loading train data. `Default: \"config\"`  \n        as_single_dataframe: Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. `Default: True` \n\n    Returns:\n        A single Pandas DataFrame containing all data from training set, or a list of DataFrames (one per time series).\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access train_dataloader.\")\n\n    if not self.dataset_config.has_train:\n        raise ValueError(\"Dataloader for training set is not available in the dataset configuration.\")\n\n    assert self.train_dataset is not None, \"The train_dataset must be initialized before accessing data from training set.\"\n\n    ts_ids, time_period = self.dataset_config._get_train()\n\n    dataloader = self.get_train_dataloader(workers=workers, take_all=not self.dataset_config.is_series_based, cache_loader=False)\n    return self._get_df(dataloader, as_single_dataframe, ts_ids, time_period)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_val_df","title":"get_val_df","text":"<pre><code>get_val_df(workers: int | Literal['config'] = 'config', as_single_dataframe: bool = True) -&gt; pd.DataFrame\n</code></pre> <p>Create a Pandas <code>DataFrame</code> containing all the data from validation set grouped by time series.</p> <p>This method uses the <code>val_dataloader</code> with a batch size set to the total number of data in the validation set. The cached <code>val_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire validation set is loaded into memory, which may lead to high memory usage. If working with large validation set, consider using <code>get_val_dataloader</code> instead to handle data in batches.</p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading validation data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>as_single_dataframe</code> <code>bool</code> <p>Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. <code>Default: True</code> </p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A single Pandas DataFrame containing all data from validation set, or a list of DataFrames (one per time series).</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_val_df(self, workers: int | Literal[\"config\"] = \"config\", as_single_dataframe: bool = True) -&gt; pd.DataFrame:\n    \"\"\"\n    Create a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) containing all the data from validation set grouped by time series.\n\n    This method uses the `val_dataloader` with a batch size set to the total number of data in the validation set. The cached `val_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire validation set is loaded into memory, which may lead to high memory usage. If working with large validation set, consider using `get_val_dataloader` instead to handle data in batches.\n\n    Parameters:\n        workers: The number of workers to use for loading validation data. `Default: \"config\"`  \n        as_single_dataframe: Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. `Default: True` \n\n    Returns:\n        A single Pandas DataFrame containing all data from validation set, or a list of DataFrames (one per time series).\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access val_dataloader.\")\n\n    if not self.dataset_config.has_val:\n        raise ValueError(\"Dataloader for validation set is not available in the dataset configuration.\")\n\n    assert self.val_dataset is not None, \"The val_dataset must be initialized before accessing data from validation set.\"\n\n    ts_ids, time_period = self.dataset_config._get_val()\n\n    dataloader = self.get_val_dataloader(workers=workers, take_all=not self.dataset_config.is_series_based, cache_loader=False)\n    return self._get_df(dataloader, as_single_dataframe, ts_ids, time_period)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_df","title":"get_test_df","text":"<pre><code>get_test_df(workers: int | Literal['config'] = 'config', as_single_dataframe: bool = True) -&gt; pd.DataFrame\n</code></pre> <p>Creates a Pandas <code>DataFrame</code> containing all the data from test set grouped by time series.</p> <p>This method uses the <code>test_dataloader</code> with a batch size set to the total number of data in the test set. The cached <code>test_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire test set is loaded into memory, which may lead to high memory usage. If working with large test set, consider using <code>get_test_dataloader</code> instead to handle data in batches.</p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading test data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>as_single_dataframe</code> <code>bool</code> <p>Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. <code>Default: True</code> </p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A single Pandas DataFrame containing all data from test set, or a list of DataFrames (one per time series).</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_test_df(self, workers: int | Literal[\"config\"] = \"config\", as_single_dataframe: bool = True) -&gt; pd.DataFrame:\n    \"\"\"\n    Creates a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) containing all the data from test set grouped by time series.\n\n    This method uses the `test_dataloader` with a batch size set to the total number of data in the test set. The cached `test_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire test set is loaded into memory, which may lead to high memory usage. If working with large test set, consider using `get_test_dataloader` instead to handle data in batches.\n\n    Parameters:\n        workers: The number of workers to use for loading test data. `Default: \"config\"`  \n        as_single_dataframe: Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. `Default: True` \n\n    Returns:\n        A single Pandas DataFrame containing all data from test set, or a list of DataFrames (one per time series).\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access test_dataloader.\")\n\n    if not self.dataset_config.has_all:\n        raise ValueError(\"Dataloader for test set is not available in the dataset configuration.\")\n\n    assert self.test_dataset is not None, \"The test_dataset must be initialized before accessing data from test set.\"\n\n    ts_ids, time_period = self.dataset_config._get_test()\n\n    dataloader = self.get_test_dataloader(workers=workers, take_all=not self.dataset_config.is_series_based, cache_loader=False)\n    return self._get_df(dataloader, as_single_dataframe, ts_ids, time_period)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_other_df","title":"get_test_other_df","text":"<pre><code>get_test_other_df(workers: int | Literal['config'] = 'config', as_single_dataframe: bool = True) -&gt; pd.DataFrame\n</code></pre> <p>Creates a Pandas <code>DataFrame</code> containing all the data from test_other set grouped by time series.</p> <p>This method uses the <code>test_other_dataloader</code> with a batch size set to the total number of data in the test_other set. The cached <code>test_other_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire test_other set is loaded into memory, which may lead to high memory usage. If working with large test_other set, consider using <code>get_test_other_dataloader</code> instead to handle data in batches.</p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading test_other data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>as_single_dataframe</code> <code>bool</code> <p>Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. <code>Default: True</code> </p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A single Pandas DataFrame containing all data from test_other set, or a list of DataFrames (one per time series).</p> Source code in <code>cesnet_tszoo\\datasets\\time_based_cesnet_dataset.py</code> <pre><code>def get_test_other_df(self, workers: int | Literal[\"config\"] = \"config\", as_single_dataframe: bool = True) -&gt; pd.DataFrame:\n    \"\"\"\n    Creates a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) containing all the data from test_other set grouped by time series.\n\n    This method uses the `test_other_dataloader` with a batch size set to the total number of data in the test_other set. The cached `test_other_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire test_other set is loaded into memory, which may lead to high memory usage. If working with large test_other set, consider using `get_test_other_dataloader` instead to handle data in batches.\n\n    Parameters:\n        workers: The number of workers to use for loading test_other data. `Default: \"config\"`  \n        as_single_dataframe: Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. `Default: True` \n\n    Returns:\n        A single Pandas DataFrame containing all data from test_other set, or a list of DataFrames (one per time series).\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access test_other_dataloader.\")\n\n    if not self.dataset_config.has_all:\n        raise ValueError(\"Dataloader for test_other set is not available in the dataset configuration.\")\n\n    assert self.test_dataset is not None, \"The test_other_dataset must be initialized before accessing data from test_other set.\"\n\n    ts_ids, time_period = self.dataset_config._get_test_other()\n\n    dataloader = self.get_test_other_dataloader(workers=workers, take_all=True, cache_loader=False)\n    return self._get_df(dataloader, as_single_dataframe, ts_ids, time_period)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_all_df","title":"get_all_df","text":"<pre><code>get_all_df(workers: int | Literal['config'] = 'config', as_single_dataframe: bool = True) -&gt; pd.DataFrame\n</code></pre> <p>Creates a Pandas <code>DataFrame</code> containing all the data from all set grouped by time series.</p> <p>This method uses the <code>all_dataloader</code> with a batch size set to the total number of data in the all set. The cached <code>all_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire all set is loaded into memory, which may lead to high memory usage. If working with large all set, consider using <code>get_all_dataloader</code> instead to handle data in batches.</p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading all data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>as_single_dataframe</code> <code>bool</code> <p>Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. <code>Default: True</code> </p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A single Pandas DataFrame containing all data from all set, or a list of DataFrames (one per time series).</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_all_df(self, workers: int | Literal[\"config\"] = \"config\", as_single_dataframe: bool = True) -&gt; pd.DataFrame:\n    \"\"\"\n    Creates a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) containing all the data from all set grouped by time series.\n\n    This method uses the `all_dataloader` with a batch size set to the total number of data in the all set. The cached `all_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire all set is loaded into memory, which may lead to high memory usage. If working with large all set, consider using `get_all_dataloader` instead to handle data in batches.\n\n    Parameters:\n        workers: The number of workers to use for loading all data. `Default: \"config\"`  \n        as_single_dataframe: Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. `Default: True` \n\n    Returns:\n        A single Pandas DataFrame containing all data from all set, or a list of DataFrames (one per time series).\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access all_dataloader.\")\n\n    if not self.dataset_config.has_all:\n        raise ValueError(\"Dataloader for all set is not available in the dataset configuration.\")\n\n    assert self.all_dataset is not None, \"The all_dataset must be initialized before accessing data from all set.\"\n\n    ts_ids, time_period = self.dataset_config._get_all()\n\n    dataloader = self.get_all_dataloader(workers=workers, take_all=not self.dataset_config.is_series_based, cache_loader=False)\n\n    return self._get_df(dataloader, as_single_dataframe, ts_ids, time_period)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_train_numpy","title":"get_train_numpy","text":"<pre><code>get_train_numpy(workers: int | Literal['config'] = 'config') -&gt; np.ndarray\n</code></pre> <p>Creates a NumPy array containing all the data from training set grouped by time series, with the shape <code>(num_time_series, num_times, num_features)</code>.</p> <p>This method uses the <code>train_dataloader</code> with a batch size set to the total number of data in the training set. The cached <code>train_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire training set is loaded into memory, which may lead to high memory usage. If working with large training set, consider using <code>get_train_dataloader</code> instead to handle data in batches.        </p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading train data. <code>Default: \"config\"</code> </p> <code>'config'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A NumPy array containing all the data in training set with the shape <code>(num_time_series, num_times, num_features)</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_train_numpy(self, workers: int | Literal[\"config\"] = \"config\") -&gt; np.ndarray:\n    \"\"\"\n    Creates a NumPy array containing all the data from training set grouped by time series, with the shape `(num_time_series, num_times, num_features)`.\n\n    This method uses the `train_dataloader` with a batch size set to the total number of data in the training set. The cached `train_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire training set is loaded into memory, which may lead to high memory usage. If working with large training set, consider using `get_train_dataloader` instead to handle data in batches.        \n\n    Parameters:\n        workers: The number of workers to use for loading train data. `Default: \"config\"`  \n\n    Returns:\n        A NumPy array containing all the data in training set with the shape `(num_time_series, num_times, num_features)`.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access train_dataloader.\")\n\n    if not self.dataset_config.has_train:\n        raise ValueError(\"Dataloader for training set is not available in the dataset configuration.\")\n\n    assert self.train_dataset is not None, \"The train_dataset must be initialized before accessing data from training set.\"\n\n    ts_ids, time_period = self.dataset_config._get_train()\n\n    dataloader = self.get_train_dataloader(workers=workers, take_all=not self.dataset_config.is_series_based, cache_loader=False)\n    return self._get_numpy(dataloader, ts_ids, time_period)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_val_numpy","title":"get_val_numpy","text":"<pre><code>get_val_numpy(workers: int | Literal['config'] = 'config') -&gt; np.ndarray\n</code></pre> <p>Creates a NumPy array containing all the data from validation set grouped by time series, with the shape <code>(num_time_series, num_times, num_features)</code>.</p> <p>This method uses the <code>val_dataloader</code> with a batch size set to the total number of data in the validation set. The cached <code>val_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire validation set is loaded into memory, which may lead to high memory usage. If working with large validation set, consider using <code>get_val_dataloader</code> instead to handle data in batches.        </p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading validation data. <code>Default: \"config\"</code> </p> <code>'config'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A NumPy array containing all the data in validation set with the shape <code>(num_time_series, num_times, num_features)</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_val_numpy(self, workers: int | Literal[\"config\"] = \"config\") -&gt; np.ndarray:\n    \"\"\"\n    Creates a NumPy array containing all the data from validation set grouped by time series, with the shape `(num_time_series, num_times, num_features)`.\n\n    This method uses the `val_dataloader` with a batch size set to the total number of data in the validation set. The cached `val_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire validation set is loaded into memory, which may lead to high memory usage. If working with large validation set, consider using `get_val_dataloader` instead to handle data in batches.        \n\n    Parameters:\n        workers: The number of workers to use for loading validation data. `Default: \"config\"`  \n\n    Returns:\n        A NumPy array containing all the data in validation set with the shape `(num_time_series, num_times, num_features)`.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access val_dataloader.\")\n\n    if not self.dataset_config.has_val:\n        raise ValueError(\"Dataloader for validation set is not available in the dataset configuration.\")\n\n    assert self.val_dataset is not None, \"The val_dataset must be initialized before accessing data from validation set.\"\n\n    ts_ids, time_period = self.dataset_config._get_val()\n\n    dataloader = self.get_val_dataloader(workers=workers, take_all=not self.dataset_config.is_series_based, cache_loader=False)\n    return self._get_numpy(dataloader, ts_ids, time_period)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_numpy","title":"get_test_numpy","text":"<pre><code>get_test_numpy(workers: int | Literal['config'] = 'config') -&gt; np.ndarray\n</code></pre> <p>Creates a NumPy array containing all the data from test set grouped by time series, with the shape <code>(num_time_series, num_times, num_features)</code>.</p> <p>This method uses the <code>test_dataloader</code> with a batch size set to the total number of data in the test set. The cached <code>test_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire test set is loaded into memory, which may lead to high memory usage. If working with large test set, consider using <code>get_test_dataloader</code> instead to handle data in batches.        </p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading test data. <code>Default: \"config\"</code> </p> <code>'config'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A NumPy array containing all the data in test set with the shape <code>(num_time_series, num_times, num_features)</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_test_numpy(self, workers: int | Literal[\"config\"] = \"config\") -&gt; np.ndarray:\n    \"\"\"\n    Creates a NumPy array containing all the data from test set grouped by time series, with the shape `(num_time_series, num_times, num_features)`.\n\n    This method uses the `test_dataloader` with a batch size set to the total number of data in the test set. The cached `test_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire test set is loaded into memory, which may lead to high memory usage. If working with large test set, consider using `get_test_dataloader` instead to handle data in batches.        \n\n    Parameters:\n        workers: The number of workers to use for loading test data. `Default: \"config\"`  \n\n    Returns:\n        A NumPy array containing all the data in test set with the shape `(num_time_series, num_times, num_features)`.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access test_dataloader.\")\n\n    if not self.dataset_config.has_all:\n        raise ValueError(\"Dataloader for test set is not available in the dataset configuration.\")\n\n    assert self.test_dataset is not None, \"The test_dataset must be initialized before accessing data from test set.\"\n\n    ts_ids, time_period = self.dataset_config._get_test()\n\n    dataloader = self.get_test_dataloader(workers=workers, take_all=not self.dataset_config.is_series_based, cache_loader=False)\n    return self._get_numpy(dataloader, ts_ids, time_period)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_other_numpy","title":"get_test_other_numpy","text":"<pre><code>get_test_other_numpy(workers: int | Literal['config'] = 'config') -&gt; np.ndarray\n</code></pre> <p>Creates a NumPy array containing all the data from test_other set grouped by time series, with the shape <code>(num_time_series, num_times, num_features)</code>.</p> <p>This method uses the <code>test_other_dataloader</code> with a batch size set to the total number of data in the test_other set. The cached <code>test_other_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire test_other set is loaded into memory, which may lead to high memory usage. If working with large test_other set, consider using <code>get_test_other_dataloader</code> instead to handle data in batches.        </p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading test_other data. <code>Default: \"config\"</code> </p> <code>'config'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A NumPy array containing all the data in test_other set with the shape <code>(num_time_series, num_times, num_features)</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\time_based_cesnet_dataset.py</code> <pre><code>def get_test_other_numpy(self, workers: int | Literal[\"config\"] = \"config\",) -&gt; np.ndarray:\n    \"\"\"\n    Creates a NumPy array containing all the data from test_other set grouped by time series, with the shape `(num_time_series, num_times, num_features)`.\n\n    This method uses the `test_other_dataloader` with a batch size set to the total number of data in the test_other set. The cached `test_other_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire test_other set is loaded into memory, which may lead to high memory usage. If working with large test_other set, consider using `get_test_other_dataloader` instead to handle data in batches.        \n\n    Parameters:\n        workers: The number of workers to use for loading test_other data. `Default: \"config\"`  \n\n    Returns:\n        A NumPy array containing all the data in test_other set with the shape `(num_time_series, num_times, num_features)`.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access test_other_dataloader.\")\n\n    if not self.dataset_config.has_all:\n        raise ValueError(\"Dataloader for test_other set is not available in the dataset configuration.\")\n\n    assert self.test_dataset is not None, \"The test_other_dataset must be initialized before accessing data from test_other set.\"\n\n    ts_ids, time_period = self.dataset_config._get_test_other()\n\n    dataloader = self.get_test_other_dataloader(workers=workers, take_all=True, cache_loader=False)\n    return self._get_numpy(dataloader, ts_ids, time_period)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_all_numpy","title":"get_all_numpy","text":"<pre><code>get_all_numpy(workers: int | Literal['config'] = 'config') -&gt; np.ndarray\n</code></pre> <p>Creates a NumPy array containing all the data from all set grouped by time series, with the shape <code>(num_time_series, num_times, num_features)</code>.</p> <p>This method uses the <code>all_dataloader</code> with a batch size set to the total number of data in the all set. The cached <code>all_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire all set is loaded into memory, which may lead to high memory usage. If working with large all set, consider using <code>get_all_dataloader</code> instead to handle data in batches.        </p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading all data. <code>Default: \"config\"</code> </p> <code>'config'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A NumPy array containing all the data in all set with the shape <code>(num_time_series, num_times, num_features)</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_all_numpy(self, workers: int | Literal[\"config\"] = \"config\") -&gt; np.ndarray:\n    \"\"\"\n    Creates a NumPy array containing all the data from all set grouped by time series, with the shape `(num_time_series, num_times, num_features)`.\n\n    This method uses the `all_dataloader` with a batch size set to the total number of data in the all set. The cached `all_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire all set is loaded into memory, which may lead to high memory usage. If working with large all set, consider using `get_all_dataloader` instead to handle data in batches.        \n\n    Parameters:\n        workers: The number of workers to use for loading all data. `Default: \"config\"`  \n\n    Returns:\n        A NumPy array containing all the data in all set with the shape `(num_time_series, num_times, num_features)`.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access all_dataloader.\")\n\n    if not self.dataset_config.has_all:\n        raise ValueError(\"Dataloader for all set is not available in the dataset configuration.\")\n\n    assert self.all_dataset is not None, \"The all_dataset must be initialized before accessing data from all set.\"\n\n    ts_ids, time_period = self.dataset_config._get_all()\n\n    dataloader = self.get_all_dataloader(workers=workers, take_all=not self.dataset_config.is_series_based, cache_loader=False)\n\n    return self._get_numpy(dataloader, ts_ids, time_period)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.display_dataset_details","title":"display_dataset_details","text":"<pre><code>display_dataset_details() -&gt; None\n</code></pre> <p>Display information about the contents of the dataset.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>    def display_dataset_details(self) -&gt; None:\n        \"\"\"Display information about the contents of the dataset.  \"\"\"\n\n        to_display = f'''\nDataset details:\n\n    {self.aggregation}\n        Time indices: {range(self.time_indices[ID_TIME_COLUMN_NAME][0], self.time_indices[ID_TIME_COLUMN_NAME][-1])}\n        Datetime: {(datetime.fromtimestamp(self.time_indices['time'][0], tz=timezone.utc), datetime.fromtimestamp(self.time_indices['time'][-1], timezone.utc))}\n\n    {self.source_type}\n        Time series indices: {get_abbreviated_list_string(self.ts_indices[self.ts_id_name])}; use 'get_available_ts_indices' for full list\n        Features with default values: {self.default_values}\n\n        Additional data: {list(self.additional_data.keys())}\n        '''\n\n        print(to_display)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.display_config","title":"display_config","text":"<pre><code>display_config() -&gt; None\n</code></pre> <p>Displays the values of the initialized configuration.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def display_config(self) -&gt; None:\n    \"\"\"Displays the values of the initialized configuration. \"\"\"\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before displaying config.\")\n\n    print(self.dataset_config)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_feature_names","title":"get_feature_names","text":"<pre><code>get_feature_names() -&gt; list[str]\n</code></pre> <p>Returns a list of all available feature names in the dataset.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_feature_names(self) -&gt; list[str]:\n    \"\"\"Returns a list of all available feature names in the dataset. \"\"\"\n\n    return get_column_names(self.dataset_path, self.source_type, self.aggregation)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_data_about_set","title":"get_data_about_set","text":"<pre><code>get_data_about_set(about: SplitType | Literal['train', 'val', 'test', 'all']) -&gt; dict\n</code></pre> <p>Retrieve data related to the specified set.</p> <p>Parameters:</p> Name Type Description Default <code>about</code> <code>SplitType | Literal['train', 'val', 'test', 'all']</code> <p>Specifies the set to retrieve data about.</p> required <p>Returned dictionary contains:</p> <ul> <li>ts_ids: Ids of time series in <code>about</code> set.</li> <li>test_ts_ids: Ids of  time series in <code>test_ts_ids</code>. Only for <code>about</code> == SplitType.TEST and when <code>test_ts_id</code> is set in used config.</li> <li>TimeFormat.ID_TIME: Times in <code>about</code> set, where time format is <code>TimeFormat.ID_TIME</code>.</li> <li>TimeFormat.DATETIME: Times in <code>about</code> set, where time format is <code>TimeFormat.DATETIME</code>.</li> <li>TimeFormat.UNIX_TIME: Times in <code>about</code> set, where time format is <code>TimeFormat.UNIX_TIME</code>.</li> <li>TimeFormat.SHIFTED_UNIX_TIME: Times in <code>about</code> set, where time format is <code>TimeFormat.SHIFTED_UNIX_TIME</code>.</li> </ul> <p>Returns:</p> Type Description <code>dict</code> <p>Returns dictionary with details about set.</p> Source code in <code>cesnet_tszoo\\datasets\\time_based_cesnet_dataset.py</code> <pre><code>def get_data_about_set(self, about: SplitType | Literal[\"train\", \"val\", \"test\", \"all\"]) -&gt; dict:\n    \"\"\"\n    Retrieve data related to the specified set.\n\n    Parameters:\n        about: Specifies the set to retrieve data about.\n\n    Returned dictionary contains:\n\n    - **ts_ids:** Ids of time series in `about` set.\n    - **test_ts_ids:** Ids of  time series in `test_ts_ids`. Only for `about` == SplitType.TEST and when `test_ts_id` is set in used config.\n    - **TimeFormat.ID_TIME:** Times in `about` set, where time format is `TimeFormat.ID_TIME`.\n    - **TimeFormat.DATETIME:** Times in `about` set, where time format is `TimeFormat.DATETIME`.\n    - **TimeFormat.UNIX_TIME:** Times in `about` set, where time format is `TimeFormat.UNIX_TIME`.\n    - **TimeFormat.SHIFTED_UNIX_TIME:** Times in `about` set, where time format is `TimeFormat.SHIFTED_UNIX_TIME`.\n\n    Returns:\n        Returns dictionary with details about set.\n    \"\"\"\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before getting data about set.\")\n\n    about = SplitType(about)\n\n    time_period = None\n\n    result = {}\n\n    if about == SplitType.TRAIN:\n        if not self.dataset_config.has_train:\n            raise ValueError(\"Train split is not used.\")\n        time_period = self.dataset_config.train_time_period\n    elif about == SplitType.VAL:\n        if not self.dataset_config.has_val:\n            raise ValueError(\"Val split is not used.\")\n        time_period = self.dataset_config.val_time_period\n    elif about == SplitType.TEST:\n        if not self.dataset_config.has_test:\n            raise ValueError(\"Test split is not used.\")\n        time_period = self.dataset_config.test_time_period\n        result[\"test_ts_ids\"] = self.dataset_config.test_ts_ids.copy() if self.dataset_config.test_ts_ids is not None else None\n    elif about == SplitType.ALL:\n        if not self.dataset_config.has_all:\n            raise ValueError(\"All split is not used.\")\n\n        time_period = self.dataset_config.all_time_period\n    else:\n        raise ValueError(\"Invalid split type!\")\n\n    datetime_temp = np.array([datetime.fromtimestamp(time, timezone.utc) for time in self.time_indices[TIME_COLUMN_NAME][time_period[ID_TIME_COLUMN_NAME]]])\n\n    result[\"ts_ids\"] = self.dataset_config.ts_ids.copy()\n    result[TimeFormat.ID_TIME] = time_period[ID_TIME_COLUMN_NAME].copy()\n    result[TimeFormat.DATETIME] = datetime_temp.copy()\n    result[TimeFormat.UNIX_TIME] = self.time_indices[TIME_COLUMN_NAME][time_period[ID_TIME_COLUMN_NAME]].copy()\n    result[TimeFormat.SHIFTED_UNIX_TIME] = self.time_indices[TIME_COLUMN_NAME][time_period[ID_TIME_COLUMN_NAME]] - self.time_indices[TIME_COLUMN_NAME][0]\n\n    return result\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_available_ts_indices","title":"get_available_ts_indices","text":"<pre><code>get_available_ts_indices()\n</code></pre> <p>Returns the available time series indices in this dataset.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_available_ts_indices(self):\n    \"\"\"Returns the available time series indices in this dataset. \"\"\"\n    return self.ts_indices\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_additional_data","title":"get_additional_data","text":"<pre><code>get_additional_data(data_name: str) -&gt; pd.DataFrame\n</code></pre> <p>Create a Pandas <code>DataFrame</code> of additional data of <code>data_name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data_name</code> <code>str</code> <p>Name of additional data to return.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Dataframe of additional data of <code>data_name</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_additional_data(self, data_name: str) -&gt; pd.DataFrame:\n    \"\"\"Create a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) of additional data of `data_name`.\n\n    Parameters:\n        data_name: Name of additional data to return.\n\n    Returns:\n        Dataframe of additional data of `data_name`.\n    \"\"\"\n\n    if data_name not in self.additional_data:\n        self.logger.error(\"%s is not available for this dataset.\", data_name)\n        raise ValueError(f\"{data_name} is not available for this dataset.\", f\"Possible options are: {self.additional_data}\")\n\n    data = get_additional_data(self.dataset_path, data_name)\n    data_df = pd.DataFrame(data)\n\n    for column, column_type in self.additional_data[data_name]:\n        if column_type == datetime:\n            data_df[column] = data_df[column].apply(lambda x: datetime.fromtimestamp(x, tz=timezone.utc))\n        else:\n            data_df[column] = data_df[column].astype(column_type)\n\n    return data_df\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.plot","title":"plot","text":"<pre><code>plot(ts_id: int, plot_type: Literal['scatter', 'line'], features: list[str] | str | Literal['config'] = 'config', feature_per_plot: bool = True, time_format: TimeFormat | Literal['config', 'id_time', 'datetime', 'unix_time', 'shifted_unix_time'] = 'config', use_scalers: bool = True, is_interactive: bool = True) -&gt; None\n</code></pre> <p>Displays a graph for the selected <code>ts_id</code> and its <code>features</code>.</p> <p>The plotting is done using the <code>Plotly</code> library, which provides interactive graphs.</p> <p>Parameters:</p> Name Type Description Default <code>ts_id</code> <code>int</code> <p>The ID of the time series to display.</p> required <code>plot_type</code> <code>Literal['scatter', 'line']</code> <p>The type of graph to plot.</p> required <code>features</code> <code>list[str] | str | Literal['config']</code> <p>The features to display in the plot. <code>Defaults: \"config\"</code>.</p> <code>'config'</code> <code>feature_per_plot</code> <code>bool</code> <p>Whether each feature should be displayed in a separate plot or combined into one. <code>Defaults: True</code>.</p> <code>True</code> <code>time_format</code> <code>TimeFormat | Literal['config', 'id_time', 'datetime', 'unix_time', 'shifted_unix_time']</code> <p>The time format to use for the x-axis. <code>Defaults: \"config\"</code>.</p> <code>'config'</code> <code>use_scalers</code> <code>bool</code> <p>Whether the data should be scaled. If <code>True</code>, scaling will be applied using the available scaler for the selected <code>ts_id</code>. <code>Defaults: True</code>.</p> <code>True</code> <code>is_interactive</code> <code>bool</code> <p>Whether the plot should be interactive (e.g., zoom, hover). <code>Defaults: True</code>.</p> <code>True</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def plot(self, ts_id: int, plot_type: Literal[\"scatter\", \"line\"], features: list[str] | str | Literal[\"config\"] = \"config\", feature_per_plot: bool = True,\n         time_format: TimeFormat | Literal[\"config\", \"id_time\", \"datetime\", \"unix_time\", \"shifted_unix_time\"] = \"config\", use_scalers: bool = True, is_interactive: bool = True) -&gt; None:\n    \"\"\"\n    Displays a graph for the selected `ts_id` and its `features`.\n\n    The plotting is done using the [`Plotly`](https://plotly.com/python/) library, which provides interactive graphs.\n\n    Parameters:\n        ts_id: The ID of the time series to display.\n        plot_type: The type of graph to plot.\n        features: The features to display in the plot. `Defaults: \"config\"`.\n        feature_per_plot: Whether each feature should be displayed in a separate plot or combined into one. `Defaults: True`.\n        time_format: The time format to use for the x-axis. `Defaults: \"config\"`.\n        use_scalers: Whether the data should be scaled. If `True`, scaling will be applied using the available scaler for the selected `ts_id`. `Defaults: True`.\n        is_interactive: Whether the plot should be interactive (e.g., zoom, hover). `Defaults: True`.\n    \"\"\"\n\n    if time_format == \"config\":\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to plot.\")\n\n        time_format = self.dataset_config.time_format\n        self.logger.debug(\"Using time format from dataset configuration: %s\", time_format)\n    else:\n        time_format = TimeFormat(time_format)\n        self.logger.debug(\"Using specified time format: %s\", time_format)\n\n    time_series, times, features = self._get_data_for_plot(ts_id, features, time_format, use_scalers)\n    self.logger.debug(\"Received data for plotting. Time series, times, and features are ready.\")\n\n    plots = []\n\n    if feature_per_plot:\n        self.logger.debug(\"Creating individual plots for each feature.\")\n        fig = make_subplots(rows=len(features), cols=1, shared_xaxes=False, x_title=time_format.value)\n\n        for i, feature in enumerate(features):\n            if plot_type == \"scatter\":\n                plot = go.Scatter(x=times, y=time_series[:, i], mode=\"markers\", name=feature, legendgroup=feature)\n                self.logger.debug(\"Creating scatter plot for feature: %s\", feature)\n            elif plot_type == \"line\":\n                plot = go.Scatter(x=times, y=time_series[:, i], mode=\"lines\", name=feature)\n                self.logger.debug(\"Creating line plot for feature: %s\", feature)\n            else:\n                raise ValueError(\"Invalid plot type.\")\n\n            fig.add_traces(plot, rows=i + 1, cols=1)\n\n        fig.update_layout(height=200 + 120 * len(features), width=2000, autosize=len(features) == 1, showlegend=True)\n        self.logger.debug(\"Created subplots for features: %s.\", features)\n    else:\n        self.logger.debug(\"Creating a combined plot for all features.\")\n        for i, feature in enumerate(features):\n            if plot_type == \"scatter\":\n                plot = go.Scatter(x=times, y=time_series[:, i], mode=\"markers\", name=feature)\n                self.logger.debug(\"Creating scatter plot for feature: %s\", feature)\n            elif plot_type == \"line\":\n                plot = go.Scatter(x=times, y=time_series[:, i], mode=\"lines\", name=feature)\n                self.logger.debug(\"Creating line plot for feature: %s\", feature)\n            else:\n                raise ValueError(\"Invalid plot type.\")\n            plots.append(plot)\n\n        fig = go.Figure(data=plots)\n        fig.update_layout(xaxis_title=time_format.value, showlegend=True, height=200 + 120 * 2)\n        self.logger.debug(\"Created combined plot for features: %s.\", features)\n\n    if not is_interactive:\n        self.logger.debug(\"Disabling interactivity for the plot.\")\n        fig.update_layout(updatemenus=[], dragmode=False, hovermode=False)\n\n    self.logger.debug(\"Displaying the plot.\")\n    fig.show()\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.add_annotation","title":"add_annotation","text":"<pre><code>add_annotation(annotation: str, annotation_group: str, ts_id: int | None, id_time: int | None, enforce_ids: bool = True) -&gt; None\n</code></pre> <p>Adds an annotation to the specified <code>annotation_group</code>.</p> <ul> <li>If the provided <code>annotation_group</code> does not exist, it will be created.</li> <li>At least one of <code>ts_id</code> or <code>id_time</code> must be provided to associate the annotation with time series or/and time point.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>annotation</code> <code>str</code> <p>The annotation to be added.</p> required <code>annotation_group</code> <code>str</code> <p>The group to which the annotation should be added.</p> required <code>ts_id</code> <code>int | None</code> <p>The time series ID to which the annotation should be added.</p> required <code>id_time</code> <code>int | None</code> <p>The time ID to which the annotation should be added.</p> required <code>enforce_ids</code> <code>bool</code> <p>Flag indicating whether the <code>ts_id</code> and <code>id_time</code> must belong to this dataset. <code>Default: True</code></p> <code>True</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def add_annotation(self, annotation: str, annotation_group: str, ts_id: int | None, id_time: int | None, enforce_ids: bool = True) -&gt; None:\n    \"\"\" \n    Adds an annotation to the specified `annotation_group`.\n\n    - If the provided `annotation_group` does not exist, it will be created.\n    - At least one of `ts_id` or `id_time` must be provided to associate the annotation with time series or/and time point.\n\n    Parameters:\n        annotation: The annotation to be added.\n        annotation_group: The group to which the annotation should be added.\n        ts_id: The time series ID to which the annotation should be added.\n        id_time: The time ID to which the annotation should be added.\n        enforce_ids: Flag indicating whether the `ts_id` and `id_time` must belong to this dataset. `Default: True`  \n    \"\"\"\n\n    if enforce_ids:\n        self._validate_annotation_ids(ts_id, id_time)\n    self.annotations.add_annotation(annotation, annotation_group, ts_id, id_time)\n\n    if ts_id is not None and id_time is not None:\n        self._update_annotations_imported_status(AnnotationType.BOTH, None)\n    elif ts_id is not None and id_time is None:\n        self._update_annotations_imported_status(AnnotationType.TS_ID, None)\n    elif ts_id is None and id_time is not None:\n        self._update_annotations_imported_status(AnnotationType.ID_TIME, None)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.remove_annotation","title":"remove_annotation","text":"<pre><code>remove_annotation(annotation_group: str, ts_id: int | None, id_time: int | None) -&gt; None\n</code></pre> <p>Removes an annotation from the specified <code>annotation_group</code>.</p> <ul> <li>At least one of <code>ts_id</code> or <code>id_time</code> must be provided to associate the annotation with time series or/and time point.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>annotation_group</code> <code>str</code> <p>The annotation group from which the annotation should be removed.</p> required <code>ts_id</code> <code>int | None</code> <p>The time series ID from which the annotation should be removed.</p> required <code>id_time</code> <code>int | None</code> <p>The time ID from which the annotation should be removed.</p> required Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def remove_annotation(self, annotation_group: str, ts_id: int | None, id_time: int | None) -&gt; None:\n    \"\"\"  \n    Removes an annotation from the specified `annotation_group`.\n\n    - At least one of `ts_id` or `id_time` must be provided to associate the annotation with time series or/and time point.\n\n    Parameters:\n        annotation_group: The annotation group from which the annotation should be removed.\n        ts_id: The time series ID from which the annotation should be removed.\n        id_time: The time ID from which the annotation should be removed. \n    \"\"\"\n\n    self.annotations.remove_annotation(annotation_group, ts_id, id_time, False)\n\n    if ts_id is not None and id_time is not None:\n        self._update_annotations_imported_status(AnnotationType.BOTH, None)\n    elif ts_id is not None and id_time is None:\n        self._update_annotations_imported_status(AnnotationType.TS_ID, None)\n    elif ts_id is None and id_time is not None:\n        self._update_annotations_imported_status(AnnotationType.ID_TIME, None)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.add_annotation_group","title":"add_annotation_group","text":"<pre><code>add_annotation_group(annotation_group: str, on: AnnotationType | Literal['id_time', 'ts_id', 'both'])\n</code></pre> <p>Adds a new <code>annotation_group</code>.</p> <p>Parameters:</p> Name Type Description Default <code>annotation_group</code> <code>str</code> <p>The name of the annotation group to be added.</p> required <code>on</code> <code>AnnotationType | Literal['id_time', 'ts_id', 'both']</code> <p>Specifies which part of the data should be annotated. If set to <code>\"both\"</code>, annotations will be applied as if <code>id_time</code> and <code>ts_id</code> were both set.</p> required Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def add_annotation_group(self, annotation_group: str, on: AnnotationType | Literal[\"id_time\", \"ts_id\", \"both\"]):\n    \"\"\" \n    Adds a new `annotation_group`.\n\n    Parameters:\n        annotation_group: The name of the annotation group to be added.\n        on: Specifies which part of the data should be annotated. If set to `\"both\"`, annotations will be applied as if `id_time` and `ts_id` were both set.\n    \"\"\"\n    on = AnnotationType(on)\n\n    self.annotations.add_annotation_group(annotation_group, on, False)\n\n    self._update_annotations_imported_status(on, None)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.remove_annotation_group","title":"remove_annotation_group","text":"<pre><code>remove_annotation_group(annotation_group: str, on: AnnotationType | Literal['id_time', 'ts_id', 'both'])\n</code></pre> <p>Removes the specified <code>annotation_group</code>.</p> <p>Parameters:</p> Name Type Description Default <code>annotation_group</code> <code>str</code> <p>The name of the annotation group to be removed.</p> required <code>on</code> <code>AnnotationType | Literal['id_time', 'ts_id', 'both']</code> <p>Specifies which part of the data the <code>annotation_group</code> should be removed from. If set to <code>\"both\"</code>, annotations will be applied as if <code>id_time</code> and <code>ts_id</code> were both set.</p> required Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def remove_annotation_group(self, annotation_group: str, on: AnnotationType | Literal[\"id_time\", \"ts_id\", \"both\"]):\n    \"\"\" \n    Removes the specified `annotation_group`.\n\n    Parameters:\n        annotation_group: The name of the annotation group to be removed.\n        on: Specifies which part of the data the `annotation_group` should be removed from. If set to `\"both\"`, annotations will be applied as if `id_time` and `ts_id` were both set.        \n    \"\"\"\n    on = AnnotationType(on)\n\n    self.annotations.remove_annotation_group(annotation_group, on, False)\n\n    self._update_annotations_imported_status(on, None)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_annotations","title":"get_annotations","text":"<pre><code>get_annotations(on: AnnotationType | Literal['id_time', 'ts_id', 'both']) -&gt; pd.DataFrame\n</code></pre> <p>Returns the annotations as a Pandas <code>DataFrame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>on</code> <code>AnnotationType | Literal['id_time', 'ts_id', 'both']</code> <p>Specifies which annotations to return. If set to <code>\"both\"</code>, annotations will be applied as if <code>id_time</code> and <code>ts_id</code> were both set.         </p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A Pandas DataFrame containing the selected annotations.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_annotations(self, on: AnnotationType | Literal[\"id_time\", \"ts_id\", \"both\"]) -&gt; pd.DataFrame:\n    \"\"\" \n    Returns the annotations as a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html).\n\n    Parameters:\n        on: Specifies which annotations to return. If set to `\"both\"`, annotations will be applied as if `id_time` and `ts_id` were both set.         \n\n    Returns:\n        A Pandas DataFrame containing the selected annotations.      \n    \"\"\"\n    on = AnnotationType(on)\n\n    return self.annotations.get_annotations(on, self.ts_id_name)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.import_annotations","title":"import_annotations","text":"<pre><code>import_annotations(identifier: str, enforce_ids: bool = True) -&gt; None\n</code></pre> <p>Imports annotations from a CSV file.</p> <p>First, it attempts to load the built-in annotations, if no built-in annotations with such an identifier exists, it attempts to load a custom annotations from the <code>\"data_root\"/tszoo/annotations/</code> directory.</p> <p><code>data_root</code> is specified when the dataset is created.     </p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The name of the CSV file.     </p> required <code>enforce_ids</code> <code>bool</code> <p>Flag indicating whether the <code>ts_id</code> and <code>id_time</code> must belong to this dataset. <code>Default: True</code></p> <code>True</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def import_annotations(self, identifier: str, enforce_ids: bool = True) -&gt; None:\n    \"\"\" \n    Imports annotations from a CSV file.\n\n    First, it attempts to load the built-in annotations, if no built-in annotations with such an identifier exists, it attempts to load a custom annotations from the `\"data_root\"/tszoo/annotations/` directory.\n\n    `data_root` is specified when the dataset is created.     \n\n    Parameters:\n        identifier: The name of the CSV file.     \n        enforce_ids: Flag indicating whether the `ts_id` and `id_time` must belong to this dataset. `Default: True`                \n    \"\"\"\n\n    annotations_file_path, is_built_in = get_annotations_path_and_whether_it_is_built_in(identifier, self.annotations_root, self.logger)\n\n    if is_built_in:\n        self.logger.info(\"Built-in annotations found: %s.\", identifier)\n        if not os.path.exists(annotations_file_path):\n            self.logger.info(\"Downloading annotations with identifier: %s\", identifier)\n            annotations_url = f\"{ANNOTATIONS_DOWNLOAD_BUCKET}&amp;file={identifier}\"  # probably will change annotations bucket... placeholder\n            resumable_download(url=annotations_url, file_path=annotations_file_path, silent=False)\n\n        self.logger.debug(\"Loading annotations from %s\", annotations_file_path)\n        temp_df = pd.read_csv(annotations_file_path)\n        self.logger.debug(\"Created DataFrame from file: %s\", annotations_file_path)\n    else:\n        self.logger.info(\"Custom annotations found: %s.\", identifier)\n        self.logger.debug(\"Loading annotations from %s\", annotations_file_path)\n        temp_df = pd.read_csv(annotations_file_path)\n        self.logger.debug(\"Created DataFrame from file: %s\", annotations_file_path)\n\n    ts_id_index = None\n    time_id_index = None\n    on = None\n\n    # Check the columns of the DataFrame to identify the type of annotation\n    if self.ts_id_name in temp_df.columns and ID_TIME_COLUMN_NAME in temp_df.columns:\n        self.annotations.clear_time_in_time_series()\n        time_id_index = temp_df.columns.tolist().index(ID_TIME_COLUMN_NAME)\n        ts_id_index = temp_df.columns.tolist().index(self.ts_id_name)\n        on = AnnotationType.BOTH\n        self.logger.info(\"Annotations detected as %s (both %s and id_time)\", AnnotationType.BOTH, self.ts_id_name)\n\n    elif self.ts_id_name in temp_df.columns:\n        self.annotations.clear_time_series()\n        ts_id_index = temp_df.columns.tolist().index(self.ts_id_name)\n        on = AnnotationType.TS_ID\n        self.logger.info(\"Annotations detected as %s (%s only)\", AnnotationType.TS_ID, self.ts_id_name)\n\n    elif ID_TIME_COLUMN_NAME in temp_df.columns:\n        self.annotations.clear_time()\n        time_id_index = temp_df.columns.tolist().index(ID_TIME_COLUMN_NAME)\n        on = AnnotationType.ID_TIME\n        self.logger.info(\"Annotations detected as %s (%s only)\", AnnotationType.ID_TIME, ID_TIME_COLUMN_NAME)\n\n    else:\n        raise ValueError(f\"Could not find {self.ts_id_name} and {ID_TIME_COLUMN_NAME} in the imported CSV.\")\n\n    # Process each row in the DataFrame and add annotations\n    for row in temp_df.itertuples(False):\n        for i, _ in enumerate(temp_df.columns):\n            if i == time_id_index or i == ts_id_index:\n                continue\n\n            ts_id = None\n            if ts_id_index is not None:\n                ts_id = row[ts_id_index]\n\n            id_time = None\n            if time_id_index is not None:\n                id_time = row[time_id_index]\n\n            self.add_annotation(row[i], temp_df.columns[i], ts_id, id_time, enforce_ids)\n\n    self._update_annotations_imported_status(on, identifier)\n    self.logger.info(\"Successfully imported annotations from %s\", annotations_file_path)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.import_config","title":"import_config","text":"<pre><code>import_config(identifier: str, display_config_details: bool = True, workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Import the dataset_config from a pickle file and initializes the dataset. Config type must correspond to dataset type.</p> <p>First, it attempts to load the built-in config, if no built-in config with such an identifier exists, it attempts to load a custom config from the <code>\"data_root\"/tszoo/configs/</code> directory.</p> <p><code>data_root</code> is specified when the dataset is created.       </p> <p>The following configuration attributes are used during initialization:</p> Dataset config Description <code>init_workers</code> Specifies the number of workers to use for initialization. Applied when <code>workers</code> = \"config\". <code>partial_fit_initialized_scalers</code> Determines whether initialized scalers should be partially fitted on the training data. <code>nan_threshold</code> Filters out time series with missing values exceeding the specified threshold. <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>Name of the pickle file.</p> required <code>display_config_details</code> <code>bool</code> <p>Flag indicating whether to display the configuration values after initialization. <code>Default: True</code> </p> <code>True</code> <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use during initialization. <code>Default: \"config\"</code></p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def import_config(self, identifier: str, display_config_details: bool = True, workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\" \n    Import the dataset_config from a pickle file and initializes the dataset. Config type must correspond to dataset type.\n\n    First, it attempts to load the built-in config, if no built-in config with such an identifier exists, it attempts to load a custom config from the `\"data_root\"/tszoo/configs/` directory.\n\n    `data_root` is specified when the dataset is created.       \n\n    The following configuration attributes are used during initialization:\n\n    | Dataset config                    | Description                                                                                    |\n    | --------------------------------- | ---------------------------------------------------------------------------------------------- |\n    | `init_workers`                    | Specifies the number of workers to use for initialization. Applied when `workers` = \"config\". |\n    | `partial_fit_initialized_scalers` | Determines whether initialized scalers should be partially fitted on the training data.        |\n    | `nan_threshold`                   | Filters out time series with missing values exceeding the specified threshold.                 |  \n\n    Parameters:\n        identifier: Name of the pickle file.\n        display_config_details: Flag indicating whether to display the configuration values after initialization. `Default: True` \n        workers: The number of workers to use during initialization. `Default: \"config\"`  \n    \"\"\"\n\n    # Load config\n    config_file_path, is_built_in = get_config_path_and_whether_it_is_built_in(identifier, self.configs_root, self.database_name, self.source_type, self.aggregation, self.logger)\n\n    if is_built_in:\n        self.logger.info(\"Built-in config found: %s. Loading it.\", identifier)\n        config = pickle_load(config_file_path)\n    else:\n        self.logger.info(\"Custom config found: %s. Loading it.\", identifier)\n        config = pickle_load(config_file_path)\n\n    self.logger.info(\"Initializing dataset configuration with the imported config.\")\n    self.set_dataset_config_and_initialize(config, display_config_details, workers)\n\n    self._update_config_imported_status(identifier)\n    self.logger.info(\"Successfully imported config from %s\", config_file_path)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.save_annotations","title":"save_annotations","text":"<pre><code>save_annotations(identifier: str, on: AnnotationType | Literal['id_time', 'ts_id', 'both'], force_write: bool = False) -&gt; None\n</code></pre> <p>Saves the annotations as a CSV file.</p> <p>The file will be saved to a path determined by the <code>data_root</code> specified when the dataset was created.</p> <p>The annotations will be saved under the directory <code>data_root/tszoo/annotations/</code>.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The name of the CSV file.</p> required <code>on</code> <code>AnnotationType | Literal['id_time', 'ts_id', 'both']</code> <p>What annotation type should be saved. If set to <code>\"both\"</code>, annotations will be applied as if <code>id_time</code> and <code>ts_id</code> were both set.   </p> required <code>force_write</code> <code>bool</code> <p>If set to <code>True</code>, will overwrite any existing files with the same name. <code>Default: False</code></p> <code>False</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def save_annotations(self, identifier: str, on: AnnotationType | Literal[\"id_time\", \"ts_id\", \"both\"], force_write: bool = False) -&gt; None:\n    \"\"\" \n    Saves the annotations as a CSV file.\n\n    The file will be saved to a path determined by the `data_root` specified when the dataset was created.\n\n    The annotations will be saved under the directory `data_root/tszoo/annotations/`.\n\n    Parameters:\n        identifier: The name of the CSV file.\n        on: What annotation type should be saved. If set to `\"both\"`, annotations will be applied as if `id_time` and `ts_id` were both set.   \n        force_write: If set to `True`, will overwrite any existing files with the same name. `Default: False`               \n    \"\"\"\n\n    if exists_built_in_annotations(identifier):\n        raise ValueError(\"Built-in annotations with this identifier already exists. Choose another identifier.\")\n\n    on = AnnotationType(on)\n\n    temp_df = self.get_annotations(on)\n\n    # Ensure the annotations root directory exists, creating it if necessary\n    if not os.path.exists(self.annotations_root):\n        os.makedirs(self.annotations_root)\n        self.logger.info(\"Created annotations directory at %s\", self.annotations_root)\n\n    path = os.path.join(self.annotations_root, f\"{identifier}.csv\")\n\n    if os.path.exists(path) and not force_write:\n        raise ValueError(f\"Annotations already exist at {path}. Set force_write=True to overwrite.\")\n    self.logger.debug(\"Annotations CSV file path: %s\", path)\n\n    temp_df.to_csv(path, index=False)\n\n    self._update_annotations_imported_status(on, identifier)\n    self.logger.info(\"Annotations successfully saved to %s\", path)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.save_config","title":"save_config","text":"<pre><code>save_config(identifier: str, create_with_details_file: bool = True, force_write: bool = False) -&gt; None\n</code></pre> <p>Saves the config as a pickle file.</p> <p>The file will be saved to a path determined by the <code>data_root</code> specified when the dataset was created.  The config will be saved under the directory <code>data_root/tszoo/configs/</code>.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The name of the pickle file.</p> required <code>create_with_details_file</code> <code>bool</code> <p>Whether to export the config along with a readable text file that provides details. <code>Defaults: True</code>. </p> <code>True</code> <code>force_write</code> <code>bool</code> <p>If set to <code>True</code>, will overwrite any existing files with the same name. <code>Default: False</code></p> <code>False</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def save_config(self, identifier: str, create_with_details_file: bool = True, force_write: bool = False) -&gt; None:\n    \"\"\" \n    Saves the config as a pickle file.\n\n    The file will be saved to a path determined by the `data_root` specified when the dataset was created. \n    The config will be saved under the directory `data_root/tszoo/configs/`.\n\n    Parameters:\n        identifier: The name of the pickle file.\n        create_with_details_file: Whether to export the config along with a readable text file that provides details. `Defaults: True`. \n        force_write: If set to `True`, will overwrite any existing files with the same name. `Default: False`            \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to save config.\")\n\n    if exists_built_in_config(identifier):\n        raise ValueError(\"Built-in config with this identifier already exists. Choose another identifier.\")\n\n    # Ensure the config directory exists\n    if not os.path.exists(self.configs_root):\n        os.makedirs(self.configs_root)\n        self.logger.info(\"Created config directory at %s\", self.configs_root)\n\n    path_pickle = os.path.join(self.configs_root, f\"{identifier}.pickle\")\n    path_details = os.path.join(self.configs_root, f\"{identifier}.txt\")\n\n    if os.path.exists(path_pickle) and not force_write:\n        raise ValueError(f\"Config at path {path_pickle} already exists. Set force_write=True to overwrite.\")\n    self.logger.debug(\"Config pickle path: %s\", path_pickle)\n\n    if create_with_details_file:\n        if os.path.exists(path_details) and not force_write:\n            raise ValueError(f\"Config details at path {path_details} already exists. Set force_write=True to overwrite.\")\n        self.logger.debug(\"Config details path: %s\", path_details)\n\n    if self.dataset_config.is_filler_custom:\n        self.logger.warning(\"You are using a custom filler. Ensure the config is distributed with the source code of the filler.\")\n\n    if self.dataset_config.is_scaler_custom:\n        self.logger.warning(\"You are using a custom scaler. Ensure the config is distributed with the source code of the scaler.\")\n\n    pickle_dump(self._export_config_copy, path_pickle)\n    self.logger.info(\"Config pickle saved to %s\", path_pickle)\n\n    if create_with_details_file:\n        with open(path_details, \"w\", encoding=\"utf-8\") as file:\n            file.write(str(self.dataset_config))\n        self.logger.info(\"Config details saved to %s\", path_details)\n\n    self._update_config_imported_status(identifier)\n    self.logger.info(\"Config successfully saved\")\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.save_benchmark","title":"save_benchmark","text":"<pre><code>save_benchmark(identifier: str, force_write: bool = False) -&gt; None\n</code></pre> <p>Saves the benchmark as a YAML file.</p> <p>The benchmark, along with any associated annotations and config files, will be saved in a path determined by the <code>data_root</code> specified when creating the dataset.  The default save path for benchmark is <code>\"data_root/tszoo/benchmarks/\"</code>.</p> <p>If you are using imported <code>annotations</code> or <code>config</code> (whether custom or built-in), their file names will be set in the <code>benchmark</code> file.  If new <code>annotations</code> or <code>config</code> are created during the process, their filenames will be derived from the provided <code>identifier</code> and set in the <code>benchmark</code> file.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The name of the YAML file.</p> required <code>force_write</code> <code>bool</code> <p>If set to <code>True</code>, will overwrite any existing files with the same name. <code>Default: False</code></p> <code>False</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def save_benchmark(self, identifier: str, force_write: bool = False) -&gt; None:\n    \"\"\" \n    Saves the benchmark as a YAML file.\n\n    The benchmark, along with any associated annotations and config files, will be saved in a path determined by the `data_root` specified when creating the dataset. \n    The default save path for benchmark is `\"data_root/tszoo/benchmarks/\"`.\n\n    If you are using imported `annotations` or `config` (whether custom or built-in), their file names will be set in the `benchmark` file. \n    If new `annotations` or `config` are created during the process, their filenames will be derived from the provided `identifier` and set in the `benchmark` file.\n\n    Parameters:\n        identifier: The name of the YAML file.\n        force_write: If set to `True`, will overwrite any existing files with the same name. `Default: False`            \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to save benchmark.\")\n\n    if exists_built_in_benchmark(identifier):\n        raise ValueError(\"Built-in benchmark with this identifier already exists. Choose another identifier.\")\n\n    # Determine annotation names based on the available annotations and whether the annotations were imported\n    if len(self.annotations.time_series_annotations) &gt; 0:\n        annotations_ts_name = self.imported_annotations_ts_identifier if self.imported_annotations_ts_identifier is not None else f\"{identifier}_{AnnotationType.TS_ID.value}\"\n    else:\n        annotations_ts_name = None\n\n    if len(self.annotations.time_annotations) &gt; 0:\n        annotations_time_name = self.imported_annotations_time_identifier if self.imported_annotations_time_identifier is not None else f\"{identifier}_{AnnotationType.ID_TIME.value}\"\n    else:\n        annotations_time_name = None\n\n    if len(self.annotations.time_in_series_annotations) &gt; 0:\n        annotations_both_name = self.imported_annotations_both_identifier if self.imported_annotations_both_identifier is not None else f\"{identifier}_{AnnotationType.BOTH.value}\"\n    else:\n        annotations_both_name = None\n\n    # Use the imported identifier if available, otherwise default to the current identifier\n    config_name = self.dataset_config.import_identifier if self.dataset_config.import_identifier is not None else identifier\n\n    export_benchmark = ExportBenchmark(self.database_name,\n                                       self.is_series_based,\n                                       self.source_type.value,\n                                       self.aggregation.value,\n                                       config_name,\n                                       annotations_ts_name,\n                                       annotations_time_name,\n                                       annotations_both_name)\n\n    # If the config was not imported, save it\n    if self.dataset_config.import_identifier is None:\n        self.save_config(export_benchmark.config_identifier, force_write=force_write)\n    else:\n        self.logger.info(\"Using already existing config with identifier: %s\", self.dataset_config.import_identifier)\n\n    # Save ts_id annotations if available and not previously imported\n    if self.imported_annotations_ts_identifier is None and len(self.annotations.time_series_annotations) &gt; 0:\n        self.save_annotations(export_benchmark.annotations_ts_identifier, AnnotationType.TS_ID, force_write=force_write)\n    elif self.imported_annotations_ts_identifier is not None:\n        self.logger.info(\"Using already existing annotations with identifier: %s; type: %s\", self.imported_annotations_ts_identifier, AnnotationType.TS_ID)\n\n    # Save id_time annotations if available and not previously imported\n    if self.imported_annotations_time_identifier is None and len(self.annotations.time_annotations) &gt; 0:\n        self.save_annotations(export_benchmark.annotations_time_identifier, AnnotationType.ID_TIME, force_write=force_write)\n    elif self.imported_annotations_time_identifier is not None:\n        self.logger.info(\"Using already existing annotations with identifier: %s; type: %s\", self.imported_annotations_time_identifier, AnnotationType.ID_TIME)\n\n    # Save both annotations if available and not previously imported\n    if self.imported_annotations_both_identifier is None and len(self.annotations.time_in_series_annotations) &gt; 0:\n        self.save_annotations(export_benchmark.annotations_both_identifier, AnnotationType.BOTH, force_write=force_write)\n    elif self.imported_annotations_both_identifier is not None:\n        self.logger.info(\"Using already existing annotations with identifier: %s; type: %s\", self.imported_annotations_both_identifier, AnnotationType.BOTH)\n\n    # Ensure the benchmark directory exists\n    if not os.path.exists(self.benchmarks_root):\n        os.makedirs(self.benchmarks_root)\n        self.logger.info(\"Created benchmarks directory at %s\", self.benchmarks_root)\n\n    benchmark_path = os.path.join(self.benchmarks_root, f\"{identifier}.yaml\")\n\n    if os.path.exists(benchmark_path) and not force_write:\n        self.logger.error(\"Benchmark file already exists at %s\", benchmark_path)\n        raise ValueError(f\"Benchmark at path {benchmark_path} already exists. Set force_write=True to overwrite.\")\n    self.logger.debug(\"Benchmark YAML file path: %s\", benchmark_path)\n\n    yaml_dump(export_benchmark.to_dict(), benchmark_path)\n    self.logger.info(\"Benchmark successfully saved to %s\", benchmark_path)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_scalers","title":"get_scalers","text":"<pre><code>get_scalers() -&gt; np.ndarray[Scaler] | Scaler | None\n</code></pre> <p>Return used scalers from config.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_scalers(self) -&gt; np.ndarray[Scaler] | Scaler | None:\n    \"\"\"Return used scalers from config. \"\"\"\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting get scalers.\")\n\n    return self.dataset_config.scalers\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.check_errors","title":"check_errors","text":"<pre><code>check_errors() -&gt; None\n</code></pre> <p>Validates whether the dataset is corrupted. </p> <p>Raises an exception if corrupted.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def check_errors(self) -&gt; None:\n    \"\"\"\n    Validates whether the dataset is corrupted. \n\n    Raises an exception if corrupted.\n    \"\"\"\n\n    dataset, _ = load_database(self.dataset_path)\n\n    try:\n        node_iter = dataset.walk_nodes()\n\n        # Process each node in the dataset\n        for node in node_iter:\n            if isinstance(node, tb.Table):\n\n                iter_by = min(LOADING_WARNING_THRESHOLD, len(node))\n                iters_done = 0\n\n                # Process the node in chunks to avoid memory issues\n                while iters_done &lt; len(node):\n                    iter_by = min(LOADING_WARNING_THRESHOLD, len(node) - iters_done)\n                    _ = node[iters_done: iters_done + iter_by]  # Fetch the data in chunks\n                    iters_done += iter_by\n\n                self.logger.info(\"Table '%s' checked successfully. (%d rows processed)\", node._v_pathname, len(node))\n\n        self.logger.info(\"Dataset check completed with no errors found.\")\n\n    except Exception as e:\n        self.logger.error(\"Error encountered during dataset check: %s\", str(e))\n\n    finally:\n        dataset.close()\n        self.logger.debug(\"Dataset connection closed.\")\n</code></pre>"},{"location":"reference_time_based_config/","title":"Time-based config class","text":""},{"location":"reference_time_based_config/#cesnet_tszoo.configs.time_based_config.TimeBasedConfig","title":"cesnet_tszoo.configs.time_based_config.TimeBasedConfig","text":"<p>               Bases: <code>DatasetConfig</code></p> <p>This class is used for configuring the <code>TimeBasedCesnetDataset</code>.</p> <p>Used to configure the following:</p> <ul> <li>Train, validation, test, test_other, all sets (time period, sizes, features, window size)</li> <li>Handling missing values (default values, <code>fillers</code>)</li> <li>Data transformation using <code>scalers</code></li> <li>Dataloader options (train/val/test/all/init workers, batch sizes)</li> <li>Plotting</li> </ul> <p>Important Notes:</p> <ul> <li>Custom fillers must inherit from the <code>fillers</code> base class.</li> <li>Fillers can carry over values from the train set to the validation and test sets. For example, <code>ForwardFiller</code> can carry over values from previous sets.    </li> <li>It is recommended to use the <code>scalers</code> base class, though this is not mandatory as long as it meets the required methods.<ul> <li>If scalers are already initialized and <code>create_scaler_per_time_series</code> is <code>True</code> and <code>partial_fit_initialized_scalers</code> is <code>True</code> then scalers must support <code>partial_fit</code>.</li> <li>If <code>create_scaler_per_time_series</code> is <code>True</code>, scalers must have a <code>fit</code> method and <code>scale_with</code> should be a list of scalers.</li> <li>If <code>create_scaler_per_time_series</code> is <code>False</code>, scalers must support <code>partial_fit</code>.</li> <li>Scalers must implement the <code>transform</code> method.</li> <li>The <code>fit/partial_fit</code> and <code>transform</code> methods must accept an input of type <code>np.ndarray</code> with shape <code>(times, features)</code>.</li> <li>Scalers are applied to <code>test_other</code> only when <code>create_scaler_per_time_series</code> is <code>False</code>.    </li> </ul> </li> <li><code>ts_ids</code> and <code>test_ts_ids</code> must not contain any overlapping time series IDs.</li> <li><code>train_time_period</code>, <code>val_time_period</code>, <code>test_time_period</code> can overlap, but they should keep order of <code>train_time_period</code> &lt; <code>val_time_period</code> &lt; <code>test_time_period</code></li> </ul> <p>For available configuration options, refer to here.</p> <p>Attributes:</p> Name Type Description <code>used_train_workers</code> <p>Tracks the number of train workers in use. Helps determine if the train dataloader should be recreated based on worker changes.</p> <code>used_val_workers</code> <p>Tracks the number of validation workers in use. Helps determine if the validation dataloader should be recreated based on worker changes.</p> <code>used_test_workers</code> <p>Tracks the number of test workers in use. Helps determine if the test dataloader should be recreated based on worker changes.</p> <code>used_test_other_workers</code> <p>Tracks the number of test_other workers in use. Helps determine if the test_other dataloader should be recreated based on worker changes.</p> <code>used_all_workers</code> <p>Tracks the total number of all workers in use. Helps determine if the all dataloader should be recreated based on worker changes.</p> <code>import_identifier</code> <p>Tracks the name of the config upon import. None if not imported.</p> <code>logger</code> <p>Logger for displaying information.     </p> <p>The following attributes are initialized when <code>set_dataset_config_and_initialize</code> is called:</p> <p>Attributes:</p> Name Type Description <code>display_train_time_period</code> <p>Used to display the configured value of <code>train_time_period</code>.</p> <code>display_val_time_period</code> <p>Used to display the configured value of <code>val_time_period</code>.</p> <code>display_test_time_period</code> <p>Used to display the configured value of <code>test_time_period</code>.</p> <code>display_all_time_period</code> <p>Used to display the configured value of <code>all_time_period</code>.</p> <code>all_time_period</code> <p>If no specific sets (train/val/test) are provided, all time IDs are used. When any set is defined, only the time IDs in defined sets are used.</p> <code>ts_row_ranges</code> <p>Initialized when <code>ts_ids</code> is set. Contains time series IDs in <code>ts_ids</code> with their respective time ID ranges (same as <code>all_time_period</code>).</p> <code>test_ts_row_ranges</code> <p>Initialized when <code>test_ts_ids</code> is set. Contains time series IDs in <code>test_ts_ids</code> with their respective time ID ranges (same as <code>test_time_period</code>).    </p> <code>other_test_fillers</code> <p>Fillers used in the test_other set. <code>None</code> if no filler is used or test_other set is not used.</p> <code>has_ts_ids</code> <p>Flag indicating whether the <code>ts_ids</code> is in use.       </p> <code>has_test_ts_ids</code> <p>Flag indicating whether the <code>test_ts_ids</code> is in use.    </p> <code>aggregation</code> <p>The aggregation period used for the data.</p> <code>source_type</code> <p>The source type of the data.</p> <code>database_name</code> <p>Specifies which database this config applies to.</p> <code>scale_with_display</code> <p>Used to display the configured type of <code>scale_with</code>.</p> <code>fill_missing_with_display</code> <p>Used to display the configured type of <code>fill_missing_with</code>.</p> <code>features_to_take_without_ids</code> <p>Features to be returned, excluding time or time series IDs.</p> <code>indices_of_features_to_take_no_ids</code> <p>Indices of non-ID features in <code>features_to_take</code>.</p> <code>is_scaler_custom</code> <p>Flag indicating whether the scaler is custom.</p> <code>is_filler_custom</code> <p>Flag indicating whether the filler is custom.</p> <code>ts_id_name</code> <p>Name of the time series ID, dependent on <code>source_type</code>.</p> <code>used_times</code> <p>List of all times used in the configuration.</p> <code>used_ts_ids</code> <p>List of all time series IDs used in the configuration.</p> <code>used_ts_row_ranges</code> <p>List of time series IDs with their respective time ID ranges.</p> <code>used_fillers</code> <p>List of all fillers used in the configuration.</p> <code>used_singular_train_time_series</code> <p>Currently used singular train set time series for dataloader.</p> <code>used_singular_val_time_series</code> <p>Currently used singular validation set time series for dataloader.</p> <code>used_singular_test_time_series</code> <p>Currently used singular test set time series for dataloader.</p> <code>used_singular_test_other_time_series</code> <p>Currently used singular test other set time series for dataloader.</p> <code>used_singular_all_time_series</code> <p>Currently used singular all set time series for dataloader.             </p> <code>scalers</code> <p>Prepared scalers for fitting/transforming. Can be one scaler, array of scalers or <code>None</code>.</p> <code>are_scalers_premade</code> <p>Indicates whether the scalers are premade.</p> <code>has_train</code> <p>Flag indicating whether the training set is in use.</p> <code>has_val</code> <p>Flag indicating whether the validation set is in use.</p> <code>has_test</code> <p>Flag indicating whether the test set is in use.</p> <code>has_all</code> <p>Flag indicating whether the all set is in use.</p> <code>train_fillers</code> <p>Fillers used in the train set. <code>None</code> if no filler is used or train set is not used.</p> <code>val_fillers</code> <p>Fillers used in the validation set. <code>None</code> if no filler is used or validation set is not used.</p> <code>test_fillers</code> <p>Fillers used in the test set. <code>None</code> if no filler is used or test set is not used.</p> <code>all_fillers</code> <p>Fillers used for the all set. <code>None</code> if no filler is used or all set is not used.</p> <code>is_initialized</code> <p>Flag indicating if the configuration has already been initialized. If true, config initialization will be skipped.      </p>"},{"location":"reference_time_based_config/#cesnet_tszoo.configs.time_based_config.TimeBasedConfig--configuration-options","title":"Configuration options","text":"<p>Attributes:</p> Name Type Description <code>ts_ids</code> <p>Defines which time series IDs are used for train/val/test/all. Can be a list of IDs, or an integer/float to specify a random selection. An <code>int</code> specifies the number of random time series, and a <code>float</code> specifies the proportion of available time series.      <code>int</code> and <code>float</code> must be greater than 0, and a float should be smaller or equal to 1.0. Using <code>int</code> or <code>float</code> guarantees that no time series from <code>test_ts_ids</code> will be used. <code>Default: None</code> </p> <code>train_time_period</code> <p>Defines the time period for training set. Can be a range of time IDs or a tuple of datetime objects. Float value is equivalent to percentage of available times with offseted position from previous used set. <code>Default: None</code></p> <code>val_time_period</code> <p>Defines the time period for validation set. Can be a range of time IDs or a tuple of datetime objects. Float value is equivalent to percentage of available times with offseted position from previous used set. <code>Default: None</code></p> <code>test_time_period</code> <p>Defines the time period for test set. Can be a range of time IDs or a tuple of datetime objects. <code>Default: None</code></p> <code>features_to_take</code> <p>Defines which features are used. <code>Default: \"all\"</code> </p> <code>test_ts_ids</code> <p>Defines which time series IDs are used in the test_other set. Same as <code>ts_ids</code> but for the test_other set. These time series only use times in <code>test_time_period</code>. <code>Default: None</code> </p> <code>default_values</code> <p>Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature. <code>Default: \"default\"</code></p> <code>sliding_window_size</code> <p>Number of times in one window. Impacts dataloader behavior. Batch sizes affects how much data will be cached for creating windows. <code>Default: None</code></p> <code>sliding_window_prediction_size</code> <p>Number of times to predict from sliding_window_size. Impacts dataloader behavior. Batch sizes affects how much data will be cached for creating windows. <code>Default: None</code></p> <code>sliding_window_step</code> <p>Number of times to move by after each window. <code>Default: 1</code></p> <code>set_shared_size</code> <p>How much times should time periods share. Order of sharing is training set &lt; validation set &lt; test set. Only in effect if sets share less values than set_shared_size. Use float value for percentage of total times or int for count. <code>Default: 0</code></p> <code>train_batch_size</code> <p>Batch size for the train dataloader. Affects number of returned times in one batch. <code>Default: 32</code></p> <code>val_batch_size</code> <p>Batch size for the validation dataloader. Affects number of returned times in one batch. <code>Default: 64</code></p> <code>test_batch_size</code> <p>Batch size for the test dataloader. Affects number of returned times in one batch. <code>Default: 128</code></p> <code>all_batch_size</code> <p>Batch size for the all dataloader. Affects number of returned times in one batch. <code>Default: 128</code> </p> <code>fill_missing_with</code> <p>Defines how to fill missing values in the dataset. Can pass enum <code>FillerType</code> for built-in filler or pass a type of custom filler that must derive from <code>Filler</code> base class. <code>Default: None</code></p> <code>scale_with</code> <p>Defines the scaler used to transform the dataset. Can pass enum <code>ScalerType</code> for built-in scaler, pass a type of custom scaler or instance of already fitted scaler(s). <code>Default: None</code></p> <code>create_scaler_per_time_series</code> <p>If <code>True</code>, a separate scaler is created for each time series. Not used when using already initialized scalers. <code>Default: True</code></p> <code>partial_fit_initialized_scalers</code> <p>If <code>True</code>, partial fitting on train set is performed when using initiliazed scalers. <code>Default: False</code></p> <code>include_time</code> <p>If <code>True</code>, time data is included in the returned values. <code>Default: True</code></p> <code>include_ts_id</code> <p>If <code>True</code>, time series IDs are included in the returned values. <code>Default: True</code></p> <code>time_format</code> <p>Format for the returned time data. When using TimeFormat.DATETIME, time will be returned as separate list along rest of the values. <code>Default: TimeFormat.ID_TIME</code></p> <code>train_workers</code> <p>Number of workers for loading training data. <code>0</code> means that the data will be loaded in the main process. <code>Default: 4</code></p> <code>val_workers</code> <p>Number of workers for loading validation data. <code>0</code> means that the data will be loaded in the main process. <code>Default: 3</code></p> <code>test_workers</code> <p>Number of workers for loading test and test_other data. <code>0</code> means that the data will be loaded in the main process. <code>Default: 2</code></p> <code>all_workers</code> <p>Number of workers for loading all data. <code>0</code> means that the data will be loaded in the main process. <code>Default: 4</code></p> <code>init_workers</code> <p>Number of workers for initial dataset processing during configuration. <code>0</code> means that the data will be loaded in the main process. <code>Default: 4</code></p> <code>nan_threshold</code> <p>Maximum allowable percentage of missing data. Time series exceeding this threshold are excluded. Time series over the threshold will not be used. Used for <code>train/val/test/all</code> separately. <code>Default: 1.0</code></p> <code>random_state</code> <p>Fixes randomness for reproducibility during configuration and dataset initialization. <code>Default: None</code></p> Source code in <code>cesnet_tszoo\\configs\\time_based_config.py</code> <pre><code>class TimeBasedConfig(DatasetConfig):\n    \"\"\"\n    This class is used for configuring the [`TimeBasedCesnetDataset`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset].\n\n    Used to configure the following:\n\n    - Train, validation, test, test_other, all sets (time period, sizes, features, window size)\n    - Handling missing values (default values, [`fillers`][cesnet_tszoo.utils.filler])\n    - Data transformation using [`scalers`][cesnet_tszoo.utils.scaler]\n    - Dataloader options (train/val/test/all/init workers, batch sizes)\n    - Plotting\n\n    **Important Notes:**\n\n    - Custom fillers must inherit from the [`fillers`][cesnet_tszoo.utils.filler.Filler] base class.\n    - Fillers can carry over values from the train set to the validation and test sets. For example, [`ForwardFiller`][cesnet_tszoo.utils.filler.ForwardFiller] can carry over values from previous sets.    \n    - It is recommended to use the [`scalers`][cesnet_tszoo.utils.scaler.Scaler] base class, though this is not mandatory as long as it meets the required methods.\n        - If scalers are already initialized and `create_scaler_per_time_series` is `True` and `partial_fit_initialized_scalers` is `True` then scalers must support `partial_fit`.\n        - If `create_scaler_per_time_series` is `True`, scalers must have a `fit` method and `scale_with` should be a list of scalers.\n        - If `create_scaler_per_time_series` is `False`, scalers must support `partial_fit`.\n        - Scalers must implement the `transform` method.\n        - The `fit/partial_fit` and `transform` methods must accept an input of type `np.ndarray` with shape `(times, features)`.\n        - Scalers are applied to `test_other` only when `create_scaler_per_time_series` is `False`.    \n    - `ts_ids` and `test_ts_ids` must not contain any overlapping time series IDs.\n    - `train_time_period`, `val_time_period`, `test_time_period` can overlap, but they should keep order of `train_time_period` &lt; `val_time_period` &lt; `test_time_period`\n\n    For available configuration options, refer to [here][cesnet_tszoo.configs.time_based_config.TimeBasedConfig--configuration-options].\n\n    Attributes:\n        used_train_workers: Tracks the number of train workers in use. Helps determine if the train dataloader should be recreated based on worker changes.\n        used_val_workers: Tracks the number of validation workers in use. Helps determine if the validation dataloader should be recreated based on worker changes.\n        used_test_workers: Tracks the number of test workers in use. Helps determine if the test dataloader should be recreated based on worker changes.\n        used_test_other_workers: Tracks the number of test_other workers in use. Helps determine if the test_other dataloader should be recreated based on worker changes.\n        used_all_workers: Tracks the total number of all workers in use. Helps determine if the all dataloader should be recreated based on worker changes.\n        import_identifier: Tracks the name of the config upon import. None if not imported.\n        logger: Logger for displaying information.     \n\n    The following attributes are initialized when [`set_dataset_config_and_initialize`][cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.set_dataset_config_and_initialize] is called:\n\n    Attributes:\n        display_train_time_period: Used to display the configured value of `train_time_period`.\n        display_val_time_period: Used to display the configured value of `val_time_period`.\n        display_test_time_period: Used to display the configured value of `test_time_period`.\n        display_all_time_period: Used to display the configured value of `all_time_period`.\n        all_time_period: If no specific sets (train/val/test) are provided, all time IDs are used. When any set is defined, only the time IDs in defined sets are used.\n        ts_row_ranges: Initialized when `ts_ids` is set. Contains time series IDs in `ts_ids` with their respective time ID ranges (same as `all_time_period`).\n        test_ts_row_ranges: Initialized when `test_ts_ids` is set. Contains time series IDs in `test_ts_ids` with their respective time ID ranges (same as `test_time_period`).    \n        other_test_fillers: Fillers used in the test_other set. `None` if no filler is used or test_other set is not used.\n        has_ts_ids: Flag indicating whether the `ts_ids` is in use.       \n        has_test_ts_ids: Flag indicating whether the `test_ts_ids` is in use.    \n\n        aggregation: The aggregation period used for the data.\n        source_type: The source type of the data.\n        database_name: Specifies which database this config applies to.\n        scale_with_display: Used to display the configured type of `scale_with`.\n        fill_missing_with_display: Used to display the configured type of `fill_missing_with`.\n        features_to_take_without_ids: Features to be returned, excluding time or time series IDs.\n        indices_of_features_to_take_no_ids: Indices of non-ID features in `features_to_take`.\n        is_scaler_custom: Flag indicating whether the scaler is custom.\n        is_filler_custom: Flag indicating whether the filler is custom.\n        ts_id_name: Name of the time series ID, dependent on `source_type`.\n        used_times: List of all times used in the configuration.\n        used_ts_ids: List of all time series IDs used in the configuration.\n        used_ts_row_ranges: List of time series IDs with their respective time ID ranges.\n        used_fillers: List of all fillers used in the configuration.\n        used_singular_train_time_series: Currently used singular train set time series for dataloader.\n        used_singular_val_time_series: Currently used singular validation set time series for dataloader.\n        used_singular_test_time_series: Currently used singular test set time series for dataloader.\n        used_singular_test_other_time_series: Currently used singular test other set time series for dataloader.\n        used_singular_all_time_series: Currently used singular all set time series for dataloader.             \n        scalers: Prepared scalers for fitting/transforming. Can be one scaler, array of scalers or `None`.\n        are_scalers_premade: Indicates whether the scalers are premade.\n        has_train: Flag indicating whether the training set is in use.\n        has_val: Flag indicating whether the validation set is in use.\n        has_test: Flag indicating whether the test set is in use.\n        has_all: Flag indicating whether the all set is in use.\n        train_fillers: Fillers used in the train set. `None` if no filler is used or train set is not used.\n        val_fillers: Fillers used in the validation set. `None` if no filler is used or validation set is not used.\n        test_fillers: Fillers used in the test set. `None` if no filler is used or test set is not used.\n        all_fillers: Fillers used for the all set. `None` if no filler is used or all set is not used.\n        is_initialized: Flag indicating if the configuration has already been initialized. If true, config initialization will be skipped.      \n\n    # Configuration options\n\n    Attributes:\n        ts_ids: Defines which time series IDs are used for train/val/test/all. Can be a list of IDs, or an integer/float to specify a random selection. An `int` specifies the number of random time series, and a `float` specifies the proportion of available time series. \n                `int` and `float` must be greater than 0, and a float should be smaller or equal to 1.0. Using `int` or `float` guarantees that no time series from `test_ts_ids` will be used. `Default: None`    \n        train_time_period: Defines the time period for training set. Can be a range of time IDs or a tuple of datetime objects. Float value is equivalent to percentage of available times with offseted position from previous used set. `Default: None`\n        val_time_period: Defines the time period for validation set. Can be a range of time IDs or a tuple of datetime objects. Float value is equivalent to percentage of available times with offseted position from previous used set. `Default: None`\n        test_time_period: Defines the time period for test set. Can be a range of time IDs or a tuple of datetime objects. `Default: None`\n        features_to_take: Defines which features are used. `Default: \"all\"` \n        test_ts_ids: Defines which time series IDs are used in the test_other set. Same as `ts_ids` but for the test_other set. These time series only use times in `test_time_period`. `Default: None`                   \n        default_values: Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature. `Default: \"default\"`\n        sliding_window_size: Number of times in one window. Impacts dataloader behavior. Batch sizes affects how much data will be cached for creating windows. `Default: None`\n        sliding_window_prediction_size: Number of times to predict from sliding_window_size. Impacts dataloader behavior. Batch sizes affects how much data will be cached for creating windows. `Default: None`\n        sliding_window_step: Number of times to move by after each window. `Default: 1`\n        set_shared_size: How much times should time periods share. Order of sharing is training set &lt; validation set &lt; test set. Only in effect if sets share less values than set_shared_size. Use float value for percentage of total times or int for count. `Default: 0`\n        train_batch_size: Batch size for the train dataloader. Affects number of returned times in one batch. `Default: 32`\n        val_batch_size: Batch size for the validation dataloader. Affects number of returned times in one batch. `Default: 64`\n        test_batch_size: Batch size for the test dataloader. Affects number of returned times in one batch. `Default: 128`\n        all_batch_size: Batch size for the all dataloader. Affects number of returned times in one batch. `Default: 128`   \n        fill_missing_with: Defines how to fill missing values in the dataset. Can pass enum [`FillerType`][cesnet_tszoo.utils.enums.FillerType] for built-in filler or pass a type of custom filler that must derive from [`Filler`][cesnet_tszoo.utils.filler.Filler] base class. `Default: None`\n        scale_with: Defines the scaler used to transform the dataset. Can pass enum [`ScalerType`][cesnet_tszoo.utils.enums.ScalerType] for built-in scaler, pass a type of custom scaler or instance of already fitted scaler(s). `Default: None`\n        create_scaler_per_time_series: If `True`, a separate scaler is created for each time series. Not used when using already initialized scalers. `Default: True`\n        partial_fit_initialized_scalers: If `True`, partial fitting on train set is performed when using initiliazed scalers. `Default: False`\n        include_time: If `True`, time data is included in the returned values. `Default: True`\n        include_ts_id: If `True`, time series IDs are included in the returned values. `Default: True`\n        time_format: Format for the returned time data. When using TimeFormat.DATETIME, time will be returned as separate list along rest of the values. `Default: TimeFormat.ID_TIME`\n        train_workers: Number of workers for loading training data. `0` means that the data will be loaded in the main process. `Default: 4`\n        val_workers: Number of workers for loading validation data. `0` means that the data will be loaded in the main process. `Default: 3`\n        test_workers: Number of workers for loading test and test_other data. `0` means that the data will be loaded in the main process. `Default: 2`\n        all_workers: Number of workers for loading all data. `0` means that the data will be loaded in the main process. `Default: 4`\n        init_workers: Number of workers for initial dataset processing during configuration. `0` means that the data will be loaded in the main process. `Default: 4`\n        nan_threshold: Maximum allowable percentage of missing data. Time series exceeding this threshold are excluded. Time series over the threshold will not be used. Used for `train/val/test/all` separately. `Default: 1.0`\n        random_state: Fixes randomness for reproducibility during configuration and dataset initialization. `Default: None`                   \n    \"\"\"\n\n    def __init__(self,\n                 ts_ids: list[int] | npt.NDArray[np.int_] | float | int,\n                 train_time_period: tuple[datetime, datetime] | range | float | None = None,\n                 val_time_period: tuple[datetime, datetime] | range | float | None = None,\n                 test_time_period: tuple[datetime, datetime] | range | float | None = None,\n                 features_to_take: list[str] | Literal[\"all\"] = \"all\",\n                 test_ts_ids: list[int] | npt.NDArray[np.int_] | float | int | None = None,\n                 default_values: list[Number] | npt.NDArray[np.number] | dict[str, Number] | Number | Literal[\"default\"] | None = \"default\",\n                 sliding_window_size: int | None = None,\n                 sliding_window_prediction_size: int | None = None,\n                 sliding_window_step: int = 1,\n                 set_shared_size: float | int = 0,\n                 train_batch_size: int = 32,\n                 val_batch_size: int = 64,\n                 test_batch_size: int = 128,\n                 all_batch_size: int = 128,\n                 fill_missing_with: type | FillerType | Literal[\"mean_filler\", \"forward_filler\", \"linear_interpolation_filler\"] | None = None,\n                 scale_with: type | list[Scaler] | np.ndarray[Scaler] | ScalerType | Scaler | Literal[\"min_max_scaler\", \"standard_scaler\", \"max_abs_scaler\", \"log_scaler\", \"robust_scaler\", \"power_transformer\", \"quantile_transformer\", \"l2_normalizer\"] | None = None,\n                 create_scaler_per_time_series: bool = True,\n                 partial_fit_initialized_scalers: bool = False,\n                 include_time: bool = True,\n                 include_ts_id: bool = True,\n                 time_format: TimeFormat | Literal[\"id_time\", \"datetime\", \"unix_time\", \"shifted_unix_time\"] = TimeFormat.ID_TIME,\n                 train_workers: int = 4,\n                 val_workers: int = 3,\n                 test_workers: int = 2,\n                 all_workers: int = 4,\n                 init_workers: int = 4,\n                 nan_threshold: float = 1.0,\n                 random_state: int | None = None):\n\n        self.ts_ids = ts_ids\n        self.train_time_period = train_time_period\n        self.val_time_period = val_time_period\n        self.test_time_period = test_time_period\n        self.test_ts_ids = test_ts_ids\n\n        self.display_train_time_period = None\n        self.display_val_time_period = None\n        self.display_test_time_period = None\n        self.display_all_time_period = None\n        self.all_time_period = None\n        self.used_test_other_workers = None\n        self.ts_row_ranges = None\n        self.test_ts_row_ranges = None\n        self.other_test_fillers = None\n        self.has_ts_ids = False\n        self.has_test_ts_ids = False\n        self.used_singular_test_other_time_series = None\n\n        super(TimeBasedConfig, self).__init__(features_to_take, default_values, sliding_window_size, sliding_window_prediction_size, sliding_window_step, set_shared_size, train_batch_size, val_batch_size, test_batch_size, all_batch_size, fill_missing_with, scale_with, partial_fit_initialized_scalers, include_time, include_ts_id, time_format,\n                                              train_workers, val_workers, test_workers, all_workers, init_workers, nan_threshold, create_scaler_per_time_series, False, DataloaderOrder.SEQUENTIAL, random_state)\n\n    def _validate_construction(self) -&gt; None:\n        \"\"\"Performs basic parameter validation to ensure correct configuration. More comprehensive validation, which requires dataset-specific data, is handled in [`_dataset_init`][cesnet_tszoo.configs.time_based_config.TimeBasedConfig._dataset_init]. \"\"\"\n\n        super(TimeBasedConfig, self)._validate_construction()\n\n        assert self.ts_ids is not None, \"ts_ids must not be None\"\n\n        if self.test_time_period is None and self.test_ts_ids is not None:\n            self.test_ts_ids = None\n            self.logger.warning(\"test_ts_ids has been ignored because test_time_period is set to None.\")\n\n        split_float_total = 0\n\n        if isinstance(self.ts_ids, (float, int)):\n            assert self.ts_ids &gt; 0, \"ts_ids must be greater than 0\"\n            if isinstance(self.ts_ids, float):\n                split_float_total += self.ts_ids\n\n        if isinstance(self.test_ts_ids, (float, int)):\n            assert self.test_ts_ids &gt; 0, \"test_ts_ids must be greater than 0\"\n            if isinstance(self.test_ts_ids, float):\n                split_float_total += self.test_ts_ids\n\n        # Check if the total of float splits exceeds 1.0\n        if split_float_total &gt; 1.0:\n            self.logger.error(\"The total of the float split sizes is greater than 1.0. Current total: %s\", split_float_total)\n            raise ValueError(\"Total value of used float split sizes can't be greater than 1.0.\")\n\n        split_time_float_total = 0\n        train_used_float = None if self.train_time_period is None else False\n        val_used_float = None if self.val_time_period is None else False\n\n        if isinstance(self.train_time_period, (float, int)):\n            self.train_time_period = float(self.train_time_period)\n            assert self.train_time_period &gt; 0.0, \"train_time_period must be greater than 0\"\n            split_time_float_total += self.train_time_period\n            train_used_float = True\n\n        if isinstance(self.val_time_period, (float, int)):\n            if train_used_float is False:\n                raise ValueError(\"val_time_period cant use float to be set, because train_time_period was set, but did not use float.\")\n            self.val_time_period = float(self.val_time_period)\n            assert self.val_time_period &gt; 0.0, \"val_time_period must be greater than 0\"\n            split_time_float_total += self.val_time_period\n            val_used_float = True\n\n        if isinstance(self.test_time_period, (float, int)):\n            if train_used_float is False or val_used_float is False:\n                raise ValueError(\"test_time_period cant use float to be set, because previous periods were set, but did not use float.\")\n            self.test_time_period = float(self.test_time_period)\n            assert self.test_time_period &gt; 0.0, \"test_time_period must be greater than 0\"\n            split_time_float_total += self.test_time_period\n\n        # Check if the total of float splits exceeds 1.0\n        if split_time_float_total &gt; 1.0:\n            self.logger.error(\"The total of the float split sizes for time periods is greater than 1.0. Current total: %s\", split_time_float_total)\n            raise ValueError(\"Total value of used float split sizes for time periods can't be greater than 1.0.\")\n\n        self.logger.debug(\"Time-based configuration validated successfully.\")\n\n    def _update_sliding_window(self, sliding_window_size: int | None, sliding_window_prediction_size: int | None, sliding_window_step: int | None, set_shared_size: float | int, all_time_ids: np.ndarray):\n        if isinstance(set_shared_size, float):\n            assert set_shared_size &gt;= 0 and set_shared_size &lt;= 1, \"set_shared_size float value must be between or equal to 0 and 1.\"\n            set_shared_size = int(len(all_time_ids) * set_shared_size)\n\n        assert set_shared_size &gt;= 0, \"set_shared_size must be of positive value.\"\n\n        # Ensure sliding_window_size is either None or a valid integer greater than 1\n        assert sliding_window_size is None or (isinstance(sliding_window_size, int) and sliding_window_size &gt; 1), \"sliding_window_size must be an integer greater than 1, or None.\"\n\n        # Ensure sliding_window_prediction_size is either None or a valid integer greater or equal to 1\n        assert sliding_window_prediction_size is None or (isinstance(sliding_window_prediction_size, int) and sliding_window_prediction_size &gt;= 1), \"sliding_window_prediction_size must be an integer greater than 1, or None.\"\n\n        # Both sliding_window_size and sliding_window_prediction_size must be set or None\n        assert (sliding_window_size is None and sliding_window_prediction_size is None) or (sliding_window_size is not None and sliding_window_prediction_size is not None), \"Both sliding_window_size and sliding_window_prediction_size must be set or None.\"\n\n        # Adjust batch sizes based on sliding_window_size\n        if sliding_window_size is not None:\n\n            if sliding_window_step &lt;= 0:\n                raise ValueError(\"sliding_window_step must be greater or equal to 1.\")\n\n            if set_shared_size == self.set_shared_size:\n                if self.has_train and len(self.train_time_period) &lt; sliding_window_size + sliding_window_prediction_size:\n                    raise ValueError(\"New sliding window size + prediction size is larger than the number of times in train_time_period.\")\n\n                if self.has_val and len(self.val_time_period) &lt; sliding_window_size + sliding_window_prediction_size:\n                    raise ValueError(\"New sliding window size + prediction size is larger than the number of times in val_time_period.\")\n\n                if self.has_test and len(self.test_time_period) &lt; sliding_window_size + sliding_window_prediction_size:\n                    raise ValueError(\"New sliding window size + prediction size is larger than the number of times in test_time_period.\")\n\n                if self.has_all and len(self.all_time_period) &lt; sliding_window_size + sliding_window_prediction_size:\n                    raise ValueError(\"New sliding window size + prediction size is larger than the number of times in all_time_period.\")\n\n            total_window_size = sliding_window_size + sliding_window_prediction_size\n\n            if total_window_size &gt; self.train_batch_size:\n                self.train_batch_size = sliding_window_size + sliding_window_prediction_size\n                self.logger.info(\"train_batch_size adjusted to %s as it should be greater than or equal to sliding_window_size + sliding_window_prediction_size.\", total_window_size)\n            if total_window_size &gt; self.val_batch_size:\n                self.val_batch_size = sliding_window_size + sliding_window_prediction_size\n                self.logger.info(\"val_batch_size adjusted to %s as it should be greater than or equal to sliding_window_size + sliding_window_prediction_size.\", total_window_size)\n            if total_window_size &gt; self.test_batch_size:\n                self.test_batch_size = sliding_window_size + sliding_window_prediction_size\n                self.logger.info(\"test_batch_size adjusted to %s as it should be greater than or equal to sliding_window_size + sliding_window_prediction_size.\", total_window_size)\n            if total_window_size &gt; self.all_batch_size:\n                self.all_batch_size = sliding_window_size + sliding_window_prediction_size\n                self.logger.info(\"all_batch_size adjusted to %s as it should be greater than or equal to sliding_window_size + sliding_window_prediction_size.\", total_window_size)\n\n        self.sliding_window_size = sliding_window_size\n        self.sliding_window_prediction_size = sliding_window_prediction_size\n        self.sliding_window_step = sliding_window_step\n        self.set_shared_size = set_shared_size\n\n    def _get_train(self) -&gt; tuple[np.ndarray, np.ndarray] | tuple[None, None]:\n        \"\"\"Returns the indices corresponding to the training set. \"\"\"\n        return self.ts_ids, self.train_time_period\n\n    def _get_val(self) -&gt; tuple[np.ndarray, np.ndarray] | tuple[None, None]:\n        \"\"\"Returns the indices corresponding to the validation set. \"\"\"\n        return self.ts_ids, self.val_time_period\n\n    def _get_test(self) -&gt; tuple[np.ndarray, np.ndarray] | tuple[None, None]:\n        \"\"\"Returns the indices corresponding to the test set. \"\"\"\n        return self.ts_ids, self.test_time_period\n\n    def _get_all(self) -&gt; tuple[np.ndarray, np.ndarray] | tuple[None, None]:\n        \"\"\"Returns the indices corresponding to the all set. \"\"\"\n        return self.ts_ids, self.all_time_period\n\n    def _get_test_other(self) -&gt; tuple[np.ndarray, np.ndarray] | tuple[None, None]:\n        \"\"\"Returns the indices corresponding to the test_other set. \"\"\"\n        return self.test_ts_ids, self.test_time_period\n\n    def _set_time_period(self, all_time_ids: np.ndarray) -&gt; None:\n        \"\"\"Validates and filters `train_time_period`, `val_time_period`, `test_time_period` and `all_time_period` based on `dataset` and `aggregation`. \"\"\"\n\n        if isinstance(self.set_shared_size, float):\n            self.set_shared_size = int(len(all_time_ids) * self.set_shared_size)\n            self.logger.debug(\"Converted set_shared_size from float to int value.\")\n\n        times_to_share = None\n\n        # Used when periods are set with float\n        start = 0\n        end = len(all_time_ids)\n\n        if isinstance(self.train_time_period, float):\n            offset_from_start = int(end * self.train_time_period)\n            self.train_time_period = range(start, start + offset_from_start)\n            start += offset_from_start\n            self.logger.debug(\"train_time_period set with float value. Using range: %s\", self.train_time_period)\n\n        # Process and validate train time period\n        self.train_time_period, self.display_train_time_period = self._process_time_period(self.train_time_period, all_time_ids, times_to_share)\n        self.has_train = self.train_time_period is not None\n\n        if self.has_train:\n            if self.sliding_window_size is not None and len(self.train_time_period) &lt; self.sliding_window_size + self.sliding_window_prediction_size:\n                raise ValueError(\"Sliding window size + prediction size is larger than the number of times in train_time_period.\")\n            self.logger.debug(\"Processed train_time_period: %s, display_train_time_period: %s\", self.train_time_period, self.display_train_time_period)\n            if self.set_shared_size &gt; 0:\n                if self.set_shared_size &gt;= len(self.train_time_period):\n                    times_to_share = self.train_time_period[0: len(self.train_time_period)]\n                    times_to_share = all_time_ids[times_to_share[ID_TIME_COLUMN_NAME]]\n                    self.logger.warning(\"Whole training set will be shared to the next set. Consider increasing train_time_period or lowering set_shared_size. Current set_shared_size in count value is %s\", self.set_shared_size)\n                else:\n                    times_to_share = self.train_time_period[-self.set_shared_size:len(self.train_time_period)]\n                    times_to_share = all_time_ids[times_to_share[ID_TIME_COLUMN_NAME]]\n\n        if isinstance(self.val_time_period, float):\n            offset_from_start = int(end * self.val_time_period)\n            self.val_time_period = range(start, start + offset_from_start)\n            start += offset_from_start\n            self.logger.debug(\"val_time_period set with float value. Using range: %s\", self.val_time_period)\n\n        # Process and validate validation time period\n        self.val_time_period, self.display_val_time_period = self._process_time_period(self.val_time_period, all_time_ids, times_to_share)\n        self.has_val = self.val_time_period is not None\n\n        if self.has_val:\n            if self.sliding_window_size is not None and len(self.val_time_period) &lt; self.sliding_window_size + self.sliding_window_prediction_size:\n                raise ValueError(\"Sliding window size + prediction size is larger than the number of times in val_time_period.\")\n            self.logger.debug(\"Processed val_time_period: %s, display_val_time_period: %s\", self.val_time_period, self.display_val_time_period)\n            if self.set_shared_size &gt; 0:\n                if self.set_shared_size &gt;= len(self.val_time_period):\n                    times_to_share = self.val_time_period[0: len(self.val_time_period)]\n                    times_to_share = all_time_ids[times_to_share[ID_TIME_COLUMN_NAME]]\n                    self.logger.warning(\"Whole validation set will be shared to the next set. Consider increasing val_time_period or lowering set_shared_size. Current set_shared_size in count value is %s\", self.set_shared_size)\n                else:\n                    times_to_share = self.val_time_period[-self.set_shared_size:len(self.val_time_period)]\n                    times_to_share = all_time_ids[times_to_share[ID_TIME_COLUMN_NAME]]\n\n        if isinstance(self.test_time_period, float):\n            offset_from_start = int(end * self.test_time_period)\n            self.test_time_period = range(start, start + offset_from_start)\n            start += offset_from_start\n            self.logger.debug(\"test_time_period set with float value. Using range: %s\", self.test_time_period)\n\n        # Process and validate test time period\n        self.test_time_period, self.display_test_time_period = self._process_time_period(self.test_time_period, all_time_ids, times_to_share)\n        self.has_test = self.test_time_period is not None\n\n        if self.has_test:\n            if self.sliding_window_size is not None and len(self.test_time_period) &lt; self.sliding_window_size + self.sliding_window_prediction_size:\n                raise ValueError(\"Sliding window size + prediction size is larger than the number of times in test_time_period.\")\n            self.logger.debug(\"Processed test_time_period: %s, display_test_time_period: %s\", self.test_time_period, self.display_test_time_period)\n\n        if not self.has_train and not self.has_val and not self.has_test:\n            self.all_time_period = all_time_ids.copy()\n            self.all_time_period = self._set_time_period_form(self.all_time_period, all_time_ids)\n            self.logger.info(\"Using all times for all_time_period because train_time_period, val_time_period, and test_time_period are all set to None.\")\n        else:\n            for temp_time_period in [self.train_time_period, self.val_time_period, self.test_time_period]:\n                if temp_time_period is None:\n                    continue\n                elif self.all_time_period is None:\n                    self.all_time_period = temp_time_period.copy()\n                else:\n                    self.all_time_period = np.concatenate((self.all_time_period, temp_time_period))\n\n            if self.has_train:\n                self.logger.debug(\"all_time_period includes values from train_time_period.\")\n            if self.has_val:\n                self.logger.debug(\"all_time_period includes values from val_time_period.\")\n            if self.has_test:\n                self.logger.debug(\"all_time_period includes values from test_time_period.\")\n\n            self.all_time_period = np.unique(self.all_time_period)\n\n        self.has_all = self.all_time_period is not None\n\n        if self.has_all:\n            self.display_all_time_period = range(self.all_time_period[ID_TIME_COLUMN_NAME][0], self.all_time_period[ID_TIME_COLUMN_NAME][-1] + 1)\n            if self.sliding_window_size is not None and len(self.all_time_period) &lt; self.sliding_window_size + self.sliding_window_prediction_size:\n                raise ValueError(\"Sliding window size + prediction size is larger than the number of times in all_time_period.\")\n            self.logger.debug(\"Processed all_time_period: %s, display_all_time_period: %s\", self.all_time_period, self.display_all_time_period)\n\n    def _set_ts(self, all_ts_ids: np.ndarray, all_ts_row_ranges: np.ndarray) -&gt; None:\n        \"\"\" Validates and filters inputted time series id from `ts_ids` and `test_ts_ids` based on `dataset` and `source_type`. Handles random set.\"\"\"\n\n        random_ts_ids = all_ts_ids[self.ts_id_name]\n        random_indices = np.arange(len(all_ts_ids))\n\n        # Process ts_ids if it was specified with times series ids\n        if not isinstance(self.ts_ids, (float, int)):\n            self.ts_ids, self.ts_row_ranges, _ = self._process_ts_ids(self.ts_ids, all_ts_ids, all_ts_row_ranges, None, None)\n            self.has_ts_ids = True\n\n            mask = np.isin(random_ts_ids, self.ts_ids, invert=True)\n            random_ts_ids = random_ts_ids[mask]\n            random_indices = random_indices[mask]\n\n            self.logger.debug(\"ts_ids set: %s\", self.ts_ids)\n\n        # Process test_ts_ids if it was specified with times series ids\n        if self.test_ts_ids is not None and not isinstance(self.test_ts_ids, (float, int)):\n            self.test_ts_ids, self.test_ts_row_ranges, _ = self._process_ts_ids(self.test_ts_ids, all_ts_ids, all_ts_row_ranges, None, None)\n            self.has_test_ts_ids = True\n\n            mask = np.isin(random_ts_ids, self.test_ts_ids, invert=True)\n            random_ts_ids = random_ts_ids[mask]\n            random_indices = random_indices[mask]\n\n            self.logger.debug(\"test_ts_ids set: %s\", self.test_ts_ids)\n\n        # Convert proportions to total values\n        if isinstance(self.ts_ids, float):\n            self.ts_ids = int(self.ts_ids * len(random_ts_ids))\n            self.logger.debug(\"ts_ids converted to total values: %s\", self.ts_ids)\n        if isinstance(self.test_ts_ids, float):\n            self.test_ts_ids = int(self.test_ts_ids * len(random_ts_ids))\n            self.logger.debug(\"test_ts_ids converted to total values: %s\", self.test_ts_ids)\n\n        # Process random ts_ids if it is to be randomly made\n        if isinstance(self.ts_ids, int):\n            self.ts_ids, self.ts_row_ranges, random_indices = self._process_ts_ids(None, all_ts_ids, all_ts_row_ranges, self.ts_ids, random_indices)\n            self.has_ts_ids = True\n            self.logger.debug(\"Random ts_ids set with %s time series.\", self.ts_ids)\n\n        # Process random test_ts_ids if it is to be randomly made\n        if isinstance(self.test_ts_ids, int):\n            self.test_ts_ids, self.test_ts_row_ranges, random_indices = self._process_ts_ids(None, all_ts_ids, all_ts_row_ranges, self.test_ts_ids, random_indices)\n            self.has_test_ts_ids = True\n            self.logger.debug(\"Random test_ts_ids set with %s time series.\", self.test_ts_ids)\n\n    def _set_feature_scalers(self) -&gt; None:\n        \"\"\"Creates and/or validates scalers based on the `scale_with` parameter. \"\"\"\n\n        if self.scale_with is None:\n            self.scale_with_display = None\n            self.are_scalers_premade = False\n            self.scalers = None\n            self.is_scaler_custom = None\n\n            self.logger.debug(\"No scaler will be used because scale_with is not set.\")\n            return\n\n        if not self.has_train:\n            if self.partial_fit_initialized_scalers:\n                self.logger.warning(\"partial_fit_initialized_scalers will be ignored because train set is not used.\")\n            self.partial_fit_initialized_scalers = False\n\n        # Treat scale_with as a list of initialized scalers\n        if isinstance(self.scale_with, (list, np.ndarray)):\n            self.create_scaler_per_time_series = True\n\n            self.scalers = np.array(self.scale_with)\n            self.scale_with = None\n\n            assert len(self.scalers) == len(self.ts_ids), \"Number of time series in ts_ids does not match with number of provided scalers.\"\n\n            # Ensure that all scalers in the list are of the same type\n            for scaler in self.scalers:\n                if isinstance(scaler, (type, ScalerType)):\n                    raise ValueError(\"scaler_with as a list of scalers must contain only initialized scalers.\")\n\n                new_scale_with, self.scale_with_display = scaler_from_input_to_scaler_type(type(scaler), check_for_fit=False, check_for_partial_fit=self.partial_fit_initialized_scalers)\n\n                if self.scale_with is None:\n                    self.scale_with = new_scale_with\n                elif self.scale_with != new_scale_with:\n                    raise ValueError(\"Scalers in scale_with must all be of the same type.\")\n\n            self.are_scalers_premade = True\n\n            self.is_scaler_custom = \"Custom\" in self.scale_with_display\n            self.logger.debug(\"Using list of initialized scalers of type: %s\", self.scale_with_display)\n\n        # Treat scale_with as already initialized scaler\n        elif not isinstance(self.scale_with, (type, ScalerType)):\n            self.create_scaler_per_time_series = False\n\n            self.scalers = self.scale_with\n\n            self.scale_with, self.scale_with_display = scaler_from_input_to_scaler_type(type(self.scale_with), check_for_fit=False, check_for_partial_fit=self.partial_fit_initialized_scalers)\n\n            self.are_scalers_premade = True\n\n            self.is_scaler_custom = \"Custom\" in self.scale_with_display\n            self.logger.debug(\"Using initialized scaler of type: %s\", self.scale_with_display)\n\n        # Treat scale_with as uninitialized scaler\n        else:\n            if not self.has_train:\n                self.scale_with = None\n                self.scale_with_display = None\n                self.are_scalers_premade = False\n                self.scalers = None\n                self.is_scaler_custom = None\n\n                self.logger.warning(\"No scaler will be used because train set is not used.\")\n                return\n\n            self.scale_with, self.scale_with_display = scaler_from_input_to_scaler_type(self.scale_with, check_for_fit=self.create_scaler_per_time_series, check_for_partial_fit=not self.create_scaler_per_time_series)\n\n            self.are_scalers_premade = False\n\n            self.is_scaler_custom = \"Custom\" in self.scale_with_display\n            if self.create_scaler_per_time_series:\n                self.scalers = np.array([self.scale_with() for _ in self.ts_ids])\n                self.logger.debug(\"Using list of uninitialized scalers of type: %s\", self.scale_with_display)\n            else:\n                self.scalers = self.scale_with()\n                self.logger.debug(\"Using uninitialized scaler of type: %s\", self.scale_with_display)\n\n    def _set_fillers(self) -&gt; None:\n        \"\"\"Creates and/or validates fillers based on the `fill_missing_with` parameter. \"\"\"\n\n        self.fill_missing_with, self.fill_missing_with_display = filler_from_input_to_type(self.fill_missing_with)\n        self.is_filler_custom = \"Custom\" in self.fill_missing_with_display if self.fill_missing_with is not None else None\n\n        if self.fill_missing_with is None:\n            self.logger.debug(\"No filler is used because fill_missing_with is set to None.\")\n            return\n\n        # Set the fillers for the training set\n        if self.has_train:\n            self.train_fillers = np.array([self.fill_missing_with(self.features_to_take_without_ids) for _ in self.ts_ids])\n            self.logger.debug(\"Fillers for training set are set.\")\n\n        # Set the fillers for the validation set\n        if self.has_val:\n            self.val_fillers = np.array([self.fill_missing_with(self.features_to_take_without_ids) for _ in self.ts_ids])\n            self.logger.debug(\"Fillers for validation set are set.\")\n\n        # Set the fillers for the test set\n        if self.has_test:\n            self.test_fillers = np.array([self.fill_missing_with(self.features_to_take_without_ids) for _ in self.ts_ids])\n            self.logger.debug(\"Fillers for test set are set.\")\n\n        # Set the fillers for the all set\n        if self.has_all:\n            self.all_fillers = np.array([self.fill_missing_with(self.features_to_take_without_ids) for _ in self.ts_ids])\n            self.logger.debug(\"Fillers for all set are set.\")\n\n        # Set the fillers for the test_other set\n        if self.has_test_ts_ids:\n            self.other_test_fillers = np.array([self.fill_missing_with(self.features_to_take_without_ids) for _ in self.test_ts_ids])\n            self.logger.debug(\"Fillers for other_test set are set.\")\n\n    def _validate_finalization(self) -&gt; None:\n        \"\"\" Performs final validation of the configuration. Validates if `train/val/test` are continuos and that there are no overlapping time series ids in `ts_ids` and `test_ts_ids`.\"\"\"\n\n        previous_first_time_id = None\n        previous_last_time_id = None\n\n        # Validates if time periods are continuos\n        for time_period in [self.train_time_period, self.val_time_period, self.test_time_period]:\n            if time_period is None:\n                continue\n\n            current_first_time_id = time_period[0][ID_TIME_COLUMN_NAME]\n            current_last_time_id = time_period[-1][ID_TIME_COLUMN_NAME]\n\n            # Check if the first time ID is valid in relation to the previous time period's first time ID\n            if previous_first_time_id is not None:\n                if current_first_time_id &lt; previous_first_time_id:\n                    self.logger.error(\"Starting time ids of train/val/test must follow this rule: train &lt; val &lt; test\")\n                    raise ValueError(f\"Starting time ids of train/val/test must follow this rule: train &lt; val &lt; test. \"\n                                     f\"Current first time ID: {current_first_time_id}, previous first time ID: {previous_first_time_id}\")\n\n                if current_first_time_id &gt; previous_last_time_id + 1:\n                    self.logger.error(\"Starting time ids of train/val/test must be smaller or equal to last_id(next_split) + 1\")\n                    raise ValueError(f\"Starting time ids of train/val/test must be smaller or equal to last_id(next_split) + 1. \"\n                                     f\"Current first time ID: {current_first_time_id}, previous last time ID: {previous_last_time_id}\")\n\n            # Check if the last time ID is valid in relation to the previous time period's last time ID\n            if previous_last_time_id is not None:\n                if current_last_time_id &lt; previous_last_time_id:\n                    self.logger.error(\"Last time ids of train/val/test must be equal or larger than last_id(next_split)\")\n                    raise ValueError(f\"Last time ids of train/val/test must be equal or larger than last_id(next_split). \"\n                                     f\"Current last time ID: {current_last_time_id}, previous last time ID: {previous_last_time_id}\")\n\n            previous_first_time_id = current_first_time_id\n            previous_last_time_id = current_last_time_id\n\n        if self.scale_with is not None and self.create_scaler_per_time_series and self.test_ts_ids is not None:\n            self.logger.warning(\"Scalers won't be used on time series in test_ts_ids, if create_scaler_per_time_series is true.\")\n\n        # Check for overlap between ts_ids and test_ts_ids\n        if self.ts_ids is not None and self.test_ts_ids is not None:\n            mask = np.isin(self.ts_ids, self.test_ts_ids)\n            if len(self.ts_ids[mask]) &gt; 0:\n                self.logger.error(\"ts_ids and test_ts_ids can't have the same IDs!\")\n                raise ValueError(f\"ts_ids and test_ts_ids can't have the same IDs. Overlapping IDs: {self.ts_ids[mask]}\")\n\n    def __str__(self) -&gt; str:\n\n        if self.scale_with is None:\n            scaler_part = f\"Scaler type: {str(self.scale_with_display)}\"\n        else:\n            scaler_part = f'''Scaler type: {str(self.scale_with_display)}\n        Is scaler per Time series: {self.create_scaler_per_time_series}\n        Are scalers premade: {self.are_scalers_premade}\n        Are premade scalers partial_fitted: {self.partial_fit_initialized_scalers}'''\n\n        if self.include_time:\n            time_part = f'''Time included: {str(self.include_time)}    \n        Time format: {str(self.time_format)}'''\n        else:\n            time_part = f\"Time included: {str(self.include_time)}\"\n\n        return f'''\nConfig Details\n    Used for database: {self.database_name}\n    Aggregation: {str(self.aggregation)}\n    Source: {str(self.source_type)}\n\n    Time series\n        Time series IDS: {get_abbreviated_list_string(self.ts_ids)}\n        Test time series IDS: {get_abbreviated_list_string(self.test_ts_ids)}\n    Time periods\n        Train time periods: {str(self.display_train_time_period)}\n        Val time periods: {str(self.display_val_time_period)}\n        Test time periods: {str(self.display_test_time_period)}\n        All time periods: {str(self.display_all_time_period)}\n    Features\n        Taken features: {str(self.features_to_take_without_ids)}\n        Default values: {self.default_values}\n        Time series ID included: {str(self.include_ts_id)}\n        {time_part}\n    Sliding window\n        Sliding window size: {self.sliding_window_size}\n        Sliding window prediction size: {self.sliding_window_prediction_size}\n        Sliding window step size: {self.sliding_window_step}\n        Set shared size: {self.set_shared_size}\n    Fillers\n        Filler type: {str(self.fill_missing_with_display)}\n    Scalers\n        {scaler_part}\n    Batch sizes\n        Train batch size: {self.train_batch_size}\n        Val batch size: {self.val_batch_size}\n        Test batch size: {self.test_batch_size}\n        All batch size: {self.all_batch_size}\n    Default workers\n        Init worker count: {str(self.init_workers)}\n        Train worker count: {str(self.train_workers)}\n        Val worker count: {str(self.val_workers)}\n        Test worker count: {str(self.test_workers)}\n        All worker count: {str(self.all_workers)}\n    Other\n        Nan threshold: {str(self.nan_threshold)}\n        Random state: {self.random_state}\n                '''\n</code></pre>"},{"location":"scalers/","title":"Scalers","text":"<p>The <code>cesnet_tszoo</code> package supports various ways of using scalers to transform data. Scaler(s) can be created and fitted (on train set) when initializing dataset with config. Or already fitted scaler(s) can be passed to transform data.</p>"},{"location":"scalers/#built-in-scalers","title":"Built-in scalers","text":"<p>The <code>cesnet_tszoo</code> package comes with multiple built-in scalers. Not all of them support <code>partial_fit</code> though. To check built-in scalers refer to <code>scalers</code>.</p>"},{"location":"scalers/#custom-scalers","title":"Custom scalers","text":"<p>It is possible to create and use own scalers. It is recommended to use prepared base class <code>Scaler</code>.</p>"},{"location":"scalers/#using-scalers-on-time-based-dataset","title":"Using scalers on time-based dataset","text":"<p>Related config parameters in <code>TimeBasedConfig</code>:</p> <ul> <li><code>scale_with</code>:  Defines the scaler(s) to transform the dataset. Can pass enum <code>ScalerType</code> for built-in scaler, pass a type of custom scaler or instance of already fitted scaler(s).</li> <li><code>create_scaler_per_time_series</code>: Whether to create a separate scaler for each time series or create one scaler for all time series.</li> <li><code>partial_fit_initialized_scalers</code>: Whether to <code>partial_fit</code> already fitted scaler(s).</li> </ul> <p>Time series in test_ts_ids</p> <p>Time series in <code>test_ts_ids</code> will not be transformed when <code>create_scaler_per_time_series</code> = <code>True</code>. But they will be transformed when <code>create_scaler_per_time_series</code> = <code>False</code>.</p> <p>fit vs partial_fit</p> <p>When <code>create_scaler_per_time_series</code> = <code>True</code> and scalers are not pre-fitted, scalers must implement <code>fit</code> method. Else if you want to fit scalers, <code>partial_fit</code> method must be implemented. Check <code>Scaler</code> for details.</p>"},{"location":"scalers/#using-scalers-on-series-based-dataset","title":"Using scalers on series-based dataset","text":"<p>Series-based dataset always uses <code>create_scaler_per_time_series</code> = <code>False</code>. Related config parameters in <code>SeriesBasedConfig</code>:</p> <ul> <li><code>scale_with</code>:  Defines the scaler to transform the dataset. Can pass enum <code>ScalerType</code> for built-in scaler, pass a type of custom scaler or instance of already fitted scaler.</li> <li><code>partial_fit_initialized_scalers</code>: Whether to <code>partial_fit</code> already fitted scaler.</li> </ul> <p>partial_fit</p> <p>Scaler must implement <code>partial_fit</code> method unless using already fitted scaler without fitting it on train data. Check <code>Scaler</code> for details.    </p>"},{"location":"using_datasets/","title":"Using datasets","text":"<p>This tutorial will look at what you need to use dataset.  Trying to use dataset you do not have downloaded, will automatically download it.</p> <p>There currently two supported datasets:</p> <ul> <li>CESNET-TimeSeries24 - supports time-based and series-based</li> <li>CESNET-AGG23 - supports only time-based</li> </ul>"},{"location":"using_datasets/#using-dataset-from-benchmark","title":"Using dataset from benchmark","text":"<p>You can refer to benchmarks for more detailed usage.</p> <pre><code>from cesnet_tszoo.benchmarks import load_benchmark                                                                       \n\n# Imports built-in benchmark\nbenchmark = load_benchmark(identifier=\"2e92831cb502\", data_root=\"/some_directory/\")\ndataset = benchmark.get_initialized_dataset(display_config_details=True, check_errors=False, workers=\"config\")\n\n# Imports custom benchmark\nbenchmark = load_benchmark(identifier=\"test2\", data_root=\"/some_directory/\")\ndataset = benchmark.get_initialized_dataset(display_config_details=True, check_errors=False, workers=\"config\")\n</code></pre>"},{"location":"using_datasets/#creating-dataset","title":"Creating dataset","text":"<p>You can refer to choosing_data for more detailed data selection via config.</p>"},{"location":"using_datasets/#using-dataset-from-cesnet_timeseries24","title":"Using dataset from CESNET_TimeSeries24","text":"<pre><code>from cesnet_tszoo.configs import TimeBasedConfig # For time-based dataset\nfrom cesnet_tszoo.configs import SeriesBasedConfig # For series-based dataset   \n\nfrom cesnet_tszoo.utils.enums import AgreggationType, SourceType # Used for specifying which dataset to use\nfrom cesnet_tszoo.datasets import CESNET_TimeSeries24\n\n# Time-based\ntime_based_dataset = CESNET_TimeSeries24.get_dataset(data_root=\"/some_directory/\", source_type=SourceType.INSTITUTIONS, aggregation=AgreggationType.AGG_1_DAY, is_series_based=False)\nconfig = TimeBasedConfig(ts_ids=50)\ntime_based_dataset.set_dataset_config_and_initialize(config)\n\n# Series-based\nseries_based_dataset = CESNET_TimeSeries24.get_dataset(data_root=\"/some_directory/\", source_type=SourceType.INSTITUTIONS, aggregation=AgreggationType.AGG_1_DAY, is_series_based=True)\nconfig = SeriesBasedDataset(time_period=range(0, 200))\nseries_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre>"},{"location":"using_datasets/#using-dataset-from-cesnet_agg23","title":"Using dataset from CESNET_AGG23","text":"<pre><code>from cesnet_tszoo.configs import TimeBasedConfig # For time-based dataset \n\nfrom cesnet_tszoo.datasets import CESNET_AGG23\n\n# Using dataset from CESNET_AGG23\n# Only time-based\ntime_based_dataset = CESNET_TimeSeries24.get_dataset(data_root=\"/some_directory/\")\nconfig = TimeBasedConfig(ts_ids=1)\ntime_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre>"},{"location":"using_scalers/","title":"Using scalers","text":"<p>This tutorial will look at some configuration options for using scalers.</p> <p>Each dataset type will have its own part because of multiple differences of available configuration values.</p>"},{"location":"using_scalers/#timebasedcesnetdataset-dataset","title":"<code>TimeBasedCesnetDataset</code> dataset","text":"<p>Note</p> <p>For every configuration and more detailed examples refer to Jupyter notebook <code>time_based_using_scalers</code></p> <p>Relevant configuration values:</p> <ul> <li><code>scale_with</code> - Defines the scaler used to transform the dataset.</li> <li><code>create_scaler_per_time_series</code> - If True, a separate scaler is created for each time series and scalers wont be used for time series on 'test_ts_id'.</li> <li><code>partial_fit_initialized_scalers</code> - If True, partial fitting on train set is performed when using initiliazed scalers.</li> </ul>"},{"location":"using_scalers/#scalers","title":"Scalers","text":"<ul> <li>Scalers are implemented as class.<ul> <li>You can create your own or use built-in one.</li> </ul> </li> <li>Scaler must implement <code>transform</code>.</li> <li>Scalers are applied after <code>default_values</code> and fillers took care of missing values.</li> <li>To use scalers, train set must be implemented (unless scalers are already fitted and <code>partial_fit_initialized_scalers</code> is False).</li> <li><code>fit</code> method on scaler:<ul> <li>must be implemented when <code>create_scaler_per_time_series</code> is True and scalers are not already fitted.</li> </ul> </li> <li><code>partial_fit</code> method on scaler:<ul> <li>must be implemented when <code>create_scaler_per_time_series</code> is False or using already fitted scalers with <code>partial_fit_initialized_scalers</code> set to True.</li> </ul> </li> <li>You can change used scaler later with <code>update_dataset_config_and_initialize</code> or <code>apply_scaler</code>.</li> </ul>"},{"location":"using_scalers/#built-in","title":"Built-in","text":"<p>To see all built-in scalers refer to <code>Scalers</code>.</p> <pre><code>from cesnet_tszoo.utils.enums import ScalerType\nfrom cesnet_tszoo.configs import TimeBasedConfig\n\nconfig = TimeBasedConfig(ts_ids=[1367, 1368], train_time_period=0.5, val_time_period=0.2, test_time_period=0.1, test_ts_ids=[1370], features_to_take=['n_flows', 'n_packets'],\n                         scale_with=ScalerType.MIN_MAX_SCALER, create_scaler_per_time_series=True)                                                                              \n\n# Call on time-based dataset to use created config\ntime_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre> <p>Or later with:</p> <pre><code>time_based_dataset.update_dataset_config_and_initialize(scale_with=ScalerType.MIN_MAX_SCALER, create_scaler_per_time_series=True, partial_fit_initialized_scalers=\"config\", workers=0)\n# Or\ntime_based_dataset.apply_scaler(scale_with=ScalerType.MIN_MAX_SCALER, create_scaler_per_time_series=True, partial_fit_initialized_scalers=\"config\", workers=0)\n</code></pre>"},{"location":"using_scalers/#custom","title":"Custom","text":"<p>You can create your own custom scaler. It is recommended to derive from 'Scaler' base class. </p> <p>To check Scaler base class refer to <code>Scaler</code></p> <pre><code>from cesnet_tszoo.utils.scaler import Scaler\nfrom cesnet_tszoo.configs import TimeBasedConfig\n\nclass CustomScaler(Scaler):\n    def __init__(self):\n        super().__init__()\n\n        self.max = None\n        self.min = None\n\n    def transform(self, data):\n        return (data - self.min) / (self.max - self.min)\n\n    def fit(self, data):\n        self.partial_fit(data)\n\n    def partial_fit(self, data):\n\n        if self.max is None and self.min is None:\n            self.max = np.max(data, axis=0)\n            self.min = np.min(data, axis=0)\n            return\n\n        temp_max = np.max(data, axis=0)\n        temp = np.vstack((self.max, temp_max)) \n        self.max = np.max(temp, axis=0)\n\n        temp_min = np.min(data, axis=0)\n        temp = np.vstack((self.min, temp_min)) \n        self.min = np.min(temp, axis=0)            \n\nconfig = TimeBasedConfig(ts_ids=[1367, 1368], train_time_period=0.5, val_time_period=0.2, test_time_period=0.1, test_ts_ids=[1370], features_to_take=['n_flows', 'n_packets'],\n                         scale_with=CustomScaler, create_scaler_per_time_series=True)                                                                        \n\ntime_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre> <p>Or later with:</p> <pre><code>time_based_dataset.update_dataset_config_and_initialize(scale_with=CustomScaler, create_scaler_per_time_series=True, partial_fit_initialized_scalers=\"config\", workers=0)\n# Or\ntime_based_dataset.apply_scaler(scale_with=CustomScaler, create_scaler_per_time_series=True, partial_fit_initialized_scalers=\"config\", workers=0)\n</code></pre>"},{"location":"using_scalers/#using-already-fitted-scalers","title":"Using already fitted scalers","text":"<pre><code>from cesnet_tszoo.configs import TimeBasedConfig         \n\nconfig = TimeBasedConfig(ts_ids=[103, 118], train_time_period=0.5, val_time_period=0.2, test_time_period=0.1, test_ts_ids=[1370], features_to_take=['n_flows', 'n_packets'],\n                         scale_with=list_of_fitted_scalers, create_scaler_per_time_series=True)    \n\n# Length of list_of_fitted_scalers must be equal to number of time series in ts_ids \n# All scalers in list_of_fitted_scalers must be of same type                                                            \n\nconfig = TimeBasedConfig(ts_ids=[103, 118], train_time_period=0.5, val_time_period=0.2, test_time_period=0.1, test_ts_ids=[1370], features_to_take=['n_flows', 'n_packets'],\n                         scale_with=one_prefitted_scaler, create_scaler_per_time_series=True)\n\n# one_prefitted_scaler must be just one scaler (not a list)                     \n\ntime_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre>"},{"location":"using_scalers/#seriesbasedcesnetdataset-dataset","title":"<code>SeriesBasedCesnetDataset</code> dataset","text":"<p>Note</p> <p>For every configuration and more detailed examples refer to Jupyter notebook <code>series_based_using_scalers</code></p> <p>Relevant configuration values:</p> <ul> <li><code>scale_with</code> - Defines the scaler used to transform the dataset.</li> <li><code>partial_fit_initialized_scalers</code> - If True, partial fitting on train set is performed when using initiliazed scaler.</li> </ul>"},{"location":"using_scalers/#scalers_1","title":"Scalers","text":"<ul> <li>Scalers are implemented as class.<ul> <li>You can create your own or use built-in one.</li> </ul> </li> <li>Scaler is applied after <code>default_values</code> and fillers took care of missing values.</li> <li>One scaler is used for all time series.</li> <li>Scaler must implement <code>transform</code>.</li> <li>Scaler must implement <code>partial_fit</code> (unless scaler is already fitted and <code>partial_fit_initialized_scalers</code> is False).</li> <li>To use scaler, train set must be implemented (unless scaler is already fitted and <code>partial_fit_initialized_scalers</code> is False).</li> <li>You can change used scaler later with <code>update_dataset_config_and_initialize</code> or <code>apply_scaler</code>.</li> </ul>"},{"location":"using_scalers/#built-in_1","title":"Built-in","text":"<p>To see all built-in scalers refer to <code>Scalers</code>.</p> <pre><code>from cesnet_tszoo.utils.enums import ScalerType\nfrom cesnet_tszoo.configs import SeriesBasedConfig\n\nconfig = SeriesBasedConfig(time_period=0.5, train_ts=500, features_to_take=[\"n_flows\", \"n_packets\"],\n                           scale_with=ScalerType.MIN_MAX_SCALER, nan_threshold=0.5, random_state=1500)                                                                          \n\n# Call on series-based dataset to use created config\nseries_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre> <p>Or later with:</p> <pre><code>series_based_dataset.update_dataset_config_and_initialize(scale_with=ScalerType.MIN_MAX_SCALER, partial_fit_initialized_scalers=\"config\", workers=0)\n# Or\nseries_based_dataset.apply_scaler(scale_with=ScalerType.MIN_MAX_SCALER, partial_fit_initialized_scalers=\"config\", workers=0)\n</code></pre>"},{"location":"using_scalers/#custom_1","title":"Custom","text":"<p>You can create your own custom scaler. It is recommended to derive from 'Scaler' base class. </p> <p>To check Scaler base class refer to <code>Scaler</code></p> <pre><code>from cesnet_tszoo.utils.scaler import Scaler\nfrom cesnet_tszoo.configs import SeriesBasedConfig\n\nclass CustomScaler(Scaler):\n    def __init__(self):\n        super().__init__()\n\n        self.max = None\n        self.min = None\n\n    def transform(self, data):\n        return (data - self.min) / (self.max - self.min)\n\n    def fit(self, data):\n        self.partial_fit(data)\n\n    def partial_fit(self, data):\n\n        if self.max is None and self.min is None:\n            self.max = np.max(data, axis=0)\n            self.min = np.min(data, axis=0)\n            return\n\n        temp_max = np.max(data, axis=0)\n        temp = np.vstack((self.max, temp_max)) \n        self.max = np.max(temp, axis=0)\n\n        temp_min = np.min(data, axis=0)\n        temp = np.vstack((self.min, temp_min)) \n        self.min = np.min(temp, axis=0)            \n\nconfig = SeriesBasedConfig(time_period=0.5, train_ts=500, features_to_take=[\"n_flows\", \"n_packets\"],\n                           scale_with=CustomScaler, nan_threshold=0.5, random_state=1500)                                                                    \n\nseries_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre> <p>Or later with:</p> <pre><code>series_based_dataset.update_dataset_config_and_initialize(scale_with=CustomScaler, partial_fit_initialized_scalers=\"config\", workers=0)\n# Or\nseries_based_dataset.apply_scaler(scale_with=CustomScaler, partial_fit_initialized_scalers=\"config\", workers=0)\n</code></pre>"},{"location":"using_scalers/#using-already-fitted-scalers_1","title":"Using already fitted scalers","text":"<pre><code>from cesnet_tszoo.configs import SeriesBasedConfig         \n\nconfig = SeriesBasedConfig(time_period=0.5, val_ts=500, features_to_take=[\"n_flows\", \"n_packets\"],\n                           scale_with=fitted_scaler, nan_threshold=0.5, random_state=999)   \n\n# fitted_scaler must be just one scaler (not a list)                                                                     \n\nseries_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre>"},{"location":"utilities/","title":"Utilities","text":"<p>This tutorial will look at various utilities.</p> <p>Only time-based will be used, because all methods work almost the same way for series-based.</p> <p>Note</p> <p>For every option and more detailed examples refer to Jupyter notebook <code>utilities</code></p>"},{"location":"utilities/#setting-logger","title":"Setting logger","text":"<p>CESNET TS-Zoo uses logger, but without setting config below, it wont log anything.</p> <pre><code>import logging                                                                       \n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"[%(asctime)s][%(name)s][%(levelname)s] - %(message)s\")\n</code></pre>"},{"location":"utilities/#checking-errors","title":"Checking errors","text":"<ul> <li>Goes through all data in dataset to check whether everything is in correct state,</li> <li>Can be called when creating dataset or with method <code>check_errors</code> on already create dataset.</li> <li>Recommended to call at least once after download</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import AgreggationType, SourceType\nfrom cesnet_tszoo.datasets import CESNET_TimeSeries24                                                                \n\n# Can be called at dataset creation\ntime_based_dataset = CESNET_TimeSeries24.get_dataset(data_root=\"/some_directory/\", source_type=SourceType.IP_ADDRESSES_SAMPLE, aggregation=AgreggationType.AGG_1_DAY, is_series_based=False, check_errors=True)\n\n# Or after it\ntime_based_dataset.check_errors()\n</code></pre>"},{"location":"utilities/#dataset-details","title":"Dataset details","text":""},{"location":"utilities/#displaying-all-data-about-selected-dataset","title":"Displaying all data about selected dataset","text":"<p>Displays available times, time series, features with their default values, additional data provided by dataset.</p> <pre><code>dataset.display_dataset_details()\n</code></pre>"},{"location":"utilities/#get-list-of-available-features","title":"Get list of available features","text":"<pre><code>dataset.get_feature_names()\n</code></pre>"},{"location":"utilities/#get-numpy-array-of-available-dataset-time-series-indices","title":"Get numpy array of available dataset time series indices","text":"<pre><code>dataset.get_available_ts_indices()\n</code></pre>"},{"location":"utilities/#get-dictionary-of-related-set-data","title":"Get dictionary of related set data","text":"<p>Returns all data in dictionary related to set.</p> <pre><code>from cesnet_tszoo.configs import TimeBasedConfig   \n\nconfig = TimeBasedConfig(20, train_time_period=0.5)\ntime_based_dataset.set_dataset_config_and_initialize(config, workers=0, display_config_details=False)\n\ntime_based_dataset.get_data_about_set(about=SplitType.TRAIN)\n</code></pre>"},{"location":"utilities/#displaying-config-details","title":"Displaying config details","text":"<p>Can be called when calling <code>set_dataset_config_and_initialize</code> or after it with <code>display_config</code></p> <pre><code>from cesnet_tszoo.configs import TimeBasedConfig   \n\nconfig = TimeBasedConfig(20)\n\n# Can be called during initialization\ntime_based_dataset.set_dataset_config_and_initialize(config, workers=0, display_config_details=True)\n\n# Or after it\ntime_based_dataset.display_config()\n</code></pre>"},{"location":"utilities/#plotting","title":"Plotting","text":"<ul> <li>Uses <code>Plotly</code> library.</li> <li>You can plot specific time series with method <code>plot</code></li> <li>You can set <code>ts_id</code> to any time series id used in config</li> <li>Plot will always contains time period of all set</li> <li>Config must be set before using</li> </ul> <pre><code># Features will be taken from config\ndataset.plot(ts_id=10, plot_type=\"line\", features=\"config\", feature_per_plot=True, time_format=\"datetime\", use_scalers=True)\n\n# Specifies features as list... features must be set in used config\ndataset.plot(ts_id=10, plot_type=\"line\", features=[\"n_flows\", \"n_packets\"], feature_per_plot=True, time_format=\"datetime\", use_scalers=True)\n\n# Can specify single feature... still must be set in used config\ndataset.plot(ts_id=10, plot_type=\"line\", features=\"n_flows\", feature_per_plot=True, time_format=\"datetime\", use_scalers=True)\n</code></pre>"},{"location":"utilities/#get-additional-data","title":"Get additional data","text":"<ul> <li>You can check whether dataset has additional data, with method <code>display_dataset_details</code>.</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import AgreggationType, SourceType\nfrom cesnet_tszoo.datasets import CESNET_TimeSeries24                                                                \n\ntime_based_dataset = CESNET_TimeSeries24.get_dataset(data_root=\"/some_directory/\", source_type=SourceType.IP_ADDRESSES_SAMPLE, aggregation=AgreggationType.AGG_1_DAY, is_series_based=False, display_details=True)\n\n# Available additional data in CESNET_TimeSeries24 database\ntime_based_dataset.get_additional_data('ids_relationship')\ntime_based_dataset.get_additional_data('weekends_and_holidays')\n</code></pre>"},{"location":"utilities/#get-fitted-scalers","title":"Get fitted scalers","text":"<p>Returns used scaler/s that are used for transforming data.</p> <pre><code>dataset.get_scalers()\n</code></pre>"},{"location":"benchmarks/device_type_classification/69270dcc1819/","title":"69270dcc1819","text":"Parameter Value Benchmark hash 0d523e69c328 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source IP_ADDRESSES_FULL Train size 0.6 Val size 0.2 Test size 0.2 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Scaler None* Sliding window train None Sliding window prediction None Sliding window step None Set shared size None All batch size 7* Train TS IDs 1.0 Test TS IDs 0.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Accuracy Precision Recall F1-score"},{"location":"benchmarks/device_type_classification/941261e8c367/","title":"941261e8c367","text":"Parameter Value Benchmark hash 0d523e69c328 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source IP_ADDRESSES_FULL Train size 0.6 Val size 0.2 Test size 0.2 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Scaler None* Sliding window train None Sliding window prediction None Sliding window step None Set shared size None All batch size 7* Train TS IDs 1.0 Test TS IDs 0.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Accuracy Precision Recall F1-score"},{"location":"benchmarks/device_type_classification/bf0aec939afe/","title":"Benchamrk bf0aec939afe","text":"Parameter Value Benchmark hash 0d523e69c328 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source IP_ADDRESSES_FULL Train size 0.6 Val size 0.2 Test size 0.2 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Scaler None* Sliding window train None Sliding window prediction None Sliding window step None Set shared size None All batch size 7* Train TS IDs 1.0 Test TS IDs 0.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Accuracy Precision Recall F1-score"},{"location":"benchmarks/multivariate_forecasting/generic_model/0197980a87c0/","title":"Benchmark 0197980a87c0","text":"Parameter Value Benchmark hash 0197980a87c0 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source IP_ADDRESSES_FULL Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Scaler None* Sliding window train 7* Sliding window prediction 1* Sliding window step 1* set_shared_size 7* train_ts_ids 0.5 test_ts_ids 0.5 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/multivariate_forecasting/generic_model/16274e0b44af/","title":"Benchmark 16274e0b44af","text":"Parameter Value Benchmark hash 16274e0b44af Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source IP_ADDRESSES_SAMPLE Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Scaler None* Sliding window train 7* Sliding window prediction 1* Sliding window step 1* set_shared_size 7* train_ts_ids 0.5 test_ts_ids 0.5 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/multivariate_forecasting/generic_model/4ae11863ee38/","title":"Benchmark 4ae11863ee38","text":"Parameter Value Benchmark hash 4ae11863ee38 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source IP_ADDRESSES_FULL Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Scaler None* Sliding window train 168* Sliding window prediction 1* Sliding window step 1* set_shared_size 168* train_ts_ids 0.5 test_ts_ids 0.5 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/multivariate_forecasting/generic_model/50eb509e1e77/","title":"Benchmark 50eb509e1e77","text":"Parameter Value Benchmark hash 50eb509e1e77 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source IP_ADDRESSES_SAMPLE Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Scaler None* Sliding window train 1008* Sliding window prediction 1* Sliding window step 1* set_shared_size 1008* train_ts_ids 0.5 test_ts_ids 0.5 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/multivariate_forecasting/generic_model/681a7fb90948/","title":"Benchmark 681a7fb90948","text":"Parameter Value Benchmark hash 681a7fb90948 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source IP_ADDRESSES_FULL Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Scaler None* Sliding window train 1008* Sliding window prediction 1* Sliding window step 1* set_shared_size 1008* train_ts_ids 0.5 test_ts_ids 0.5 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/multivariate_forecasting/generic_model/7cd4e41b05ec/","title":"Benchmark 7cd4e41b05ec","text":"Parameter Value Benchmark hash 7cd4e41b05ec Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source INSTITUTION_SUBNETS Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Scaler None* Sliding window train 1008* Sliding window prediction 1* Sliding window step 1* set_shared_size 1008* train_ts_ids 0.5 test_ts_ids 0.5 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/multivariate_forecasting/generic_model/88fd173619b2/","title":"Benchmark 88fd173619b2","text":"Parameter Value Benchmark hash 88fd173619b2 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source IP_ADDRESSES_SAMPLE Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Scaler None* Sliding window train 168* Sliding window prediction 1* Sliding window step 1* set_shared_size 168* train_ts_ids 0.5 test_ts_ids 0.5 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/multivariate_forecasting/generic_model/9ac2b87c9a7c/","title":"Benchmark 9ac2b87c9a7c","text":"Parameter Value Benchmark hash 9ac2b87c9a7c Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source INSTITUTIONS Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Scaler None* Sliding window train 1008* Sliding window prediction 1* Sliding window step 1* set_shared_size 1008* train_ts_ids 0.5 test_ts_ids 0.5 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/multivariate_forecasting/generic_model/ab8183ea80af/","title":"Benchmark ab8183ea80af","text":"Parameter Value Benchmark hash ab8183ea80af Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTIONS Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Scaler None* Sliding window train 168* Sliding window prediction 1* Sliding window step 1* set_shared_size 168* train_ts_ids 0.5 test_ts_ids 0.5 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/multivariate_forecasting/generic_model/c95d66b0baf5/","title":"Benchmark c95d66b0baf5","text":"Parameter Value Benchmark hash c95d66b0baf5 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source INSTITUTION_SUBNETS Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Scaler None* Sliding window train 7* Sliding window prediction 1* Sliding window step 1* set_shared_size 7* train_ts_ids 0.5 test_ts_ids 0.5 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/multivariate_forecasting/generic_model/cdb79dbf54ea/","title":"Benchmark cdb79dbf54ea","text":"Parameter Value Benchmark hash cdb79dbf54ea Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source INSTITUTIONS Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Scaler None* Sliding window train 7* Sliding window prediction 1* Sliding window step 1* set_shared_size 7* train_ts_ids 0.5 test_ts_ids 0.5 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/multivariate_forecasting/generic_model/f9bd005c7efe/","title":"Benchmark f9bd005c7efe","text":"Parameter Value Benchmark hash f9bd005c7efe Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTION_SUBNETS Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Scaler None* Sliding window train 168* Sliding window prediction 1* Sliding window step 1* set_shared_size 168* train_ts_ids 0.5 test_ts_ids 0.5 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/multivariate_forecasting/unique_model/084f368f4c82/","title":"Benchmark 084f368f4c82","text":"Parameter Value Benchmark hash 084f368f4c82 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source IP_ADDRESSES_FULL Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Scaler None* Sliding window train 7* Sliding window prediction 1* Sliding window step 1* Set shared size 7* Train TS IDs 1 Test TS IDs 0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/multivariate_forecasting/unique_model/15737f3fceec/","title":"Benchmark 15737f3fceec","text":"Parameter Value Benchmark hash 15737f3fceec Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source IP_ADDRESSES_SAMPLE Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Scaler None* Sliding window train 7* Sliding window prediction 1* Sliding window step 1* Set shared size 7* Train TS IDs 1 Test TS IDs 0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/multivariate_forecasting/unique_model/18d04cab63e4/","title":"Benchmark 18d04cab63e4","text":"Parameter Value Benchmark hash 18d04cab63e4 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source IP_ADDRESSES_SAMPLE Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Scaler None* Sliding window train 168* Sliding window prediction 1* Sliding window step 1* Set shared size 168* Train TS IDs 1 Test TS IDs 0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/multivariate_forecasting/unique_model/3687fb52c433/","title":"Benchmark 3687fb52c433","text":"Parameter Value Benchmark hash 3687fb52c433 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source IP_ADDRESSES_FULL Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Scaler None* Sliding window train 1008* Sliding window prediction 1* Sliding window step 1* Set shared size 1008* Train TS IDs 1 Test TS IDs 0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/multivariate_forecasting/unique_model/63e1f696e7c5/","title":"Benchmark 63e1f696e7c5","text":"Parameter Value Benchmark hash 63e1f696e7c5 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source INSTITUTIONS Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Scaler None* Sliding window train 7* Sliding window prediction 1* Sliding window step 1* Set shared size 7* Train TS IDs 1 Test TS IDs 0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/multivariate_forecasting/unique_model/71d17ad3550f/","title":"Benchmark 71d17ad3550f","text":"Parameter Value Benchmark hash 71d17ad3550f Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source INSTITUTION_SUBNETS Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Scaler None* Sliding window train 7* Sliding window prediction 1* Sliding window step 1* Set shared size 7* Train TS IDs 1 Test TS IDs 0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/multivariate_forecasting/unique_model/7495b16f5fe6/","title":"Benchmark 7495b16f5fe6","text":"Parameter Value Benchmark hash 7495b16f5fe6 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source IP_ADDRESSES_SAMPLE Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Scaler None* Sliding window train 1008* Sliding window prediction 1* Sliding window step 1* Set shared size 1008* Train TS IDs 1 Test TS IDs 0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/multivariate_forecasting/unique_model/930f0b401065/","title":"Benchmark 930f0b401065","text":"Parameter Value Benchmark hash 930f0b401065 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source INSTITUTIONS Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Scaler None* Sliding window train 1008* Sliding window prediction 1* Sliding window step 1* Set shared size 1008* Train TS IDs 1 Test TS IDs 0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/multivariate_forecasting/unique_model/a6e56f99ab8a/","title":"Benchmark a6e56f99ab8a","text":"Parameter Value Benchmark hash a6e56f99ab8a Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTIONS Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Scaler None* Sliding window train 168* Sliding window prediction 1* Sliding window step 1* Set shared size 168* Train TS IDs 1 Test TS IDs 0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/multivariate_forecasting/unique_model/b0ea46897cae/","title":"Benchmark b0ea46897cae","text":"Parameter Value Benchmark hash b0ea46897cae Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source IP_ADDRESSES_FULL Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Scaler None* Sliding window train 168* Sliding window prediction 1* Sliding window step 1* Set shared size 168* Train TS IDs 1 Test TS IDs 0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/multivariate_forecasting/unique_model/ca6999ea7e24/","title":"Benchmark ca6999ea7e24","text":"Parameter Value Benchmark hash ca6999ea7e24 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source INSTITUTION_SUBNETS Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Scaler None* Sliding window train 1008* Sliding window prediction 1* Sliding window step 1* Set shared size 1008* Train TS IDs 1 Test TS IDs 0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/multivariate_forecasting/unique_model/e44334732033/","title":"Benchmark e44334732033","text":"Parameter Value Benchmark hash e44334732033 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTION_SUBNETS Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Scaler None* Sliding window train 168* Sliding window prediction 1* Sliding window step 1* Set shared size 168* Train TS IDs 1 Test TS IDs 0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/univariate_forecasting/generic_model/09de83e89e42/","title":"Benchmark 09de83e89e42","text":"Parameter Value Benchmark hash 09de83e89e42 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTIONS Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Scaler None* Sliding window train 168* Sliding window prediction 1* Sliding window step 1* Set shared size 168* Train TS IDs 0.5 Test TS IDs 0.5 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/univariate_forecasting/generic_model/570b215d790d/","title":"Benchmark 570b215d790d","text":"Parameter Value Benchmark hash 570b215d790d Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source IP_ADDRESSES_FULL Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Scaler None* Sliding window train 7* Sliding window prediction 1* Sliding window step 1* Set shared size 7* Train TS IDs 0.5 Test TS IDs 0.5 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/univariate_forecasting/generic_model/6249383544ef/","title":"Benchmark 6249383544ef","text":"Parameter Value Benchmark hash 6249383544ef Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source IP_ADDRESSES_SAMPLE Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Scaler None* Sliding window train 168* Sliding window prediction 1* Sliding window step 1* Set shared size 168* Train TS IDs 0.5 Test TS IDs 0.5 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/univariate_forecasting/generic_model/73a9add2c4af/","title":"Benchmark 73a9add2c4af","text":"Parameter Value Benchmark hash 73a9add2c4af Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTION_SUBNETS Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Scaler None* Sliding window train 168* Sliding window prediction 1* Sliding window step 1* Set shared size 168* Train TS IDs 0.5 Test TS IDs 0.5 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/univariate_forecasting/generic_model/7706f1087922/","title":"Benchmark 7706f1087922","text":"Parameter Value Benchmark hash 7706f1087922 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source INSTITUTIONS Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Scaler None* Sliding window train 1008* Sliding window prediction 1* Sliding window step 1* Set shared size 1008* Train TS IDs 0.5 Test TS IDs 0.5 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/univariate_forecasting/generic_model/8b03d0d508ce/","title":"Benchmark 8b03d0d508ce","text":"Parameter Value Benchmark hash 8b03d0d508ce Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source IP_ADDRESSES_FULL Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Scaler None* Sliding window train 1008* Sliding window prediction 1* Sliding window step 1* Set shared size 1008* Train TS IDs 0.5 Test TS IDs 0.5 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/univariate_forecasting/generic_model/9f7047902d66/","title":"Benchmark 9f7047902d66","text":"Parameter Value Benchmark hash 9f7047902d66 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source IP_ADDRESSES_SAMPLE Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Scaler None* Sliding window train 7* Sliding window prediction 1* Sliding window step 1* Set shared size 7* Train TS IDs 0.5 Test TS IDs 0.5 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/univariate_forecasting/generic_model/a642915953ad/","title":"Benchmark a642915953ad","text":"Parameter Value Benchmark hash a642915953ad Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source INSTITUTION_SUBNETS Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Scaler None* Sliding window train 1008* Sliding window prediction 1* Sliding window step 1* Set shared size 1008* Train TS IDs 0.5 Test TS IDs 0.5 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/univariate_forecasting/generic_model/b8098753b97b/","title":"Benchmark b8098753b97b","text":"Parameter Value Benchmark hash b8098753b97b Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source IP_ADDRESSES_FULL Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Scaler None* Sliding window train 168* Sliding window prediction 1* Sliding window step 1* Set shared size 168* Train TS IDs 0.5 Test TS IDs 0.5 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/univariate_forecasting/generic_model/ce63551ffaab/","title":"Benchmark ce63551ffaab","text":"Parameter Value Benchmark hash ce63551ffaab Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source INSTITUTION_SUBNETS Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Scaler None* Sliding window train 7* Sliding window prediction 1* Sliding window step 1* Set shared size 7* Train TS IDs 0.5 Test TS IDs 0.5 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/univariate_forecasting/generic_model/e3de1fc0a44e/","title":"Benchmark e3de1fc0a44e","text":"Parameter Value Benchmark hash e3de1fc0a44e Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source IP_ADDRESSES_SAMPLE Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Scaler None* Sliding window train 1008* Sliding window prediction 1* Sliding window step 1* Set shared size 1008* Train TS IDs 0.5 Test TS IDs 0.5 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/univariate_forecasting/generic_model/ef632e70c252/","title":"Benchmark ef632e70c252","text":"Parameter Value Benchmark hash ef632e70c252 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source INSTITUTIONS Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Scaler None* Sliding window train 7* Sliding window prediction 1* Sliding window step 1* Set shared size 7* Train TS IDs 0.5 Test TS IDs 0.5 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/univariate_forecasting/unique_model/080582bcd519/","title":"Benchmark 080582bcd519","text":"Parameter Value Benchmark hash 080582bcd519 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTION_SUBNETS Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Scaler None* Sliding window train 168* Sliding window prediction 1* Sliding window step 1* Set shared size 168* Train TS IDs 1.0 Test TS IDs 0.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/univariate_forecasting/unique_model/095f847ca755/","title":"Benchmark 095f847ca755","text":"Parameter Value Benchmark hash 095f847ca755 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source INSTITUTION_SUBNETS Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Scaler None* Sliding window train 1008* Sliding window prediction 1* Sliding window step 1* Set shared size 1008* Train TS IDs 1.0 Test TS IDs 0.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/univariate_forecasting/unique_model/0d523e69c328/","title":"Benchmark 0d523e69c328","text":"Parameter Value Benchmark hash 0d523e69c328 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source INSTITUTIONS Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Scaler None* Sliding window train 7* Sliding window prediction 1* Sliding window step 1* Set shared size 7* Train TS IDs 1.0 Test TS IDs 0.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/univariate_forecasting/unique_model/22c5a8e8ffd3/","title":"Benchmark 22c5a8e8ffd3","text":"Parameter Value Benchmark hash 22c5a8e8ffd3 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source INSTITUTIONS Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Scaler None* Sliding window train 1008* Sliding window prediction 1* Sliding window step 1* Set shared size 1008* Train TS IDs 1.0 Test TS IDs 0.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/univariate_forecasting/unique_model/871f5972109e/","title":"Benchmark 871f5972109e","text":"Parameter Value Benchmark hash 871f5972109e Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTIONS Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Scaler None* Sliding window train 168* Sliding window prediction 1* Sliding window step 1* Set shared size 168* Train TS IDs 1.0 Test TS IDs 0.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/univariate_forecasting/unique_model/8e2a07fb3177/","title":"Benchmark 8e2a07fb3177","text":"Parameter Value Benchmark hash 8e2a07fb3177 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source INSTITUTION_SUBNETS Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Scaler None* Sliding window train 7* Sliding window prediction 1* Sliding window step 1* Set shared size 7* Train TS IDs 1.0 Test TS IDs 0.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/univariate_forecasting/unique_model/b5e5ea044b81/","title":"Benchmark b5e5ea044b81","text":"Parameter Value Benchmark hash b5e5ea044b81 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source IP_ADDRESSES_SAMPLE Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Scaler None* Sliding window train 7* Sliding window prediction 1* Sliding window step 1* Set shared size 78* Train TS IDs 1.0 Test TS IDs 0.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/univariate_forecasting/unique_model/c2970e89d824/","title":"Benchmark c2970e89d824","text":"Parameter Value Benchmark hash c2970e89d824 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source IP_ADDRESSES_SAMPLE Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Scaler None* Sliding window train 1008* Sliding window prediction 1* Sliding window step 1* Set shared size 1008* Train TS IDs 1.0 Test TS IDs 0.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/univariate_forecasting/unique_model/d19ba386743f/","title":"Benchmark d19ba386743f","text":"Parameter Value Benchmark hash d19ba386743f Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source IP_ADDRESSES_FULL Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Scaler None* Sliding window train 7* Sliding window prediction 1* Sliding window step 1* Set shared size 7* Train TS IDs 1.0 Test TS IDs 0.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/univariate_forecasting/unique_model/ddb1f02dae43/","title":"Benchmark ddb1f02dae43","text":"Parameter Value Benchmark hash ddb1f02dae43 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source IP_ADDRESSES_FULL Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Scaler None* Sliding window train 1008* Sliding window prediction 1* Sliding window step 1* Set shared size 1008* Train TS IDs 1.0 Test TS IDs 0.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/univariate_forecasting/unique_model/e268fa9957f2/","title":"Benchmark e268fa9957f2","text":"Parameter Value Benchmark hash e268fa9957f2 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source IP_ADDRESSES_FULL Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Scaler None* Sliding window train 168* Sliding window prediction 1* Sliding window step 1* Set shared size 168* Train TS IDs 1.0 Test TS IDs 0.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/univariate_forecasting/unique_model/f3fc14310e2e/","title":"Benchmark f3fc14310e2e","text":"Parameter Value Benchmark hash f3fc14310e2e Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source IP_ADDRESSES_SAMPLE Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Scaler None* Sliding window train 168* Sliding window prediction 1* Sliding window step 1* Set shared size 168* Train TS IDs 1.0 Test TS IDs 0.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"related_works/arXiv_2503.17410/","title":"Comparative Analysis of Deep Learning Models for Real-World ISP Network Traffic Forecasting","text":"<p>These configs were used in the paper \"Koumar, J., Smole\u0148, T., Je\u0159\u00e1bek, K. and \u010cejka, T., 2025. Comparative Analysis of Deep Learning Models for Real-World ISP Network Traffic Forecasting. arXiv preprint arXiv:2503.17410.\".</p> Benchmark hash Dataset Aggregation Source 2439d12c2292 CESNET-TimeSeries24 1 HOUR INSTITUTIONS 63882fe052f8 CESNET-TimeSeries24 1 HOUR INSTITUTIONS 5a79f6cd3506 CESNET-TimeSeries24 1 HOUR INSTITUTIONS 5a7471b2c70b CESNET-TimeSeries24 1 HOUR INSTITUTIONS 0f4fbc0419ce CESNET-TimeSeries24 1 HOUR INSTITUTIONS d166d3b19a87 CESNET-TimeSeries24 1 HOUR INSTITUTION_SUBNETS 2112383abab7 CESNET-TimeSeries24 1 HOUR INSTITUTION_SUBNETS 5d9e6e63cbf0 CESNET-TimeSeries24 1 HOUR INSTITUTION_SUBNETS d82139cd671f CESNET-TimeSeries24 1 HOUR INSTITUTION_SUBNETS d03ba8db5892 CESNET-TimeSeries24 1 HOUR INSTITUTION_SUBNETS 2e92831cb502 CESNET-TimeSeries24 1 HOUR IP_ADDRESSES_SAMPLE 702e58166879 CESNET-TimeSeries24 1 HOUR IP_ADDRESSES_SAMPLE 394603854070 CESNET-TimeSeries24 1 HOUR IP_ADDRESSES_SAMPLE 420d4303f949 CESNET-TimeSeries24 1 HOUR IP_ADDRESSES_SAMPLE e2c2148a178c CESNET-TimeSeries24 1 HOUR IP_ADDRESSES_SAMPLE <p>Example of usage of this related works configs:</p> <pre><code>from cesnet_tszoo.benchmarks import load_benchmark\nfrom cesnet_tszoo.utils.enums import FillerType, ScalerType\n\nbenchmark = load_benchmark(\"2439d12c2292\", \"../\")\ndataset = benchmark.get_initialized_dataset()\n\n# Get related results\nrelated_results = benchmark.get_related_results()\nprint(related_results)\n\n# Process with your own defined model\nresults = []\nfor ts_id in tqdm.tqdm(dataset.get_data_about_set(about='train')['ts_ids']):\n    model = SimpleLSTM().to(device)\n    model.fit(\n        dataset.get_train_dataloader(ts_id), \n        dataset.get_val_dataloader(ts_id), \n        n_epochs=5, \n        device=device,\n    )\n    y_pred, y_true = model.predict(\n        dataset.get_test_dataloader(ts_id), \n        device=device,\n    )\n\n    rmse = mean_squared_error(y_true, y_pred)\n    results.append(rmse)\n\n_mean = round(np.mean(results), 3)\n_std = round(np.std(results), 3)\nprint(f\"Mean RMSE: {_mean}\")\nprint(f\"Std RMSE: {_std}\") \n\n# Compare with related works results\nbetter_works = related_results[related_results['Avg. RMSE'] &lt; _mean]\nworse_works = related_results[related_results['Avg. RMSE'] &lt;= _mean]\nprint(better_works)\nprint(worse_works)\n</code></pre>"},{"location":"related_works/arXiv_2503.17410/0f4fbc0419ce/","title":"Config 0f4fbc0419ce","text":"Parameter Value Benchmark hash 0f4fbc0419ce Original paper https://doi.org/10.48550/arXiv.2503.17410 Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTIONS Train size 0.35 Val size 0.05 Test size 0.6 Uni/Multi variate Univariate Metrics n_bytes Default value 0 Filler None Scaler MinMaxScaler Sliding window train 744 Sliding window prediction 168 Sliding window step 168 Set shared size 744 Train TS IDs 1 Test TS IDs 0 Related work Model Average RMSE Std RMSE Average R2-score Std R2-score https://arxiv.org/abs/2503.17410 GRU 0.165 0.82 -0.53 1.3 https://arxiv.org/abs/2503.17410 GRU_FCN 0.165 0.82 -1.08 2.8 https://arxiv.org/abs/2503.17410 INCEPTIONTIME 0.519 0.78 -9.18 2.3 https://arxiv.org/abs/2503.17410 LSTM 0.165 0.82 -0.49 1.2 https://arxiv.org/abs/2503.17410 LSTM_FCN 0.164 0.82 -0.54 1.4 https://arxiv.org/abs/2503.17410 MEAN 0.152 0.76 -0.01 0.1 https://arxiv.org/abs/2503.17410 RCLSTM 0.193 0.83 -1.8 2.4 https://arxiv.org/abs/2503.17410 RESNET 0.173 0.82 -0.74 1.5"},{"location":"related_works/arXiv_2503.17410/2112383abab7/","title":"Config 2112383abab7","text":"Parameter Value Benchmark hash 2112383abab7 Original paper https://doi.org/10.48550/arXiv.2503.17410 Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTION_SUBNETS Train size 0.35 Val size 0.05 Test size 0.6 Uni/Multi variate Univariate Metrics n_bytes Default value 0 Filler None Scaler MinMaxScaler Sliding window train 168 Sliding window prediction 1 Sliding window step 1 Set shared size 168 Train TS IDs 1 Test TS IDs 0 Related work Model Average RMSE Std RMSE Average R2-score Std R2-score https://arxiv.org/abs/2503.17410 GRU 0.219 1.15 -0.1 1.2 https://arxiv.org/abs/2503.17410 GRU_FCN 0.219 1.15 0.05 0.9 https://arxiv.org/abs/2503.17410 INCEPTIONTIME 0.238 1.15 -1.57 3.0 https://arxiv.org/abs/2503.17410 LSTM 0.22 1.15 -0.07 1.2 https://arxiv.org/abs/2503.17410 LSTM_FCN 0.219 1.15 -0.01 1.2 https://arxiv.org/abs/2503.17410 MEAN 0.392 1.66 0.06 0.1 https://arxiv.org/abs/2503.17410 RCLSTM 0.277 1.36 0.09 1.0 https://arxiv.org/abs/2503.17410 RESNET 0.235 1.15 -0.68 2.1"},{"location":"related_works/arXiv_2503.17410/2439d12c2292/","title":"Config 2439d12c2292","text":"Parameter Value Benchmark hash 2439d12c2292 Original paper https://doi.org/10.48550/arXiv.2503.17410 Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTIONS Train size 0.35 Val size 0.05 Test size 0.6 Uni/Multi variate Univariate Metrics n_bytes Default value 0 Filler None Scaler MinMaxScaler Sliding window train 24 Sliding window prediction 1 Sliding window step 1 Set shared size 24 Train TS IDs 1 Test TS IDs 0 Related work Model Average RMSE Std RMSE Average R2-score Std R2-score https://arxiv.org/abs/2503.17410 GRU 0.104 0.53 0.08 0.8 https://arxiv.org/abs/2503.17410 GRU_FCN 0.102 0.55 0.15 1.0 https://arxiv.org/abs/2503.17410 INCEPTIONTIME 0.112 0.55 -0.77 2.5 https://arxiv.org/abs/2503.17410 LSTM 0.105 0.54 0.09 0.8 https://arxiv.org/abs/2503.17410 LSTM_FCN 0.102 0.54 0.19 0.7 https://arxiv.org/abs/2503.17410 MEAN 0.146 0.75 0.09 0.1 https://arxiv.org/abs/2503.17410 RCLSTM 0.14 0.8 0.2 0.9 https://arxiv.org/abs/2503.17410 RESNET 0.106 0.55 0.06 1.1"},{"location":"related_works/arXiv_2503.17410/2e92831cb502/","title":"Config 2e92831cb502","text":"Parameter Value Benchmark hash 2e92831cb502 Original paper https://doi.org/10.48550/arXiv.2503.17410 Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source IP_ADDRESSES_SAMPLE Train size 0.35 Val size 0.05 Test size 0.6 Uni/Multi variate Univariate Metrics n_bytes Default value 0 Filler None Scaler MinMaxScaler Sliding window train 24 Sliding window prediction 1 Sliding window step 1 Set shared size 24 Train TS IDs 1 Test TS IDs 0 Related work Model Average RMSE Std RMSE Average R2-score Std R2-score https://arxiv.org/abs/2503.17410 GRU 0.149 0.82 -0.46 1.9 https://arxiv.org/abs/2503.17410 GRU_FCN 0.15 0.82 -0.12 1.1 https://arxiv.org/abs/2503.17410 INCEPTIONTIME 0.165 0.82 -2.7 3.9 https://arxiv.org/abs/2503.17410 LSTM 0.15 0.82 -0.41 1.8 https://arxiv.org/abs/2503.17410 LSTM_FCN 0.151 0.82 -0.44 1.9 https://arxiv.org/abs/2503.17410 MEAN 1.01 2.86 0.0 0.1 https://arxiv.org/abs/2503.17410 RCLSTM 0.221 1.08 -0.09 1.0 https://arxiv.org/abs/2503.17410 RESNET 0.152 0.82 -0.81 2.4"},{"location":"related_works/arXiv_2503.17410/394603854070/","title":"Config 394603854070","text":"Parameter Value Benchmark hash 394603854070 Original paper https://doi.org/10.48550/arXiv.2503.17410 Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source IP_ADDRESSES_SAMPLE Train size 0.35 Val size 0.05 Test size 0.6 Uni/Multi variate Univariate Metrics n_bytes Default value 0 Filler None Scaler MinMaxScaler Sliding window train 168 Sliding window prediction 24 Sliding window step 24 Set shared size 168 Train TS IDs 1 Test TS IDs 0 Related work Model Average RMSE Std RMSE Average R2-score Std R2-score https://arxiv.org/abs/2503.17410 GRU 0.154 0.82 -0.31 1.5 https://arxiv.org/abs/2503.17410 GRU_FCN 0.154 0.82 -0.61 2.1 https://arxiv.org/abs/2503.17410 INCEPTIONTIME 0.573 0.77 -9.22 2.5 https://arxiv.org/abs/2503.17410 LSTM 0.154 0.82 -0.35 1.6 https://arxiv.org/abs/2503.17410 LSTM_FCN 0.158 0.82 -1.28 3.0 https://arxiv.org/abs/2503.17410 MEAN 1.011 2.86 0.01 0.1 https://arxiv.org/abs/2503.17410 RCLSTM 0.247 1.17 -0.99 2.6 https://arxiv.org/abs/2503.17410 RESNET 0.159 0.82 -0.92 2.6"},{"location":"related_works/arXiv_2503.17410/420d4303f949/","title":"Config 420d4303f949","text":"Parameter Value Benchmark hash 420d4303f949 Original paper https://doi.org/10.48550/arXiv.2503.17410 Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source IP_ADDRESSES_SAMPLE Train size 0.35 Val size 0.05 Test size 0.6 Uni/Multi variate Univariate Metrics n_bytes Default value 0 Filler None Scaler MinMaxScaler Sliding window train 744 Sliding window prediction 1 Sliding window step 1 Set shared size 744 Train TS IDs 1 Test TS IDs 0 Related work Model Average RMSE Std RMSE Average R2-score Std R2-score https://arxiv.org/abs/2503.17410 GRU 0.15 0.82 -0.38 1.7 https://arxiv.org/abs/2503.17410 GRU_FCN 0.153 0.82 -0.25 1.3 https://arxiv.org/abs/2503.17410 INCEPTIONTIME 0.183 0.82 -3.57 4.2 https://arxiv.org/abs/2503.17410 LSTM 0.151 0.82 -0.39 1.8 https://arxiv.org/abs/2503.17410 LSTM_FCN 0.154 0.82 -0.62 2.2 https://arxiv.org/abs/2503.17410 MEAN 1.013 2.86 -0.08 0.9 https://arxiv.org/abs/2503.17410 RCLSTM 0.166 0.89 -0.29 1.6 https://arxiv.org/abs/2503.17410 RESNET 0.163 0.82 -1.17 3.0"},{"location":"related_works/arXiv_2503.17410/5a7471b2c70b/","title":"Config 5a7471b2c70b","text":"Parameter Value Benchmark hash 5a7471b2c70b Original paper https://doi.org/10.48550/arXiv.2503.17410 Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTIONS Train size 0.35 Val size 0.05 Test size 0.6 Uni/Multi variate Univariate Metrics n_bytes Default value 0 Filler None Scaler MinMaxScaler Sliding window train 744 Sliding window prediction 1 Sliding window step 1 Set shared size 744 Train TS IDs 1 Test TS IDs 0 <p>|:-----------------|:-----------------|:-----------------:|:-----------------:|:-----------------:|:-----------------:| | Related work | Model | Average RMSE | Std RMSE | Average R2-score | Std R2-score | | https://arxiv.org/abs/2503.17410 | GRU | 0.106 | 0.53 | 0.06 | 0.7 | | https://arxiv.org/abs/2503.17410 | GRU_FCN | 0.111 | 0.55 | -0.05 | 1.0 | | https://arxiv.org/abs/2503.17410 | INCEPTIONTIME | 0.136 | 0.55 | -1.59 | 2.8 | | https://arxiv.org/abs/2503.17410 | LSTM | 0.106 | 0.53 | 0.07 | 0.8 | | https://arxiv.org/abs/2503.17410 | LSTM_FCN | 0.11 | 0.53 | -0.09 | 1.2 | | https://arxiv.org/abs/2503.17410 | MEAN | 0.151 | 0.75 | 0.01 | 0.1 | | https://arxiv.org/abs/2503.17410 | RCLSTM | 0.112 | 0.58 | 0.23 | 0.7 | | https://arxiv.org/abs/2503.17410 | RESNET | 0.146 | 0.56 | -0.92 | 2.4 |</p>"},{"location":"related_works/arXiv_2503.17410/5a79f6cd3506/","title":"Config 5a79f6cd3506","text":"Parameter Value Benchmark hash 5a79f6cd3506 Original paper https://doi.org/10.48550/arXiv.2503.17410 Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTIONS Train size 0.35 Val size 0.05 Test size 0.6 Uni/Multi variate Univariate Metrics n_bytes Default value 0 Filler None Scaler MinMaxScaler Sliding window train 168 Sliding window prediction 24 Sliding window step 24 Set shared size 168 Train TS IDs 1 Test TS IDs 0 Related work Model Average RMSE Std RMSE Average R2-score Std R2-score https://arxiv.org/abs/2503.17410 GRU 0.123 0.55 -0.45 1.2 https://arxiv.org/abs/2503.17410 GRU_FCN 0.115 0.55 -0.18 1.1 https://arxiv.org/abs/2503.17410 INCEPTIONTIME 0.483 0.51 -9.22 2.2 https://arxiv.org/abs/2503.17410 LSTM 0.124 0.55 -0.45 1.3 https://arxiv.org/abs/2503.17410 LSTM_FCN 0.117 0.55 -0.29 1.3 https://arxiv.org/abs/2503.17410 MEAN 0.15 0.75 0.03 0.1 https://arxiv.org/abs/2503.17410 RCLSTM 0.163 0.81 -0.37 1.5 https://arxiv.org/abs/2503.17410 RESNET 0.131 0.55 -0.73 1.7"},{"location":"related_works/arXiv_2503.17410/5d9e6e63cbf0/","title":"Config 5d9e6e63cbf0","text":"Parameter Value Benchmark hash 5d9e6e63cbf0 Original paper https://doi.org/10.48550/arXiv.2503.17410 Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTION_SUBNETS Train size 0.35 Val size 0.05 Test size 0.6 Uni/Multi variate Univariate Metrics n_bytes Default value 0 Filler None Scaler MinMaxScaler Sliding window train 168 Sliding window prediction 24 Sliding window step 24 Set shared size 168 Train TS IDs 1 Test TS IDs 0 Related work Model Average RMSE Std RMSE Average R2-score Std R2-score https://arxiv.org/abs/2503.17410 GRU 0.237 1.15 -0.55 1.4 https://arxiv.org/abs/2503.17410 GRU_FCN 0.229 1.15 -0.27 1.1 https://arxiv.org/abs/2503.17410 INCEPTIONTIME 0.59 1.1 -8.9 2.7 https://arxiv.org/abs/2503.17410 LSTM 0.237 1.15 -0.53 1.3 https://arxiv.org/abs/2503.17410 LSTM_FCN 0.231 1.15 -0.55 1.8 https://arxiv.org/abs/2503.17410 MEAN 0.394 1.66 0.05 0.1 https://arxiv.org/abs/2503.17410 RCLSTM 0.334 1.48 -0.64 1.9 https://arxiv.org/abs/2503.17410 RESNET 0.244 1.15 -0.77 1.6"},{"location":"related_works/arXiv_2503.17410/63882fe052f8/","title":"Config 63882fe052f8","text":"Parameter Value Benchmark hash 63882fe052f8 Original paper https://doi.org/10.48550/arXiv.2503.17410 Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTIONS Train size 0.35 Val size 0.05 Test size 0.6 Uni/Multi variate Univariate Metrics n_bytes Default value 0 Filler None Scaler MinMaxScaler Sliding window train 168 Sliding window prediction 1 Sliding window step 1 Set shared size 168 Train TS IDs 1 Test TS IDs 0 Related work Model Average RMSE Std RMSE Average R2-score Std R2-score https://arxiv.org/abs/2503.17410 GRU 0.105 0.53 0.03 0.9 https://arxiv.org/abs/2503.17410 GRU_FCN 0.104 0.55 0.17 0.7 https://arxiv.org/abs/2503.17410 INCEPTIONTIME 0.124 0.55 -1.08 2.5 https://arxiv.org/abs/2503.17410 LSTM 0.106 0.54 0.1 0.7 https://arxiv.org/abs/2503.17410 LSTM_FCN 0.103 0.54 0.14 0.9 https://arxiv.org/abs/2503.17410 MEAN 0.149 0.75 0.04 0.1 https://arxiv.org/abs/2503.17410 RCLSTM 0.14 0.8 0.2 0.9 https://arxiv.org/abs/2503.17410 RESNET 0.127 0.55 -0.63 2.2"},{"location":"related_works/arXiv_2503.17410/702e58166879/","title":"Config 702e58166879","text":"Parameter Value Benchmark hash 702e58166879 Original paper https://doi.org/10.48550/arXiv.2503.17410 Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source IP_ADDRESSES_SAMPLE Train size 0.35 Val size 0.05 Test size 0.6 Uni/Multi variate Univariate Metrics n_bytes Default value 0 Filler None Scaler MinMaxScaler Sliding window train 168 Sliding window prediction 1 Sliding window step 1 Set shared size 168 Train TS IDs 1 Test TS IDs 0 Related work Model Average RMSE Std RMSE Average R2-score Std R2-score https://arxiv.org/abs/2503.17410 GRU 0.15 0.82 -0.41 1.8 https://arxiv.org/abs/2503.17410 GRU_FCN 0.152 0.82 -0.18 1.2 https://arxiv.org/abs/2503.17410 INCEPTIONTIME 0.168 0.82 -2.82 3.9 https://arxiv.org/abs/2503.17410 LSTM 0.151 0.82 -0.4 1.8 https://arxiv.org/abs/2503.17410 LSTM_FCN 0.152 0.82 -0.66 2.3 https://arxiv.org/abs/2503.17410 MEAN 1.011 2.86 0.01 0.1 https://arxiv.org/abs/2503.17410 RCLSTM 0.226 1.12 -0.12 1.1 https://arxiv.org/abs/2503.17410 RESNET 0.158 0.82 -1.03 2.8"},{"location":"related_works/arXiv_2503.17410/d03ba8db5892/","title":"Config d03ba8db5892","text":"Parameter Value Benchmark hash d03ba8db5892 Original paper https://doi.org/10.48550/arXiv.2503.17410 Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTION_SUBNETS Train size 0.35 Val size 0.05 Test size 0.6 Uni/Multi variate Univariate Metrics n_bytes Default value 0 Filler None Scaler MinMaxScaler Sliding window train 744 Sliding window prediction 168 Sliding window step 168 Set shared size 744 Train TS IDs 1 Test TS IDs 0 Related work Model Average RMSE Std RMSE Average R2-score Std R2-score https://arxiv.org/abs/2503.17410 GRU 0.265 1.23 -0.61 1.4 https://arxiv.org/abs/2503.17410 GRU_FCN 0.269 1.23 -1.67 3.3 https://arxiv.org/abs/2503.17410 INCEPTIONTIME 0.613 1.18 -8.85 2.8 https://arxiv.org/abs/2503.17410 LSTM 0.265 1.23 -0.55 1.3 https://arxiv.org/abs/2503.17410 LSTM_FCN 0.265 1.23 -0.94 2.2 https://arxiv.org/abs/2503.17410 MEAN 0.402 1.69 0.0 0.1 https://arxiv.org/abs/2503.17410 RCLSTM 0.326 1.37 -2.26 2.9 https://arxiv.org/abs/2503.17410 RESNET 0.275 1.23 -0.91 1.8"},{"location":"related_works/arXiv_2503.17410/d166d3b19a87/","title":"Config d166d3b19a87","text":"Parameter Value Benchmark hash d166d3b19a87 Original paper https://doi.org/10.48550/arXiv.2503.17410 Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTION_SUBNETS Train size 0.35 Val size 0.05 Test size 0.6 Uni/Multi variate Univariate Metrics n_bytes Default value 0 Filler None Scaler MinMaxScaler Sliding window train 24 Sliding window prediction 1 Sliding window step 1 Set shared size 24 Train TS IDs 1 Test TS IDs 0 Related work Model Average RMSE Std RMSE Average R2-score Std R2-score https://arxiv.org/abs/2503.17410 GRU 0.218 1.14 -0.1 1.3 https://arxiv.org/abs/2503.17410 GRU_FCN 0.217 1.15 0.06 0.8 https://arxiv.org/abs/2503.17410 INCEPTIONTIME 0.23 1.15 -1.29 3.1 https://arxiv.org/abs/2503.17410 LSTM 0.219 1.15 -0.05 1.1 https://arxiv.org/abs/2503.17410 LSTM_FCN 0.217 1.15 0.08 0.8 https://arxiv.org/abs/2503.17410 MEAN 0.383 1.63 0.12 0.2 https://arxiv.org/abs/2503.17410 RCLSTM 0.323 1.5 0.08 1.1 https://arxiv.org/abs/2503.17410 RESNET 0.22 1.15 -0.15 1.4"},{"location":"related_works/arXiv_2503.17410/d82139cd671f/","title":"Config d82139cd671f","text":"Parameter Value Benchmark hash d82139cd671f Original paper https://doi.org/10.48550/arXiv.2503.17410 Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTION_SUBNETS Train size 0.35 Val size 0.05 Test size 0.6 Uni/Multi variate Univariate Metrics n_bytes Default value 0 Filler None Scaler MinMaxScaler Sliding window train 744 Sliding window prediction 1 Sliding window step 1 Set shared size 744 Train TS IDs 1 Test TS IDs 0 Related work Model Average RMSE Std RMSE Average R2-score Std R2-score https://arxiv.org/abs/2503.17410 GRU 0.22 1.14 -0.1 1.2 https://arxiv.org/abs/2503.17410 GRU_FCN 0.228 1.15 -0.19 1.1 https://arxiv.org/abs/2503.17410 INCEPTIONTIME 0.253 1.15 -2.19 3.4 https://arxiv.org/abs/2503.17410 LSTM 0.22 1.15 -0.09 1.1 https://arxiv.org/abs/2503.17410 LSTM_FCN 0.228 1.15 -0.24 1.3 https://arxiv.org/abs/2503.17410 MEAN 0.4 1.68 0.02 0.1 https://arxiv.org/abs/2503.17410 RCLSTM 0.225 1.16 0.1 1.0 https://arxiv.org/abs/2503.17410 RESNET 0.255 1.15 -1.05 2.5"},{"location":"related_works/arXiv_2503.17410/e2c2148a178c/","title":"Config e2c2148a178c","text":"Parameter Value Benchmark hash e2c2148a178c Original paper https://doi.org/10.48550/arXiv.2503.17410 Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source IP_ADDRESSES_SAMPLE Train size 0.35 Val size 0.05 Test size 0.6 Uni/Multi variate Univariate Metrics n_bytes Default value 0 Filler None Scaler MinMaxScaler Sliding window train 744 Sliding window prediction 168 Sliding window step 168 Set shared size 744 Train TS IDs 1 Test TS IDs 0 Related work Model Average RMSE Std RMSE Average R2-score Std R2-score https://arxiv.org/abs/2503.17410 GRU 0.179 0.93 -0.5 1.8 https://arxiv.org/abs/2503.17410 GRU_FCN 0.214 0.92 -3.97 4.2 https://arxiv.org/abs/2503.17410 INCEPTIONTIME 0.593 0.88 -9.17 2.5 https://arxiv.org/abs/2503.17410 LSTM 0.179 0.93 -0.34 1.5 https://arxiv.org/abs/2503.17410 LSTM_FCN 0.189 0.93 -1.52 3.2 https://arxiv.org/abs/2503.17410 MEAN 1.014 2.86 -0.09 0.9 https://arxiv.org/abs/2503.17410 RCLSTM 0.234 1.08 -2.77 3.8 https://arxiv.org/abs/2503.17410 RESNET 0.182 0.93 -0.52 1.9"}]}