{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CESNET TSZoo","text":"<p>This is documentation of the CESNET TSZoo project. </p> <p>The goal of <code>cesnet-tszoo</code> project is to provide time series datasets with useful tools for preprocessing and reproducibility. Such as:</p> <ul> <li>API for downloading, configuring and loading CESNET-TimeSeries24, CESNET-AGG23 datasets. Each with various sources and aggregations. Check dataset overview page for details about datasets.</li> <li>Example of configuration options:<ul> <li>Data can be split into train/val/test sets. Split can be done by time series, check <code>SeriesBasedCesnetDataset</code>, by time periods, check <code>TimeBasedCesnetDataset</code>, or by both, check <code>DisjointTimeBasedCesnetDataset</code>.</li> <li>Transforming of data with built-in transformers or with custom transformers. Check <code>transformers</code> for details.</li> <li>Handling missing values built-in fillers or with custom fillers. Check <code>fillers</code> for details.</li> <li>Handling anomalies with built-in anomaly handlers or with custom anomaly handlers. Check <code>anomaly handlers</code> for details.</li> <li>Applying custom handlers. Check <code>custom handlers</code> for details.</li> <li>Changing order of when are preprocesses applied/fitted</li> </ul> </li> <li>Creation and import of benchmarks, for easy reproducibility of experiments.</li> <li>Creation and import of annotations. Can create annotations for specific time series, specific time or specific time in specific time series.</li> </ul>"},{"location":"annotations/","title":"Annotations","text":"<p>This tutorial will look at how to use annotations.</p> <p>Note</p> <p>For every option and more detailed examples refer to Jupyter notebook <code>annotations</code>.</p>"},{"location":"annotations/#basics","title":"Basics","text":"<ul> <li>You can get annotations for specific type with <code>get_annotations</code> method. </li> <li>Method <code>get_annotations</code> returns annotations as Pandas Dataframe.</li> </ul> <p>There are three annotation types:</p> <ol> <li>AnnotationType.TS_ID -&gt; Annotations for whole specific time series</li> <li>AnnotationType.ID_TIME -&gt; Annotations for specific time... independent on time series</li> <li>AnnotationType.BOTH -&gt; Annotations for specific time in specific time series</li> </ol> <pre><code>from cesnet_tszoo.utils.enums import AnnotationType                                                                          \n\ndataset.get_annotations(on=AnnotationType.TS_ID)\ndataset.get_annotations(on=AnnotationType.ID_TIME)\ndataset.get_annotations(on=AnnotationType.BOTH)\n</code></pre>"},{"location":"annotations/#annotation-groups","title":"Annotation groups","text":"<ul> <li>Annotation group could be understood as column names in Dataframe/CSV.</li> <li>You can add annotation groups or remove them.</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import AnnotationType                                                                          \n\n# Adding groups\ndataset.add_annotation_group(annotation_group=\"test1\", on=AnnotationType.TS_ID)\ndataset.add_annotation_group(annotation_group=\"test2\", on=AnnotationType.ID_TIME)\ndataset.add_annotation_group(annotation_group=\"test3\", on=AnnotationType.BOTH)\n\n# Removing groups\ndataset.remove_annotation_group(annotation_group=\"test1\", on=AnnotationType.TS_ID)\ndataset.remove_annotation_group(annotation_group=\"test2\", on=AnnotationType.ID_TIME)\ndataset.remove_annotation_group(annotation_group=\"test3\", on=AnnotationType.BOTH)\n</code></pre>"},{"location":"annotations/#annotation-values","title":"Annotation values","text":"<ul> <li>Annotations are specific values for selected annotation group and AnnotationType.</li> <li>You can add annotations or remove them.</li> <li>Adding annotation<ul> <li>When adding annotation to annotation group that does not exist, it will be created.</li> <li>To override existing annotation, you just need to specify same <code>annotation_group</code>, <code>ts_id</code>, <code>id_time</code> and new annotation.</li> <li>Setting <code>enforce_ids</code> to True, ensures that inputted <code>ts_id</code> and <code>id_time</code> must belong to used dataset.</li> </ul> </li> <li>Removing annotations<ul> <li>Removing annotation from every annotation group of a row, removes that row from Dataframe.</li> </ul> </li> </ul> <pre><code># Adding annotations\ndataset.add_annotation(annotation=\"test_annotation1_3\", annotation_group=\"test1\", ts_id=3, id_time=None, enforce_ids=True) # Adds to AnnotationType.TS_ID\ndataset.add_annotation(annotation=\"test_annotation2_0\", annotation_group=\"test2\", ts_id=None, id_time=0, enforce_ids=True) # Adds to AnnotationType.ID_TIME\ndataset.add_annotation(annotation=\"test_annotation3_3_0\", annotation_group=\"test3\", ts_id=3, id_time=0, enforce_ids=True) # Adds to AnnotationType.BOTH\n\n# Removing annotations\ndataset.remove_annotation(annotation_group=\"test1\", ts_id=3, id_time=None) # Removes from AnnotationType.TS_ID\ndataset.remove_annotation(annotation_group=\"test2\", ts_id=None, id_time=0 ) # Removes from AnnotationType.ID_TIME\ndataset.remove_annotation(annotation_group=\"test3\", ts_id=3, id_time=0 ) # Removes from AnnotationType.BOTH\n</code></pre>"},{"location":"annotations/#exporting-annotations","title":"Exporting annotations","text":"<ul> <li>You can export your created annotation with <code>save_annotations</code> method.</li> <li><code>save_annotations</code> creates CSV file at: <code>os.path.join(dataset.metadata.annotations_root, identifier)</code>.</li> <li>When parameter <code>force_write</code> is True, existing files with same name will be overwritten.</li> <li>You should not add \".csv\" to identifier, because it will be added automatically.</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import AnnotationType   \n\ndataset.save_annotations(identifier=\"test_name\", on=AnnotationType.BOTH, force_write=True)\n</code></pre>"},{"location":"annotations/#importing-annotations","title":"Importing annotations","text":"<ul> <li>You can import already existing annotations, be it your own or already built-in one.</li> <li>Setting <code>enforce_ids</code> to True, ensures that all <code>ts_id</code> or <code>id_time</code> from imported annotations must belong to used dataset.</li> <li>Method <code>import_annotations</code> automatically detects what AnnotationType imported annotations is, based on existing ts_id (expects name of ts_id for used dataset) or id_time columns.</li> <li>First, it attempts to load the built-in annotations, if no built-in annotations with such an identifier exists, it attempts to load a custom annotations from the <code>\"data_root\"/tszoo/annotations/</code> directory.</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import AnnotationType   \n\ndataset.import_annotations(identifier=\"test_name\", enforce_ids=True)\n</code></pre>"},{"location":"anomaly_handlers/","title":"Anomaly handlers","text":"<p>The <code>cesnet_tszoo</code> package supports various ways of using anomaly handlers to handle anomalies. Anomaly handlers can be created and fitted (on train set) when initializing dataset with config and each time series has its own anomaly handler instance.</p> <p>Possible config parameters in <code>TimeBasedConfig</code>, <code>DisjointTimeBasedConfig</code> and <code>SeriesBasedConfig</code>:</p> <ul> <li><code>handle_anomalies_with</code>:  Defines the anomaly handlers to handle anomalies in the train set. Can pass enum <code>AnomalyHandlerType</code> for built-in anomaly handler or pass a type of custom anomaly handler.</li> </ul>"},{"location":"anomaly_handlers/#built-in-anomaly-handlers","title":"Built-in anomaly handlers","text":"<p>The <code>cesnet_tszoo</code> package comes with multiple built-in anomaly handlers. To check built-in anomaly handlers refer to <code>anomaly handlers</code>.</p>"},{"location":"anomaly_handlers/#custom-anomaly-handlers","title":"Custom anomaly handlers","text":"<p>It is possible to create and use own anomaly handlers. It is recommended to use prepared base class <code>AnomalyHandler</code>.</p>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>CESNET-TS-Zoo enables easy sharing and reuse of configuration files to support open science, reproducibility, and transparent comparison of time series modeling approaches.</p> <p>We provide a collection of pre-defined configurations that serve as benchmarks, including use cases like network traffic forecasting and anomaly detection.</p> <p>The library includes tools for both importing and exporting configurations as benchmarks. This allows researchers to cite a specific benchmark via its unique hash or to share their own approach as a configuration file.</p> <p>To load and use a benchmark in your code, simply use the following snippet:</p> <pre><code>from cesnet_tszoo.benchmarks import load_benchmark\n\nbenchmark = load_benchmark(\"&lt;benchmark_hash&gt;\", \"&lt;path-to-datasets&gt;\")\ndataset = benchmark.get_initialized_dataset()\n</code></pre> <p>Note</p> <p>More detailed tutorial how to use benchmarks is available <code>here</code></p>"},{"location":"benchmarks/#available-benchmarks","title":"Available benchmarks","text":""},{"location":"benchmarks/#network-traffic-forecasting-benchmarks","title":"Network Traffic Forecasting Benchmarks","text":"<p>Network traffic forecasting plays a crucial role in network management and security. Therefore, we prepared several benchmarks for evaluation of network traffic forecasting methods for both management and security tasks. We split the <code>Network Traffic Forecasting Benchmarks</code> into these two groups:</p> <ul> <li>\"Univariate forecasting - Transmitted data size\": Benchmarks in this group are designed to support mostly used forecasting task for network management.</li> <li>\"Multivariate forecasting\": Benchmarks in this group are designed to multivariate forecasting of network traffic features which is more often usable in network security for anomaly/outlier detection.</li> </ul>"},{"location":"benchmarks/#network-device-type-classification-benchmarks","title":"Network Device Type Classification Benchmarks","text":"<p>Network device type classification focuses on evaluating the performance of models for classifying types of network devices. The goal of this benchmark is to allow comparison of various classification algorithms and methods in the context of network devices. This task is valuable in environments where it is essential to quickly and efficiently identify devices in a network for monitoring, security, and traffic optimization purposes. Analyzing the benchmarks helps determine which methods are most suitable for deployment in real-world scenarios.</p> <p>The network device type classification benchmarks are described in detail: here</p>"},{"location":"benchmarks/#anomaly-detection-benchmarks","title":"Anomaly Detection Benchmarks","text":"<p>This benchmarks are in process of making and they will be added soon.</p>"},{"location":"benchmarks/#similarity-search-benchmarks","title":"Similarity Search Benchmarks","text":"<p>This benchmarks are in process of making and they will be added soon.</p>"},{"location":"benchmarks/#available-dataset-benchmarks-from-related-works","title":"Available dataset benchmarks from related works","text":"<p>For supporting reproducibility of approaches, the CESNET-TS-Zoo allows to share ts-zoo benchmarks with others using pull request from forked repository.</p> <p>Each related work contains benchmarks and example of usage. Please follow authors instruction in example to ensure comparable results. Following benchmarks are already included in the ts-zoo:</p> DOI Task Benchmarks link https://doi.org/10.48550/arXiv.2503.17410 Univariate forecasting benchmarks"},{"location":"benchmarks_tutorial/","title":"Benchmarks","text":"<p>This tutorial will look at how to use benchmarks.</p> <p>Only time-based will be used, because all methods work almost the same way for other dataset types.</p> <p>Note</p> <p>For every option and more detailed examples refer to Jupyter notebook <code>benchmarks</code></p> <p>Benchmarks can consist of various parts:</p> <ul> <li>identifier of used config</li> <li>identifier of used annotations (for each AnnotationType)</li> <li>identifier of related_results (only available for premade benchmarks)</li> <li>Used SourceType, AggregationType and DatasetType</li> <li>Database name (here it would be CESNET_TimeSeries24)</li> <li>Whether config or annotations are built-in</li> </ul>"},{"location":"benchmarks_tutorial/#importing-benchmarks","title":"Importing benchmarks","text":"<ul> <li>You can import your own or built-in benchmark with <code>load_benchmark</code> function.</li> <li>When importing benchmark with annotations that exist, but are not downloaded, they will be downloaded (only works for built-in annotations),</li> <li>First, it attempts to load the built-in benchmark, if no built-in benchmark with such an identifier exists, it attempts to load a custom benchmark from the <code>\"data_root\"/tszoo/benchmarks/</code> directory.</li> </ul> <pre><code>from cesnet_tszoo.benchmarks import load_benchmark                                                                       \n\n# Imports built-in benchmark\n# Can get related_results with `get_related_results` method.\n# Method `get_related_results` returns pandas Dataframe. \nbenchmark = load_benchmark(identifier=\"2e92831cb502\", data_root=\"/some_directory/\")\ndataset = benchmark.get_initialized_dataset(display_config_details=\"text\", check_errors=False, workers=\"config\")\n\n# Imports custom benchmark\n# Looks for benchmark at: `os.path.join(\"/some_directory/\", \"tszoo\", \"benchmarks\", identifier)`\nbenchmark = load_benchmark(identifier=\"test2\", data_root=\"/some_directory/\")\ndataset = benchmark.get_initialized_dataset(display_config_details=\"text\", check_errors=False, workers=\"config\")\n</code></pre>"},{"location":"benchmarks_tutorial/#exporting-benchmarks","title":"Exporting benchmarks","text":"<ul> <li>You can use method <code>save_benchmark</code> to save benchmark.</li> <li>Saving benchmark creates YAML file, which hold metadata, at: <code>os.path.join(dataset.metadata.benchmarks_root, identifier)</code>.</li> <li>Saving benchmark automatically creates files for config and annotations with identifiers matching benchmark identifier</li> <li>config will be saved at: <code>os.path.join(dataset.metadata.configs_root, identifier)</code></li> <li>annotations will be saved at: <code>os.path.join(dataset.metadata.annotations_root, identifier, str(AnnotationType))</code></li> <li>When parameter <code>force_write</code> is True, existing files with the same name will be overwritten.</li> <li>When using imported config or annotations, only their identifier will be passed to benchmark and no new files will get created</li> <li>if calling anything that changes annotations, it will no longer be taken as imported</li> <li>Only annotations with at least one value will be exported.</li> <li>You can export benchmarks with custom transformers or fillers, but should share their source code along with benchmark</li> </ul> <pre><code>from cesnet_tszoo.datasets import CESNET_TimeSeries24\nfrom cesnet_tszoo.utils.enums import SourceType, AgreggationType, DatasetType\nfrom cesnet_tszoo.configs import TimeBasedConfig                                                                            \n\ntime_based_dataset = CESNET_TimeSeries24.get_dataset(data_root=\"/some_directory/\", source_type=SourceType.IP_ADDRESSES_FULL, aggregation=AgreggationType.AGG_1_DAY, dataset_type=DatasetType.TIME_BASED, display_details=True)\nconfig = TimeBasedConfig([1548925, 443967], train_time_period=1.0, features_to_take=[\"n_flows\", \"n_packets\", \"n_bytes\"], transform_with=None)\n\n# Call on time-based dataset to use created config -&gt; must be done before saving exporting benchmark\ntime_based_dataset.set_dataset_config_and_initialize(config, workers=0, display_config_details=\"text\")\n\ntime_based_dataset.save_benchmark(identifier=\"test1\", force_write=True)\n</code></pre>"},{"location":"benchmarks_tutorial/#other","title":"Other","text":"<p>Instead of exporting or importing whole benchmark you can do for specific config or annotations.</p>"},{"location":"benchmarks_tutorial/#config","title":"Config","text":"<ul> <li>Saving config<ul> <li>When parameter <code>force_write</code> is True, existing files with the same name will be overwritten.</li> <li>Config will be saved as pickle file at: <code>os.path.join(dataset.metadata.configs_root, identifier)</code></li> <li>When parameter <code>create_with_details_file</code> is True, text file with config details will be exported along pickle config.</li> </ul> </li> <li>Importing config<ul> <li> <ul> <li>First, it attempts to load the built-in config, if no built-in config with such an identifier exists, it attempts to load a custom config from the <code>\"data_root\"/tszoo/configs/</code> directory.</li> </ul> </li> </ul> </li> </ul> <pre><code>from cesnet_tszoo.configs import TimeBasedConfig                                                                      \n\nconfig = TimeBasedConfig([1548925, 443967], train_time_period=1.0, features_to_take=[\"n_flows\", \"n_packets\", \"n_bytes\"], transform_with=None)\n\ntime_based_dataset.set_dataset_config_and_initialize(config, workers=0, display_config_details=\"text\")\n\n# Exports config\ntime_based_dataset.save_config(identifier=\"test_config1\", create_with_details_file=True, force_write=True)\n\n# Imports custom config\ntime_based_dataset.import_config(identifier=\"test_config1\", display_config_details=\"text\", workers=\"config\")\n</code></pre>"},{"location":"benchmarks_tutorial/#annotations","title":"Annotations","text":"<ul> <li>Saving annotation<ul> <li>When parameter <code>force_write</code> is True, existing files with the same name will be overwritten.</li> <li>Annotations will be saved as CSV file at: <code>os.path.join(dataset.metadata.annotations_root, identifier)</code>.</li> </ul> </li> <li>Importing annotation<ul> <li>First, it attempts to load the built-in annotations, if no built-in annotations with such an identifier exists, it attempts to load a custom annotations from the <code>\"data_root\"/tszoo/annotations/</code> directory.</li> </ul> </li> </ul> <pre><code>from cesnet_tszoo.utils.enums import AnnotationType                                                                    \n\ndataset.add_annotation(annotation=\"test_annotation3_3_0\", annotation_group=\"test3\", ts_id=3, id_time=0, enforce_ids=True)\ndataset.add_annotation(annotation=\"test_annotation3_3_5\", annotation_group=\"test3_2\", ts_id=3, id_time=5, enforce_ids=True)\ndataset.add_annotation(annotation=\"test_annotation3_5_0\", annotation_group=\"test3\", ts_id=5, id_time=0, enforce_ids=True)\ndataset.add_annotation(annotation=\"test_annotation3_5_1\", annotation_group=\"test3_2\", ts_id=5, id_time=1, enforce_ids=True)\ndataset.get_annotations(on=AnnotationType.BOTH)\n\n# Exports annotation of type BOTH\ndataset.save_annotations(identifier=\"test_annotations1\", on=AnnotationType.BOTH, force_write=True)\n\n# Imports custom annotations\ndataset.import_annotations(identifier=\"test_annotations1\", enforce_ids=True)\n</code></pre>"},{"location":"cesnet_agg23/","title":"CESNET-AGG23","text":""},{"location":"cesnet_agg23/#data-capture","title":"Data capture","text":"<p>The data was captured in the flow monitoring infrastructure of the CESNET2 network. The capturing was done for two months between 25.2.2023 and 3.5.2023.</p>"},{"location":"cesnet_agg23/#data-description","title":"Data description","text":"<p>The dataset consists of rules set used by the Scalar aggregator to aggregate information from incoming flows gathered by ipfixprobe. Each row in the dataset represents a single time 1-minute window interval.</p>"},{"location":"cesnet_agg23/#ipfixprobe-parameters","title":"ipfixprobe parameters","text":"<pre><code>Active timeout: 10min\nInactive timeout: 1min\n</code></pre>"},{"location":"cesnet_agg23/#list-of-time-series-metrics","title":"List of time series metrics","text":"Time Series Metric Description id_time Unique identifier for each aggregation interval within the time series, used to segment the dataset into specific time periods for analysis. avr_duration The average duration of all flows. avr_duration_ipv4 The average duration of IPV4 flows. avr_duration_ipv6 The average duration of IPV6 flows. avr_duration_tcp The average duration of TCP flows. avr_duration_udp The average duration of UDP flows. byte_avg The average number of bytes of all flows. byte_avg_ipv4 The average number of bytes of IPV4 flows. byte_avg_ipv6 The average number of bytes of IPV6 flows. byte_avg_tcp The average number of bytes of TCP flows. byte_avg_udp The average number of bytes of UDP flows. byte_rate Byte rate estimation on the network of all flows. byte_rate_ipv4 Byte rate estimation on the network of IPV4 flows. byte_rate_ipv6 Byte rate estimation on the network of IPV6 flows. byte_rate_tcp Byte rate estimation on the network of TCP flows. byte_rate_udp Byte rate estimation on the network of UDP flows. bytes The sum of bytes of all flows. bytes_ipv4 The sum of bytes of IPV4 flows. bytes_ipv6 The sum of bytes of IPV6 flows. bytes_tcp The sum of bytes of TCP flows. bytes_udp The sum of bytes of UDP flows. no_flows The number of all active flows. no_flows_ipv4 The number of IPV4 active flows. no_flows_ipv6 The number of IPV6 active flows. no_flows_tcp The number of TCP active flows. no_flows_tcp_synonly The number of flows containing only SYN packets. no_flows_udp The number of UDP active flows. no_uniq_biflows The number of all unique biflows. no_uniq_flows The number of all unique flows. packet_avg The average packets of all flows. packet_avg_ipv4 The average packets of IPV4 flows. packet_avg_ipv6 The average packets of IPV6 flows. packet_avg_tcp The average packets of TCP flows. packet_avg_udp The average packets of UDP flows. packet_rate Packet rate estimation on the network of all flows. packet_rate_ipv4 Packet rate estimation on the network of IPV4 flows. packet_rate_ipv6 Packet rate estimation on the network of IPV6 flows. packet_rate_tcp Packet rate estimation on the network of TCP flows. packet_rate_udp Packet rate estimation on the network of UDP flows. packets The sum of packets of all flows. packets_ipv4 The sum of packets of IPV4 flows. packets_ipv6 The sum of packets of IPV6 flows. packets_tcp The sum of packets of TCP flows. packets_udp The sum of packets of UDP flows. <p>More detailed description is available in the paper or you can contact dataset author Jaroslav Pesek.</p>"},{"location":"cesnet_timeseries24/","title":"CESNET-TimeSeries24","text":""},{"location":"cesnet_timeseries24/#data-capture","title":"Data capture","text":"<p>The dataset called CESNET-TimeSeries24 was collected by long-term monitoring of selected statistical metrics for 40 weeks for each IP address on the ISP network CESNET3 (Czech Education and Science Network). The dataset encompasses network traffic from more than 275,000 active IP addresses, assigned to a wide variety of devices, including office computers, NATs, servers, WiFi routers, honeypots, and video-game consoles found in dormitories. Moreover, the dataset is also rich in network anomaly types since it contains all types of anomalies, ensuring a comprehensive evaluation of anomaly detection methods.</p> <p>Last but not least, the CESNET-TimeSeries24 dataset provides traffic time series on institutional and IP subnet levels to cover all possible anomaly detection or forecasting scopes. Overall, the time series dataset was created from the 66 billion IP flows that contain 4 trillion packets that carry approximately 3.7 petabytes of data. The CESNET-TimeSeries24 dataset is a complex real-world dataset that will finally bring insights into the evaluation of forecasting models in real-world environments.</p>"},{"location":"cesnet_timeseries24/#data-description","title":"Data description","text":"<p>We create evenly spaced time series for each IP address by aggregating IP flow records into time series datapoints. The created datapoints represent the behavior of IP addresses within a defined time window of 10 minutes. The time series are built from multivariate datapoints.  </p> <p>Datapoints created by the aggregation of IP flows contain the following time-series metrics:</p> <ul> <li>Simple volumetric metrics: the number of IP flows, the number of packets, and the transmitted data size (i.e. number of bytes)</li> <li>Unique volumetric metrics: the number of unique destination IP addresses, the number of unique destination Autonomous System Numbers (ASNs), and the number of unique destination transport layer ports. The aggregation of Unique volumetric metrics is memory intensive since all unique values must be stored in an array. We used a server with 41 GB of RAM, which was enough for 10-minute aggregation on the ISP network.</li> <li>Ratios metrics: the ratio of UDP/TCP packets, the ratio of UDP/TCP transmitted data size, the direction ratio of packets, and the direction ratio of transmitted data size</li> <li>Average metrics: the average flow duration, and the average Time To Live (TTL)</li> </ul>"},{"location":"cesnet_timeseries24/#multiple-time-aggregation","title":"Multiple time aggregation","text":"<p>The original datapoints in the dataset are aggregated by 10 minutes of network traffic. The size of the aggregation interval influences anomaly detection procedures, mainly the training speed of the detection model. However, the 10-minute intervals can be too short for longitudinal anomaly detection methods. Therefore, we added two more aggregation intervals to the datasets--1 hour and 1 day.</p>"},{"location":"cesnet_timeseries24/#time-series-of-institutions","title":"Time series of institutions","text":"<p>We identify 283 institutions inside the CESNET3 network. These time series aggregated per each institution ID provide a view of the institution's data.</p>"},{"location":"cesnet_timeseries24/#time-series-of-institutional-subnets","title":"Time series of institutional subnets","text":"<p>We identify 548 institution subnets inside the CESNET3 network. These time series aggregated per each institution ID provide a view of the institution subnet's data.</p>"},{"location":"cesnet_timeseries24/#ipfixprobe-parameters","title":"ipfixprobe parameters","text":"<pre><code>Active timeout: 5min\nInactive timeout: 65s\n</code></pre>"},{"location":"cesnet_timeseries24/#list-of-time-series-metrics","title":"List of time series metrics","text":"<p>The following list describes time series metrics in dataset:</p> Time Series Metric Description id_time Unique identifier for each aggregation interval within the time series, used to segment the dataset into specific time periods for analysis. n_flows Total number of flows observed in the aggregation interval, indicating the volume of distinct sessions or connections for the IP address. n_packets Total number of packets transmitted during the aggregation interval, reflecting the packet-level traffic volume for the IP address. n_bytes Total number of bytes transmitted during the aggregation interval, representing the data volume for the IP address. n_dest_ip Number of unique destination IP addresses contacted by the IP address during the aggregation interval, showing the diversity of endpoints reached. n_dest_asn Number of unique destination Autonomous System Numbers (ASNs) contacted by the IP address during the aggregation interval, indicating the diversity of networks reached. n_dest_port Number of unique destination transport layer ports contacted by the IP address during the aggregation interval, representing the variety of services accessed. tcp_udp_ratio_packets Ratio of packets sent using TCP versus UDP by the IP address during the aggregation interval, providing insight into the transport protocol usage pattern. This metric belongs to the interval &lt;0, 1&gt; where 1 is when all packets are sent over TCP, and 0 is when all packets are sent over UDP. tcp_udp_ratio_bytes Ratio of bytes sent using TCP versus UDP by the IP address during the aggregation interval, highlighting the data volume distribution between protocols. This metric belongs to the interval &lt;0, 1&gt;  with same rule as tcp_udp_ratio_packets. dir_ratio_packets Ratio of packet directions (inbound versus outbound) for the IP address during the aggregation interval, indicating the balance of traffic flow directions. This metric belongs to the interval &lt;0, 1&gt;, where 1 is when all packets are sent in the outgoing direction from the monitored IP address, and 0 is when all packets are sent in the incoming direction to the monitored IP address. dir_ratio_bytes Ratio of byte directions (inbound versus outbound) for the IP address during the aggregation interval, showing the data volume distribution in traffic flows. This metric belongs to the interval &lt;0, 1&gt; with the same rule as dir_ratio_packets. avg_duration Average duration of IP flows for the IP address during the aggregation interval, measuring the typical session length. avg_ttl Average Time To Live (TTL) of IP flows for the IP address during the aggregation interval, providing insight into the lifespan of packets. <p>Moreover, the time series created by re-aggregation contains following time series metrics instead of n_dest_ip, n_dest_asn, and n_dest_port:</p> Time Series Metric Description sum_n_dest_ip Sum of numbers of unique destination IP addresses. avg_n_dest_ip The average number of unique destination IP addresses. std_n_dest_ip Standard deviation of numbers of unique destination IP addresses. sum_n_dest_asn Sum of numbers of unique destination ASNs. avg_n_dest_asn The average number of unique destination ASNs. std_n_dest_asn Standard deviation of numbers of unique destination ASNs. sum_n_dest_ports Sum of numbers of unique destination transport layer ports. avg_n_dest_ports The average number of unique destination transport layer ports. std_n_dest_ports Standard deviation of numbers of unique destination transport layer ports. <p>More detailed description is available in the dataset paper or you can contact dataset author Josef Koumar.</p>"},{"location":"choosing_data/","title":"Choosing data","text":"<p>This tutorial will look at some configuration options for choosing data you wish to load. </p> <p>Each dataset type will have its own part because of multiple differences of available configuration values.</p>"},{"location":"choosing_data/#timebasedcesnetdataset-dataset","title":"<code>TimeBasedCesnetDataset</code> dataset","text":"<p>Note</p> <p>For every configuration and more detailed examples refer to Jupyter notebook <code>time_based_choosing_data</code></p> <p>Relevant configuration values:</p> <ul> <li><code>ts_ids</code> - Defines which time series IDs are used for train/val/test/all.</li> <li><code>train_time_period</code>/<code>val_time_period</code>/<code>test_time_period</code> - Defines time periods for train/val/test sets.</li> <li><code>features_to_take</code> - Defines which features are used.</li> <li><code>include_time</code> - If True, time data is included in the returned values.</li> <li><code>include_ts_id</code> - If True, time series IDs are included in the returned values.</li> <li><code>time_format</code> - Format for the returned time data.</li> <li><code>random_state</code> - Fixes randomness for reproducibility when setting <code>ts_ids</code></li> </ul>"},{"location":"choosing_data/#selecting-which-time-series-to-load","title":"Selecting which time series to load","text":"<ul> <li>Sets time series that will be used for train/val/test/all sets</li> </ul> <pre><code>from cesnet_tszoo.configs import TimeBasedConfig\n\n# Sets time series used in sets with count. Chosen randomly from available time series.\n# Affected by random_state.\nconfig = TimeBasedConfig(ts_ids=54, random_state = 111)\n\n# Sets time series used in sets with percentage of time series in dataset. Chosen randomly from available time series.\n# Affected by random_state.\nconfig = TimeBasedConfig(ts_ids=0.1, random_state = 111)\n\n# Sets ts_ids with specific time series\nconfig = TimeBasedConfig(ts_ids=[0,1,2,3,4,5])\n\n# Call on time-based dataset to use created config\ntime_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre>"},{"location":"choosing_data/#creating-trainvaltest-sets","title":"Creating train/val/test sets","text":"<ul> <li>Sets time period in set for every time series in <code>ts_ids</code></li> <li>You can leave any set value set as None.</li> <li>Can use <code>nan_threshold</code> to set how many nan values will be tolerated.<ul> <li><code>nan_threshold</code> = 1.0, means that time series can be completely empty.</li> <li>is applied after sets.</li> <li>Is checked seperately for every set.</li> </ul> </li> <li>Sets must follow these rules:<ul> <li>Used time periods must be connected.</li> <li>Sets can share subset of times.</li> <li>start of <code>train_time_period</code> &lt; start of <code>val_time_period</code> &lt; start of <code>test_time_period</code>.</li> </ul> </li> </ul> <pre><code>from datetime import datetime\n\nfrom cesnet_tszoo.configs import TimeBasedConfig\n\n# Sets sets as range of time indices.\nconfig = TimeBasedConfig(ts_ids=54, train_time_period=range(0, 2000), val_time_period=range(2000, 4000), test_time_period=range(4000, 5000))\n\n# Sets sets with tuple of datetime objects.\n# Datetime objects are expected to be of UTC.\nconfig = TimeBasedConfig(ts_ids=54, train_time_period=(datetime(2023, 10, 9, 0), datetime(2023, 11, 9, 23)), val_time_period=(datetime(2023, 11, 9, 23), datetime(2023, 12, 9, 23)), test_time_period=(datetime(2023, 12, 9, 23), datetime(2023, 12, 25, 23)))\n\n# Sets sets a percentage of whole time period from dataset.\n# Always starts from first time.\nconfig = TimeBasedConfig(ts_ids=54, train_time_period=0.5, val_time_period=0.3, test_time_period=0.2)\n\ntime_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre>"},{"location":"choosing_data/#selecting-features","title":"Selecting features","text":"<ul> <li>Affects which features will be returned when loading data.</li> <li>Setting <code>include_time</code> as True will add time to features that return when loading data.</li> <li>Setting <code>include_ts_id</code> as True will add time series id to features that return when loading data.</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import TimeFormat\nfrom cesnet_tszoo.configs import TimeBasedConfig\n\nconfig = TimeBasedConfig(ts_ids=54, features_to_take=\"all\")\n\nconfig = TimeBasedConfig(ts_ids=54, features_to_take=[\"n_flows\", \"n_packets\"])\n\nconfig = TimeBasedConfig(ts_ids=54, features_to_take=[\"n_flows\", \"n_packets\"], include_time=True, include_ts_id=True, time_format=TimeFormat.ID_TIME)\n\ntime_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre>"},{"location":"choosing_data/#selecting-all-set","title":"Selecting all set","text":"<ul> <li>Contains time series from <code>ts_ids</code>.</li> </ul> <pre><code>from cesnet_tszoo.configs import TimeBasedConfig\n\n# All set will contain whole time period of dataset.\nconfig = TimeBasedConfig(ts_ids=54, train_time_period=None, val_time_period=None, test_time_period=None)\n\n# All set will contain total time period of train + val + test.\nconfig = TimeBasedConfig(ts_ids=54, train_time_period=0.5, val_time_period=0.3, test_time_period=0.2)\n\ntime_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre>"},{"location":"choosing_data/#disjointtimebasedcesnetdataset-dataset","title":"<code>DisjointTimeBasedCesnetDataset</code> dataset","text":"<p>Note</p> <p>For every configuration and more detailed examples refer to Jupyter notebook <code>disjoint_time_based_choosing_data</code></p> <p>Relevant configuration values:</p> <ul> <li><code>train_ts</code>/<code>val_ts</code>/<code>test_ts</code> - Defines time series for train/val/test.</li> <li><code>train_time_period</code>/<code>val_time_period</code>/<code>test_time_period</code> - Defines time periods for train/val/test sets.</li> <li><code>features_to_take</code> - Defines which features are used.</li> <li><code>include_time</code> - If True, time data is included in the returned values.</li> <li><code>include_ts_id</code> - If True, time series IDs are included in the returned values.</li> <li><code>time_format</code> - Format for the returned time data.</li> <li><code>random_state</code> - Fixes randomness for reproducibility when setting <code>train_ts</code>/<code>val_ts</code>/<code>test_ts</code></li> </ul>"},{"location":"choosing_data/#selecting-which-time-series-to-load_1","title":"Selecting which time series to load","text":"<ul> <li>Sets time series that will be used for train/val/test/all sets</li> </ul> <pre><code>from cesnet_tszoo.configs import DisjointTimeBasedConfig\n\n# Sets time series used in sets with count. Chosen randomly from available time series.\n# Affected by random_state.\nconfig = DisjointTimeBasedConfig(train_ts=100, val_ts=50, test_ts=20, train_time_period=0.7, val_time_period=0.2, test_time_period=0.1, random_state = 111)\n\n# Sets time series used in sets with percentage of time series in dataset. Chosen randomly from available time series.\n# Affected by random_state.\nconfig = DisjointTimeBasedConfig(train_ts=0.5, val_ts=0.2, test_ts=0.1, train_time_period=0.7, val_time_period=0.2, test_time_period=0.1, random_state = 111)\n\n# Sets with specific time series\nconfig = DisjointTimeBasedConfig(train_ts=[0], val_ts=[1], test_ts=[2], train_time_period=0.7, val_time_period=0.2, test_time_period=0.1, random_state = 111)\n\n# Call on disjoint-time-based dataset to use created config\ndisjoint_dataset.set_dataset_config_and_initialize(config)\n</code></pre>"},{"location":"choosing_data/#selecting-which-time-period-to-use-for-each-set","title":"Selecting which time period to use for each set","text":"<ul> <li>Sets time period for every set and their time series</li> <li><code>train_time_period</code> is used for <code>train_ts</code></li> <li><code>val_time_period</code> is used for <code>val_ts</code></li> <li><code>test_time_period</code> is used for <code>test_ts</code></li> <li>Either both time series and their time period must be set or both has to be None</li> <li>Can use <code>nan_threshold</code> to set how many nan values will be tolerated for time series and their time period.<ul> <li><code>nan_threshold</code> = 1.0, means that time series can be completely empty.</li> <li>is applied after sets.</li> <li>Is checked seperately for every set.</li> </ul> </li> <li>Sets must follow these rules:<ul> <li>Used time periods must be connected.</li> <li>Sets can share subset of times.</li> <li>start of <code>train_time_period</code> &lt; start of <code>val_time_period</code> &lt; start of <code>test_time_period</code>.</li> </ul> </li> </ul> <pre><code>from datetime import datetime\n\nfrom cesnet_tszoo.configs import DisjointTimeBasedConfig\n\n# Sets sets as range of time indices.\nconfig = DisjointTimeBasedConfig(train_ts=0.5, val_ts=0.2, test_ts=0.1, train_time_period=range(0, 2000), val_time_period=range(2000, 4000), test_time_period=range(4000, 5000))\n\n# Sets sets with tuple of datetime objects.\n# Datetime objects are expected to be of UTC.\nconfig = DisjointTimeBasedConfig(train_ts=0.5, val_ts=0.2, test_ts=0.1, train_time_period=(datetime(2023, 10, 9, 0), datetime(2023, 11, 9, 23)), val_time_period=(datetime(2023, 11, 9, 23), datetime(2023, 12, 9, 23)), test_time_period=(datetime(2023, 12, 9, 23), datetime(2023, 12, 25, 23)))\n\n# Sets sets a percentage of whole time period from dataset.\n# Always starts from first time.\nconfig = DisjointTimeBasedConfig(train_ts=0.5, val_ts=0.2, test_ts=0.1, train_time_period=0.5, val_time_period=0.3, test_time_period=0.2)\n\ndisjoint_dataset.set_dataset_config_and_initialize(config)\n</code></pre>"},{"location":"choosing_data/#selecting-features_1","title":"Selecting features","text":"<ul> <li>Affects which features will be returned when loading data.</li> <li>Setting <code>include_time</code> as True will add time to features that return when loading data.</li> <li>Setting <code>include_ts_id</code> as True will add time series id to features that return when loading data.</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import TimeFormat\nfrom cesnet_tszoo.configs import DisjointTimeBasedConfig\n\nconfig = DisjointTimeBasedConfig(train_ts=0.5, val_ts=0.2, test_ts=0.1, train_time_period=0.5, val_time_period=0.3, test_time_period=0.2, features_to_take=\"all\")\n\nconfig = DisjointTimeBasedConfig(train_ts=0.5, val_ts=0.2, test_ts=0.1, train_time_period=0.5, val_time_period=0.3, test_time_period=0.2, features_to_take=[\"n_flows\", \"n_packets\"])\n\nconfig = DisjointTimeBasedConfig(train_ts=0.5, val_ts=0.2, test_ts=0.1, train_time_period=0.5, val_time_period=0.3, test_time_period=0.2, features_to_take=[\"n_flows\", \"n_packets\"], include_time=True, include_ts_id=True, time_format=TimeFormat.ID_TIME)\n\ndisjoint_dataset.set_dataset_config_and_initialize(config)\n</code></pre>"},{"location":"choosing_data/#seriesbasedcesnetdataset-dataset","title":"<code>SeriesBasedCesnetDataset</code> dataset","text":"<p>Note</p> <p>For every configuration and more detailed examples refer to Jupyter notebook <code>series_based_choosing_data</code></p> <p>Relevant configuration values:</p> <ul> <li><code>time_period</code> - Defines the time period for train/val/test/all sets.</li> <li><code>train_ts</code>/<code>val_ts</code>/<code>test_ts</code> - Defines time series for train/val/test</li> <li><code>features_to_take</code> - Defines which features are used.</li> <li><code>include_time</code> - If True, time data is included in the returned values.</li> <li><code>include_ts_id</code> - If True, time series IDs are included in the returned values.</li> <li><code>time_format</code> - Format for the returned time data.</li> <li><code>random_state</code> - Fixes randomness for reproducibility when setting <code>train_ts</code>, <code>val_ts</code>, <code>test_ts</code>.</li> </ul>"},{"location":"choosing_data/#selecting-time-period","title":"Selecting time period","text":"<ul> <li><code>time_period</code> sets time period for all sets (used time series).</li> </ul> <pre><code>from datetime import datetime\n\nfrom cesnet_tszoo.configs import SeriesBasedConfig\n\n# Sets time period for time series as a whole time period from dataset.\nconfig = SeriesBasedConfig(time_period=\"all\")\n\n# Sets time period for time series as range of time indices.\nconfig = SeriesBasedConfig(time_period=range(0, 2000))\n\n# Sets time period for time series with tuple of datetime objects.\n# Datetime objects are expected to be of UTC.\nconfig = SeriesBasedConfig(time_period=(datetime(2023, 10, 9, 0), datetime(2023, 11, 9, 23)))\n\n# Sets time period for time series as a percentage of whole time period from dataset.\n# Always starts from first time.\nconfig = SeriesBasedConfig(time_period=0.5)\n\n# Call on series-based dataset to use created config\nseries_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre>"},{"location":"choosing_data/#creating-trainvaltest-sets_1","title":"Creating train/val/test sets","text":"<ul> <li>Sets how many time series will be in each set.</li> <li>You can leave any set value set as None.</li> <li>Each set must have unique time series</li> <li>Can use <code>nan_threshold</code> to set how many nan values will be tolerated.<ul> <li><code>nan_threshold</code> = 1.0, means that time series can be completely empty.</li> <li>is applied after sets.</li> </ul> </li> </ul> <pre><code>from cesnet_tszoo.configs import SeriesBasedConfig\n\n# Sets time series in set with count. Chosen randomly from available time series.\n# Each set will contain unique time series.\n# Affected by random_state.\nconfig = SeriesBasedConfig(time_period=0.5, train_ts=54, val_ts=25, test_ts=10, random_state=None, nan_threshold=1.0)\n\n# Sets time series in set with percentage of time series in dataset. Chosen randomly from available time series.\n# Each set will contain unique time series.\n# Affected by random_state.\nconfig = SeriesBasedConfig(time_period=0.5, train_ts=0.5, val_ts=0.2, test_ts=0.1, random_state=None, nan_threshold=1.0)\n\n# Sets sets with specific time series\nconfig = SeriesBasedConfig(time_period=0.5, train_ts=[0,1,2,3,4], val_ts=[5,6,7,8,9], test_ts=[10,11,12,13,14], nan_threshold=1.0)\n\nseries_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre>"},{"location":"choosing_data/#selecting-features_2","title":"Selecting features","text":"<ul> <li>Affects which features will be returned when loading data.</li> <li>Setting <code>include_time</code> as True will add time to features that return when loading data.</li> <li>Setting <code>include_ts_id</code> as True will add time series id to features that return when loading data.</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import TimeFormat\nfrom cesnet_tszoo.configs import SeriesBasedConfig\n\nconfig = SeriesBasedConfig(time_period=0.5, features_to_take=\"all\")\n\nconfig = SeriesBasedConfig(time_period=0.5, features_to_take=[\"n_flows\", \"n_packets\"])\n\nconfig = SeriesBasedConfig(time_period=0.5, features_to_take=[\"n_flows\", \"n_packets\"], include_time=True, include_ts_id=True, time_format=TimeFormat.ID_TIME)\n\n# Call on series-based dataset to use created config\nseries_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre>"},{"location":"choosing_data/#selecting-all-set_1","title":"Selecting all set","text":"<pre><code>from cesnet_tszoo.configs import SeriesBasedConfig\n\n# All set will contain all time series from dataset.\nconfig = SeriesBasedConfig(time_period=0.5, train_ts=None, val_ts=None, test_ts=None)\n\n# All set will contain all time series that were set by other sets.\nconfig = SeriesBasedConfig(time_period=0.5, train_ts=54, val_ts=25, test_ts=10)\n\nseries_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre>"},{"location":"custom_handlers/","title":"Custom handlers","text":"<p>The <code>cesnet_tszoo</code> package supports adding custom way of handling data. It is possible to create custom handler for per time series, custom handler for all time series by subclassing from <code>custom handlers</code>. They also support applying only to specified sets.</p> <p>Related config parameters in <code>TimeBasedConfig</code>, <code>DisjointTimeBasedConfig</code> and <code>SeriesBasedConfig</code>:</p> <ul> <li><code>preprocess_order</code>: Mainly used for changing order of preprocesses. Is also used as a way of adding custom handlers by adding type their type between the preprocesses.</li> </ul>"},{"location":"custom_handlers/#creating-custom-handlers","title":"Creating custom handlers","text":"<p>You can create custom handler by subclassing from one of the below classes:</p> <ul> <li><code>PerSeriesCustomHandler</code><ul> <li>Instance is created for every time series separately</li> <li>Fits on their respective time series train set part</li> </ul> </li> <li><code>AllSeriesCustomHandler</code> <ul> <li>Only one instance is created for all used time series</li> <li>Fits on train set</li> </ul> </li> <li><code>NoFitCustomHandler</code> <ul> <li>Instance is created for every time series separately </li> <li>Does not fit</li> </ul> </li> </ul> <p>PerSeriesCustomHandler</p> <p><code>PerSeriesCustomHandler</code> is only supported for <code>Time-based</code>, because of its nature.</p>"},{"location":"dataset_approaches/","title":"Approaches to datasets","text":"<p>The <code>cesnet-tszoo</code> library provides multiple splitting strategies to accommodate different TSA tasks, such as forecasting, classification, and similarity search.</p>"},{"location":"dataset_approaches/#time-based","title":"<code>Time-based</code>","text":"<p>Time-based approach splits each time series separately based on the time axis. The times of splits into train, validation, and test sets can be selected in multiple ways, for example, exact timestamp or classical percentage split (i.e., 60:20:20). The train set is always before the validation and test set, and the validation set is always before the test set. This splitting approach is practical, for example, for forecasting or anomaly detection, where we need to predict future data from historical data. </p>"},{"location":"dataset_approaches/#time-based-splitting-with-disjoint-identifiers","title":"<code>Time-based splitting with disjoint identifiers</code>","text":"<p>Time-based splitting with disjoint identifiers was implemented to support better generalization of algorithms. Time series are split into train, validation, and test sets not only by time but simultaneously by identifiers. This approach allows robust evaluation of a model trained and validated on different time series in a different time span.  </p>"},{"location":"dataset_approaches/#series-based","title":"<code>Series-based</code>","text":"<p>Series-based splitting procedure splits time series based on the different time series identifiers into train, validation, and test sets. The series-based splitting is valuable, for example, for classification based on time series behavior or similarity detection in the same time frame. </p>"},{"location":"datasets_overview/","title":"Overview of datasets","text":""},{"location":"datasets_overview/#cesnet-timeseries24","title":"CESNET-TimeSeries24","text":"<p>CESNET-TimeSeries24</p> <ul> <li>Collected in 2023-2024</li> <li>Spans 40 weeks</li> <li>Contains multivariate time series for 283 institutions, 548 institution subnets, 275 124 IP addresses</li> <li>Three aggregation windows: 1 day, 1 hour, 10 minute</li> </ul> <p>This dataset was published in \"CESNET-TimeSeries24: Time Series Dataset for Network Traffic Anomaly Detection and Forecasting\" (DOI). It was built from live traffic collected using high-speed monitoring probes at the perimeter of the CESNET3 network.</p> <p>For detailed information about the dataset, please refer to the linked paper and the CESNET-TimeSeries24 page.</p>"},{"location":"datasets_overview/#cesnet-agg23","title":"CESNET-AGG23","text":"<p>CESNET-AGG23</p> <ul> <li>Collected in 2023</li> <li>Spans 10 weeks</li> <li>Contains overall multivariate time series for CESNET2 network</li> <li>One aggregation window: 1 minute</li> </ul> <p>This dataset was published in \"Look at my Network: An Insight into the ISP Backbone Traffic\" (DOI). It was built from live traffic collected using high-speed monitoring probes at the perimeter of the CESNET2 network.</p> <p>For detailed information about the dataset, please refer to the linked paper and the CESNET-AGG23 page.</p>"},{"location":"device_type_classification/","title":"Network Device Type Classification","text":"<p>Network device type classification focuses on evaluating the performance of models for classifying types of network devices. The goal of this benchmark is to allow comparison of various classification algorithms and methods in the context of network devices. This task is valuable in environments where it is essential to quickly and efficiently identify devices in a network for monitoring, security, and traffic optimization purposes. Analyzing the benchmarks helps determine which methods are most suitable for deployment in real-world scenarios.</p> <p>Available benchmarks for training unique model per each time series are here:</p> Benchmark hash Dataset Aggregation Source Original paper 69270dcc1819 CESNET-TimeSeries24 10 MINUTES IP_ADDRESSES_FULL None 941261e8c367 CESNET-TimeSeries24 1 HOUR IP_ADDRESSES_FULL None bf0aec939afe CESNET-TimeSeries24 1 DAY IP_ADDRESSES_FULL None <p>We encourage users to change default value for missing values, filler, transformer, sliding window step,  and batch sizes. However, users may not change the rest of the arguments. Usage of these benchmarks are following:</p> <pre><code>from cesnet_tszoo.benchmarks import load_benchmark\nfrom cesnet_tszoo.utils.enums import AnnotationType, FillerType, TransformerType, SplitType\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\n\nbenchmark = load_benchmark(\"bf0aec939afe\", \"../\")\ndataset = benchmark.get_initialized_dataset()\n\n# Get annotations\nannotations = benchmark.get_annotations(on=AnnotationType.TS_ID)\n\n# Prepare annotations\nencoder = LabelEncoder()\nannotations['group_encoded'] = encoder.fit_transform(annotations['group'])\n\ntrain_annotations = annotations[annotations['id_ip'].isin(dataset.get_data_about_set(about=SplitType.TRAIN)['ts_ids'])]\ntrain_target = train_annotations['group_encoded'].to_numpy()\n\ntest_annotations = annotations[annotations['id_ip'].isin(dataset.get_data_about_set(about=SplitType.TEST)['ts_ids'])]\ntest_target = test_annotations['group_encoded'].to_numpy()\n\n# (optional) Set default value for missing data \ndataset.set_default_values(0)\n\n# (optional) Set filler for filling missing data \ndataset.apply_filler(FillerType.MEAN_FILLER)\n\n# (optional) Set transformer for data\ndataset.apply_transformer(TransformerType.MIN_MAX_SCALER)\n\n# (optional) Change sliding window setting\ndataset.set_sliding_window(sliding_window_size=80, sliding_window_prediction_size=24, sliding_window_step=1, set_shared_size=80)\n\n# or to update all at once which is usually faster\n# dataset.update_dataset_config_and_initialize(default_values=0, sliding_window_size=80, sliding_window_prediction_size=24, sliding_window_step=1, set_shared_size=80, \n#                                              fill_missing_with=FillerType.MEAN_FILLER, transform_with=TransformerType.MIN_MAX_SCALER)\n\n# Process with your own defined model\nmodel = Model()\nmodel.fit(\n    dataset.get_train_dataloader(), \n    dataset.get_val_dataloader(),\n    train_target,\n)\n\n# Predict for time series which data are not in training\ny_pred = model.predict(\n    dataset.get_test_dataloader()\n)\n\n# Evaluate predictions, for example, with RMSE\naccuracy = accuracy_score(test_target, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n</code></pre>"},{"location":"fillers/","title":"Fillers","text":"<p>The <code>cesnet_tszoo</code> package supports various ways of dealing with missing data in dataset. Possible config parameters in <code>TimeBasedConfig</code>, <code>DisjointTimeBasedConfig</code> and <code>SeriesBasedConfig</code>:</p> <ul> <li><code>fill_missing_with</code>: Can pass enum <code>FillerType</code> for built-in filler or pass a type of custom filler that must derive from <code>Filler</code> base class.</li> <li><code>default_values</code>: Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature.</li> </ul> <p>Note for <code>Time-based</code></p> <p>Fillers can carry over values from the train set to the validation and test sets. For example, <code>ForwardFiller</code> can carry over values from previous sets. </p>"},{"location":"fillers/#built-in-fillers","title":"Built-in fillers","text":"<p>The <code>cesnet_tszoo</code> package comes with multiple built-in fillers. To check built-in fillers refer to <code>fillers</code>.</p>"},{"location":"fillers/#custom-fillers","title":"Custom fillers","text":"<p>It is possible to create and use own fillers. But custom filler must derive from <code>Filler</code> base class.</p>"},{"location":"forecasting_multivariate/","title":"Multivariate forecasting","text":"<p>We divided these benchmarks into two groups.</p>"},{"location":"forecasting_multivariate/#unique-model-for-each-time-series","title":"Unique model for each time series","text":"<p>First group target on training model for each time series. Available benchmarks for training unique model per each time series are here:</p> Benchmark hash Dataset Aggregation Source Original paper 930f0b401065 CESNET-TimeSeries24 10 MINUTES INSTITUTIONS None ca6999ea7e24 CESNET-TimeSeries24 10 MINUTES INSTITUTION_SUBNETS None 7495b16f5fe6 CESNET-TimeSeries24 10 MINUTES IP_ADDRESSES_FULL None 3687fb52c433 CESNET-TimeSeries24 10 MINUTES IP_ADDRESSES_SAMPLE None a6e56f99ab8a CESNET-TimeSeries24 1 HOUR INSTITUTION_SUBNETS None e44334732033 CESNET-TimeSeries24 1 HOUR INSTITUTIONS None 18d04cab63e4 CESNET-TimeSeries24 1 HOUR IP_ADDRESSES_FULL None b0ea46897cae CESNET-TimeSeries24 1 HOUR IP_ADDRESSES_SAMPLE None 63e1f696e7c5 CESNET-TimeSeries24 1 DAY INSTITUTION_SUBNETS None 71d17ad3550f CESNET-TimeSeries24 1 DAY INSTITUTIONS None 15737f3fceec CESNET-TimeSeries24 1 DAY IP_ADDRESSES_FULL None 084f368f4c82 CESNET-TimeSeries24 1 DAY IP_ADDRESSES_SAMPLE None <p>We encourage users to change default value for missing values, filler, transformer, sliding window step,  and batch sizes. However, users may not change the rest of the arguments. Usage of these benchmarks are following:</p> <pre><code>from cesnet_tszoo.benchmarks import load_benchmark\nfrom cesnet_tszoo.utils.enums import FillerType, TransformerType\nfrom sklearn.metrics import mean_squared_error\n\nbenchmark = load_benchmark(\"871f5972109e\", \"../\")\ndataset = benchmark.get_initialized_dataset()\n\n# (optional) Set default value for missing data \ndataset.set_default_values(0)\n\n# (optional) Set filler for filling missing data \ndataset.apply_filler(FillerType.MEAN_FILLER)\n\n# (optional) Set transformer for data\ndataset.apply_transformer(TransformerType.MIN_MAX_SCALER)\n\n# (optional) Change sliding window setting\ndataset.set_sliding_window(sliding_window_size=744, sliding_window_prediction_size=24, sliding_window_step=1, set_shared_size=744)\n\n# or to update all at once which is usually faster\n# dataset.update_dataset_config_and_initialize(default_values=0, sliding_window_size=744, sliding_window_prediction_size=24, sliding_window_step=1, set_shared_size=744, \n#                                              fill_missing_with=FillerType.MEAN_FILLER, transform_with=TransformerType.MIN_MAX_SCALER)\n\n# Process with model per each time series individualy \nresults = []\nfor ts_id in dataset.get_data_about_set(about='train')['ts_ids']:\n    # Define your own class Model uses dataloaders for perform training and prediction\n    model = Model()\n    model.fit(\n        dataset.get_train_dataloader(ts_id), \n        dataset.get_val_dataloader(ts_id),\n    )\n    y_pred, y_true = model.predict(\n        dataset.get_test_dataloader(ts_id), \n    )\n\n    # Evaluate predictions, for example, with RMSE\n    rmse = mean_squared_error(y_true, y_pred)\n\n    # Add individual result into all results\n    results.append(rmse)\n\nprint(f\"Mean RMSE: {np.mean(rmse):.4f}\")\nprint(f\"Std RMSE: {np.std(rmse):.4f}\")\n</code></pre>"},{"location":"forecasting_multivariate/#generic-model-for-multiple-time-series","title":"Generic model for multiple time series","text":"<p>Second group target on training one generic model which learns generic paterns in several time series and then it can forecast multiple other time series. Available benchmarks for training unique model per each time series are here:</p> Benchmark hash Dataset Aggregation Source Original paper 9ac2b87c9a7c CESNET-TimeSeries24 10 MINUTES INSTITUTIONS None 7cd4e41b05ec CESNET-TimeSeries24 10 MINUTES INSTITUTION_SUBNETS None 50eb509e1e77 CESNET-TimeSeries24 10 MINUTES IP_ADDRESSES_FULL None 681a7fb90948 CESNET-TimeSeries24 10 MINUTES IP_ADDRESSES_SAMPLE None ab8183ea80af CESNET-TimeSeries24 1 HOUR INSTITUTION_SUBNETS None f9bd005c7efe CESNET-TimeSeries24 1 HOUR INSTITUTIONS None 88fd173619b2 CESNET-TimeSeries24 1 HOUR IP_ADDRESSES_FULL None 4ae11863ee38 CESNET-TimeSeries24 1 HOUR IP_ADDRESSES_SAMPLE None cdb79dbf54ea CESNET-TimeSeries24 1 DAY INSTITUTION_SUBNETS None c95d66b0baf5 CESNET-TimeSeries24 1 DAY INSTITUTIONS None 16274e0b44af CESNET-TimeSeries24 1 DAY IP_ADDRESSES_FULL None 0197980a87c0 CESNET-TimeSeries24 1 DAY IP_ADDRESSES_SAMPLE None <p>We encourage users to change default value for missing values, filler, transformer, sliding window step,  and batch sizes. However, users may not change the rest of the arguments. Usage of these benchmarks are following:</p> <pre><code>from cesnet_tszoo.benchmarks import load_benchmark\nfrom cesnet_tszoo.utils.enums import FillerType, TransformerType\nfrom sklearn.metrics import mean_squared_error\n\nbenchmark = load_benchmark(\"09de83e89e42\", \"../\")\ndataset = benchmark.get_initialized_dataset()\n\n# (optional) Set default value for missing data \ndataset.set_default_values(0)\n\n# (optional) Set filler for filling missing data \ndataset.apply_filler(FillerType.MEAN_FILLER)\n\n# (optional) Set transformer for data\ndataset.apply_transformer(TransformerType.MIN_MAX_SCALER)\n\n# (optional) Change sliding window setting\ndataset.set_sliding_window(sliding_window_size=744, sliding_window_prediction_size=24, sliding_window_step=1, set_shared_size=744)\n\n# or to update all at once which is usually faster\n# dataset.update_dataset_config_and_initialize(default_values=0, sliding_window_size=744, sliding_window_prediction_size=24, sliding_window_step=1, set_shared_size=744, \n#                                              fill_missing_with=FillerType.MEAN_FILLER, transform_with=TransformerType.MIN_MAX_SCALER)\n\n# Process with your own defined model\nmodel = Model()\nmodel.fit(\n    dataset.get_train_dataloader(), \n    dataset.get_val_dataloader(),\n)\n\n# Predict for time series which data are not in training\ny_pred, y_true = model.predict(\n    dataset.get_test_dataloader(), \n)\n\n# Evaluate predictions, for example, with RMSE\nrmse = mean_squared_error(y_true, y_pred)\nprint(f\"RMSE: {rmse:.4f}\")\n</code></pre>"},{"location":"forecasting_univariate/","title":"Univariate forecasting","text":"<p>Benchmarks in this group are designed to support mostly used forecasting task for network management. We divided these benchmarks into two groups.</p>"},{"location":"forecasting_univariate/#unique-model-for-each-time-series","title":"Unique model for each time series","text":"<p>First group target on training model for each time series. Available benchmarks for training unique model per each time series are here:</p> Benchmark hash Dataset Aggregation Source Original paper 22c5a8e8ffd3 CESNET-TimeSeries24 10 MINUTES INSTITUTIONS None 095f847ca755 CESNET-TimeSeries24 10 MINUTES INSTITUTION_SUBNETS None c2970e89d824 CESNET-TimeSeries24 10 MINUTES IP_ADDRESSES_SAMPLE None ddb1f02dae43 CESNET-TimeSeries24 10 MINUTES IP_ADDRESSES_FULL None 871f5972109e CESNET-TimeSeries24 1 HOUR INSTITUTIONS None 080582bcd519 CESNET-TimeSeries24 1 HOUR INSTITUTION_SUBNETS None f3fc14310e2e CESNET-TimeSeries24 1 HOUR IP_ADDRESSES_SAMPLE None e268fa9957f2 CESNET-TimeSeries24 1 HOUR IP_ADDRESSES_FULL None 0d523e69c328 CESNET-TimeSeries24 1 DAY INSTITUTIONS None 8e2a07fb3177 CESNET-TimeSeries24 1 DAY INSTITUTION_SUBNETS None b5e5ea044b81 CESNET-TimeSeries24 1 DAY IP_ADDRESSES_SAMPLE None d19ba386743f CESNET-TimeSeries24 1 DAY IP_ADDRESSES_FULL None <p>We encourage users to change default value for missing values, filler, transformer, sliding window step,  and batch sizes. However, users may not change the rest of the arguments. Usage of these benchmarks are following:</p> <pre><code>from cesnet_tszoo.benchmarks import load_benchmark\nfrom cesnet_tszoo.utils.enums import FillerType, TransformerType\nfrom sklearn.metrics import mean_squared_error\n\nbenchmark = load_benchmark(\"871f5972109e\", \"../\")\ndataset = benchmark.get_initialized_dataset()\n\n# (optional) Set default value for missing data \ndataset.set_default_values(0)\n\n# (optional) Set filler for filling missing data \ndataset.apply_filler(FillerType.MEAN_FILLER)\n\n# (optional) Set transformer for data\ndataset.apply_transformer(TransformerType.MIN_MAX_SCALER)\n\n# (optional) Change sliding window setting\ndataset.set_sliding_window(sliding_window_size=744, sliding_window_prediction_size=24, sliding_window_step=1, set_shared_size=744)\n\n# or to update all at once which is usually faster\n# dataset.update_dataset_config_and_initialize(default_values=0, sliding_window_size=744, sliding_window_prediction_size=24, sliding_window_step=1, set_shared_size=744, \n#                                              fill_missing_with=FillerType.MEAN_FILLER, transform_with=TransformerType.MIN_MAX_SCALER)\n\n# Process with model per each time series individualy \nresults = []\nfor ts_id in dataset.get_data_about_set(about='train')['ts_ids']:\n    # Define your own class Model uses dataloaders for perform training and prediction\n    model = Model()\n    model.fit(\n        dataset.get_train_dataloader(ts_id), \n        dataset.get_val_dataloader(ts_id),\n    )\n    y_pred, y_true = model.predict(\n        dataset.get_test_dataloader(ts_id), \n    )\n\n    # Evaluate predictions, for example, with RMSE\n    rmse = mean_squared_error(y_true, y_pred)\n\n    # Add individual result into all results\n    results.append(rmse)\n\nprint(f\"Mean RMSE: {np.mean(rmse):.4f}\")\nprint(f\"Std RMSE: {np.std(rmse):.4f}\")\n</code></pre>"},{"location":"forecasting_univariate/#generic-model-for-multiple-time-series","title":"Generic model for multiple time series","text":"<p>Second group target on training one generic model which learns generic paterns in several time series and then it can forecast multiple other time series.  Available benchmarks for training unique model per each time series are here:</p> Benchmark hash Dataset Aggregation Source Original paper 7706f1087922 CESNET-TimeSeries24 10 MINUTES INSTITUTIONS None a642915953ad CESNET-TimeSeries24 10 MINUTES INSTITUTION_SUBNETS None e3de1fc0a44e CESNET-TimeSeries24 10 MINUTES IP_ADDRESSES_SAMPLE None 8b03d0d508ce CESNET-TimeSeries24 10 MINUTES IP_ADDRESSES_FULL None 09de83e89e42 CESNET-TimeSeries24 1 HOUR INSTITUTIONS None 73a9add2c4af CESNET-TimeSeries24 1 HOUR INSTITUTION_SUBNETS None 6249383544ef CESNET-TimeSeries24 1 HOUR IP_ADDRESSES_SAMPLE None b8098753b97b CESNET-TimeSeries24 1 HOUR IP_ADDRESSES_FULL None ef632e70c252 CESNET-TimeSeries24 1 DAY INSTITUTIONS None ce63551ffaab CESNET-TimeSeries24 1 DAY INSTITUTION_SUBNETS None 9f7047902d66 CESNET-TimeSeries24 1 DAY IP_ADDRESSES_SAMPLE None 570b215d790d CESNET-TimeSeries24 1 DAY IP_ADDRESSES_FULL None <p>We encourage users to change default value for missing values, filler, transformer, sliding window step,  and batch sizes. However, users may not change the rest of the arguments. Usage of these benchmarks are following:</p> <pre><code>from cesnet_tszoo.benchmarks import load_benchmark\nfrom cesnet_tszoo.utils.enums import FillerType, TransformerType\nfrom sklearn.metrics import mean_squared_error\n\nbenchmark = load_benchmark(\"09de83e89e42\", \"../\")\ndataset = benchmark.get_initialized_dataset()\n\n# (optional) Set default value for missing data \ndataset.set_default_values(0)\n\n# (optional) Set filler for filling missing data \ndataset.apply_filler(FillerType.MEAN_FILLER)\n\n# (optional) Set transformer for data\ndataset.apply_transformer(TransformerType.MIN_MAX_SCALER)\n\n# (optional) Change sliding window setting\ndataset.set_sliding_window(sliding_window_size=744, sliding_window_prediction_size=24, sliding_window_step=1, set_shared_size=744)\n\n# or to update all at once which is usually faster\n# dataset.update_dataset_config_and_initialize(default_values=0, sliding_window_size=744, sliding_window_prediction_size=24, sliding_window_step=1, set_shared_size=744, \n#                                              fill_missing_with=FillerType.MEAN_FILLER, transform_with=TransformerType.MIN_MAX_SCALER)\n\n# Process with your own defined model\nmodel = Model()\nmodel.fit(\n    dataset.get_train_dataloader(), \n    dataset.get_val_dataloader(),\n)\n\n# Predict for time series which data are not in training\ny_pred, y_true = model.predict(\n    dataset.get_test_dataloader(), \n)\n\n# Evaluate predictions, for example, with RMSE\nrmse = mean_squared_error(y_true, y_pred)\nprint(f\"RMSE: {rmse::4f}\")\n</code></pre>"},{"location":"getting_started/","title":"Getting started","text":"<p>Note</p> <p>For a demonstration of usage for simple forecasting refer to Jupyter notebook <code>simple_forecasting</code></p>"},{"location":"getting_started/#code-snippets","title":"Code snippets","text":""},{"location":"getting_started/#download-a-dataset","title":"Download a dataset","text":"<pre><code>from cesnet_tszoo.datasets import CESNET_TimeSeries24\nfrom cesnet_tszoo.utils.enums import SourceType, AgreggationType, DatasetType\n\ndataset = CESNET_TimeSeries24.get_dataset(\"/some_directory/\", source_type=SourceType.INSTITUTIONS, aggregation=AgreggationType.AGG_1_DAY, dataset_type=DatasetType.TIME_BASED)\n</code></pre> <p>Alternatively you can use <code>load_benchmark</code>.</p> <pre><code>from cesnet_tszoo.benchmarks import load_benchmark\n\nbenchmark = load_benchmark(\"SOME_BUILT_IN_IDENTIFIER\", \"/some_directory/\")\ndataset = benchmark.get_dataset()\n</code></pre> <p>This will create following directories:</p> <ul> <li>\"/some_directory/tszoo\"<ul> <li>\"/some_directory/tszoo/annotations\"</li> <li>\"/some_directory/tszoo/benchmarks\"</li> <li>\"/some_directory/tszoo/configs\"</li> <li>\"/some_directory/tszoo/databases\"</li> </ul> </li> </ul> <p>Dataset will be downloaded to \"/some_directory/tszoo/databases/CESNET_TimeSeries24/\".</p>"},{"location":"getting_started/#enable-logging","title":"Enable logging","text":"<p><pre><code>import logging\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"[%(asctime)s][%(name)s][%(levelname)s] - %(message)s\")\n</code></pre> Set up logging to get more information from the package.</p>"},{"location":"getting_started/#initialize-dataset-to-create-train-validation-and-test-sets","title":"Initialize dataset to create train, validation, and test sets","text":""},{"location":"getting_started/#using-timebasedcesnetdataset-dataset","title":"Using <code>TimeBasedCesnetDataset</code> dataset","text":"<p><pre><code>from cesnet_tszoo.datasets import CESNET_TimeSeries24\nfrom cesnet_tszoo.utils.enums import SourceType, AgreggationType, DatasetType\nfrom cesnet_tszoo.configs import TimeBasedConfig\n\ndataset = CESNET_TimeSeries24.get_dataset(\"/some_directory/\", source_type=SourceType.INSTITUTIONS, aggregation=AgreggationType.AGG_1_DAY, dataset_type=DatasetType.TIME_BASED)\nconfig = TimeBasedConfig(\n    ts_ids=50, # number of randomly selected time series from dataset\n    train_time_period=range(0, 100), \n    val_time_period=range(100, 150), \n    test_time_period=range(150, 250), \n    features_to_take=[\"n_flows\", \"n_packets\"])\ndataset.set_dataset_config_and_initialize(config)\n</code></pre> Time-based datasets are configured with <code>TimeBasedConfig</code>. Can load data using:</p> <ul> <li> <p><code>get_train_dataloader</code>, <code>get_val_dataloader</code>, <code>get_test_dataloader</code>, <code>get_all_dataloader</code></p> </li> <li> <p><code>get_train_df</code>, <code>get_val_df</code>, <code>get_test_df</code>, <code>get_all_df</code></p> </li> <li> <p><code>get_train_numpy</code>, <code>get_val_numpy</code>, <code>get_test_numpy</code>, <code>get_all_numpy</code></p> </li> </ul>"},{"location":"getting_started/#using-disjointtimebasedcesnetdataset-dataset","title":"Using <code>DisjointTimeBasedCesnetDataset</code> dataset","text":"<p><pre><code>from cesnet_tszoo.datasets import CESNET_TimeSeries24\nfrom cesnet_tszoo.utils.enums import SourceType, AgreggationType, DatasetType\nfrom cesnet_tszoo.configs import DisjointTimeBasedConfig\n\ndataset = CESNET_TimeSeries24.get_dataset(\"/some_directory/\", source_type=SourceType.INSTITUTIONS, aggregation=AgreggationType.AGG_1_DAY, dataset_type=DatasetType.DISJOINT_TIME_BASED)\nconfig = TimeBasedConfig(\n    train_ts=50, # number of randomly selected time series from dataset that are not in val_ts and test_ts\n    val_ts=20, # number of randomly selected time series from dataset that are not in train_ts and test_ts\n    test_ts=10, # number of randomly selected time series from dataset that are not in train_ts and val_ts\n    train_time_period=range(0, 100), \n    val_time_period=range(100, 150), \n    test_time_period=range(150, 250), \n    features_to_take=[\"n_flows\", \"n_packets\"])\ndataset.set_dataset_config_and_initialize(config)\n</code></pre> Disjoint-time-based datasets are configured with <code>DisjointTimeBasedConfig</code>. Can load data using:</p> <ul> <li> <p><code>get_train_dataloader</code>, <code>get_val_dataloader</code>, <code>get_test_dataloader</code></p> </li> <li> <p><code>get_train_df</code>, <code>get_val_df</code>, <code>get_test_df</code></p> </li> <li> <p><code>get_train_numpy</code>, <code>get_val_numpy</code>, <code>get_test_numpy</code></p> </li> </ul>"},{"location":"getting_started/#using-seriesbasedcesnetdataset-dataset","title":"Using <code>SeriesBasedCesnetDataset</code> dataset","text":"<p><pre><code>from cesnet_tszoo.datasets import CESNET_TimeSeries24\nfrom cesnet_tszoo.utils.enums import SourceType, AgreggationType, DatasetType\nfrom cesnet_tszoo.configs import SeriesBasedConfig\n\ndataset = CESNET_TimeSeries24.get_dataset(\"/some_directory/\", source_type=SourceType.INSTITUTIONS, aggregation=AgreggationType.AGG_1_DAY, dataset_type=DatasetType.SERIES_BASED)\nconfig = SeriesBasedConfig(\n    time_period=range(0, 250), \n    train_ts=50, # number of randomly selected time series from dataset that are not in val_ts and test_ts\n    val_ts=20, # number of randomly selected time series from dataset that are not in train_ts and test_ts\n    test_ts=10, # number of randomly selected time series from dataset that are not in train_ts and val_ts\n    features_to_take=[\"n_flows\", \"n_packets\"])\ndataset.set_dataset_config_and_initialize(config)\n</code></pre> Series-based datasets are configured with <code>SeriesBasedConfig</code>. Can load data using:</p> <ul> <li> <p><code>get_train_dataloader</code>, <code>get_val_dataloader</code>, <code>get_test_dataloader</code>, <code>get_all_dataloader</code></p> </li> <li> <p><code>get_train_df</code>, <code>get_val_df</code>, <code>get_test_df</code>, <code>get_all_df</code></p> </li> <li> <p><code>get_train_numpy</code>, <code>get_val_numpy</code>, <code>get_test_numpy</code>, <code>get_all_numpy</code></p> </li> </ul>"},{"location":"getting_started/#using-load_benchmark","title":"Using <code>load_benchmark</code>","text":"<p><pre><code>from cesnet_tszoo.benchmarks import load_benchmark\n\nbenchmark = load_benchmark(\"SOME_BUILT_IN_IDENTIFIER\", \"/some_directory/\")\ndataset = benchmark.get_initialized_dataset()\n</code></pre> Whether loaded dataset is series-based or time-based depends on the benchmark. What can be loaded corresponds to previous datasets.</p>"},{"location":"handling_missing_data/","title":"Handling missing data","text":"<p>This tutorial will look at some configuration options used for handling missing data.</p> <p>Only time-based will be used, because all methods work almost the same way for other dataset types.</p> <p>Note</p> <p>For every configuration and more detailed examples refer to Jupyter notebook <code>handling_missing_data</code></p> <p>Relevant configuration values:</p> <ul> <li><code>default_values</code> - Default values for missing data, applied before fillers.</li> <li><code>fill_missing_with</code> - Defines how to fill missing values in the dataset. Can pass FillerType enum or custom Filler type.</li> </ul>"},{"location":"handling_missing_data/#default-values","title":"Default values","text":"<ul> <li>Default values are set to missing values before filler is used.</li> <li>You can change used default values later with <code>update_dataset_config_and_initialize</code> or <code>set_default_values</code>.</li> </ul> <pre><code>from cesnet_tszoo.configs import TimeBasedConfig\n\n# Default values are provided from used dataset.\nconfig = TimeBasedConfig(ts_ids=[1200], train_time_period=range(0, 30), test_time_period=range(30, 80), features_to_take=['n_flows', 'n_packets'],\n                         default_values=\"default\")\n\n# All missing values will be set as None\nconfig = TimeBasedConfig(ts_ids=[1200], train_time_period=range(0, 30), test_time_period=range(30, 80), features_to_take=['n_flows', 'n_packets'],\n                         default_values=None)     \n\n# All missing values will be set with 0\nconfig = TimeBasedConfig(ts_ids=[1200], train_time_period=range(0, 30), test_time_period=range(30, 80), features_to_take=['n_flows', 'n_packets'],\n                         default_values=0) \n\n# Using list to specify default values for each used feature\n# Position of values in list correspond to order of features in `features_to_take`.\nconfig = TimeBasedConfig(ts_ids=[1200], train_time_period=range(0, 30), test_time_period=range(30, 80), features_to_take=['n_flows', 'n_packets'],\n                         default_values=[1, None])       \n\n# Using dictionary with key as name for used feature and value as a default value for missing data\n# Dictionary must contain key and value for every feature in `features_to_take`.\nconfig = TimeBasedConfig(ts_ids=[1200], train_time_period=range(0, 30), test_time_period=range(30, 80), features_to_take=['n_flows', 'n_packets'],\n                         default_values={\"n_flows\" : 1, \"n_packets\": None})                                                                                       \n\n# Call on time-based dataset to use created config\ntime_based_dataset.set_dataset_config_and_initialize(config)\n\ntime_based_dataset.update_dataset_config_and_initialize(default_values=\"default\", workers=0)\n# Or\ntime_based_dataset.set_default_values(default_values=\"default\", workers=0)\n</code></pre>"},{"location":"handling_missing_data/#fillers","title":"Fillers","text":"<ul> <li>Fillers are implemented as classes.<ul> <li>You can create your own or use built-in one.</li> </ul> </li> <li>One filler per time series is created.</li> <li>Filler is applied after default values and usually overrides them.</li> <li>Fillers in time-based dataset can carry over values from train -&gt; val -&gt; test. Example is in Jupyter notebook.</li> <li>You can change used filler later with <code>update_dataset_config_and_initialize</code> or <code>apply_filler</code>.</li> </ul>"},{"location":"handling_missing_data/#built-in","title":"Built-in","text":"<p>To see all built-in fillers refer to <code>Fillers</code>.</p> <pre><code>from cesnet_tszoo.utils.enums import FillerType\nfrom cesnet_tszoo.configs import TimeBasedConfig\n\nconfig = TimeBasedConfig(ts_ids=[1200], train_time_period=range(0, 30), test_time_period=range(30, 80), features_to_take=['n_flows', 'n_packets'],\n                         default_values=None, fill_missing_with=FillerType.FORWARD_FILLER)                                                                                \n\n# Call on time-based dataset to use created config\ntime_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre> <p>Or later with:</p> <pre><code>time_based_dataset.update_dataset_config_and_initialize(fill_missing_with=FillerType.FORWARD_FILLER, workers=0)\n# Or\ntime_based_dataset.apply_filler(fill_missing_with=FillerType.FORWARD_FILLER, workers=0)\n</code></pre>"},{"location":"handling_missing_data/#custom","title":"Custom","text":"<p>You can create your own custom filler, which must derive from 'Filler' base class. </p> <p>To check Filler base class refer to <code>Filler</code></p> <pre><code>from cesnet_tszoo.utils.filler import Filler\nfrom cesnet_tszoo.configs import TimeBasedConfig\n\nclass CustomFiller(Filler):\n    def fill(self, batch_values: np.ndarray, existing_indices: np.ndarray, missing_indices: np.ndarray, **kwargs):\n        batch_values[missing_indices] = -1\n\nconfig = TimeBasedConfig(ts_ids=[1200], train_time_period=range(0, 30), test_time_period=range(30, 80), features_to_take=['n_flows', 'n_packets'],\n                         default_values=None, fill_missing_with=CustomFiller)                                                                            \n\n# Call on time-based dataset to use created config\ntime_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre> <p>Or later with:</p> <pre><code>time_based_dataset.update_dataset_config_and_initialize(fill_missing_with=CustomFiller, workers=0)\n# Or\ntime_based_dataset.apply_filler(CustomFiller, workers=0)\n</code></pre>"},{"location":"handling_missing_data/#changing-when-are-missing-values-handled","title":"Changing when are missing values handled","text":"<ul> <li>You can change when are <code>default_values</code> and filler applied with <code>preprocess_order</code> parameter</li> <li><code>default_values</code> are always applied before filler and filler considers values filled with <code>default_values</code>, as still missing</li> </ul> <pre><code>from cesnet_tszoo.utils.utils.enums import FillerType\nfrom cesnet_tszoo.configs import TimeBasedConfig\n\nconfig = TimeBasedConfig(ts_ids=[1200], train_time_period=range(0, 30), test_time_period=range(30, 80), features_to_take=['n_flows', 'n_packets'],\n                         default_values=None, fill_missing_with=FillerType.FORWARD_FILLER, preprocess_order=[\"handling_anomalies\", \"filling_gaps\", \"transforming\"])\n\n# Call on time-based dataset to use created config\ntime_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre> <p>Or later with:</p> <pre><code>time_based_dataset.update_dataset_config_and_initialize(preprocess_order=[\"filling_gaps\", \"handling_anomalies\", \"transforming\"], workers=0)\n# Or\ntime_based_dataset.set_preprocess_order(preprocess_order=[\"filling_gaps\", \"handling_anomalies\", \"transforming\"], workers=0)\n</code></pre>"},{"location":"installation/","title":"Installation","text":"<p>Install the package from pip with:</p> <pre><code>pip install cesnet-tszoo\n</code></pre> <p>or for editable install with:</p> <pre><code>pip install -e git+https://github.com/CESNET/cesnet-tszoo\n</code></pre>"},{"location":"installation/#requirements","title":"Requirements","text":"<p>The <code>cesnet-tszoo</code> package requires Python &gt;=3.10.</p>"},{"location":"installation/#dependencies","title":"Dependencies","text":"Name Version matplotlib numpy pandas scikit-learn tables &gt;=3.10.0 torch &gt;=1.10 tqdm plotly PyYAML requests packaging importlib nbformat &gt;=4.2.0"},{"location":"loading_data/","title":"Loading data","text":"<p>This tutorial will look at some configuration options used for loading data.</p> <p>Each dataset type will have its own part because of multiple differences of available configuration values.</p>"},{"location":"loading_data/#timebasedcesnetdataset-dataset","title":"<code>TimeBasedCesnetDataset</code> dataset","text":"<p>Note</p> <p>For every configuration and more detailed examples refer to Jupyter notebook <code>time_based_loading_data</code></p> <p>Relevant configuration values:</p> <ul> <li><code>sliding_window_size</code> - Number of times in one window.</li> <li><code>sliding_window_prediction_size</code> - Number of times to predict from <code>sliding_window_size</code>.</li> <li><code>sliding_window_step</code> - Number of times to move by after each window.</li> <li><code>set_shared_size</code> - How much times should time periods share. Order of sharing is training set &lt; validation set &lt; test set.</li> <li><code>train_batch_size</code>/<code>val_batch_size</code>/<code>test_batch_size</code>/<code>all_batch_size</code> - How many times for every time series will be in one batch (differs when sliding window is used).</li> <li><code>train_workers</code>/<code>val_workers</code>/<code>test_workers</code>/<code>all_workers</code> - Defines how many workers (processes) will be used for loading specific set.</li> </ul>"},{"location":"loading_data/#loading-data-with-dataloader","title":"Loading data with DataLoader","text":"<ul> <li>Load data using Pytorch Dataloader.</li> <li>Affected by workers and batch sizes.</li> <li>Last batch is never dropped (unless sliding window is used)</li> <li>Returned batch shape changes when used <code>time_format</code> is TimeFormat.DATETIME compare to other time formats. Check Jupyter notebook for details.</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import TimeFormat\nfrom cesnet_tszoo.configs import TimeBasedConfig\n\nconfig = TimeBasedConfig(ts_ids=54, train_time_period=0.5, val_time_period=0.3, test_time_period=0.2, features_to_take=\"all\", time_format=TimeFormat.ID_TIME,\n                         train_workers=0, val_workers=0, test_workers=0, all_workers=0, init_workers=0,\n                         train_batch_size=32, val_batch_size=64, test_batch_size=128, all_batch_size=128)\n\n# Call on time-based dataset to use created config -&gt; must be called before attempting to load data.\ntime_based_dataset.set_dataset_config_and_initialize(config)\n\n# train_time_period must be set\ndataloader = time_based_dataset.get_train_dataloader(workers=\"config\")\n\n# val_time_period must be set\ndataloader = time_based_dataset.get_val_dataloader(workers=\"config\")\n\n# test_time_period must be set\ndataloader = time_based_dataset.get_test_dataloader(workers=\"config\")\n\n# Always usable\ndataloader = time_based_dataset.get_all_dataloader(workers=\"config\")\n\n# Example of usage -&gt; loads all batches into list\nbatches = []\n\nfor batch in tqdm(dataloader):\n    # batch is a Numpy array of shape (ts_ids, batch_size, features_to_take + used ids)\n    batches.append(batch)\n</code></pre> <p>You can also change set batch sizes later with <code>update_dataset_config_and_initialize</code> or <code>set_batch_sizes</code>.</p> <pre><code>time_based_dataset.update_dataset_config_and_initialize(train_batch_size=33, val_batch_size=65, test_batch_size=\"config\", all_batch_size=\"config\")\n# Or\ntime_based_dataset.set_batch_sizes(train_batch_size=33, val_batch_size=65, test_batch_size=\"config\", all_batch_size=\"config\")\n</code></pre> <p>You can also change set workers later with <code>update_dataset_config_and_initialize</code> or <code>set_workers</code>.</p> <pre><code>time_based_dataset.update_dataset_config_and_initialize(train_workers=0, val_workers=0, test_workers=0, all_workers=0, init_workers=0)\n# Or\ntime_based_dataset.set_workers(train_workers=0, val_workers=0, test_workers=0, all_workers=0, init_workers=0)\n</code></pre> <p>You can also specify which time series to load from set.</p> <pre><code>from cesnet_tszoo.utils.enums import TimeFormat\nfrom cesnet_tszoo.configs import TimeBasedConfig\n\nconfig = TimeBasedConfig(ts_ids=54, train_time_period=[0,1,2,3,4], features_to_take=\"all\", time_format=TimeFormat.ID_TIME,\n                         train_workers=0, train_batch_size=32,)\n\ntime_based_dataset.set_dataset_config_and_initialize(config)\n\n# train_time_period must be set; load time series with id == 1\ndataloader = time_based_dataset.get_train_dataloader(ts_id=1, workers=\"config\")\n\n# Example of usage -&gt; loads all batches into list\nbatches = []\n\nfor batch in tqdm(dataloader):\n    # batch is a Numpy array of shape (1, batch_size, features_to_take + used ids)\n    batches.append(batch)\n</code></pre>"},{"location":"loading_data/#sliding-window","title":"Sliding window","text":"<ul> <li>When <code>sliding_window_prediction_size</code> is set then <code>sliding_window_size</code> must be set too if you want to use sliding window.</li> <li>Batch sizes are used for background caching.</li> <li>You can modify sliding window step size with <code>sliding_window_step</code></li> <li>You can use <code>set_shared_size</code> to set how many times time periods should share.<ul> <li><code>val_time_period</code> takes from <code>train_time_period</code></li> <li><code>test_time_period</code> takes from <code>val_time_period</code> or <code>train_time_period</code></li> </ul> </li> </ul> <pre><code>from cesnet_tszoo.utils.enums import TimeFormat\nfrom cesnet_tszoo.configs import TimeBasedConfig\n\nconfig = TimeBasedConfig(ts_ids=54, train_time_period=range(0, 1000), val_time_period=range(1000, 1500), test_time_period=range(1500, 2000), features_to_take=[\"n_flows\"], time_format=TimeFormat.ID_TIME,\n                         train_workers=0, val_workers=0, test_workers=0, all_workers=0, init_workers=0,\n                         train_batch_size=32, val_batch_size=64, test_batch_size=128, all_batch_size=128,\n                         sliding_window_size=22, sliding_window_prediction_size=2, sliding_window_step=2, set_shared_size=0.05)\n\ntime_based_dataset.set_dataset_config_and_initialize(config)\n\ndataloader = time_based_dataset.get_train_dataloader(workers=\"config\")\n\n# Example of usage -&gt; loads all batches into list\nbatches = []\n\nfor sliding_window, sliding_window_prediction in tqdm(dataloader):\n    # sliding_window is a Numpy array of shape (ts_ids, sliding_window_size, features_to_take + used ids)\n    # sliding_window_prediction is a Numpy array of shape (ts_ids, sliding_window_prediction_size, features_to_take + used ids)\n    batches.append((sliding_window, sliding_window_prediction))    \n</code></pre> <p>You can also change sliding window parameters later with <code>update_dataset_config_and_initialize</code> or <code>set_sliding_window</code>.</p> <pre><code>time_based_dataset.update_dataset_config_and_initialize(sliding_window_size=22, sliding_window_prediction_size=3, sliding_window_step=\"config\", set_shared_size=\"config\", workers=0)\n# Or\ntime_based_dataset.set_sliding_window(sliding_window_size=22, sliding_window_prediction_size=3, sliding_window_step=\"config\", set_shared_size=\"config\", workers=0)\n</code></pre>"},{"location":"loading_data/#loading-data-as-dataframe","title":"Loading data as Dataframe","text":"<ul> <li>Batch size has no effect.</li> <li>Sliding window has no effect.</li> <li>Returns every time series in <code>ts_ids</code> with sets specified time period.</li> <li>Data is returned as Pandas Dataframe.</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import TimeFormat\nfrom cesnet_tszoo.configs import TimeBasedConfig\n\nconfig = TimeBasedConfig(ts_ids=54, train_time_period=0.5, val_time_period=0.3, test_time_period=0.2, features_to_take=\"all\", time_format=TimeFormat.ID_TIME,\n                         train_workers=0, val_workers=0, test_workers=0, all_workers=0, init_workers=0)\n\ntime_based_dataset.set_dataset_config_and_initialize(config)\n\n# train_time_period must be set\ndf = time_based_dataset.get_train_df(as_single_dataframe=True, workers=\"config\") # loads time series from ts_ids of train_time_period into one Pandas Dataframe\ndfs = time_based_dataset.get_train_df(as_single_dataframe=False, workers=\"config\") # loads time series from ts_ids of train_time_period into seperate Pandas Dataframes\n\n# val_time_period must be set\ndf = time_based_dataset.get_val_df(as_single_dataframe=True, workers=\"config\") # loads time series from ts_ids of val_time_period into one Pandas Dataframe\ndfs = time_based_dataset.get_val_df(as_single_dataframe=False, workers=\"config\") # loads time series from ts_ids of val_time_period into seperate Pandas Dataframes\n\n# test_time_period must be set\ndf = time_based_dataset.get_test_df(as_single_dataframe=True, workers=\"config\") # loads time series from ts_ids of test_time_period into one Pandas Dataframe\ndfs = time_based_dataset.get_test_df(as_single_dataframe=False, workers=\"config\") # loads time series from ts_ids of test_time_period into seperate Pandas Dataframes\n\n# Always usable\ndf = time_based_dataset.get_all_df(as_single_dataframe=True, workers=\"config\") # loads time series from ts_ids of all time period into one Pandas Dataframe\ndfs = time_based_dataset.get_all_df(as_single_dataframe=False, workers=\"config\") # loads time series from ts_ids of all time period into seperate Pandas Dataframes\n</code></pre>"},{"location":"loading_data/#loading-data-as-singular-numpy-array","title":"Loading data as singular Numpy array","text":"<ul> <li>Batch size has no effect.</li> <li>Sliding window has no effect.</li> <li>Returns every time series in <code>ts_ids</code> with sets specified time period.</li> <li>Data is returned as one Numpy array.</li> <li>Follows similar rules to Dataloader batches, regarding shape (excluding sliding window parameters).</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import TimeFormat\nfrom cesnet_tszoo.configs import TimeBasedConfig\n\nconfig = TimeBasedConfig(ts_ids=54, train_time_period=0.5, val_time_period=0.3, test_time_period=0.2, features_to_take=\"all\", time_format=TimeFormat.ID_TIME,\n                         train_workers=0, val_workers=0, test_workers=0, all_workers=0, init_workers=0)\n\ntime_based_dataset.set_dataset_config_and_initialize(config)\n\n# train_time_period must be set\nnumpy_array = time_based_dataset.get_train_numpy(workers=\"config\")\n\n# val_time_period must be set\nnumpy_array = time_based_dataset.get_val_numpy(workers=\"config\")\n\n# test_time_period must be set\nnumpy_array = time_based_dataset.get_test_numpy(workers=\"config\")\n\n# Always usable\nnumpy_array = time_based_dataset.get_all_numpy(workers=\"config\")\n</code></pre>"},{"location":"loading_data/#disjointtimebasedcesnetdataset-dataset","title":"<code>DisjointTimeBasedCesnetDataset</code> dataset","text":"<p>Note</p> <p>For every configuration and more detailed examples refer to Jupyter notebook <code>disjoint_time_based_loading_data</code></p> <p>Relevant configuration values:</p> <ul> <li><code>sliding_window_size</code> - Number of times in one window.</li> <li><code>sliding_window_prediction_size</code> - Number of times to predict from <code>sliding_window_size</code>.</li> <li><code>sliding_window_step</code> - Number of times to move by after each window.</li> <li><code>set_shared_size</code> - How much times should time periods share. Order of sharing is training set &lt; validation set &lt; test set.</li> <li><code>train_batch_size</code>/<code>val_batch_size</code>/<code>test_batch_size</code> - How many times for every time series will be in one batch (differs when sliding window is used).</li> <li><code>train_workers</code>/<code>val_workers</code>/<code>test_workers</code> - Defines how many workers (processes) will be used for loading specific set.</li> </ul>"},{"location":"loading_data/#loading-data-with-dataloader_1","title":"Loading data with DataLoader","text":"<ul> <li>Load data using Pytorch Dataloader.</li> <li>Affected by workers and batch sizes.</li> <li>Last batch is never dropped (unless sliding window is used)</li> <li>Returned batch shape changes when used <code>time_format</code> is TimeFormat.DATETIME compare to other time formats. Check Jupyter notebook for details.</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import TimeFormat\nfrom cesnet_tszoo.configs import DisjointTimeBasedConfig\n\nconfig = DisjointTimeBasedConfig(train_ts=0.5, val_ts=0.2, test_ts=0.1, train_time_period=0.5, val_time_period=0.3, test_time_period=0.2, features_to_take=\"all\", time_format=TimeFormat.ID_TIME,\n                         train_workers=0, val_workers=0, test_workers=0, init_workers=0,\n                         train_batch_size=32, val_batch_size=64, test_batch_size=128)\n\n# Call on disjoint-time-based dataset to use created config -&gt; must be called before attempting to load data.\ndisjoint_dataset.set_dataset_config_and_initialize(config)\n\n# train_ts and train_time_period must be set\ndataloader = disjoint_dataset.get_train_dataloader(workers=\"config\")\n\n# val_ts and val_time_period must be set\ndataloader = disjoint_dataset.get_val_dataloader(workers=\"config\")\n\n# test_ts and test_time_period must be set\ndataloader = disjoint_dataset.get_test_dataloader(workers=\"config\")\n\n# Example of usage -&gt; loads all batches into list\nbatches = []\n\nfor batch in tqdm(dataloader):\n    # batch is a Numpy array of shape (train_ts/val_ts/test_ts, batch_size, features_to_take + used ids)\n    batches.append(batch)\n</code></pre> <p>You can also change set batch sizes later with <code>update_dataset_config_and_initialize</code> or <code>set_batch_sizes</code>.</p> <pre><code>disjoint_dataset.update_dataset_config_and_initialize(train_batch_size=33, val_batch_size=65, test_batch_size=\"config\")\n# Or\ndisjoint_dataset.set_batch_sizes(train_batch_size=33, val_batch_size=65, test_batch_size=\"config\")\n</code></pre> <p>You can also change set workers later with <code>update_dataset_config_and_initialize</code> or <code>set_workers</code>.</p> <pre><code>disjoint_dataset.update_dataset_config_and_initialize(train_workers=0, val_workers=0, test_workers=0, init_workers=0)\n# Or\ndisjoint_dataset.set_workers(train_workers=0, val_workers=0, test_workers=0, init_workers=0)\n</code></pre> <p>You can also specify which time series to load from set.</p> <pre><code>from cesnet_tszoo.utils.enums import TimeFormat\nfrom cesnet_tszoo.configs import DisjointTimeBasedConfig\n\nconfig = DisjointTimeBasedConfig(train_ts=[177, 176, 319, 267], val_ts=None, test_ts=None, train_time_period=0.5, features_to_take=\"all\", time_format=TimeFormat.ID_TIME,\n                         train_workers=0, val_workers=0, test_workers=0, init_workers=0,\n                         train_batch_size=32, val_batch_size=64, test_batch_size=128)\n\ndisjoint_dataset.set_dataset_config_and_initialize(config)\n\n# train_time_period must be set; load time series with id == 177\ndataloader = disjoint_dataset.get_train_dataloader(ts_id=177, workers=\"config\")\n\n# Example of usage -&gt; loads all batches into list\nbatches = []\n\nfor batch in tqdm(dataloader):\n    # batch is a Numpy array of shape (1, batch_size, features_to_take + used ids)\n    batches.append(batch)\n</code></pre>"},{"location":"loading_data/#sliding-window_1","title":"Sliding window","text":"<ul> <li>When <code>sliding_window_prediction_size</code> is set then <code>sliding_window_size</code> must be set too if you want to use sliding window.</li> <li>Batch sizes are used for background caching.</li> <li>You can modify sliding window step size with <code>sliding_window_step</code></li> <li>You can use <code>set_shared_size</code> to set how many times time periods should share.<ul> <li><code>val_time_period</code> takes from <code>train_time_period</code></li> <li><code>test_time_period</code> takes from <code>val_time_period</code> or <code>train_time_period</code></li> </ul> </li> </ul> <pre><code>from cesnet_tszoo.utils.enums import TimeFormat\nfrom cesnet_tszoo.configs import DisjointTimeBasedConfig\n\nconfig = DisjointTimeBasedConfig(train_ts=0.5, val_ts=0.2, test_ts=0.1, train_time_period=range(0, 1000), val_time_period=range(1000, 1500), test_time_period=range(1500, 2000), \n                         features_to_take=[\"n_flows\"], time_format=TimeFormat.ID_TIME,\n                         train_workers=0, val_workers=0, test_workers=0, init_workers=0,\n                         train_batch_size=32, val_batch_size=64, test_batch_size=128,\n                         sliding_window_size=22, sliding_window_prediction_size=2, sliding_window_step=2, set_shared_size=0.05)\n\ndisjoint_dataset.set_dataset_config_and_initialize(config)\n\ndataloader = disjoint_dataset.get_train_dataloader(workers=\"config\")\n\n# Example of usage -&gt; loads all batches into list\nbatches = []\n\nfor sliding_window, sliding_window_prediction in tqdm(dataloader):\n    # sliding_window is a Numpy array of shape (train_ts/val_ts/test_ts, sliding_window_size, features_to_take + used ids)\n    # sliding_window_prediction is a Numpy array of shape (train_ts/val_ts/test_ts, sliding_window_prediction_size, features_to_take + used ids)\n    batches.append((sliding_window, sliding_window_prediction))    \n</code></pre> <p>You can also change sliding window parameters later with <code>update_dataset_config_and_initialize</code> or <code>set_sliding_window</code>.</p> <pre><code>disjoint_dataset.update_dataset_config_and_initialize(sliding_window_size=22, sliding_window_prediction_size=3, sliding_window_step=\"config\", set_shared_size=\"config\", workers=0)\n# Or\ndisjoint_dataset.set_sliding_window(sliding_window_size=22, sliding_window_prediction_size=3, sliding_window_step=\"config\", set_shared_size=\"config\", workers=0)\n</code></pre>"},{"location":"loading_data/#loading-data-as-dataframe_1","title":"Loading data as Dataframe","text":"<ul> <li>Batch size has no effect.</li> <li>Sliding window has no effect.</li> <li>Returns every time series in <code>train_ts</code>/<code>val_ts</code>/<code>test_ts</code> with sets specified time period.</li> <li>Data is returned as Pandas Dataframe.</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import TimeFormat\nfrom cesnet_tszoo.configs import DisjointTimeBasedConfig\n\nconfig = DisjointTimeBasedConfig(train_ts=0.5, val_ts=0.2, test_ts=0.1, train_time_period=0.5, val_time_period=0.3, test_time_period=0.2, features_to_take=\"all\", time_format=TimeFormat.ID_TIME,\n                         train_workers=0, val_workers=0, test_workers=0, init_workers=0)\n\ndisjoint_dataset.set_dataset_config_and_initialize(config)\n\n# train_ts and train_time_period must be set\ndf = disjoint_dataset.get_train_df(as_single_dataframe=True, workers=\"config\") # loads time series from train_ts of train_time_period into one Pandas Dataframe\ndfs = disjoint_dataset.get_train_df(as_single_dataframe=False, workers=\"config\") # loads time series from train_ts of train_time_period into seperate Pandas Dataframes\n\n# val_ts and val_time_period must be set\ndf = disjoint_dataset.get_val_df(as_single_dataframe=True, workers=\"config\") # loads time series from val_ts of val_time_period into one Pandas Dataframe\ndfs = disjoint_dataset.get_val_df(as_single_dataframe=False, workers=\"config\") # loads time series from val_ts of val_time_period into seperate Pandas Dataframes\n\n# test_ts and test_time_period must be set\ndf = disjoint_dataset.get_test_df(as_single_dataframe=True, workers=\"config\") # loads time series from test_ts of test_time_period into one Pandas Dataframe\ndfs = disjoint_dataset.get_test_df(as_single_dataframe=False, workers=\"config\") # loads time series from test_ts of test_time_period into seperate Pandas Dataframes\n</code></pre>"},{"location":"loading_data/#loading-data-as-singular-numpy-array_1","title":"Loading data as singular Numpy array","text":"<ul> <li>Batch size has no effect.</li> <li>Sliding window has no effect.</li> <li>Returns every time series in <code>train_ts</code>/<code>val_ts</code>/<code>test_ts</code> with sets specified time period.</li> <li>Data is returned as one Numpy array.</li> <li>Follows similar rules to Dataloader batches, regarding shape (excluding sliding window parameters).</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import TimeFormat\nfrom cesnet_tszoo.configs import DisjointTimeBasedConfig\n\nconfig = DisjointTimeBasedConfig(train_ts=0.5, val_ts=0.2, test_ts=0.1, train_time_period=0.5, val_time_period=0.3, test_time_period=0.2, features_to_take=\"all\", time_format=TimeFormat.ID_TIME,\n                         train_workers=0, val_workers=0, test_workers=0, init_workers=0)\n\ndisjoint_dataset.set_dataset_config_and_initialize(config)\n\n# train_ts and train_time_period must be set\nnumpy_array = disjoint_dataset.get_train_numpy(workers=\"config\")\n\n# val_ts and val_time_period must be set\nnumpy_array = disjoint_dataset.get_val_numpy(workers=\"config\")\n\n# test_ts and test_time_period must be set\nnumpy_array = disjoint_dataset.get_test_numpy(workers=\"config\")\n</code></pre>"},{"location":"loading_data/#seriesbasedcesnetdataset-dataset","title":"<code>SeriesBasedCesnetDataset</code> dataset","text":"<p>Note</p> <p>For every configuration and more detailed examples refer to Jupyter notebook <code>series_based_loading_data</code></p> <p>Relevant configuration values:</p> <ul> <li><code>train_batch_size</code>/<code>val_batch_size</code>/<code>test_batch_size</code>/<code>all_batch_size</code> - How many time series will be in one batch.</li> <li><code>train_workers</code>/<code>val_workers</code>/<code>test_workers</code>/<code>all_workers</code> - Defines how many workers (processes) will be used for loading specific set.</li> <li><code>train_dataloader_order</code> - Affects order of time series in loaded batch.</li> <li><code>random_state</code> - When set, batches will be same when using random order type for <code>train_dataloader_order</code>. </li> </ul>"},{"location":"loading_data/#loading-data-with-dataloader_2","title":"Loading data with DataLoader","text":"<ul> <li>Load data using Pytorch Dataloader.</li> <li>Affected by workers and batch sizes.</li> <li>Last batch is never dropped.</li> <li>Returned batch shape changes when used <code>time_format</code> is TimeFormat.DATETIME compare to other time formats. Check Jupyter notebook for details.</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import TimeFormat\nfrom cesnet_tszoo.configs import SeriesBasedConfig\n\nconfig = SeriesBasedConfig(time_period=0.5, train_ts=54, val_ts=25, test_ts=10, features_to_take=[\"n_flows\"], time_format=TimeFormat.ID_TIME,\n                           train_workers=0, val_workers=0, test_workers=0, all_workers=0, init_workers=0,\n                           train_batch_size=32, val_batch_size=64, test_batch_size=128, all_batch_size=128)\n\n# Call on series-based dataset to use created config -&gt; must be called before attempting to load data.\nseries_based_dataset.set_dataset_config_and_initialize(config)\n\n# train_ts must be set\ndataloader = series_based_dataset.get_train_dataloader(workers=\"config\")\n\n# val_ts must be set\ndataloader = series_based_dataset.get_val_dataloader(workers=\"config\")\n\n# test_ts must be set\ndataloader = series_based_dataset.get_test_dataloader(workers=\"config\")\n\n# Always usable\ndataloader = series_based_dataset.get_all_dataloader(workers=\"config\")\n\n# Example of usage -&gt; loads all batches into list\nbatches = []\n\nfor batch in tqdm(dataloader):\n    # batch is a Numpy array of shape (batch_size, time_period, features_to_take + used ids)\n    batches.append(batch)\n</code></pre> <p>You can also change set batch sizes later with <code>update_dataset_config_and_initialize</code> or <code>set_batch_sizes</code>.</p> <pre><code>series_based_dataset.update_dataset_config_and_initialize(train_batch_size=33, val_batch_size=65, test_batch_size=\"config\", all_batch_size=\"config\")\n# Or\nseries_based_dataset.set_batch_sizes(train_batch_size=33, val_batch_size=65, test_batch_size=\"config\", all_batch_size=\"config\")\n</code></pre> <p>You can also change set workers later with <code>update_dataset_config_and_initialize</code> or <code>set_workers</code>.</p> <pre><code>series_based_dataset.update_dataset_config_and_initialize(train_workers=0, val_workers=0, test_workers=0, all_workers=0, init_workers=0)\n# Or\nseries_based_dataset.set_workers(train_workers=0, val_workers=0, test_workers=0, all_workers=0, init_workers=0)\n</code></pre> <p>You can also specify which time series to load from set.</p> <pre><code>from cesnet_tszoo.utils.enums import TimeFormat\nfrom cesnet_tszoo.configs import SeriesBasedConfig\n\nconfig = SeriesBasedConfig(time_period=0.5, train_ts=[177, 176, 319, 267], features_to_take=[\"n_flows\"], time_format=TimeFormat.ID_TIME,\n                           train_workers=0, train_batch_size=32)\n\nseries_based_dataset.set_dataset_config_and_initialize(config)\n\n# train_ts must be set; load time series with id == 176\ndataloader = series_based_dataset.get_train_dataloader(ts_id=176, workers=\"config\")\n\n# Example of usage -&gt; loads one whole time series\nbatches = []\n\nfor batch in tqdm(dataloader):\n    # batch is a Numpy array of shape (1, time_period, features_to_take + used ids)\n    batches.append(batch)\n</code></pre>"},{"location":"loading_data/#loading-data-as-dataframe_2","title":"Loading data as Dataframe","text":"<ul> <li>Batch size has no effect.</li> <li>Returns every time series in set with specified <code>time_period</code>.</li> <li>Data is returned as Pandas Dataframe.</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import TimeFormat\nfrom cesnet_tszoo.configs import SeriesBasedConfig\n\nconfig = SeriesBasedConfig(time_period=0.5, train_ts=54, val_ts=25, test_ts=10, features_to_take=\"all\", time_format=TimeFormat.ID_TIME,\n                           train_workers=0, val_workers=0, test_workers=0, all_workers=0, init_workers=0)\n\nseries_based_dataset.set_dataset_config_and_initialize(config)\n\n# train_ts must be set\ndf = series_based_dataset.get_train_df(as_single_dataframe=True, workers=\"config\") # loads every time series from train_ts with time_period into one Pandas Dataframe\ndfs = series_based_dataset.get_train_df(as_single_dataframe=False, workers=\"config\") # loads every time series from train_ts with time_period into seperate Pandas Dataframe\n\n# val_ts must be set\ndf = series_based_dataset.get_val_df(as_single_dataframe=True, workers=\"config\") # loads every time series with from val_ts time_period into one Pandas Dataframe\ndfs = series_based_dataset.get_val_df(as_single_dataframe=False, workers=\"config\") # loads every time series from val_ts with time_period into seperate Pandas Dataframe\n\n# test_ts must be set\ndf = series_based_dataset.get_test_df(as_single_dataframe=True, workers=\"config\") # loads every time series from test_ts with time_period into one Pandas Dataframe\ndfs = series_based_dataset.get_test_df(as_single_dataframe=False, workers=\"config\") # loads every time series from test_ts with time_period into seperate Pandas Dataframe\n\n# Always usable\ndf = series_based_dataset.get_all_df(as_single_dataframe=True, workers=\"config\") # loads every time series from all set with time_period into one Pandas Dataframe\ndfs = series_based_dataset.get_all_df(as_single_dataframe=False, workers=\"config\") # loads every time series from all set with time_period into seperate Pandas Dataframe\n</code></pre>"},{"location":"loading_data/#loading-data-as-singular-numpy-array_2","title":"Loading data as singular Numpy array","text":"<ul> <li>Batch size has no effect.</li> <li>Returns every time series in set with specified <code>time_period</code>.</li> <li>Data is returned as one Numpy array.</li> <li>Follows similar rules to Dataloader batches, regarding shape.</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import TimeFormat\nfrom cesnet_tszoo.configs import SeriesBasedConfig\n\nconfig = SeriesBasedConfig(time_period=0.5, train_ts=54, val_ts=25, test_ts=10, features_to_take=\"all\", time_format=TimeFormat.ID_TIME,\n                           train_workers=0, val_workers=0, test_workers=0, all_workers=0, init_workers=0)\n\nseries_based_dataset.set_dataset_config_and_initialize(config)\n\n# train_ts must be set\nnumpy_array = series_based_dataset.get_train_numpy(workers=\"config\")\n\n# val_ts must be set\nnumpy_array = series_based_dataset.get_val_numpy(workers=\"config\")\n\n# test_ts must be set\nnumpy_array = series_based_dataset.get_test_numpy(workers=\"config\")\n\n# Always usable\nnumpy_array = series_based_dataset.get_all_numpy(workers=\"config\")\n</code></pre>"},{"location":"reference_anomaly_handlers/","title":"Anomaly handlers","text":""},{"location":"reference_anomaly_handlers/#cesnet_tszoo.utils.anomaly_handler.anomaly_handler","title":"cesnet_tszoo.utils.anomaly_handler.anomaly_handler","text":""},{"location":"reference_anomaly_handlers/#cesnet_tszoo.utils.anomaly_handler.anomaly_handler.AnomalyHandler","title":"AnomalyHandler","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for anomaly handlers, used for handling anomalies in the data.</p> <p>This class serves as the foundation for creating custom anomaly handlers. To implement a custom anomaly handler, this class is recommended to be subclassed and extended.</p> <p>Example:</p> <pre><code>import numpy as np\n\nclass InterquartileRange(AnomalyHandler):\n\n    def __init__(self):\n        self.lower_bound = None\n        self.upper_bound = None\n        self.iqr = None\n\n    def fit(self, data: np.ndarray) -&gt; None:\n        q25, q75 = np.percentile(data, [25, 75], axis=0)\n        self.iqr = q75 - q25\n\n        self.lower_bound = q25 - 1.5 * self.iqr\n        self.upper_bound = q75 + 1.5 * self.iqr\n\n    def transform_anomalies(self, data: np.ndarray) -&gt; np.ndarray:\n        mask_lower_outliers = data &lt; self.lower_bound\n        mask_upper_outliers = data &gt; self.upper_bound\n\n        data[mask_lower_outliers] = np.take(self.lower_bound, np.where(mask_lower_outliers)[1])\n        data[mask_upper_outliers] = np.take(self.upper_bound, np.where(mask_upper_outliers)[1])\n</code></pre> Source code in <code>cesnet_tszoo\\utils\\anomaly_handler\\anomaly_handler.py</code> <pre><code>class AnomalyHandler(ABC):\n    \"\"\"\n    Base class for anomaly handlers, used for handling anomalies in the data.\n\n    This class serves as the foundation for creating custom anomaly handlers. To implement a custom anomaly handler, this class is recommended to be subclassed and extended.\n\n    Example:\n\n        import numpy as np\n\n        class InterquartileRange(AnomalyHandler):\n\n            def __init__(self):\n                self.lower_bound = None\n                self.upper_bound = None\n                self.iqr = None\n\n            def fit(self, data: np.ndarray) -&gt; None:\n                q25, q75 = np.percentile(data, [25, 75], axis=0)\n                self.iqr = q75 - q25\n\n                self.lower_bound = q25 - 1.5 * self.iqr\n                self.upper_bound = q75 + 1.5 * self.iqr\n\n            def transform_anomalies(self, data: np.ndarray) -&gt; np.ndarray:\n                mask_lower_outliers = data &lt; self.lower_bound\n                mask_upper_outliers = data &gt; self.upper_bound\n\n                data[mask_lower_outliers] = np.take(self.lower_bound, np.where(mask_lower_outliers)[1])\n                data[mask_upper_outliers] = np.take(self.upper_bound, np.where(mask_upper_outliers)[1])\n\n    \"\"\"\n\n    @abstractmethod\n    def fit(self, data: np.ndarray) -&gt; None:\n        \"\"\"\n        Sets the anomaly handler values for a given time series part.\n\n        This method must be implemented.\n\n        Parameters:\n            data: A numpy array representing data for a single time series with shape `(times, features)` excluding any identifiers.  \n        \"\"\"\n        ...\n\n    @abstractmethod\n    def transform_anomalies(self, data: np.ndarray):\n        \"\"\"\n        Transforms anomalies the input data for a given time series part.\n\n        This method must be implemented.\n        Anomaly transformation is done in-place.\n\n        Parameters:\n            data: A numpy array representing data for a single time series with shape `(times, features)` excluding any identifiers.            \n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference_anomaly_handlers/#cesnet_tszoo.utils.anomaly_handler.anomaly_handler.AnomalyHandler.fit","title":"fit  <code>abstractmethod</code>","text":"<pre><code>fit(data: ndarray) -&gt; None\n</code></pre> <p>Sets the anomaly handler values for a given time series part.</p> <p>This method must be implemented.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>A numpy array representing data for a single time series with shape <code>(times, features)</code> excluding any identifiers.</p> required Source code in <code>cesnet_tszoo\\utils\\anomaly_handler\\anomaly_handler.py</code> <pre><code>@abstractmethod\ndef fit(self, data: np.ndarray) -&gt; None:\n    \"\"\"\n    Sets the anomaly handler values for a given time series part.\n\n    This method must be implemented.\n\n    Parameters:\n        data: A numpy array representing data for a single time series with shape `(times, features)` excluding any identifiers.  \n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference_anomaly_handlers/#cesnet_tszoo.utils.anomaly_handler.anomaly_handler.AnomalyHandler.transform_anomalies","title":"transform_anomalies  <code>abstractmethod</code>","text":"<pre><code>transform_anomalies(data: ndarray)\n</code></pre> <p>Transforms anomalies the input data for a given time series part.</p> <p>This method must be implemented. Anomaly transformation is done in-place.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>A numpy array representing data for a single time series with shape <code>(times, features)</code> excluding any identifiers.</p> required Source code in <code>cesnet_tszoo\\utils\\anomaly_handler\\anomaly_handler.py</code> <pre><code>@abstractmethod\ndef transform_anomalies(self, data: np.ndarray):\n    \"\"\"\n    Transforms anomalies the input data for a given time series part.\n\n    This method must be implemented.\n    Anomaly transformation is done in-place.\n\n    Parameters:\n        data: A numpy array representing data for a single time series with shape `(times, features)` excluding any identifiers.            \n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference_anomaly_handlers/#cesnet_tszoo.utils.anomaly_handler.anomaly_handler.ZScore","title":"ZScore","text":"<p>               Bases: <code>AnomalyHandler</code></p> <p>Fitting calculates mean and standard deviation of values used for fitting.  Calculated mean and standard deviation calculated when fitting will be used for calculating z-score for every value and those with z-score over or below threshold (3) will be clipped to the threshold value.</p> <p>Corresponds to enum <code>AnomalyHandlerType.Z_SCORE</code> or literal <code>z-score</code>.</p> Source code in <code>cesnet_tszoo\\utils\\anomaly_handler\\anomaly_handler.py</code> <pre><code>class ZScore(AnomalyHandler):\n    \"\"\"\n    Fitting calculates mean and standard deviation of values used for fitting. \n    Calculated mean and standard deviation calculated when fitting will be used for calculating z-score for every value and those with z-score over or below threshold (3) will be clipped to the threshold value.\n\n    Corresponds to enum [`AnomalyHandlerType.Z_SCORE`][cesnet_tszoo.utils.enums.AnomalyHandlerType] or literal `z-score`.\n    \"\"\"\n\n    def __init__(self):\n        self.mean = None\n        self.std = None\n        self.threshold = 3\n\n    def fit(self, data: np.ndarray) -&gt; None:\n        warnings.filterwarnings(\"ignore\")\n        self.mean = np.nanmean(data, axis=0)\n        self.std = np.nanstd(data, axis=0)\n        warnings.filterwarnings(\"always\")\n\n    def transform_anomalies(self, data: np.ndarray):\n        temp = data - self.mean\n        z_score = np.divide(temp, self.std, out=np.zeros_like(temp, dtype=float), where=self.std != 0)\n        mask_outliers = np.abs(z_score) &gt; self.threshold\n\n        clipped_values = self.mean + np.sign(z_score) * self.threshold * self.std\n\n        data[mask_outliers] = clipped_values[mask_outliers]\n</code></pre>"},{"location":"reference_anomaly_handlers/#cesnet_tszoo.utils.anomaly_handler.anomaly_handler.InterquartileRange","title":"InterquartileRange","text":"<p>               Bases: <code>AnomalyHandler</code></p> <p>Fitting calculates 25th percentile, 75th percentile from the values used for fitting. From those percentiles the interquartile range, lower and upper bound will be calculated. Lower and upper bounds will then be used for detecting anomalies (values below lower bound or above upper bound). Anomalies will then be clipped to closest bound.</p> <p>Corresponds to enum <code>AnomalyHandlerType.INTERQUARTILE_RANGE</code> or literal <code>interquartile_range</code>.</p> Source code in <code>cesnet_tszoo\\utils\\anomaly_handler\\anomaly_handler.py</code> <pre><code>class InterquartileRange(AnomalyHandler):\n    \"\"\"\n    Fitting calculates 25th percentile, 75th percentile from the values used for fitting. From those percentiles the interquartile range, lower and upper bound will be calculated.\n    Lower and upper bounds will then be used for detecting anomalies (values below lower bound or above upper bound). Anomalies will then be clipped to closest bound.\n\n    Corresponds to enum [`AnomalyHandlerType.INTERQUARTILE_RANGE`][cesnet_tszoo.utils.enums.AnomalyHandlerType] or literal `interquartile_range`.\n    \"\"\"\n\n    def __init__(self):\n        self.lower_bound = None\n        self.upper_bound = None\n        self.iqr = None\n\n    def fit(self, data: np.ndarray) -&gt; None:\n        q25, q75 = np.percentile(data, [25, 75], axis=0)\n        self.iqr = q75 - q25\n\n        self.lower_bound = q25 - 1.5 * self.iqr\n        self.upper_bound = q75 + 1.5 * self.iqr\n\n    def transform_anomalies(self, data: np.ndarray):\n        mask_lower_outliers = data &lt; self.lower_bound\n        mask_upper_outliers = data &gt; self.upper_bound\n\n        data[mask_lower_outliers] = np.take(self.lower_bound, np.where(mask_lower_outliers)[1])\n        data[mask_upper_outliers] = np.take(self.upper_bound, np.where(mask_upper_outliers)[1])\n</code></pre>"},{"location":"reference_anomaly_handlers/#cesnet_tszoo.utils.anomaly_handler.anomaly_handler.NoAnomalyHandler","title":"NoAnomalyHandler","text":"<p>               Bases: <code>AnomalyHandler</code></p> <p>Does nothing. </p> <p>Corresponds to enum <code>AnomalyHandlerType.NO_ANOMALY_HANDLER</code> or literal <code>no_anomaly_handler</code>.</p> Source code in <code>cesnet_tszoo\\utils\\anomaly_handler\\anomaly_handler.py</code> <pre><code>class NoAnomalyHandler(AnomalyHandler):\n    \"\"\"\n    Does nothing. \n\n    Corresponds to enum [`AnomalyHandlerType.NO_ANOMALY_HANDLER`][cesnet_tszoo.utils.enums.AnomalyHandlerType] or literal `no_anomaly_handler`.\n    \"\"\"\n\n    def fit(self, data: np.ndarray) -&gt; None:\n        ...\n\n    def transform_anomalies(self, data: np.ndarray):\n        ...\n</code></pre>"},{"location":"reference_benchmarks/","title":"Benchmark","text":""},{"location":"reference_benchmarks/#cesnet_tszoo.benchmarks","title":"cesnet_tszoo.benchmarks","text":""},{"location":"reference_benchmarks/#cesnet_tszoo.benchmarks.Benchmark","title":"Benchmark","text":"<p>Used as wrapper for imported <code>dataset</code>, <code>config</code>, <code>annotations</code> and <code>related_results</code>.</p> <p>Intended usage:</p> <p>For time-based:</p> <p>When using <code>TimeBasedCesnetDataset</code> (<code>dataset_type</code> = <code>DatasetType.TIME_BASED</code>):</p> <ol> <li>Create an instance of the dataset with the desired data root by calling <code>get_dataset</code>. This will download the dataset if it has not been previously downloaded and return instance of dataset.</li> <li>Create an instance of <code>TimeBasedConfig</code> and set it using <code>set_dataset_config_and_initialize</code>.     This initializes the dataset, including data splitting (train/validation/test), fitting transformers (if needed), selecting features, and more. This is cached for later use.</li> <li>Use <code>get_train_dataloader</code>/<code>get_train_df</code>/<code>get_train_numpy</code> to get training data for chosen model.</li> <li>Validate the model and perform the hyperparameter optimalization on <code>get_val_dataloader</code>/<code>get_val_df</code>/<code>get_val_numpy</code>.</li> <li>Evaluate the model on <code>get_test_dataloader</code>/<code>get_test_df</code>/<code>get_test_numpy</code>.     </li> </ol> <p>When using <code>SeriesBasedCesnetDataset</code> (<code>dataset_type</code> = <code>DatasetType.SERIES_BASED</code>):</p> <ol> <li>Create an instance of the dataset with the desired data root by calling <code>get_dataset</code>. This will download the dataset if it has not been previously downloaded and return instance of dataset.</li> <li>Create an instance of <code>SeriesBasedConfig</code> and set it using <code>set_dataset_config_and_initialize</code>.     This initializes the dataset, including data splitting (train/validation/test), fitting transformers (if needed), selecting features, and more. This is cached for later use.</li> <li>Use <code>get_train_dataloader</code>/<code>get_train_df</code>/<code>get_train_numpy</code> to get training data for chosen model.</li> <li>Validate the model and perform the hyperparameter optimalization on <code>get_val_dataloader</code>/<code>get_val_df</code>/<code>get_val_numpy</code>.</li> <li>Evaluate the model on <code>get_test_dataloader</code>/<code>get_test_df</code>/<code>get_test_numpy</code>.   </li> </ol> <p>When using <code>DisjointTimeBasedCesnetDataset</code> (<code>dataset_type</code> = <code>DatasetType.DISJOINT_TIME_BASED</code>):</p> <ol> <li>Create an instance of the dataset with the desired data root by calling <code>get_dataset</code>. This will download the dataset if it has not been previously downloaded and return instance of dataset.</li> <li>Create an instance of <code>DisjointTimeBasedConfig</code> and set it using <code>set_dataset_config_and_initialize</code>.     This initializes the dataset, including data splitting (train/validation/test), fitting transformers (if needed), selecting features, and more. This is cached for later use.</li> <li>Use <code>get_train_dataloader</code>/<code>get_train_df</code>/<code>get_train_numpy</code> to get training data for chosen model.</li> <li>Validate the model and perform the hyperparameter optimalization on <code>get_val_dataloader</code>/<code>get_val_df</code>/<code>get_val_numpy</code>.</li> <li>Evaluate the model on <code>get_test_dataloader</code>/<code>get_test_df</code>/<code>get_test_numpy</code>.      </li> </ol> <p>You can create custom time-based benchmarks with <code>save_benchmark</code>, series-based benchmarks with <code>save_benchmark</code> or disjoint-time-based with <code>save_benchmark</code>. They will be saved to <code>\"data_root\"/tszoo/benchmarks/</code> directory, where <code>data_root</code> was set when you created instance of dataset.</p> Source code in <code>cesnet_tszoo\\benchmarks.py</code> <pre><code>class Benchmark:\n    \"\"\"\n    Used as wrapper for imported `dataset`, `config`, `annotations` and `related_results`.\n\n    **Intended usage:**\n\n    For time-based:\n\n    When using [`TimeBasedCesnetDataset`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset) (`dataset_type` = `DatasetType.TIME_BASED`):\n\n    1. Create an instance of the dataset with the desired data root by calling [`get_dataset`](reference_cesnet_database.md#cesnet_tszoo.datasets.databases.cesnet_database.CesnetDatabase.get_dataset). This will download the dataset if it has not been previously downloaded and return instance of dataset.\n    2. Create an instance of [`TimeBasedConfig`](reference_time_based_config.md#references.TimeBasedConfig) and set it using [`set_dataset_config_and_initialize`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.set_dataset_config_and_initialize). \n       This initializes the dataset, including data splitting (train/validation/test), fitting transformers (if needed), selecting features, and more. This is cached for later use.\n    3. Use [`get_train_dataloader`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset)/[`get_train_df`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_train_df)/[`get_train_numpy`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_train_numpy) to get training data for chosen model.\n    4. Validate the model and perform the hyperparameter optimalization on [`get_val_dataloader`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_val_dataloader)/[`get_val_df`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_val_df)/[`get_val_numpy`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_val_numpy).\n    5. Evaluate the model on [`get_test_dataloader`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_dataloader)/[`get_test_df`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_df)/[`get_test_numpy`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_numpy).     \n\n    When using [`SeriesBasedCesnetDataset`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset) (`dataset_type` = `DatasetType.SERIES_BASED`):\n\n    1. Create an instance of the dataset with the desired data root by calling [`get_dataset`](reference_cesnet_database.md#cesnet_tszoo.datasets.databases.cesnet_database.CesnetDatabase.get_dataset). This will download the dataset if it has not been previously downloaded and return instance of dataset.\n    2. Create an instance of [`SeriesBasedConfig`](reference_series_based_config.md#references.SeriesBasedConfig) and set it using [`set_dataset_config_and_initialize`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.set_dataset_config_and_initialize). \n       This initializes the dataset, including data splitting (train/validation/test), fitting transformers (if needed), selecting features, and more. This is cached for later use.\n    3. Use [`get_train_dataloader`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_train_dataloader)/[`get_train_df`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_train_df)/[`get_train_numpy`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_train_numpy) to get training data for chosen model.\n    4. Validate the model and perform the hyperparameter optimalization on [`get_val_dataloader`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_val_dataloader)/[`get_val_df`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_val_df)/[`get_val_numpy`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_val_numpy).\n    5. Evaluate the model on [`get_test_dataloader`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_test_dataloader)/[`get_test_df`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_test_df)/[`get_test_numpy`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_test_numpy).   \n\n    When using [`DisjointTimeBasedCesnetDataset`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset) (`dataset_type` = `DatasetType.DISJOINT_TIME_BASED`):\n\n    1. Create an instance of the dataset with the desired data root by calling [`get_dataset`](reference_cesnet_database.md#cesnet_tszoo.datasets.databases.cesnet_database.CesnetDatabase.get_dataset). This will download the dataset if it has not been previously downloaded and return instance of dataset.\n    2. Create an instance of [`DisjointTimeBasedConfig`](reference_disjoint_time_based_config.md#references.DisjointTimeBasedConfig) and set it using [`set_dataset_config_and_initialize`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.set_dataset_config_and_initialize). \n       This initializes the dataset, including data splitting (train/validation/test), fitting transformers (if needed), selecting features, and more. This is cached for later use.\n    3. Use [`get_train_dataloader`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_train_dataloader)/[`get_train_df`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_train_df)/[`get_train_numpy`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_train_numpy) to get training data for chosen model.\n    4. Validate the model and perform the hyperparameter optimalization on [`get_val_dataloader`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_val_dataloader)/[`get_val_df`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_val_df)/[`get_val_numpy`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_val_numpy).\n    5. Evaluate the model on [`get_test_dataloader`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_test_dataloader)/[`get_test_df`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_test_df)/[`get_test_numpy`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_test_numpy).      \n\n    You can create custom time-based benchmarks with [`save_benchmark`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.save_benchmark), series-based benchmarks with [`save_benchmark`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.save_benchmark) or disjoint-time-based with [`save_benchmark`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.save_benchmark).\n    They will be saved to `\"data_root\"/tszoo/benchmarks/` directory, where `data_root` was set when you created instance of dataset.\n    \"\"\"\n\n    def __init__(self, config: DatasetConfig, dataset: CesnetDataset, description: str = None):\n        self.config = config\n        self.dataset = dataset\n        self.description = description\n        self.related_results = None\n        self.logger = logging.getLogger(\"benchmark\")\n\n    def get_config(self) -&gt; SeriesBasedConfig | TimeBasedConfig | DisjointTimeBasedConfig:\n        \"\"\"Returns config made for this benchmark. \"\"\"\n\n        return self.config\n\n    def get_initialized_dataset(self, display_config_details: Optional[Literal[\"text\", \"diagram\"]] = \"text\", check_errors: bool = False, workers: Literal[\"config\"] | int = \"config\") -&gt; TimeBasedCesnetDataset | SeriesBasedCesnetDataset | DisjointTimeBasedCesnetDataset:\n        \"\"\"\n        Returns dataset with intialized sets, transformers, fillers etc..\n\n        This method uses following config attributes:\n\n        Dataset config | Description\n        -------------- | -----------\n        `init_workers` | Specifies the number of workers to use for initialization. Applied when `workers` = \"config\".\n        `partial_fit_initialized_transformers` | Determines whether initialized transformers should be partially fitted on the training data.\n        `nan_threshold` | Filters out time series with missing values exceeding the specified threshold.\n\n        Parameters:\n            display_config_details: Flag indicating whether to display the configuration values after initialization. `Default: True`   \n            check_errors: Whether to validate if dataset is not corrupted. `Default: False`\n            workers: The number of workers to use during initialization. `Default: \"config\"`        \n\n        Returns:\n            Returns initialized dataset.\n        \"\"\"\n\n        if display_config_details is not None:\n            display_config_details = DisplayType(display_config_details)\n\n        if check_errors:\n            self.dataset.check_errors()\n\n        self.dataset.set_dataset_config_and_initialize(self.config, display_config_details, workers)\n\n        return self.dataset\n\n    def get_dataset(self, check_errors: bool = False) -&gt; TimeBasedCesnetDataset | SeriesBasedCesnetDataset | DisjointTimeBasedCesnetDataset:\n        \"\"\"Returns dataset without initializing it.\n\n        Parameters:\n            check_errors: Whether to validate if dataset is not corrupted. `Default: False`\n\n        Returns:\n            Returns dataset used for this benchmark.\n        \"\"\"\n\n        if check_errors:\n            self.dataset.check_errors()\n\n        return self.dataset\n\n    def get_annotations(self, on: AnnotationType | Literal[\"id_time\", \"ts_id\", \"both\"]) -&gt; pd.DataFrame:\n        \"\"\" \n        Returns the annotations as a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html).\n\n        Parameters:\n            on: Specifies which annotations to return. If set to `\"both\"`, annotations will be applied as if `id_time` and `ts_id` were both set.         \n\n        Returns:\n            A Pandas DataFrame containing the selected annotations.      \n        \"\"\"\n\n        return self.dataset.get_annotations(on)\n\n    def get_related_results(self) -&gt; pd.DataFrame | None:\n        \"\"\"\n        Returns the related results as a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), if they exist. \n\n        Returns:\n            A Pandas DataFrame containing related results or None if not related results exist. \n        \"\"\"\n\n        return self.related_results\n</code></pre>"},{"location":"reference_benchmarks/#cesnet_tszoo.benchmarks.Benchmark.get_annotations","title":"get_annotations","text":"<pre><code>get_annotations(on: AnnotationType | Literal['id_time', 'ts_id', 'both']) -&gt; pd.DataFrame\n</code></pre> <p>Returns the annotations as a Pandas <code>DataFrame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>on</code> <code>AnnotationType | Literal['id_time', 'ts_id', 'both']</code> <p>Specifies which annotations to return. If set to <code>\"both\"</code>, annotations will be applied as if <code>id_time</code> and <code>ts_id</code> were both set.         </p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A Pandas DataFrame containing the selected annotations.</p> Source code in <code>cesnet_tszoo\\benchmarks.py</code> <pre><code>def get_annotations(self, on: AnnotationType | Literal[\"id_time\", \"ts_id\", \"both\"]) -&gt; pd.DataFrame:\n    \"\"\" \n    Returns the annotations as a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html).\n\n    Parameters:\n        on: Specifies which annotations to return. If set to `\"both\"`, annotations will be applied as if `id_time` and `ts_id` were both set.         \n\n    Returns:\n        A Pandas DataFrame containing the selected annotations.      \n    \"\"\"\n\n    return self.dataset.get_annotations(on)\n</code></pre>"},{"location":"reference_benchmarks/#cesnet_tszoo.benchmarks.Benchmark.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; SeriesBasedConfig | TimeBasedConfig | DisjointTimeBasedConfig\n</code></pre> <p>Returns config made for this benchmark.</p> Source code in <code>cesnet_tszoo\\benchmarks.py</code> <pre><code>def get_config(self) -&gt; SeriesBasedConfig | TimeBasedConfig | DisjointTimeBasedConfig:\n    \"\"\"Returns config made for this benchmark. \"\"\"\n\n    return self.config\n</code></pre>"},{"location":"reference_benchmarks/#cesnet_tszoo.benchmarks.Benchmark.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(check_errors: bool = False) -&gt; TimeBasedCesnetDataset | SeriesBasedCesnetDataset | DisjointTimeBasedCesnetDataset\n</code></pre> <p>Returns dataset without initializing it.</p> <p>Parameters:</p> Name Type Description Default <code>check_errors</code> <code>bool</code> <p>Whether to validate if dataset is not corrupted. <code>Default: False</code></p> <code>False</code> <p>Returns:</p> Type Description <code>TimeBasedCesnetDataset | SeriesBasedCesnetDataset | DisjointTimeBasedCesnetDataset</code> <p>Returns dataset used for this benchmark.</p> Source code in <code>cesnet_tszoo\\benchmarks.py</code> <pre><code>def get_dataset(self, check_errors: bool = False) -&gt; TimeBasedCesnetDataset | SeriesBasedCesnetDataset | DisjointTimeBasedCesnetDataset:\n    \"\"\"Returns dataset without initializing it.\n\n    Parameters:\n        check_errors: Whether to validate if dataset is not corrupted. `Default: False`\n\n    Returns:\n        Returns dataset used for this benchmark.\n    \"\"\"\n\n    if check_errors:\n        self.dataset.check_errors()\n\n    return self.dataset\n</code></pre>"},{"location":"reference_benchmarks/#cesnet_tszoo.benchmarks.Benchmark.get_initialized_dataset","title":"get_initialized_dataset","text":"<pre><code>get_initialized_dataset(display_config_details: Optional[Literal['text', 'diagram']] = 'text', check_errors: bool = False, workers: Literal['config'] | int = 'config') -&gt; TimeBasedCesnetDataset | SeriesBasedCesnetDataset | DisjointTimeBasedCesnetDataset\n</code></pre> <p>Returns dataset with intialized sets, transformers, fillers etc..</p> <p>This method uses following config attributes:</p> Dataset config Description <code>init_workers</code> Specifies the number of workers to use for initialization. Applied when <code>workers</code> = \"config\". <code>partial_fit_initialized_transformers</code> Determines whether initialized transformers should be partially fitted on the training data. <code>nan_threshold</code> Filters out time series with missing values exceeding the specified threshold. <p>Parameters:</p> Name Type Description Default <code>display_config_details</code> <code>Optional[Literal['text', 'diagram']]</code> <p>Flag indicating whether to display the configuration values after initialization. <code>Default: True</code> </p> <code>'text'</code> <code>check_errors</code> <code>bool</code> <p>Whether to validate if dataset is not corrupted. <code>Default: False</code></p> <code>False</code> <code>workers</code> <code>Literal['config'] | int</code> <p>The number of workers to use during initialization. <code>Default: \"config\"</code> </p> <code>'config'</code> <p>Returns:</p> Type Description <code>TimeBasedCesnetDataset | SeriesBasedCesnetDataset | DisjointTimeBasedCesnetDataset</code> <p>Returns initialized dataset.</p> Source code in <code>cesnet_tszoo\\benchmarks.py</code> <pre><code>def get_initialized_dataset(self, display_config_details: Optional[Literal[\"text\", \"diagram\"]] = \"text\", check_errors: bool = False, workers: Literal[\"config\"] | int = \"config\") -&gt; TimeBasedCesnetDataset | SeriesBasedCesnetDataset | DisjointTimeBasedCesnetDataset:\n    \"\"\"\n    Returns dataset with intialized sets, transformers, fillers etc..\n\n    This method uses following config attributes:\n\n    Dataset config | Description\n    -------------- | -----------\n    `init_workers` | Specifies the number of workers to use for initialization. Applied when `workers` = \"config\".\n    `partial_fit_initialized_transformers` | Determines whether initialized transformers should be partially fitted on the training data.\n    `nan_threshold` | Filters out time series with missing values exceeding the specified threshold.\n\n    Parameters:\n        display_config_details: Flag indicating whether to display the configuration values after initialization. `Default: True`   \n        check_errors: Whether to validate if dataset is not corrupted. `Default: False`\n        workers: The number of workers to use during initialization. `Default: \"config\"`        \n\n    Returns:\n        Returns initialized dataset.\n    \"\"\"\n\n    if display_config_details is not None:\n        display_config_details = DisplayType(display_config_details)\n\n    if check_errors:\n        self.dataset.check_errors()\n\n    self.dataset.set_dataset_config_and_initialize(self.config, display_config_details, workers)\n\n    return self.dataset\n</code></pre>"},{"location":"reference_benchmarks/#cesnet_tszoo.benchmarks.Benchmark.get_related_results","title":"get_related_results","text":"<pre><code>get_related_results() -&gt; pd.DataFrame | None\n</code></pre> <p>Returns the related results as a Pandas <code>DataFrame</code>, if they exist. </p> <p>Returns:</p> Type Description <code>DataFrame | None</code> <p>A Pandas DataFrame containing related results or None if not related results exist.</p> Source code in <code>cesnet_tszoo\\benchmarks.py</code> <pre><code>def get_related_results(self) -&gt; pd.DataFrame | None:\n    \"\"\"\n    Returns the related results as a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), if they exist. \n\n    Returns:\n        A Pandas DataFrame containing related results or None if not related results exist. \n    \"\"\"\n\n    return self.related_results\n</code></pre>"},{"location":"reference_benchmarks/#cesnet_tszoo.benchmarks.load_benchmark","title":"load_benchmark","text":"<pre><code>load_benchmark(identifier: str, data_root: str) -&gt; Benchmark\n</code></pre> <p>Load a benchmark using the identifier.</p> <p>First, it attempts to load the built-in benchmark, if no built-in benchmark with such an identifier exists, it attempts to load a custom benchmark from the <code>\"data_root\"/tszoo/benchmarks/</code> directory.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The name of the benchmark YAML file.</p> required <code>data_root</code> <code>str</code> <p>Path to the folder where the dataset will be stored. Each database has its own subfolder <code>\"data_root\"/tszoo/databases/database_name/</code>.</p> required <p>Returns:</p> Type Description <code>Benchmark</code> <p>Returns benchmark with <code>config</code>, <code>annotations</code>, <code>dataset</code> and <code>related_results</code>.</p> Source code in <code>cesnet_tszoo\\benchmarks.py</code> <pre><code>def load_benchmark(identifier: str, data_root: str) -&gt; Benchmark:\n    \"\"\"\n    Load a benchmark using the identifier.\n\n    First, it attempts to load the built-in benchmark, if no built-in benchmark with such an identifier exists, it attempts to load a custom benchmark from the `\"data_root\"/tszoo/benchmarks/` directory.\n\n    Parameters:\n        identifier: The name of the benchmark YAML file.\n        data_root: Path to the folder where the dataset will be stored. Each database has its own subfolder `\"data_root\"/tszoo/databases/database_name/`.\n\n    Returns:\n        Returns benchmark with `config`, `annotations`, `dataset` and `related_results`.\n    \"\"\"\n\n    logger = logging.getLogger(\"benchmark\")\n\n    data_root = os.path.normpath(os.path.expanduser(data_root))\n\n    # For anything else\n    if isinstance(identifier, str):\n        _, is_built_in = get_benchmark_path_and_whether_it_is_built_in(identifier, data_root, logger)\n\n        if is_built_in:\n            logger.info(\"Built-in benchmark found: %s. Loading it.\", identifier)\n            return _get_built_in_benchmark(identifier, data_root)\n        else:\n            logger.info(\"Custom benchmark found: %s. Loading it.\", identifier)\n            return _get_custom_benchmark(identifier, data_root)\n\n    else:\n        logger.error(\"Invalid identifier.\")\n        raise ValueError(\"Invalid identifier.\")\n</code></pre>"},{"location":"reference_cesnet_database/","title":"CesnetDatabase","text":""},{"location":"reference_cesnet_database/#cesnet_tszoo.datasets.databases.cesnet_database.CesnetDatabase","title":"cesnet_tszoo.datasets.databases.cesnet_database.CesnetDatabase","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for cesnet databases. This class should not be used directly. Use it as base for adding new databases.</p> <p>Derived databases are used by calling class method <code>get_dataset</code> which will create a new instance of specified CesnetDataset. Check them for more info about how to use them.</p> <p>Intended usage:</p> <p>When using <code>TimeBasedCesnetDataset</code> (<code>dataset_type</code> = <code>DatasetType.TIME_BASED</code>):</p> <ol> <li>Create an instance of the dataset with the desired data root by calling <code>get_dataset</code>. This will download the dataset if it has not been previously downloaded and return instance of dataset.</li> <li>Create an instance of <code>TimeBasedConfig</code> and set it using <code>set_dataset_config_and_initialize</code>.     This initializes the dataset, including data splitting (train/validation/test), fitting transformers (if needed), selecting features, and more. This is cached for later use.</li> <li>Use <code>get_train_dataloader</code>/<code>get_train_df</code>/<code>get_train_numpy</code> to get training data for chosen model.</li> <li>Validate the model and perform the hyperparameter optimalization on <code>get_val_dataloader</code>/<code>get_val_df</code>/<code>get_val_numpy</code>.</li> <li>Evaluate the model on <code>get_test_dataloader</code>/<code>get_test_df</code>/<code>get_test_numpy</code>.     </li> </ol> <p>When using <code>SeriesBasedCesnetDataset</code> (<code>dataset_type</code> = <code>DatasetType.SERIES_BASED</code>):</p> <ol> <li>Create an instance of the dataset with the desired data root by calling <code>get_dataset</code>. This will download the dataset if it has not been previously downloaded and return instance of dataset.</li> <li>Create an instance of <code>SeriesBasedConfig</code> and set it using <code>set_dataset_config_and_initialize</code>.     This initializes the dataset, including data splitting (train/validation/test), fitting transformers (if needed), selecting features, and more. This is cached for later use.</li> <li>Use <code>get_train_dataloader</code>/<code>get_train_df</code>/<code>get_train_numpy</code> to get training data for chosen model.</li> <li>Validate the model and perform the hyperparameter optimalization on <code>get_val_dataloader</code>/<code>get_val_df</code>/<code>get_val_numpy</code>.</li> <li>Evaluate the model on <code>get_test_dataloader</code>/<code>get_test_df</code>/<code>get_test_numpy</code>.   </li> </ol> <p>When using <code>DisjointTimeBasedCesnetDataset</code> (<code>dataset_type</code> = <code>DatasetType.DISJOINT_TIME_BASED</code>):</p> <ol> <li>Create an instance of the dataset with the desired data root by calling <code>get_dataset</code>. This will download the dataset if it has not been previously downloaded and return instance of dataset.</li> <li>Create an instance of <code>DisjointTimeBasedConfig</code> and set it using <code>set_dataset_config_and_initialize</code>.     This initializes the dataset, including data splitting (train/validation/test), fitting transformers (if needed), selecting features, and more. This is cached for later use.</li> <li>Use <code>get_train_dataloader</code>/<code>get_train_df</code>/<code>get_train_numpy</code> to get training data for chosen model.</li> <li>Validate the model and perform the hyperparameter optimalization on <code>get_val_dataloader</code>/<code>get_val_df</code>/<code>get_val_numpy</code>.</li> <li>Evaluate the model on <code>get_test_dataloader</code>/<code>get_test_df</code>/<code>get_test_numpy</code>.   </li> </ol> <p>Used class attributes:</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the database.</p> <code>bucket_url</code> <code>str</code> <p>URL of the bucket where the dataset is stored.</p> <code>tszoo_root</code> <code>str</code> <p>Path to folder where all databases are saved. Set after <code>get_dataset</code> was called at least once.</p> <code>database_root</code> <code>str</code> <p>Path to the folder where datasets belonging to the database are saved. Set after <code>get_dataset</code> was called at least once.</p> <code>configs_root</code> <code>str</code> <p>Path to the folder where configurations are saved. Set after <code>get_dataset</code> was called at least once.</p> <code>benchmarks_root</code> <code>str</code> <p>Path to the folder where benchmarks are saved. Set after <code>get_dataset</code> was called at least once.</p> <code>annotations_root</code> <code>str</code> <p>Path to the folder where annotations are saved. Set after <code>get_dataset</code> was called at least once.</p> <code>id_names</code> <code>dict</code> <p>Names for time series IDs for each <code>source_type</code>.</p> <code>default_values</code> <code>dict</code> <p>Default values for each available feature.</p> <code>source_types</code> <code>list[SourceType]</code> <p>Available source types for the database.</p> <code>aggregations</code> <code>list[AgreggationType]</code> <p>Available aggregations for the database.   </p> <code>additional_data</code> <code>dict[str, tuple]</code> <p>Available small datasets for each dataset.</p> Source code in <code>cesnet_tszoo\\datasets\\databases\\cesnet_database.py</code> <pre><code>class CesnetDatabase(ABC):\n    \"\"\"\n    Base class for cesnet databases. This class should **not** be used directly. Use it as base for adding new databases.\n\n    Derived databases are used by calling class method [`get_dataset`](reference_cesnet_database.md#cesnet_tszoo.datasets.databases.cesnet_database.CesnetDatabase.get_dataset) which will create a new instance of specified CesnetDataset. Check them for more info about how to use them.\n\n    **Intended usage:**\n\n    When using [`TimeBasedCesnetDataset`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset) (`dataset_type` = `DatasetType.TIME_BASED`):\n\n    1. Create an instance of the dataset with the desired data root by calling [`get_dataset`](reference_cesnet_database.md#cesnet_tszoo.datasets.databases.cesnet_database.CesnetDatabase.get_dataset). This will download the dataset if it has not been previously downloaded and return instance of dataset.\n    2. Create an instance of [`TimeBasedConfig`](reference_time_based_config.md#references.TimeBasedConfig) and set it using [`set_dataset_config_and_initialize`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.set_dataset_config_and_initialize). \n       This initializes the dataset, including data splitting (train/validation/test), fitting transformers (if needed), selecting features, and more. This is cached for later use.\n    3. Use [`get_train_dataloader`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset)/[`get_train_df`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_train_df)/[`get_train_numpy`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_train_numpy) to get training data for chosen model.\n    4. Validate the model and perform the hyperparameter optimalization on [`get_val_dataloader`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_val_dataloader)/[`get_val_df`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_val_df)/[`get_val_numpy`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_val_numpy).\n    5. Evaluate the model on [`get_test_dataloader`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_dataloader)/[`get_test_df`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_df)/[`get_test_numpy`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_numpy).     \n\n    When using [`SeriesBasedCesnetDataset`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset) (`dataset_type` = `DatasetType.SERIES_BASED`):\n\n    1. Create an instance of the dataset with the desired data root by calling [`get_dataset`](reference_cesnet_database.md#cesnet_tszoo.datasets.databases.cesnet_database.CesnetDatabase.get_dataset). This will download the dataset if it has not been previously downloaded and return instance of dataset.\n    2. Create an instance of [`SeriesBasedConfig`](reference_series_based_config.md#references.SeriesBasedConfig) and set it using [`set_dataset_config_and_initialize`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.set_dataset_config_and_initialize). \n       This initializes the dataset, including data splitting (train/validation/test), fitting transformers (if needed), selecting features, and more. This is cached for later use.\n    3. Use [`get_train_dataloader`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_train_dataloader)/[`get_train_df`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_train_df)/[`get_train_numpy`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_train_numpy) to get training data for chosen model.\n    4. Validate the model and perform the hyperparameter optimalization on [`get_val_dataloader`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_val_dataloader)/[`get_val_df`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_val_df)/[`get_val_numpy`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_val_numpy).\n    5. Evaluate the model on [`get_test_dataloader`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_test_dataloader)/[`get_test_df`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_test_df)/[`get_test_numpy`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_test_numpy).   \n\n    When using [`DisjointTimeBasedCesnetDataset`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset) (`dataset_type` = `DatasetType.DISJOINT_TIME_BASED`):\n\n    1. Create an instance of the dataset with the desired data root by calling [`get_dataset`](reference_cesnet_database.md#cesnet_tszoo.datasets.databases.cesnet_database.CesnetDatabase.get_dataset). This will download the dataset if it has not been previously downloaded and return instance of dataset.\n    2. Create an instance of [`DisjointTimeBasedConfig`](reference_disjoint_time_based_config.md#references.DisjointTimeBasedConfig) and set it using [`set_dataset_config_and_initialize`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.set_dataset_config_and_initialize). \n       This initializes the dataset, including data splitting (train/validation/test), fitting transformers (if needed), selecting features, and more. This is cached for later use.\n    3. Use [`get_train_dataloader`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_train_dataloader)/[`get_train_df`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_train_df)/[`get_train_numpy`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_train_numpy) to get training data for chosen model.\n    4. Validate the model and perform the hyperparameter optimalization on [`get_val_dataloader`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_val_dataloader)/[`get_val_df`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_val_df)/[`get_val_numpy`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_val_numpy).\n    5. Evaluate the model on [`get_test_dataloader`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_test_dataloader)/[`get_test_df`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_test_df)/[`get_test_numpy`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_test_numpy).   \n\n    Used class attributes:\n\n    Attributes:\n        name: Name of the database.\n        bucket_url: URL of the bucket where the dataset is stored.\n        tszoo_root: Path to folder where all databases are saved. Set after [`get_dataset`](reference_cesnet_database.md#cesnet_tszoo.datasets.databases.cesnet_database.CesnetDatabase.get_dataset) was called at least once.\n        database_root: Path to the folder where datasets belonging to the database are saved. Set after [`get_dataset`](reference_cesnet_database.md#cesnet_tszoo.datasets.databases.cesnet_database.CesnetDatabase.get_dataset) was called at least once.\n        configs_root: Path to the folder where configurations are saved. Set after [`get_dataset`](reference_cesnet_database.md#cesnet_tszoo.datasets.databases.cesnet_database.CesnetDatabase.get_dataset) was called at least once.\n        benchmarks_root: Path to the folder where benchmarks are saved. Set after [`get_dataset`](reference_cesnet_database.md#cesnet_tszoo.datasets.databases.cesnet_database.CesnetDatabase.get_dataset) was called at least once.\n        annotations_root: Path to the folder where annotations are saved. Set after [`get_dataset`](reference_cesnet_database.md#cesnet_tszoo.datasets.databases.cesnet_database.CesnetDatabase.get_dataset) was called at least once.\n        id_names: Names for time series IDs for each `source_type`.\n        default_values: Default values for each available feature.\n        source_types: Available source types for the database.\n        aggregations: Available aggregations for the database.   \n        additional_data: Available small datasets for each dataset. \n    \"\"\"\n\n    name: str\n    bucket_url: str\n\n    tszoo_root: str\n    database_root: str\n    configs_root: str\n    benchmarks_root: str\n    annotations_root: str\n\n    id_names: dict = None\n    default_values: dict = None\n    source_types: list[SourceType] = []\n    aggregations: list[AgreggationType] = []\n    additional_data: dict[str, tuple] = {}\n\n    def __init__(self):\n        raise ValueError(\"To create dataset instance use class method 'get_dataset' instead.\")\n\n    @classmethod\n    def get_dataset(cls, data_root: str, source_type: SourceType | str, aggregation: AgreggationType | str, dataset_type: DatasetType | str, check_errors: bool = False, display_details: bool = False) -&gt; CesnetDataset:\n        \"\"\"\n        Create new dataset instance.\n\n        Parameters:\n            data_root: Path to the folder where the dataset will be stored. Each database has its own subfolder `data_root/tszoo/databases/database_name/`.\n            source_type: The source type of the desired dataset.\n            aggregation: The aggregation type for the selected source type.\n            dataset_type: Type of a dataset you want to create. Can be [`TimeBasedCesnetDataset`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset), [`SeriesBasedCesnetDataset`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset) or [`DisjointTimeBasedCesnetDataset`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset).\n            check_errors: Whether to validate if the dataset is corrupted. `Default: False`\n            display_details: Whether to display details about the available data in chosen dataset. `Default: False`\n\n        Returns:\n            [`TimeBasedCesnetDataset`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset), [`SeriesBasedCesnetDataset`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset) or [`DisjointTimeBasedCesnetDataset`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset).\n        \"\"\"\n\n        logger = logging.getLogger(\"wrapper_dataset\")\n\n        source_type = SourceType(source_type)\n        aggregation = AgreggationType(aggregation)\n        dataset_type = DatasetType(dataset_type)\n\n        if source_type not in cls.source_types:\n            raise ValueError(f\"Unsupported source type: {source_type}\")\n\n        if aggregation not in cls.aggregations:\n            raise ValueError(f\"Unsupported aggregation type: {aggregation}\")\n\n        # Dataset paths setup\n        cls.tszoo_root = os.path.normpath(os.path.expanduser(os.path.join(data_root, \"tszoo\")))\n        cls.database_root = os.path.join(cls.tszoo_root, \"databases\", cls.name)\n        cls.configs_root = os.path.join(cls.tszoo_root, \"configs\")\n        cls.benchmarks_root = os.path.join(cls.tszoo_root, \"benchmarks\")\n        cls.annotations_root = os.path.join(cls.tszoo_root, \"annotations\")\n        dataset_name = f\"{cls.name}-{source_type.value}-{AgreggationType._to_str_without_number(aggregation)}\"\n        dataset_path = os.path.join(cls.database_root, f\"{dataset_name}.h5\")\n\n        # Ensure necessary directories exist\n        for directory in [cls.database_root, cls.configs_root, cls.annotations_root, cls.benchmarks_root]:\n            if not os.path.exists(directory):\n                logger.info(\"Creating directory: %s\", directory)\n                os.makedirs(directory)\n\n        if not cls._is_downloaded(dataset_path):\n            cls._download(dataset_name, dataset_path)\n\n        dataset_metadata = DatasetMetadata(cls.name, dataset_type, dataset_path, cls.configs_root, cls.benchmarks_root, cls.annotations_root, source_type, aggregation, cls.id_names[source_type], cls.default_values, cls.additional_data)\n\n        dataset_factory = dataset_factories.get_dataset_factory(dataset_type)\n        dataset = dataset_factory.create_dataset(dataset_metadata)\n\n        if check_errors:\n            dataset.check_errors()\n\n        if display_details:\n            dataset.display_dataset_details()\n\n        return dataset\n\n    @classmethod\n    def get_expected_paths(cls, data_root: str, database_name: str) -&gt; dict:\n        \"\"\"Returns expected path for the provided `data_root` and `database_name`\n\n        Args:\n            data_root: Path to the folder where the dataset will be stored. Each database has its own subfolder `data_root/tszoo/databases/database_name/`.\n            database_name: Name of the expected database.\n\n        Returns:\n            str: Dictionary of paths.\n        \"\"\"\n\n        paths = {}\n\n        paths[\"tszoo_root\"] = os.path.normpath(os.path.expanduser(os.path.join(data_root, \"tszoo\")))\n        paths[\"database_root\"] = os.path.join(paths[\"tszoo_root\"], \"databases\", database_name)\n        paths[\"configs_root\"] = os.path.join(paths[\"tszoo_root\"], \"configs\")\n        paths[\"benchmarks_root\"] = os.path.join(paths[\"tszoo_root\"], \"benchmarks\")\n        paths[\"annotations_root\"] = os.path.join(paths[\"tszoo_root\"], \"annotations\")\n\n        return paths\n\n    @classmethod\n    def _is_downloaded(cls, dataset_path: str) -&gt; bool:\n        \"\"\"Check whether the dataset at path has already been downloaded. \"\"\"\n\n        return os.path.exists(dataset_path)\n\n    @classmethod\n    def _download(cls, dataset_name: str, dataset_path: str) -&gt; None:\n        \"\"\"Download the dataset file. \"\"\"\n\n        logger = logging.getLogger(\"wrapper_dataset\")\n\n        logger.info(\"Downloading %s dataset.\", dataset_name)\n        database_url = f\"{cls.bucket_url}&amp;file={dataset_name}.h5\"\n        resumable_download(url=database_url, file_path=dataset_path, silent=False)\n</code></pre>"},{"location":"reference_cesnet_database/#cesnet_tszoo.datasets.databases.cesnet_database.CesnetDatabase.get_dataset","title":"get_dataset  <code>classmethod</code>","text":"<pre><code>get_dataset(data_root: str, source_type: SourceType | str, aggregation: AgreggationType | str, dataset_type: DatasetType | str, check_errors: bool = False, display_details: bool = False) -&gt; CesnetDataset\n</code></pre> <p>Create new dataset instance.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the folder where the dataset will be stored. Each database has its own subfolder <code>data_root/tszoo/databases/database_name/</code>.</p> required <code>source_type</code> <code>SourceType | str</code> <p>The source type of the desired dataset.</p> required <code>aggregation</code> <code>AgreggationType | str</code> <p>The aggregation type for the selected source type.</p> required <code>dataset_type</code> <code>DatasetType | str</code> <p>Type of a dataset you want to create. Can be <code>TimeBasedCesnetDataset</code>, <code>SeriesBasedCesnetDataset</code> or <code>DisjointTimeBasedCesnetDataset</code>.</p> required <code>check_errors</code> <code>bool</code> <p>Whether to validate if the dataset is corrupted. <code>Default: False</code></p> <code>False</code> <code>display_details</code> <code>bool</code> <p>Whether to display details about the available data in chosen dataset. <code>Default: False</code></p> <code>False</code> <p>Returns:</p> Type Description <code>CesnetDataset</code> <p><code>TimeBasedCesnetDataset</code>, <code>SeriesBasedCesnetDataset</code> or <code>DisjointTimeBasedCesnetDataset</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\databases\\cesnet_database.py</code> <pre><code>@classmethod\ndef get_dataset(cls, data_root: str, source_type: SourceType | str, aggregation: AgreggationType | str, dataset_type: DatasetType | str, check_errors: bool = False, display_details: bool = False) -&gt; CesnetDataset:\n    \"\"\"\n    Create new dataset instance.\n\n    Parameters:\n        data_root: Path to the folder where the dataset will be stored. Each database has its own subfolder `data_root/tszoo/databases/database_name/`.\n        source_type: The source type of the desired dataset.\n        aggregation: The aggregation type for the selected source type.\n        dataset_type: Type of a dataset you want to create. Can be [`TimeBasedCesnetDataset`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset), [`SeriesBasedCesnetDataset`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset) or [`DisjointTimeBasedCesnetDataset`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset).\n        check_errors: Whether to validate if the dataset is corrupted. `Default: False`\n        display_details: Whether to display details about the available data in chosen dataset. `Default: False`\n\n    Returns:\n        [`TimeBasedCesnetDataset`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset), [`SeriesBasedCesnetDataset`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset) or [`DisjointTimeBasedCesnetDataset`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset).\n    \"\"\"\n\n    logger = logging.getLogger(\"wrapper_dataset\")\n\n    source_type = SourceType(source_type)\n    aggregation = AgreggationType(aggregation)\n    dataset_type = DatasetType(dataset_type)\n\n    if source_type not in cls.source_types:\n        raise ValueError(f\"Unsupported source type: {source_type}\")\n\n    if aggregation not in cls.aggregations:\n        raise ValueError(f\"Unsupported aggregation type: {aggregation}\")\n\n    # Dataset paths setup\n    cls.tszoo_root = os.path.normpath(os.path.expanduser(os.path.join(data_root, \"tszoo\")))\n    cls.database_root = os.path.join(cls.tszoo_root, \"databases\", cls.name)\n    cls.configs_root = os.path.join(cls.tszoo_root, \"configs\")\n    cls.benchmarks_root = os.path.join(cls.tszoo_root, \"benchmarks\")\n    cls.annotations_root = os.path.join(cls.tszoo_root, \"annotations\")\n    dataset_name = f\"{cls.name}-{source_type.value}-{AgreggationType._to_str_without_number(aggregation)}\"\n    dataset_path = os.path.join(cls.database_root, f\"{dataset_name}.h5\")\n\n    # Ensure necessary directories exist\n    for directory in [cls.database_root, cls.configs_root, cls.annotations_root, cls.benchmarks_root]:\n        if not os.path.exists(directory):\n            logger.info(\"Creating directory: %s\", directory)\n            os.makedirs(directory)\n\n    if not cls._is_downloaded(dataset_path):\n        cls._download(dataset_name, dataset_path)\n\n    dataset_metadata = DatasetMetadata(cls.name, dataset_type, dataset_path, cls.configs_root, cls.benchmarks_root, cls.annotations_root, source_type, aggregation, cls.id_names[source_type], cls.default_values, cls.additional_data)\n\n    dataset_factory = dataset_factories.get_dataset_factory(dataset_type)\n    dataset = dataset_factory.create_dataset(dataset_metadata)\n\n    if check_errors:\n        dataset.check_errors()\n\n    if display_details:\n        dataset.display_dataset_details()\n\n    return dataset\n</code></pre>"},{"location":"reference_cesnet_database/#cesnet_tszoo.datasets.databases.cesnet_database.CesnetDatabase.get_expected_paths","title":"get_expected_paths  <code>classmethod</code>","text":"<pre><code>get_expected_paths(data_root: str, database_name: str) -&gt; dict\n</code></pre> <p>Returns expected path for the provided <code>data_root</code> and <code>database_name</code></p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the folder where the dataset will be stored. Each database has its own subfolder <code>data_root/tszoo/databases/database_name/</code>.</p> required <code>database_name</code> <code>str</code> <p>Name of the expected database.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>dict</code> <p>Dictionary of paths.</p> Source code in <code>cesnet_tszoo\\datasets\\databases\\cesnet_database.py</code> <pre><code>@classmethod\ndef get_expected_paths(cls, data_root: str, database_name: str) -&gt; dict:\n    \"\"\"Returns expected path for the provided `data_root` and `database_name`\n\n    Args:\n        data_root: Path to the folder where the dataset will be stored. Each database has its own subfolder `data_root/tszoo/databases/database_name/`.\n        database_name: Name of the expected database.\n\n    Returns:\n        str: Dictionary of paths.\n    \"\"\"\n\n    paths = {}\n\n    paths[\"tszoo_root\"] = os.path.normpath(os.path.expanduser(os.path.join(data_root, \"tszoo\")))\n    paths[\"database_root\"] = os.path.join(paths[\"tszoo_root\"], \"databases\", database_name)\n    paths[\"configs_root\"] = os.path.join(paths[\"tszoo_root\"], \"configs\")\n    paths[\"benchmarks_root\"] = os.path.join(paths[\"tszoo_root\"], \"benchmarks\")\n    paths[\"annotations_root\"] = os.path.join(paths[\"tszoo_root\"], \"annotations\")\n\n    return paths\n</code></pre>"},{"location":"reference_cesnet_dataset/","title":"Cesnet dataset class","text":""},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset","title":"cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for cesnet datasets. This class should not be used directly. Instead, use one of the derived classes, such as <code>TimeBasedCesnetDataset</code>, <code>SeriesBasedCesnetDataset</code> or <code>DisjointTimeBasedCesnetDataset</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>@dataclass\nclass CesnetDataset(ABC):\n    \"\"\"\n    Base class for cesnet datasets. This class should **not** be used directly. Instead, use one of the derived classes, such as [`TimeBasedCesnetDataset`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset), [`SeriesBasedCesnetDataset`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset) or [`DisjointTimeBasedCesnetDataset`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset).\n    \"\"\"\n\n    metadata: DatasetMetadata\n    \"\"\"Holds various metadata used in dataset for its creation, loading data and similar.\"\"\"\n\n    dataset_config: Optional[DatasetConfig] = field(default=None, init=False)\n    \"\"\"Configuration of the dataset.\"\"\"\n\n    train_dataset: Optional[Dataset] = field(default=None, init=False)\n    \"\"\"Training set as a `BaseDataset` instance wrapping the PyTables database.\"\"\"\n\n    val_dataset: Optional[Dataset] = field(default=None, init=False)\n    \"\"\"Validation set as a `BaseDataset` instance wrapping the PyTables database.\"\"\"\n\n    test_dataset: Optional[Dataset] = field(default=None, init=False)\n    \"\"\"Test set as a `BaseDataset` instance wrapping the PyTables database.\"\"\"\n\n    all_dataset: Optional[Dataset] = field(default=None, init=False)\n    \"\"\"All set as a `BaseDataset` instance wrapping the PyTables database.\"\"\"\n\n    dataloader_factory: Optional[DataloaderFactory] = field(default=None, init=False)\n    \"\"\"Factory used to create Dataloaders for specific CesnetDataset subclass.  \"\"\"\n\n    train_dataloader: Optional[DataLoader] = field(default=None, init=False)\n    \"\"\"Iterable PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for training set.\"\"\"\n\n    val_dataloader: Optional[DataLoader] = field(default=None, init=False)\n    \"\"\"Iterable PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for validation set.\"\"\"\n\n    test_dataloader: Optional[DataLoader] = field(default=None, init=False)\n    \"\"\"Iterable PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for test set.\"\"\"\n\n    all_dataloader: Optional[DataLoader] = field(default=None, init=False)\n    \"\"\"Iterable PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for all set.    \"\"\"\n\n    dataset_type: Optional[DatasetType] = field(default=None, init=False)\n\n    related_to: Optional[str] = field(default=None, init=False)\n    \"\"\"Name of file with relevant results to used benchmark.\"\"\"\n\n    _collate_fn: Optional[Callable] = field(default=None, init=False)\n    _export_config_copy: Optional[DatasetConfig] = field(default=None, init=False)\n\n    def __post_init__(self):\n        self.logger = logging.getLogger(\"cesnet_dataset\")\n\n        self._collate_fn = dataset_loaders.collate_fn_simple\n        self.annotations = Annotations()\n\n        # Initialize annotation states\n        self.imported_annotations_ts_identifier = None\n        self.imported_annotations_time_identifier = None\n        self.imported_annotations_both_identifier = None\n\n    def set_dataset_config_and_initialize(self, dataset_config: DatasetConfig, display_config_details: Optional[Literal[\"text\", \"diagram\"]] = \"text\", workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n        \"\"\"\n        Initialize training set, validation set, test set etc.. This method must be called before any data can be accessed. It is required for the final initialization of [`dataset_config`](reference_dataset_config.md#references.DatasetConfig).\n\n        The following configuration attributes are used during initialization:\n\n        Dataset config | Description\n        -------------- | -----------\n        `init_workers` | Specifies the number of workers to use for initialization. Applied when `workers` = \"config\".\n        `partial_fit_initialized_transformers` | Determines whether initialized transformers should be partially fitted on the training data.\n        `nan_threshold` | Filters out time series with missing values exceeding the specified threshold.\n\n        Parameters:\n            dataset_config: Desired configuration of the dataset.\n            display_config_details: Flag indicating whether and how to display the configuration values after initialization. `Default: text`  \n            workers: The number of workers to use during initialization. `Default: \"config\"`  \n        \"\"\"\n\n        if display_config_details is not None:\n            display_config_details = DisplayType(display_config_details)\n\n        self._clear()\n        self.dataset_config = dataset_config\n\n        # If the config is not initialized, set a copy of the configuration for export\n        if not self.dataset_config.is_initialized:\n            self.dataset_config._update_identifiers_from_dataset_metadata(self.metadata)\n            self._export_config_copy = deepcopy(self.dataset_config)\n            self.logger.debug(\"New export_config_copy created.\")\n\n        self._validate_config_for_dataset(self.dataset_config)\n\n        if workers == \"config\":\n            workers = self.dataset_config.init_workers\n\n        if not self.dataset_config.is_initialized:\n\n            self.dataset_config._dataset_init(self.metadata)\n            self._initialize_transformers_and_details(workers)\n\n            self.dataset_config.is_initialized = True\n            self.logger.info(\"Config initialized successfully.\")\n        else:\n            self.logger.info(\"Config already initialized. Skipping re-initialization.\")\n\n        # Initialize datasets\n        self._initialize_datasets()\n        self.logger.debug(\"Datasets have been successfully initialized.\")\n\n        self._update_export_config_copy()\n        self.logger.debug(\"Export config copy updated with the latest dataset configuration.\")\n\n        if display_config_details is not None:\n            self.summary(display_config_details)\n\n    def get_train_dataloader(self, ts_id: int | None = None, workers: int | Literal[\"config\"] = \"config\", **kwargs) -&gt; DataLoader:\n        \"\"\"\n        Returns a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for training set.\n\n        The `DataLoader` is created on the first call and cached for subsequent use. &lt;br/&gt;\n        The cached dataloader is cleared when either [`get_train_df`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_train_df) or [`get_train_numpy`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_train_numpy) is called.\n\n        The structure of the returned batch depends on the `time_format` and whether `sliding_window_size` is used:\n\n        - When `sliding_window_size` is used:\n            - With `time_format` == TimeFormat.DATETIME and included time:\n                - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n                - `np.ndarray` of shape `(num_time_series, 1, features)`\n                - `np.ndarray` of times with shape `(times - 1)`\n                - `np.ndarray` of time with shape `(1)`\n            - When `time_format` != TimeFormat.DATETIME or time is not included:\n                - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n                - `np.ndarray` of shape `(num_time_series, 1, features)`\n        - When `sliding_window_size` is not used:\n            - With `time_format` == TimeFormat.DATETIME and included time:\n                - `np.ndarray` of shape `(num_time_series, times, features)`\n                - `np.ndarray` of time with shape `(times)`\n            - When `time_format` != TimeFormat.DATETIME or time is not included:\n                - `np.ndarray` of shape `(num_time_series, times, features)`\n\n        The `DataLoader` is configured with the following config attributes:\n\n        Dataset config | Description\n        -------------- | -----------\n        `train_batch_size` | Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n        `sliding_window_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n        `sliding_window_prediction_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n        `sliding_window_step` | Available only for time-based datasets. Number of times to move by after each window.\n        `train_workers` | Specifies the number of workers to use for loading train data. Applied when `workers` = \"config\".\n        `train_dataloader_order` | Available only for series-based datasets. Whether to load train data in sequential or random order.\n        `random_state` | Seed for loading train data in random order.\n\n        Parameters:\n            workers: The number of workers to use for loading train data. `Default: \"config\"` \n            ts_id: Specifies time series to take. If None returns all time series as normal. `Default: \"None\"`\n\n        Returns:\n            An iterable `DataLoader` containing data from training set.          \n        \"\"\"\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access train_dataloader.\")\n\n        if not self.dataset_config.has_train():\n            raise ValueError(\"Dataloader for training set is not available in the dataset configuration.\")\n\n        assert self.train_dataset is not None, \"The train_dataset must be initialized before accessing data from training set.\"\n\n        default_kwargs = {'take_all': False, \"cache_loader\": True}\n        kwargs = {**default_kwargs, **kwargs}\n\n        if ts_id is not None:\n\n            if ts_id == self.dataset_config.used_singular_train_time_series and self.train_dataloader is not None:\n                self.logger.debug(\"Returning cached train_dataloader.\")\n                return self.train_dataloader\n\n            dataset = self._get_singular_time_series_dataset(self.train_dataset, ts_id)\n            self.dataset_config.used_singular_train_time_series = ts_id\n            if self.train_dataloader:\n                del self.train_dataloader\n                self.train_dataloader = None\n                self.logger.info(\"Destroyed previous cached train_dataloader.\")\n\n            self.dataset_config.used_train_workers = 0\n            self.train_dataloader = self.dataloader_factory.create_dataloader(dataset, self.dataset_config, 0, False, self.dataset_config.train_batch_size)\n            self.logger.info(\"Created new cached train_dataloader.\")\n            return self.train_dataloader\n        elif self.dataset_config.used_singular_train_time_series is not None and self.train_dataloader is not None:\n            del self.train_dataloader\n            self.train_dataloader = None\n            self.dataset_config.used_singular_train_time_series = None\n            self.logger.info(\"Destroyed previous cached train_dataloader.\")\n\n        if workers == \"config\":\n            workers = self.dataset_config.train_workers\n\n        # If the dataloader is cached and number of used workers did not change, return the cached dataloader\n        if self.train_dataloader and kwargs[\"cache_loader\"] and workers == self.dataset_config.used_train_workers:\n            self.logger.debug(\"Returning cached train_dataloader.\")\n            return self.train_dataloader\n\n        # Update the used workers count\n        self.dataset_config.used_train_workers = workers\n\n        # If there's a previously cached dataloader, destroy it\n        if self.train_dataloader:\n            del self.train_dataloader\n            self.train_dataloader = None\n            self.logger.info(\"Destroyed previous cached train_dataloader.\")\n\n        # If caching is enabled, create a new cached dataloader\n        if kwargs[\"cache_loader\"]:\n            self.train_dataloader = self.dataloader_factory.create_dataloader(self.train_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.train_batch_size, order=self.dataset_config.train_dataloader_order)\n            self.logger.info(\"Created new cached train_dataloader.\")\n            return self.train_dataloader\n\n        # If caching is disabled, create a new uncached dataloader\n        self.logger.debug(\"Created new uncached train_dataloader.\")\n        return self.dataloader_factory.create_dataloader(self.train_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.train_batch_size, order=self.dataset_config.train_dataloader_order)\n\n    def get_val_dataloader(self, ts_id: int | None = None, workers: int | Literal[\"config\"] = \"config\", **kwargs) -&gt; DataLoader:\n        \"\"\"\n        Returns a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for validation set.\n\n        The `DataLoader` is created on the first call and cached for subsequent use. &lt;br/&gt;\n        The cached dataloader is cleared when either [`get_val_df`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_val_df) or [`get_val_numpy`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_val_numpy) is called.\n\n        The structure of the returned batch depends on the `time_format` and whether `sliding_window_size` is used:\n\n        - When `sliding_window_size` is used:\n            - With `time_format` == TimeFormat.DATETIME and included time:\n                - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n                - `np.ndarray` of shape `(num_time_series, 1, features)`\n                - `np.ndarray` of times with shape `(times - 1)`\n                - `np.ndarray` of time with shape `(1)`\n            - When `time_format` != TimeFormat.DATETIME or time is not included:\n                - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n                - `np.ndarray` of shape `(num_time_series, 1, features)`\n        - When `sliding_window_size` is not used:\n            - With `time_format` == TimeFormat.DATETIME and included time:\n                - `np.ndarray` of shape `(num_time_series, times, features)`\n                - `np.ndarray` of time with shape `(times)`\n            - When `time_format` != TimeFormat.DATETIME or time is not included:\n                - `np.ndarray` of shape `(num_time_series, times, features)`\n\n        The `DataLoader` is configured with the following config attributes:\n\n        Dataset config | Description\n        -------------- | -----------\n        `val_batch_size` | Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n        `sliding_window_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n        `sliding_window_prediction_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n        `sliding_window_step` | Available only for time-based datasets. Number of times to move by after each window.\n        `val_workers` | Specifies the number of workers to use for loading validation data. Applied when `workers` = \"config\".\n\n\n        Parameters:\n            workers: The number of workers to use for loading validation data. `Default: \"config\"`  \n            ts_id: Specifies time series to take. If None returns all time series as normal. `Default: \"None\"`\n\n        Returns:\n            An iterable `DataLoader` containing data from validation set.        \n        \"\"\"\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access val_dataloader.\")\n\n        if not self.dataset_config.has_val():\n            raise ValueError(\"Dataloader for validation set is not available in the dataset configuration.\")\n\n        assert self.val_dataset is not None, \"The val_dataset must be initialized before accessing data from validation set.\"\n\n        default_kwargs = {'take_all': False, \"cache_loader\": True}\n        kwargs = {**default_kwargs, **kwargs}\n\n        if ts_id is not None:\n\n            if ts_id == self.dataset_config.used_singular_val_time_series and self.val_dataloader is not None:\n                self.logger.debug(\"Returning cached val_dataloader.\")\n                return self.val_dataloader\n\n            dataset = self._get_singular_time_series_dataset(self.val_dataset, ts_id)\n            self.dataset_config.used_singular_val_time_series = ts_id\n            if self.val_dataloader:\n                del self.val_dataloader\n                self.val_dataloader = None\n                self.logger.info(\"Destroyed previous cached val_dataloader.\")\n\n            self.dataset_config.used_val_workers = 0\n            self.val_dataloader = self.dataloader_factory.create_dataloader(dataset, self.dataset_config, 0, False, self.dataset_config.val_batch_size)\n            self.logger.info(\"Created new cached val_dataloader.\")\n            return self.val_dataloader\n        elif self.dataset_config.used_singular_val_time_series is not None and self.val_dataloader is not None:\n            del self.val_dataloader\n            self.val_dataloader = None\n            self.dataset_config.used_singular_val_time_series = None\n            self.logger.info(\"Destroyed previous cached val_dataloader.\")\n\n        if workers == \"config\":\n            workers = self.dataset_config.val_workers\n\n        # If the dataloader is cached and number of used workers did not change, return the cached dataloader\n        if self.val_dataloader and kwargs[\"cache_loader\"] and workers == self.dataset_config.used_val_workers:\n            self.logger.debug(\"Returning cached val_dataloader.\")\n            return self.val_dataloader\n\n        # Update the used workers count\n        self.dataset_config.used_val_workers = workers\n\n        # If there's a previously cached dataloader, destroy it\n        if self.val_dataloader:\n            del self.val_dataloader\n            self.val_dataloader = None\n            self.logger.info(\"Destroyed previous cached val_dataloader.\")\n\n        # If caching is enabled, create a new cached dataloader\n        if kwargs[\"cache_loader\"]:\n            self.val_dataloader = self.dataloader_factory.create_dataloader(self.val_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.val_batch_size)\n            self.logger.info(\"Created new cached val_dataloader.\")\n            return self.val_dataloader\n\n        # If caching is disabled, create a new uncached dataloader\n        self.logger.debug(\"Created new uncached val_dataloader.\")\n        return self.dataloader_factory.create_dataloader(self.val_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.val_batch_size)\n\n    def get_test_dataloader(self, ts_id: int | None = None, workers: int | Literal[\"config\"] = \"config\", **kwargs) -&gt; DataLoader:\n        \"\"\"\n        Returns a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for test set.\n\n        The `DataLoader` is created on the first call and cached for subsequent use. &lt;br/&gt;\n        The cached dataloader is cleared when either [`get_test_df`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_test_df) or [`get_test_numpy`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_test_numpy) is called.\n\n        The structure of the returned batch depends on the `time_format` and whether `sliding_window_size` is used:\n\n        - When `sliding_window_size` is used:\n            - With `time_format` == TimeFormat.DATETIME and included time:\n                - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n                - `np.ndarray` of shape `(num_time_series, 1, features)`\n                - `np.ndarray` of times with shape `(times - 1)`\n                - `np.ndarray` of time with shape `(1)`\n            - When `time_format` != TimeFormat.DATETIME or time is not included:\n                - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n                - `np.ndarray` of shape `(num_time_series, 1, features)`\n        - When `sliding_window_size` is not used:\n            - With `time_format` == TimeFormat.DATETIME and included time:\n                - `np.ndarray` of shape `(num_time_series, times, features)`\n                - `np.ndarray` of time with shape `(times)`\n            - When `time_format` != TimeFormat.DATETIME or time is not included:\n                - `np.ndarray` of shape `(num_time_series, times, features)`\n\n        The `DataLoader` is configured with the following config attributes:\n\n        Dataset config | Description\n        -------------- | -----------\n        `test_batch_size` | Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n        `sliding_window_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n        `sliding_window_prediction_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n        `sliding_window_step` | Available only for time-based datasets. Number of times to move by after each window.\n        `test_workers` | Specifies the number of workers to use for loading test data. Applied when `workers` = \"config\".\n\n\n        Parameters:\n            workers: The number of workers to use for loading test data. `Default: \"config\"`  \n            ts_id: Specifies time series to take. If None returns all time series as normal. `Default: \"None\"`\n\n        Returns:\n            An iterable `DataLoader` containing data from test set.        \n        \"\"\"\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access test_dataloader.\")\n\n        if not self.dataset_config.has_test():\n            raise ValueError(\"Dataloader for test set is not available in the dataset configuration.\")\n\n        assert self.test_dataset is not None, \"The test_dataset must be initialized before accessing data from test set.\"\n\n        default_kwargs = {'take_all': False, \"cache_loader\": True}\n        kwargs = {**default_kwargs, **kwargs}\n\n        if ts_id is not None:\n\n            if ts_id == self.dataset_config.used_singular_test_time_series and self.test_dataloader is not None:\n                self.logger.debug(\"Returning cached test_dataloader.\")\n                return self.test_dataloader\n\n            dataset = self._get_singular_time_series_dataset(self.test_dataset, ts_id)\n            self.dataset_config.used_singular_test_time_series = ts_id\n            if self.test_dataloader:\n                del self.test_dataloader\n                self.test_dataloader = None\n                self.logger.info(\"Destroyed previous cached test_dataloader.\")\n\n            self.dataset_config.used_test_workers = 0\n            self.test_dataloader = self.dataloader_factory.create_dataloader(dataset, self.dataset_config, 0, False, self.dataset_config.test_batch_size)\n            self.logger.info(\"Created new cached test_dataloader.\")\n            return self.test_dataloader\n        elif self.dataset_config.used_singular_test_time_series is not None and self.test_dataloader is not None:\n            del self.test_dataloader\n            self.test_dataloader = None\n            self.dataset_config.used_singular_test_time_series = None\n            self.logger.info(\"Destroyed previous cached test_dataloader.\")\n\n        if workers == \"config\":\n            workers = self.dataset_config.test_workers\n\n        # If the dataloader is cached and number of used workers did not change, return the cached dataloader\n        if self.test_dataloader and kwargs[\"cache_loader\"] and workers == self.dataset_config.used_test_workers:\n            self.logger.debug(\"Returning cached test_dataloader.\")\n            return self.test_dataloader\n\n        # Update the used workers count\n        self.dataset_config.used_test_workers = workers\n\n        # If there's a previously cached dataloader, destroy it\n        if self.test_dataloader:\n            del self.test_dataloader\n            self.test_dataloader = None\n            self.logger.info(\"Destroyed previous cached test_dataloader.\")\n\n        # If caching is enabled, create a new cached dataloader\n        if kwargs[\"cache_loader\"]:\n            self.test_dataloader = self.dataloader_factory.create_dataloader(self.test_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.test_batch_size)\n            self.logger.info(\"Created new cached test_dataloader.\")\n            return self.test_dataloader\n\n        # If caching is disabled, create a new uncached dataloader\n        self.logger.debug(\"Created new uncached test_dataloader.\")\n        return self.dataloader_factory.create_dataloader(self.test_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.test_batch_size)\n\n    def get_all_dataloader(self, ts_id: int | None = None, workers: int | Literal[\"config\"] = \"config\", **kwargs) -&gt; DataLoader:\n        \"\"\"\n        Returns a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for all set.\n\n        The `DataLoader` is created on the first call and cached for subsequent use. &lt;br/&gt;\n        The cached dataloader is cleared when either [`get_all_df`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_all_df) or [`get_all_numpy`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_all_numpy) is called.\n\n        The structure of the returned batch depends on the `time_format` and whether `sliding_window_size` is used:\n\n        - When `sliding_window_size` is used:\n            - With `time_format` == TimeFormat.DATETIME and included time:\n                - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n                - `np.ndarray` of shape `(num_time_series, 1, features)`\n                - `np.ndarray` of times with shape `(times - 1)`\n                - `np.ndarray` of time with shape `(1)`\n            - When `time_format` != TimeFormat.DATETIME or time is not included:\n                - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n                - `np.ndarray` of shape `(num_time_series, 1, features)`\n        - When `sliding_window_size` is not used:\n            - With `time_format` == TimeFormat.DATETIME and included time:\n                - `np.ndarray` of shape `(num_time_series, times, features)`\n                - `np.ndarray` of time with shape `(times)`\n            - When `time_format` != TimeFormat.DATETIME or time is not included:\n                - `np.ndarray` of shape `(num_time_series, times, features)`\n\n        The `DataLoader` is configured with the following config attributes:\n\n        Dataset config | Description\n        -------------- | -----------\n        `all_batch_size` | Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n        `sliding_window_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n        `sliding_window_prediction_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n        `sliding_window_step` | Available only for time-based datasets. Number of times to move by after each window.\n        `all_workers` | Specifies the number of workers to use for loading all data. Applied when `workers` = \"config\".\n\n\n        Parameters:\n            workers: The number of workers to use for loading all data. `Default: \"config\"`  \n            ts_id: Specifies time series to take. If None returns all time series as normal. `Default: \"None\"`\n\n        Returns:\n            An iterable `DataLoader` containing data from all set.       \n        \"\"\"\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access all_dataloader.\")\n\n        if not self.dataset_config.has_all():\n            raise ValueError(\"Dataloader for all set is not available in the dataset configuration.\")\n\n        assert self.all_dataset is not None, \"The all_dataset must be initialized before accessing data from all set.\"\n\n        default_kwargs = {'take_all': False, \"cache_loader\": True}\n        kwargs = {**default_kwargs, **kwargs}\n\n        if ts_id is not None:\n\n            if ts_id == self.dataset_config.used_singular_all_time_series and self.all_dataloader is not None:\n                self.logger.debug(\"Returning cached all_dataloader.\")\n                return self.all_dataloader\n\n            dataset = self._get_singular_time_series_dataset(self.all_dataset, ts_id)\n            self.dataset_config.used_singular_all_time_series = ts_id\n            if self.all_dataloader:\n                del self.all_dataloader\n                self.all_dataloader = None\n                self.logger.info(\"Destroyed previous cached all_dataloader.\")\n\n            self.dataset_config.used_all_workers = 0\n            self.all_dataloader = self.dataloader_factory.create_dataloader(dataset, self.dataset_config, 0, False, self.dataset_config.all_batch_size)\n            self.logger.info(\"Created new cached all_dataloader.\")\n            return self.all_dataloader\n        elif self.dataset_config.used_singular_all_time_series is not None and self.all_dataloader is not None:\n            del self.all_dataloader\n            self.all_dataloader = None\n            self.dataset_config.used_singular_all_time_series = None\n            self.logger.info(\"Destroyed previous cached all_dataloader.\")\n\n        if workers == \"config\":\n            workers = self.dataset_config.all_workers\n\n        # If the dataloader is cached and number of used workers did not change, return the cached dataloader\n        if self.all_dataloader and kwargs[\"cache_loader\"] and workers == self.dataset_config.used_all_workers:\n            self.logger.debug(\"Returning cached all_dataloader.\")\n            return self.all_dataloader\n\n        # Update the used workers count\n        self.dataset_config.used_all_workers = workers\n\n        # If there's a previously cached dataloader, destroy it\n        if self.all_dataloader:\n            del self.all_dataloader\n            self.all_dataloader = None\n            self.logger.info(\"Destroyed previous cached all_dataloader.\")\n\n        # If caching is enabled, create a new cached dataloader\n        if kwargs[\"cache_loader\"]:\n            self.all_dataloader = self.dataloader_factory.create_dataloader(self.all_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.all_batch_size)\n            self.logger.info(\"Created new cached all_dataloader.\")\n            return self.all_dataloader\n\n        # If caching is disabled, create a new uncached dataloader\n        self.logger.debug(\"Creating new uncached all_dataloader.\")\n        return self.dataloader_factory.create_dataloader(self.all_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.all_batch_size)\n\n    def get_train_df(self, workers: int | Literal[\"config\"] = \"config\", as_single_dataframe: bool = True) -&gt; pd.DataFrame:\n        \"\"\"\n        Creates a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) containing all the data from training set grouped by time series.\n\n        This method uses the `train_dataloader` with a batch size set to the total number of data in the training set. The cached `train_dataloader` is cleared during this operation.\n\n        !!! warning \"Memory usage\"\n            The entire training set is loaded into memory, which may lead to high memory usage. If working with large training set, consider using `get_train_dataloader` instead to handle data in batches.\n\n        Parameters:\n            workers: The number of workers to use for loading train data. `Default: \"config\"`  \n            as_single_dataframe: Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. `Default: True` \n\n        Returns:\n            A single Pandas DataFrame containing all data from training set, or a list of DataFrames (one per time series).\n        \"\"\"\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access train_dataloader.\")\n\n        if not self.dataset_config.has_train():\n            raise ValueError(\"Dataloader for training set is not available in the dataset configuration.\")\n\n        assert self.train_dataset is not None, \"The train_dataset must be initialized before accessing data from training set.\"\n\n        ts_ids, time_period = self.dataset_config._get_train()\n\n        should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n        dataloader = self.get_train_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n        return self._get_df(dataloader, as_single_dataframe, ts_ids, time_period)\n\n    def get_val_df(self, workers: int | Literal[\"config\"] = \"config\", as_single_dataframe: bool = True) -&gt; pd.DataFrame:\n        \"\"\"\n        Create a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) containing all the data from validation set grouped by time series.\n\n        This method uses the `val_dataloader` with a batch size set to the total number of data in the validation set. The cached `val_dataloader` is cleared during this operation.\n\n        !!! warning \"Memory usage\"\n            The entire validation set is loaded into memory, which may lead to high memory usage. If working with large validation set, consider using `get_val_dataloader` instead to handle data in batches.\n\n        Parameters:\n            workers: The number of workers to use for loading validation data. `Default: \"config\"`  \n            as_single_dataframe: Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. `Default: True` \n\n        Returns:\n            A single Pandas DataFrame containing all data from validation set, or a list of DataFrames (one per time series).\n        \"\"\"\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access val_dataloader.\")\n\n        if not self.dataset_config.has_val():\n            raise ValueError(\"Dataloader for validation set is not available in the dataset configuration.\")\n\n        assert self.val_dataset is not None, \"The val_dataset must be initialized before accessing data from validation set.\"\n\n        ts_ids, time_period = self.dataset_config._get_val()\n\n        should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n        dataloader = self.get_val_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n        return self._get_df(dataloader, as_single_dataframe, ts_ids, time_period)\n\n    def get_test_df(self, workers: int | Literal[\"config\"] = \"config\", as_single_dataframe: bool = True) -&gt; pd.DataFrame:\n        \"\"\"\n        Creates a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) containing all the data from test set grouped by time series.\n\n        This method uses the `test_dataloader` with a batch size set to the total number of data in the test set. The cached `test_dataloader` is cleared during this operation.\n\n        !!! warning \"Memory usage\"\n            The entire test set is loaded into memory, which may lead to high memory usage. If working with large test set, consider using `get_test_dataloader` instead to handle data in batches.\n\n        Parameters:\n            workers: The number of workers to use for loading test data. `Default: \"config\"`  \n            as_single_dataframe: Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. `Default: True` \n\n        Returns:\n            A single Pandas DataFrame containing all data from test set, or a list of DataFrames (one per time series).\n        \"\"\"\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access test_dataloader.\")\n\n        if not self.dataset_config.has_test():\n            raise ValueError(\"Dataloader for test set is not available in the dataset configuration.\")\n\n        assert self.test_dataset is not None, \"The test_dataset must be initialized before accessing data from test set.\"\n\n        ts_ids, time_period = self.dataset_config._get_test()\n\n        should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n        dataloader = self.get_test_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n        return self._get_df(dataloader, as_single_dataframe, ts_ids, time_period)\n\n    def get_all_df(self, workers: int | Literal[\"config\"] = \"config\", as_single_dataframe: bool = True) -&gt; pd.DataFrame:\n        \"\"\"\n        Creates a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) containing all the data from all set grouped by time series.\n\n        This method uses the `all_dataloader` with a batch size set to the total number of data in the all set. The cached `all_dataloader` is cleared during this operation.\n\n        !!! warning \"Memory usage\"\n            The entire all set is loaded into memory, which may lead to high memory usage. If working with large all set, consider using `get_all_dataloader` instead to handle data in batches.\n\n        Parameters:\n            workers: The number of workers to use for loading all data. `Default: \"config\"`  \n            as_single_dataframe: Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. `Default: True` \n\n        Returns:\n            A single Pandas DataFrame containing all data from all set, or a list of DataFrames (one per time series).\n        \"\"\"\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access all_dataloader.\")\n\n        if not self.dataset_config.has_all():\n            raise ValueError(\"Dataloader for all set is not available in the dataset configuration.\")\n\n        assert self.all_dataset is not None, \"The all_dataset must be initialized before accessing data from all set.\"\n\n        ts_ids, time_period = self.dataset_config._get_all()\n\n        should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n        dataloader = self.get_all_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n        return self._get_df(dataloader, as_single_dataframe, ts_ids, time_period)\n\n    def get_train_numpy(self, workers: int | Literal[\"config\"] = \"config\") -&gt; np.ndarray:\n        \"\"\"\n        Creates a NumPy array containing all the data from training set grouped by time series, with the shape `(num_time_series, num_times, num_features)`.\n\n        This method uses the `train_dataloader` with a batch size set to the total number of data in the training set. The cached `train_dataloader` is cleared during this operation.\n\n        !!! warning \"Memory usage\"\n            The entire training set is loaded into memory, which may lead to high memory usage. If working with large training set, consider using `get_train_dataloader` instead to handle data in batches.        \n\n        Parameters:\n            workers: The number of workers to use for loading train data. `Default: \"config\"`  \n\n        Returns:\n            A NumPy array containing all the data in training set with the shape `(num_time_series, num_times, num_features)`.\n        \"\"\"\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access train_dataloader.\")\n\n        if not self.dataset_config.has_train():\n            raise ValueError(\"Dataloader for training set is not available in the dataset configuration.\")\n\n        assert self.train_dataset is not None, \"The train_dataset must be initialized before accessing data from training set.\"\n\n        ts_ids, time_period = self.dataset_config._get_train()\n\n        should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n        dataloader = self.get_train_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n        return self._get_numpy(dataloader, ts_ids, time_period)\n\n    def get_val_numpy(self, workers: int | Literal[\"config\"] = \"config\") -&gt; np.ndarray:\n        \"\"\"\n        Creates a NumPy array containing all the data from validation set grouped by time series, with the shape `(num_time_series, num_times, num_features)`.\n\n        This method uses the `val_dataloader` with a batch size set to the total number of data in the validation set. The cached `val_dataloader` is cleared during this operation.\n\n        !!! warning \"Memory usage\"\n            The entire validation set is loaded into memory, which may lead to high memory usage. If working with large validation set, consider using `get_val_dataloader` instead to handle data in batches.        \n\n        Parameters:\n            workers: The number of workers to use for loading validation data. `Default: \"config\"`  \n\n        Returns:\n            A NumPy array containing all the data in validation set with the shape `(num_time_series, num_times, num_features)`.\n        \"\"\"\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access val_dataloader.\")\n\n        if not self.dataset_config.has_val():\n            raise ValueError(\"Dataloader for validation set is not available in the dataset configuration.\")\n\n        assert self.val_dataset is not None, \"The val_dataset must be initialized before accessing data from validation set.\"\n\n        ts_ids, time_period = self.dataset_config._get_val()\n\n        should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n        dataloader = self.get_val_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n        return self._get_numpy(dataloader, ts_ids, time_period)\n\n    def get_test_numpy(self, workers: int | Literal[\"config\"] = \"config\") -&gt; np.ndarray:\n        \"\"\"\n        Creates a NumPy array containing all the data from test set grouped by time series, with the shape `(num_time_series, num_times, num_features)`.\n\n        This method uses the `test_dataloader` with a batch size set to the total number of data in the test set. The cached `test_dataloader` is cleared during this operation.\n\n        !!! warning \"Memory usage\"\n            The entire test set is loaded into memory, which may lead to high memory usage. If working with large test set, consider using `get_test_dataloader` instead to handle data in batches.        \n\n        Parameters:\n            workers: The number of workers to use for loading test data. `Default: \"config\"`  \n\n        Returns:\n            A NumPy array containing all the data in test set with the shape `(num_time_series, num_times, num_features)`.\n        \"\"\"\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access test_dataloader.\")\n\n        if not self.dataset_config.has_test():\n            raise ValueError(\"Dataloader for test set is not available in the dataset configuration.\")\n\n        assert self.test_dataset is not None, \"The test_dataset must be initialized before accessing data from test set.\"\n\n        ts_ids, time_period = self.dataset_config._get_test()\n\n        should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n        dataloader = self.get_test_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n        return self._get_numpy(dataloader, ts_ids, time_period)\n\n    def get_all_numpy(self, workers: int | Literal[\"config\"] = \"config\") -&gt; np.ndarray:\n        \"\"\"\n        Creates a NumPy array containing all the data from all set grouped by time series, with the shape `(num_time_series, num_times, num_features)`.\n\n        This method uses the `all_dataloader` with a batch size set to the total number of data in the all set. The cached `all_dataloader` is cleared during this operation.\n\n        !!! warning \"Memory usage\"\n            The entire all set is loaded into memory, which may lead to high memory usage. If working with large all set, consider using `get_all_dataloader` instead to handle data in batches.        \n\n        Parameters:\n            workers: The number of workers to use for loading all data. `Default: \"config\"`  \n\n        Returns:\n            A NumPy array containing all the data in all set with the shape `(num_time_series, num_times, num_features)`.\n        \"\"\"\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access all_dataloader.\")\n\n        if not self.dataset_config.has_all():\n            raise ValueError(\"Dataloader for all set is not available in the dataset configuration.\")\n\n        assert self.all_dataset is not None, \"The all_dataset must be initialized before accessing data from all set.\"\n\n        ts_ids, time_period = self.dataset_config._get_all()\n\n        should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n        dataloader = self.get_all_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n        return self._get_numpy(dataloader, ts_ids, time_period)\n\n    def _update_dataset_config_and_initialize(self, config_editor: ConfigEditor, workers: int | Literal[\"config\"] = \"config\", display_config_details: Optional[Literal[\"test\", \"diagram\"]] = None):\n        \"\"\"Updates config via passed config editor. \"\"\"\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating dataset configuration.\")\n\n        if display_config_details is not None:\n            display_config_details = DisplayType(display_config_details)\n\n        original_config = deepcopy(self.dataset_config)\n        original_export_config = deepcopy(self._export_config_copy)\n\n        try:\n            if config_editor.requires_init:\n                self.logger.info(\"Re-initialization is required.\")\n                config_editor.modify_dataset_config(self._export_config_copy, self.metadata)\n                self.set_dataset_config_and_initialize(self._export_config_copy, None, workers)\n\n            else:\n                config_editor.modify_dataset_config(self.dataset_config, self.metadata)\n\n        except Exception:\n            self.dataset_config = original_config\n            self._export_config_copy = original_export_config\n            self.logger.error(\"Error occured, reverting changes.\")\n            raise\n\n        if self.train_dataloader is not None:\n            del self.train_dataloader\n            self.train_dataloader = None\n            self.logger.info(\"Destroyed cached train_dataloader.\")\n\n        if self.val_dataloader is not None:\n            del self.val_dataloader\n            self.val_dataloader = None\n            self.logger.info(\"Destroyed cached val_dataloader.\")\n\n        if self.test_dataloader is not None:\n            del self.test_dataloader\n            self.test_dataloader = None\n            self.logger.info(\"Destroyed cached test_dataloader.\")\n\n        if self.all_dataloader is not None:\n            del self.all_dataloader\n            self.all_dataloader = None\n            self.logger.info(\"Destroyed cached all_dataloader.\")\n\n        self._update_config_imported_status(None)\n        self._update_export_config_copy()\n\n        self.logger.info(\"Configuration has been changed successfuly.\")\n\n        if display_config_details is not None:\n            self.summary(display_config_details)\n\n    @abstractmethod\n    def update_dataset_config_and_initialize(self, **kwargs):\n        \"\"\"Used to modify selected configurations set in config.\"\"\"\n        ...\n\n    def apply_filler(self, fill_missing_with: type | FillerType | Literal[\"mean_filler\", \"forward_filler\", \"linear_interpolation_filler\"] | None, workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n        \"\"\"Used for updating filler set in config.\n        Set parameter to `config` to keep it as it is config.\n        If exception is thrown during set, no changes are made.\n\n        Affects following configuration: \n\n        Dataset config | Description\n        -------------- | -----------\n        `fill_missing_with` | Defines how to fill missing values in the dataset.\n\n        Parameters:\n            fill_missing_with: Defines how to fill missing values in the dataset. `Defaults: config`.  \n            workers: How many workers to use when setting new filler. `Defaults: config`.      \n        \"\"\"\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating filler.\")\n\n        self.update_dataset_config_and_initialize(fill_missing_with=fill_missing_with, workers=workers)\n        self.logger.info(\"Filler has been changed successfuly.\")\n\n    def apply_anomaly_handler(self, handle_anomalies_with: type | AnomalyHandlerType | Literal[\"z-score\", \"interquartile_range\"] | None | Literal[\"config\"], workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n        \"\"\"Used for updating anomaly handler set in config.\n\n        Set parameter to `config` to keep it as it is config.\n\n        If exception is thrown during set, no changes are made.\n\n        Affects following configuration:\n\n        Dataset config | Description\n        -------------- | -----------\n        `handle_anomalies_with` | Defines the anomaly handler to handle anomalies in the dataset.\n\n        Parameters:\n            handle_anomalies_with: Defines the anomaly handler to handle anomalies in the dataset. `Defaults: config`.  \n            workers: How many workers to use when setting new filler. `Defaults: config`.      \n        \"\"\"\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating anomaly handler.\")\n\n        self.update_dataset_config_and_initialize(handle_anomalies_with=handle_anomalies_with, workers=workers)\n        self.logger.info(\"Anomaly handler has been changed successfuly.\")\n\n    def apply_transformer(self, transform_with: type | list[Transformer] | np.ndarray[Transformer] | TransformerType | Transformer | Literal[\"min_max_scaler\", \"standard_scaler\", \"max_abs_scaler\", \"log_transformer\", \"robust_scaler\", \"power_transformer\", \"quantile_transformer\", \"l2_normalizer\"] | None | Literal[\"config\"] = \"config\",\n                          create_transformer_per_time_series: bool | Literal[\"config\"] = \"config\", partial_fit_initialized_transformers: bool | Literal[\"config\"] = \"config\", workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n        \"\"\"Used for updating transformer and relevenat configurations set in config.\n        Set parameter to `config` to keep it as it is config.\n        If exception is thrown during set, no changes are made.\n\n        Affects following configuration:\n\n        Dataset config | Description\n        -------------- | -----------\n        `transform_with` | Defines the transformer to transform the dataset.\n        `create_transformer_per_time_series` | If `True`, a separate transformer is created for each time series. Not used when using already initialized transformers.\n        `partial_fit_initialized_transformers` | If `True`, partial fitting on train set is performed when using initialized transformers.\n\n        Parameters:\n            transform_with: Defines the transformer to transform the dataset. `Defaults: config`.  \n            create_transformer_per_time_series: If `True`, a separate transformer is created for each time series. Not used when using already initialized transformers. `Defaults: config`.  \n            partial_fit_initialized_transformers: If `True`, partial fitting on train set is performed when using initiliazed transformers. `Defaults: config`.  \n            workers: How many workers to use when setting new transformer. `Defaults: config`.      \n        \"\"\"\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating transformer values.\")\n\n        self.update_dataset_config_and_initialize(transform_with=transform_with, create_transformer_per_time_series=create_transformer_per_time_series, partial_fit_initialized_transformers=partial_fit_initialized_transformers, workers=workers)\n        self.logger.info(\"Transformer configuration has been changed successfuly.\")\n\n    def set_default_values(self, default_values: list[Number] | npt.NDArray[np.number] | dict[str, Number] | Number | Literal[\"default\"] | None, workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n        \"\"\"Used for updating default values set in config.\n        Set parameter to `config` to keep it as it is config.\n        If exception is thrown during set, no changes are made.\n\n        Affects following configuration:\n\n        Dataset config | Description\n        -------------- | -----------\n        `default_values` | Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature.\n\n        Parameters:\n            default_values: Default values for missing data, applied before fillers. `Defaults: config`.  \n            workers: How many workers to use when setting new default values. `Defaults: config`.      \n        \"\"\"\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating default values.\")\n\n        self.update_dataset_config_and_initialize(default_values=default_values, workers=workers)\n        self.logger.info(\"Default values has been changed successfuly.\")\n\n    def set_preprocess_order(self, preprocess_order: list[str, type] | Literal[\"config\"] = \"config\", workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n        \"\"\"Used for updating preprocess_order set in config.\n        Set parameter to `config` to keep it as it is config.\n        If exception is thrown during set, no changes are made.\n\n        Affects following configuration: \n\n        Dataset config | Description\n        -------------- | -----------\n        `preprocess_order` | Used order of when preprocesses are applied. Can be also used to add/remove custom handlers.\n\n        Parameters:\n            preprocess_order: Used order of when preprocesses are applied. Can be also used to add/remove custom handlers. `Defaults: config`.  \n            workers: How many workers to use when setting new default values. `Defaults: config`.      \n        \"\"\"\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating preprocess order.\")\n\n        self.update_dataset_config_and_initialize(preprocess_order=preprocess_order, workers=workers)\n        self.logger.info(\"Preprocess order has been changed successfuly.\")\n\n    def set_workers(self, train_workers: int | Literal[\"config\"] = \"config\", val_workers: int | Literal[\"config\"] = \"config\",\n                    test_workers: int | Literal[\"config\"] = \"config\", all_workers: int | Literal[\"config\"] = \"config\", init_workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n        \"\"\"Used for updating workers set in config.\n        Set parameter to `config` to keep it as it is config.\n        If exception is thrown during set, no changes are made.\n\n        Affects following configuration:\n\n        Dataset config | Description\n        -------------- | -----------\n        `train_workers` | Number of workers for loading training data.\n        `val_workers` | Number of workers for loading validation data.\n        `test_workers` | Number of workers for loading test data.\n        `all_workers` | Number of workers for loading all data.\n        `init_workers` | Number of workers for dataset configuration.\n\n        Parameters:\n            train_workers: Number of workers for loading training data. `Defaults: config`.\n            val_workers: Number of workers for loading validation data. `Defaults: config`.\n            test_workers: Number of workers for loading test data. `Defaults: config`.\n            all_workers: Number of workers for loading all data.  `Defaults: config`.\n            init_workers: Number of workers for dataset configuration. `Defaults: config`.            \n        \"\"\"\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating workers.\")\n\n        self.update_dataset_config_and_initialize(train_workers=train_workers, val_workers=val_workers, test_workers=test_workers, all_workers=all_workers, init_workers=init_workers, workers=\"config\")\n        self.logger.info(\"Workers has been changed successfuly.\")\n\n    def set_batch_sizes(self, train_batch_size: int | Literal[\"config\"] = \"config\", val_batch_size: int | Literal[\"config\"] = \"config\",\n                        test_batch_size: int | Literal[\"config\"] = \"config\", all_batch_size: int | Literal[\"config\"] = \"config\") -&gt; None:\n        \"\"\"Used for updating batch sizes set in config.\n        Set parameter to `config` to keep it as it is config.\n        If exception is thrown during set, no changes are made.\n\n        Affects following configuration:\n\n        Dataset config | Description\n        -------------- | -----------\n        `train_batch_size` | Number of samples per batch for train set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n        `val_batch_size` | Number of samples per batch for val set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n        `test_batch_size` | Number of samples per batch for test set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n        `all_batch_size` | Number of samples per batch for all set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n\n        Parameters:\n            train_batch_size: Number of samples per batch for train set. `Defaults: config`.\n            val_batch_size: Number of samples per batch for val set. `Defaults: config`.\n            test_batch_size: Number of samples per batch for test set. `Defaults: config`.\n            all_batch_size: Number of samples per batch for all set. `Defaults: config`.\n        \"\"\"\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating batch sizes.\")\n\n        self.update_dataset_config_and_initialize(train_batch_size=train_batch_size, val_batch_size=val_batch_size, test_batch_size=test_batch_size, all_batch_size=all_batch_size, workers=\"config\")\n        self.logger.info(\"Batch sizes has been changed successfuly.\")\n\n    def display_dataset_details(self) -&gt; None:\n        \"\"\"Display information about the contents of the dataset.  \"\"\"\n\n        to_display = f'''\nDataset details:\n\n    {self.metadata.aggregation}\n        Time indices: {range(self.metadata.time_indices[ID_TIME_COLUMN_NAME][0], self.metadata.time_indices[ID_TIME_COLUMN_NAME][-1])}\n        Datetime: {(datetime.fromtimestamp(self.metadata.time_indices['time'][0], tz=timezone.utc), datetime.fromtimestamp(self.metadata.time_indices['time'][-1], timezone.utc))}\n\n    {self.metadata.source_type}\n        Time series indices: {get_abbreviated_list_string(self.metadata.ts_indices[self.metadata.ts_id_name])}; use 'get_available_ts_indices' for full list\n        Features with default values: {self.metadata.default_values}\n\n        Additional data: {list(self.metadata.additional_data.keys())}\n        '''\n\n        print(to_display)\n\n    def summary(self, display_type: Literal[\"text\", \"diagram\"]) -&gt; None:\n        \"\"\"Used to display used configurations. Can be displayed as interactive html diagram or text summary.\n\n        Parameters:\n            display_type: Whether configuration should be display as diagram or text summary.\n        \"\"\"\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to display summary.\")\n\n        display_type = DisplayType(display_type)\n\n        if display_type == DisplayType.TEXT:\n            print(self.dataset_config)\n        elif display_type == DisplayType.DIAGRAM:\n            steps = self.dataset_config._get_summary_steps()\n            return css_utils.display_summary_diagram(steps)\n        else:\n            raise NotImplementedError()\n\n    def save_summary_diagram_as_html(self, path: str):\n        \"\"\"Saves diagram produces from `summary` method as html file to specified path. \"\"\"\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to save summary diagram.\")\n\n        steps = self.dataset_config._get_summary_steps()\n        html = css_utils.get_summary_diagram(steps)\n\n        with open(path, \"w\", encoding=\"utf-8\") as f:\n            f.write(html)\n\n    def get_feature_names(self) -&gt; list[str]:\n        \"\"\"Returns a list of all available feature names in the dataset. \"\"\"\n\n        return list(self.metadata.features.keys())\n\n    @abstractmethod\n    def get_data_about_set(self, about: SplitType | Literal[\"train\", \"val\", \"test\", \"all\"]) -&gt; dict:\n        \"\"\"\n        Retrieves data related to the specified set.\n\n        Parameters:\n            about: Specifies the set to retrieve data about.\n\n        Returns:\n            A dictionary containing the requested data for the set.\n        \"\"\"\n        ...\n\n    def get_available_ts_indices(self) -&gt; np.ndarray:\n        \"\"\"Returns the available time series indices in this dataset. \"\"\"\n        return self.metadata.ts_indices\n\n    def get_additional_data(self, data_name: str) -&gt; pd.DataFrame:\n        \"\"\"Create a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) of additional data of `data_name`.\n\n        Parameters:\n            data_name: Name of additional data to return.\n\n        Returns:\n            Dataframe of additional data of `data_name`.\n        \"\"\"\n\n        if data_name not in self.metadata.additional_data:\n            self.logger.error(\"%s is not available for this dataset.\", data_name)\n            raise ValueError(f\"{data_name} is not available for this dataset.\", f\"Possible options are: {self.metadata.additional_data}\")\n\n        data = get_additional_data(self.metadata.dataset_path, data_name)\n        data_df = pd.DataFrame(data)\n\n        for column, column_type in self.metadata.additional_data[data_name]:\n            if column_type == datetime:\n                data_df[column] = data_df[column].apply(lambda x: datetime.fromtimestamp(x, tz=timezone.utc))\n            else:\n                data_df[column] = data_df[column].astype(column_type)\n\n        return data_df\n\n    def plot(self, ts_id: int, plot_type: Literal[\"scatter\", \"line\"], features: list[str] | str | Literal[\"config\"] = \"config\", feature_per_plot: bool = True,\n             time_format: TimeFormat | Literal[\"config\", \"id_time\", \"datetime\", \"unix_time\", \"shifted_unix_time\"] = \"config\", is_interactive: bool = True) -&gt; None:\n        \"\"\"\n        Displays a graph for the selected `ts_id` and its `features`.\n\n        The plotting is done using the [`Plotly`](https://plotly.com/python/) library, which provides interactive graphs.\n\n        Parameters:\n            ts_id: The ID of the time series to display.\n            plot_type: The type of graph to plot.\n            features: The features to display in the plot. `Defaults: \"config\"`.\n            feature_per_plot: Whether each feature should be displayed in a separate plot or combined into one. `Defaults: True`.\n            time_format: The time format to use for the x-axis. `Defaults: \"config\"`.\n            is_interactive: Whether the plot should be interactive (e.g., zoom, hover). `Defaults: True`.\n        \"\"\"\n\n        if time_format == \"config\":\n\n            if self.dataset_config is None or not self.dataset_config.is_initialized:\n                raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to plot.\")\n\n            time_format = self.dataset_config.time_format\n            self.logger.debug(\"Using time format from dataset configuration: %s\", time_format)\n        else:\n            time_format = TimeFormat(time_format)\n            self.logger.debug(\"Using specified time format: %s\", time_format)\n\n        time_series, times, features = self.__get_data_for_plot(ts_id, features, time_format)\n        self.logger.debug(\"Received data for plotting. Time series, times, and features are ready.\")\n\n        plots = []\n\n        if feature_per_plot:\n            self.logger.debug(\"Creating individual plots for each feature.\")\n            fig = make_subplots(rows=len(features), cols=1, shared_xaxes=False, x_title=time_format.value)\n\n            for i, feature in enumerate(features):\n                if plot_type == \"scatter\":\n                    plot = go.Scatter(x=times, y=time_series[:, i], mode=\"markers\", name=feature, legendgroup=feature)\n                    self.logger.debug(\"Creating scatter plot for feature: %s\", feature)\n                elif plot_type == \"line\":\n                    plot = go.Scatter(x=times, y=time_series[:, i], mode=\"lines\", name=feature)\n                    self.logger.debug(\"Creating line plot for feature: %s\", feature)\n                else:\n                    raise ValueError(\"Invalid plot type.\")\n\n                fig.add_traces(plot, rows=i + 1, cols=1)\n\n            fig.update_layout(height=200 + 120 * len(features), width=2000, autosize=len(features) == 1, showlegend=True)\n            self.logger.debug(\"Created subplots for features: %s.\", features)\n        else:\n            self.logger.debug(\"Creating a combined plot for all features.\")\n            for i, feature in enumerate(features):\n                if plot_type == \"scatter\":\n                    plot = go.Scatter(x=times, y=time_series[:, i], mode=\"markers\", name=feature)\n                    self.logger.debug(\"Creating scatter plot for feature: %s\", feature)\n                elif plot_type == \"line\":\n                    plot = go.Scatter(x=times, y=time_series[:, i], mode=\"lines\", name=feature)\n                    self.logger.debug(\"Creating line plot for feature: %s\", feature)\n                else:\n                    raise ValueError(\"Invalid plot type.\")\n                plots.append(plot)\n\n            fig = go.Figure(data=plots)\n            fig.update_layout(xaxis_title=time_format.value, showlegend=True, height=200 + 120 * 2)\n            self.logger.debug(\"Created combined plot for features: %s.\", features)\n\n        if not is_interactive:\n            self.logger.debug(\"Disabling interactivity for the plot.\")\n            fig.update_layout(updatemenus=[], dragmode=False, hovermode=False)\n\n        self.logger.debug(\"Displaying the plot.\")\n        fig.show()\n\n    def add_annotation(self, annotation: str, annotation_group: str, ts_id: int | None, id_time: int | None, enforce_ids: bool = True) -&gt; None:\n        \"\"\" \n        Adds an annotation to the specified `annotation_group`.\n\n        - If the provided `annotation_group` does not exist, it will be created.\n        - At least one of `ts_id` or `id_time` must be provided to associate the annotation with time series or/and time point.\n\n        Parameters:\n            annotation: The annotation to be added.\n            annotation_group: The group to which the annotation should be added.\n            ts_id: The time series ID to which the annotation should be added.\n            id_time: The time ID to which the annotation should be added.\n            enforce_ids: Flag indicating whether the `ts_id` and `id_time` must belong to this dataset. `Default: True`  \n        \"\"\"\n\n        if enforce_ids:\n            self._validate_annotation_ids(ts_id, id_time)\n        self.annotations.add_annotation(annotation, annotation_group, ts_id, id_time)\n\n        if ts_id is not None and id_time is not None:\n            self._update_annotations_imported_status(AnnotationType.BOTH, None)\n        elif ts_id is not None and id_time is None:\n            self._update_annotations_imported_status(AnnotationType.TS_ID, None)\n        elif ts_id is None and id_time is not None:\n            self._update_annotations_imported_status(AnnotationType.ID_TIME, None)\n\n    def remove_annotation(self, annotation_group: str, ts_id: int | None, id_time: int | None) -&gt; None:\n        \"\"\"  \n        Removes an annotation from the specified `annotation_group`.\n\n        - At least one of `ts_id` or `id_time` must be provided to associate the annotation with time series or/and time point.\n\n        Parameters:\n            annotation_group: The annotation group from which the annotation should be removed.\n            ts_id: The time series ID from which the annotation should be removed.\n            id_time: The time ID from which the annotation should be removed. \n        \"\"\"\n\n        self.annotations.remove_annotation(annotation_group, ts_id, id_time, False)\n\n        if ts_id is not None and id_time is not None:\n            self._update_annotations_imported_status(AnnotationType.BOTH, None)\n        elif ts_id is not None and id_time is None:\n            self._update_annotations_imported_status(AnnotationType.TS_ID, None)\n        elif ts_id is None and id_time is not None:\n            self._update_annotations_imported_status(AnnotationType.ID_TIME, None)\n\n    def add_annotation_group(self, annotation_group: str, on: AnnotationType | Literal[\"id_time\", \"ts_id\", \"both\"]):\n        \"\"\" \n        Adds a new `annotation_group`.\n\n        Parameters:\n            annotation_group: The name of the annotation group to be added.\n            on: Specifies which part of the data should be annotated. If set to `\"both\"`, annotations will be applied as if `id_time` and `ts_id` were both set.\n        \"\"\"\n        on = AnnotationType(on)\n\n        self.annotations.add_annotation_group(annotation_group, on, False)\n\n        self._update_annotations_imported_status(on, None)\n\n    def remove_annotation_group(self, annotation_group: str, on: AnnotationType | Literal[\"id_time\", \"ts_id\", \"both\"]):\n        \"\"\" \n        Removes the specified `annotation_group`.\n\n        Parameters:\n            annotation_group: The name of the annotation group to be removed.\n            on: Specifies which part of the data the `annotation_group` should be removed from. If set to `\"both\"`, annotations will be applied as if `id_time` and `ts_id` were both set.        \n        \"\"\"\n        on = AnnotationType(on)\n\n        self.annotations.remove_annotation_group(annotation_group, on, False)\n\n        self._update_annotations_imported_status(on, None)\n\n    def get_annotations(self, on: AnnotationType | Literal[\"id_time\", \"ts_id\", \"both\"]) -&gt; pd.DataFrame:\n        \"\"\" \n        Returns the annotations as a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html).\n\n        Parameters:\n            on: Specifies which annotations to return. If set to `\"both\"`, annotations will be applied as if `id_time` and `ts_id` were both set.         \n\n        Returns:\n            A Pandas DataFrame containing the selected annotations.      \n        \"\"\"\n        on = AnnotationType(on)\n\n        return self.annotations.get_annotations(on, self.metadata.ts_id_name)\n\n    def import_annotations(self, identifier: str, enforce_ids: bool = True) -&gt; None:\n        \"\"\" \n        Imports annotations from a CSV file.\n\n        First, it attempts to load the built-in annotations, if no built-in annotations with such an identifier exists, it attempts to load a custom annotations from the `\"data_root\"/tszoo/annotations/` directory.\n\n        `data_root` is specified when the dataset is created.     \n\n        Parameters:\n            identifier: The name of the CSV file.     \n            enforce_ids: Flag indicating whether the `ts_id` and `id_time` must belong to this dataset. `Default: True`                \n        \"\"\"\n\n        annotations_file_path, is_built_in = get_annotations_path_and_whether_it_is_built_in(identifier, self.metadata.annotations_root, self.logger)\n\n        if is_built_in:\n            self.logger.info(\"Built-in annotations found: %s.\", identifier)\n            if not os.path.exists(annotations_file_path):\n                self.logger.info(\"Downloading annotations with identifier: %s\", identifier)\n                annotations_url = f\"{ANNOTATIONS_DOWNLOAD_BUCKET}&amp;file={identifier}\"  # probably will change annotations bucket... placeholder\n                resumable_download(url=annotations_url, file_path=annotations_file_path, silent=False)\n\n            self.logger.debug(\"Loading annotations from %s\", annotations_file_path)\n            temp_df = pd.read_csv(annotations_file_path)\n            self.logger.debug(\"Created DataFrame from file: %s\", annotations_file_path)\n        else:\n            self.logger.info(\"Custom annotations found: %s.\", identifier)\n            self.logger.debug(\"Loading annotations from %s\", annotations_file_path)\n            temp_df = pd.read_csv(annotations_file_path)\n            self.logger.debug(\"Created DataFrame from file: %s\", annotations_file_path)\n\n        ts_id_index = None\n        time_id_index = None\n        on = None\n\n        # Check the columns of the DataFrame to identify the type of annotation\n        if self.metadata.ts_id_name in temp_df.columns and ID_TIME_COLUMN_NAME in temp_df.columns:\n            self.annotations.clear_time_in_time_series()\n            time_id_index = temp_df.columns.tolist().index(ID_TIME_COLUMN_NAME)\n            ts_id_index = temp_df.columns.tolist().index(self.metadata.ts_id_name)\n            on = AnnotationType.BOTH\n            self.logger.info(\"Annotations detected as %s (both %s and id_time)\", AnnotationType.BOTH, self.metadata.ts_id_name)\n\n        elif self.metadata.ts_id_name in temp_df.columns:\n            self.annotations.clear_time_series()\n            ts_id_index = temp_df.columns.tolist().index(self.metadata.ts_id_name)\n            on = AnnotationType.TS_ID\n            self.logger.info(\"Annotations detected as %s (%s only)\", AnnotationType.TS_ID, self.metadata.ts_id_name)\n\n        elif ID_TIME_COLUMN_NAME in temp_df.columns:\n            self.annotations.clear_time()\n            time_id_index = temp_df.columns.tolist().index(ID_TIME_COLUMN_NAME)\n            on = AnnotationType.ID_TIME\n            self.logger.info(\"Annotations detected as %s (%s only)\", AnnotationType.ID_TIME, ID_TIME_COLUMN_NAME)\n\n        else:\n            raise ValueError(f\"Could not find {self.metadata.ts_id_name} and {ID_TIME_COLUMN_NAME} in the imported CSV.\")\n\n        # Process each row in the DataFrame and add annotations\n        for row in temp_df.itertuples(False):\n            for i, _ in enumerate(temp_df.columns):\n                if i == time_id_index or i == ts_id_index:\n                    continue\n\n                ts_id = None\n                if ts_id_index is not None:\n                    ts_id = row[ts_id_index]\n\n                id_time = None\n                if time_id_index is not None:\n                    id_time = row[time_id_index]\n\n                self.add_annotation(row[i], temp_df.columns[i], ts_id, id_time, enforce_ids)\n\n        self._update_annotations_imported_status(on, identifier)\n        self.logger.info(\"Successfully imported annotations from %s\", annotations_file_path)\n\n    def import_config(self, identifier: str, display_config_details: Optional[Literal[\"text\", \"diagram\"]] = \"text\", workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n        \"\"\" \n        Import the dataset_config from a pickle file and initializes the dataset. Config type must correspond to dataset type.\n\n        First, it attempts to load the built-in config, if no built-in config with such an identifier exists, it attempts to load a custom config from the `\"data_root\"/tszoo/configs/` directory.\n\n        `data_root` is specified when the dataset is created.       \n\n        The following configuration attributes are used during initialization:\n\n        Dataset config | Description\n        -------------- | -----------\n        `init_workers` | Specifies the number of workers to use for initialization. Applied when `workers` = \"config\".\n        `partial_fit_initialized_transformers` | Determines whether initialized transformers should be partially fitted on the training data.\n        `nan_threshold` | Filters out time series with missing values exceeding the specified threshold.\n\n        Parameters:\n            identifier: Name of the pickle file.\n            display_config_details: Flag indicating whether to display the configuration values after initialization. `Default: True` \n            workers: The number of workers to use during initialization. `Default: \"config\"`  \n        \"\"\"\n\n        if display_config_details is not None:\n            display_config_details = DisplayType(display_config_details)\n\n        # Load config\n        config = load_config(identifier, self.metadata.configs_root, self.metadata.database_name, self.metadata.source_type, self.metadata.aggregation, self.logger)\n\n        self.logger.info(\"Initializing dataset configuration with the imported config.\")\n        self.set_dataset_config_and_initialize(config, display_config_details, workers)\n\n        self._update_config_imported_status(identifier)\n        self.logger.info(\"Successfully used config with identifier %s\", identifier)\n\n    def save_annotations(self, identifier: str, on: AnnotationType | Literal[\"id_time\", \"ts_id\", \"both\"], force_write: bool = False) -&gt; None:\n        \"\"\" \n        Saves the annotations as a CSV file.\n\n        The file will be saved to a path determined by the `data_root` specified when the dataset was created.\n\n        The annotations will be saved under the directory `data_root/tszoo/annotations/`.\n\n        Parameters:\n            identifier: The name of the CSV file.\n            on: What annotation type should be saved. If set to `\"both\"`, annotations will be applied as if `id_time` and `ts_id` were both set.   \n            force_write: If set to `True`, will overwrite any existing files with the same name. `Default: False`               \n        \"\"\"\n\n        if exists_built_in_annotations(identifier):\n            raise ValueError(\"Built-in annotations with this identifier already exists. Choose another identifier.\")\n\n        on = AnnotationType(on)\n\n        temp_df = self.get_annotations(on)\n\n        # Ensure the annotations root directory exists, creating it if necessary\n        if not os.path.exists(self.metadata.annotations_root):\n            os.makedirs(self.metadata.annotations_root)\n            self.logger.info(\"Created annotations directory at %s\", self.metadata.annotations_root)\n\n        path = os.path.join(self.metadata.annotations_root, f\"{identifier}.csv\")\n\n        if os.path.exists(path) and not force_write:\n            raise ValueError(f\"Annotations already exist at {path}. Set force_write=True to overwrite.\")\n        self.logger.debug(\"Annotations CSV file path: %s\", path)\n\n        temp_df.to_csv(path, index=False)\n\n        self._update_annotations_imported_status(on, identifier)\n        self.logger.info(\"Annotations successfully saved to %s\", path)\n\n    def save_config(self, identifier: str, create_with_details_file: bool = True, force_write: bool = False, **kwargs) -&gt; None:\n        \"\"\" \n        Saves the config as a pickle file.\n\n        The file will be saved to a path determined by the `data_root` specified when the dataset was created. \n        The config will be saved under the directory `data_root/tszoo/configs/`.\n\n        Parameters:\n            identifier: The name of the pickle file.\n            create_with_details_file: Whether to export the config along with a readable text file that provides details. `Defaults: True`. \n            force_write: If set to `True`, will overwrite any existing files with the same name. `Default: False`            \n        \"\"\"\n\n        default_kwargs = {'hard_force': False}\n        kwargs = {**default_kwargs, **kwargs}\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to save config.\")\n\n        if not kwargs[\"hard_force\"] and exists_built_in_config(identifier):\n            raise ValueError(\"Built-in config with this identifier already exists. Choose another identifier.\")\n\n        # Ensure the config directory exists\n        if not os.path.exists(self.metadata.configs_root):\n            os.makedirs(self.metadata.configs_root)\n            self.logger.info(\"Created config directory at %s\", self.metadata.configs_root)\n\n        path_pickle = os.path.join(self.metadata.configs_root, f\"{identifier}.pickle\")\n        path_details = os.path.join(self.metadata.configs_root, f\"{identifier}.txt\")\n\n        if os.path.exists(path_pickle) and not force_write:\n            raise ValueError(f\"Config at path {path_pickle} already exists. Set force_write=True to overwrite.\")\n        self.logger.debug(\"Config pickle path: %s\", path_pickle)\n\n        if create_with_details_file:\n            if os.path.exists(path_details) and not force_write:\n                raise ValueError(f\"Config details at path {path_details} already exists. Set force_write=True to overwrite.\")\n            self.logger.debug(\"Config details path: %s\", path_details)\n\n        if not self.dataset_config.filler_factory.creates_built_in:\n            self.logger.warning(\"You are using a custom filler. Ensure the config is distributed with the source code of the filler.\")\n\n        if not self.dataset_config.anomaly_handler_factory.creates_built_in:\n            self.logger.warning(\"You are using a custom anomaly handler. Ensure the config is distributed with the source code of the anomaly handler.\")\n\n        if not self.dataset_config.transformer_factory.creates_built_in:\n            self.logger.warning(\"You are using a custom transformer. Ensure the config is distributed with the source code of the transformer.\")\n\n        if len(self.dataset_config.preprocess_order) != len(MANDATORY_PREPROCESSES_ORDER):\n            self.logger.warning(\"You are using at least one custom handler. Ensure the config is distributed with the source code of every custom handler.\")\n\n        pickle_dump(self._export_config_copy, path_pickle)\n        self.logger.info(\"Config pickle saved to %s\", path_pickle)\n\n        if create_with_details_file:\n            with open(path_details, \"w\", encoding=\"utf-8\") as file:\n                file.write(str(self.dataset_config))\n            self.logger.info(\"Config details saved to %s\", path_details)\n\n        self._update_config_imported_status(identifier)\n        self.dataset_config.export_update_needed = False\n        self.logger.info(\"Config successfully saved\")\n\n    def save_benchmark(self, identifier: str, force_write: bool = False, **kwargs) -&gt; None:\n        \"\"\" \n        Saves the benchmark as a YAML file.\n\n        The benchmark, along with any associated annotations and config files, will be saved in a path determined by the `data_root` specified when creating the dataset. \n        The default save path for benchmark is `\"data_root/tszoo/benchmarks/\"`.\n\n        If you are using imported `annotations` or `config` (whether custom or built-in), their file names will be set in the `benchmark` file. \n        If new `annotations` or `config` are created during the process, their filenames will be derived from the provided `identifier` and set in the `benchmark` file.\n\n        Parameters:\n            identifier: The name of the YAML file.\n            force_write: If set to `True`, will overwrite any existing files with the same name. `Default: False`            \n        \"\"\"\n\n        default_kwargs = {'hard_force': False}\n        kwargs = {**default_kwargs, **kwargs}\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to save benchmark.\")\n\n        if not kwargs[\"hard_force\"] and exists_built_in_benchmark(identifier):\n            raise ValueError(\"Built-in benchmark with this identifier already exists. Choose another identifier.\")\n\n        # Determine annotation names based on the available annotations and whether the annotations were imported\n        if len(self.annotations.time_series_annotations) &gt; 0:\n            annotations_ts_name = self.imported_annotations_ts_identifier if self.imported_annotations_ts_identifier is not None else f\"{identifier}_{AnnotationType.TS_ID.value}\"\n        else:\n            annotations_ts_name = None\n\n        if len(self.annotations.time_annotations) &gt; 0:\n            annotations_time_name = self.imported_annotations_time_identifier if self.imported_annotations_time_identifier is not None else f\"{identifier}_{AnnotationType.ID_TIME.value}\"\n        else:\n            annotations_time_name = None\n\n        if len(self.annotations.time_in_series_annotations) &gt; 0:\n            annotations_both_name = self.imported_annotations_both_identifier if self.imported_annotations_both_identifier is not None else f\"{identifier}_{AnnotationType.BOTH.value}\"\n        else:\n            annotations_both_name = None\n\n        # Use the imported identifier if available and update is not necessary, otherwise default to the current identifier\n        config_name = self.dataset_config.import_identifier if (self.dataset_config.import_identifier is not None and not self.dataset_config.export_update_needed) else identifier\n\n        export_benchmark = ExportBenchmark(self.metadata.database_name,\n                                           self.metadata.source_type.value,\n                                           self.metadata.aggregation.value,\n                                           self.metadata.dataset_type.value,\n                                           config_name,\n                                           annotations_ts_name,\n                                           annotations_time_name,\n                                           annotations_both_name,\n                                           related_results_identifier=self.related_to,\n                                           version=version.config_and_benchmarks_current_version)\n\n        # If the config was not imported, save it\n        if self.dataset_config.import_identifier is None or self.dataset_config.export_update_needed:\n            self.save_config(export_benchmark.config_identifier, force_write=force_write, hard_force=kwargs[\"hard_force\"])\n        else:\n            self.logger.info(\"Using already existing config with identifier: %s\", self.dataset_config.import_identifier)\n\n        # Save ts_id annotations if available and not previously imported\n        if self.imported_annotations_ts_identifier is None and len(self.annotations.time_series_annotations) &gt; 0:\n            self.save_annotations(export_benchmark.annotations_ts_identifier, AnnotationType.TS_ID, force_write=force_write)\n        elif self.imported_annotations_ts_identifier is not None:\n            self.logger.info(\"Using already existing annotations with identifier: %s; type: %s\", self.imported_annotations_ts_identifier, AnnotationType.TS_ID)\n\n        # Save id_time annotations if available and not previously imported\n        if self.imported_annotations_time_identifier is None and len(self.annotations.time_annotations) &gt; 0:\n            self.save_annotations(export_benchmark.annotations_time_identifier, AnnotationType.ID_TIME, force_write=force_write)\n        elif self.imported_annotations_time_identifier is not None:\n            self.logger.info(\"Using already existing annotations with identifier: %s; type: %s\", self.imported_annotations_time_identifier, AnnotationType.ID_TIME)\n\n        # Save both annotations if available and not previously imported\n        if self.imported_annotations_both_identifier is None and len(self.annotations.time_in_series_annotations) &gt; 0:\n            self.save_annotations(export_benchmark.annotations_both_identifier, AnnotationType.BOTH, force_write=force_write)\n        elif self.imported_annotations_both_identifier is not None:\n            self.logger.info(\"Using already existing annotations with identifier: %s; type: %s\", self.imported_annotations_both_identifier, AnnotationType.BOTH)\n\n        # Ensure the benchmark directory exists\n        if not os.path.exists(self.metadata.benchmarks_root):\n            os.makedirs(self.metadata.benchmarks_root)\n            self.logger.info(\"Created benchmarks directory at %s\", self.metadata.benchmarks_root)\n\n        benchmark_path = os.path.join(self.metadata.benchmarks_root, f\"{identifier}.yaml\")\n\n        if os.path.exists(benchmark_path) and not force_write:\n            self.logger.error(\"Benchmark file already exists at %s\", benchmark_path)\n            raise ValueError(f\"Benchmark at path {benchmark_path} already exists. Set force_write=True to overwrite.\")\n        self.logger.debug(\"Benchmark YAML file path: %s\", benchmark_path)\n\n        yaml_dump(export_benchmark.to_dict(), benchmark_path)\n        self.logger.info(\"Benchmark successfully saved to %s\", benchmark_path)\n\n    def get_transformers(self) -&gt; np.ndarray[Transformer] | Transformer | None:\n        \"\"\"Returns used transformers from config. \"\"\"\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting get transformers.\")\n\n        for i, preprocess_type in enumerate(self.dataset_config.preprocess_order):\n            if preprocess_type == PreprocessType.TRANSFORMING:\n                holder: TransformerHolder = self.dataset_config.train_preprocess_order[i].holder\n                return holder.transformers\n\n        return None\n\n    def check_errors(self) -&gt; None:\n        \"\"\"\n        Validates whether the dataset is corrupted. \n\n        Raises an exception if corrupted.\n        \"\"\"\n\n        dataset, _ = load_database(self.metadata.dataset_path)\n\n        try:\n            node_iter = dataset.walk_nodes()\n\n            # Process each node in the dataset\n            for node in node_iter:\n                if isinstance(node, tb.Table):\n\n                    iter_by = min(LOADING_WARNING_THRESHOLD, len(node))\n                    iters_done = 0\n\n                    # Process the node in chunks to avoid memory issues\n                    while iters_done &lt; len(node):\n                        iter_by = min(LOADING_WARNING_THRESHOLD, len(node) - iters_done)\n                        _ = node[iters_done: iters_done + iter_by]  # Fetch the data in chunks\n                        iters_done += iter_by\n\n                    self.logger.info(\"Table '%s' checked successfully. (%d rows processed)\", node._v_pathname, len(node))\n\n            self.logger.info(\"Dataset check completed with no errors found.\")\n\n        except Exception as e:\n            self.logger.error(\"Error encountered during dataset check: %s\", str(e))\n\n        finally:\n            dataset.close()\n            self.logger.debug(\"Dataset connection closed.\")\n\n    @abstractmethod\n    def _get_data_for_plot(self, ts_id: int, feature_indices: np.ndarray[int], time_format: TimeFormat) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"Dataset type specific retrieval of data. \"\"\"\n        ...\n\n    def __get_data_for_plot(self, ts_id: int, features: list[str] | str, time_format: TimeFormat) -&gt; tuple[np.ndarray, np.ndarray, list[str]]:\n        \"\"\"Returns prepared data for plotting. \"\"\"\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before getting data for plotting.\")\n\n        features_indices = []\n\n        if features == \"config\":\n            features = deepcopy(self.dataset_config.features_to_take_without_ids)\n            features_indices = np.arange(len(features))\n            self.logger.debug(\"Features set from dataset config: %s\", features)\n        else:\n            if isinstance(features, str):\n                features = [features]\n\n            if len(features) == 0:\n                raise ValueError(\"No features specified to plot. Please provide valid features.\")\n            if len(set(features)) != len(features):\n                raise ValueError(\"Duplicate features detected. All features must be unique.\")\n\n            for feature in features:\n                if feature not in self.dataset_config.features_to_take_without_ids:\n                    raise ValueError(f\"Feature '{feature}' is not valid. It is not present in the dataset configuration.\", self.dataset_config.features_to_take_without_ids)\n\n                index_in_config_features = self.dataset_config.features_to_take_without_ids.index(feature)\n                features_indices.append(index_in_config_features)\n\n        real_feature_indices = np.array(self.dataset_config.indices_of_features_to_take_no_ids)[features_indices]\n        real_feature_indices = real_feature_indices.astype(int)\n\n        time_series, time_period = self._get_data_for_plot(ts_id, real_feature_indices, time_format)\n        self.logger.debug(\"Time series data and corresponding time values retrieved.\")\n\n        return time_series, time_period, features\n\n    def _validate_annotation_ids(self, ts_id: int | None, id_time: int | None) -&gt; None:\n        \"\"\"Validates whether the `ts_id` and `id_time` belong to this dataset. \"\"\"\n\n        assert ts_id is not None or id_time is not None, \"Either ts_id or id_time must be provided.\"\n\n        # Handle when id_time is provided\n        if id_time is not None:\n            time_indices = self.metadata.time_indices\n            if id_time &lt; time_indices[ID_TIME_COLUMN_NAME][0] or id_time &gt; time_indices[ID_TIME_COLUMN_NAME][-1]:\n                valid_range = range(time_indices[ID_TIME_COLUMN_NAME][0], time_indices[ID_TIME_COLUMN_NAME][-1])\n                raise ValueError(f\"id_time {id_time} does not fall within the valid range for {self.metadata.aggregation}. \"\n                                 f\"Valid id_time range: {valid_range}.\")\n\n        # Handle when ts_id is provided\n        if ts_id is not None:\n            ts_indices = self.metadata.ts_indices[self.metadata.ts_id_name]\n\n            if ts_id not in ts_indices:\n                valid_ts_range = self.metadata.ts_indices[self.metadata.ts_id_name]\n                raise ValueError(f\"ts_id {ts_id} does not exist in the available range for {self.metadata.source_type}. \"\n                                 f\"Valid ts_id values: {valid_ts_range}.\")\n\n    @abstractmethod\n    def _get_singular_time_series_dataset(self, parent_dataset: Dataset, ts_id: int) -&gt; Dataset:\n        \"\"\"Returns dataset for single time series \"\"\"\n        ...\n\n    def _get_df(self, dataloader: DataLoader, as_single_dataframe: bool, ts_ids: np.ndarray, time_period: np.ndarray) -&gt; pd.DataFrame:\n        \"\"\"Returns all data from the DataLoader as a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html). \"\"\"\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before getting DataFrame.\")\n\n        total_samples = len(ts_ids) * len(time_period)\n        if total_samples &gt;= LOADING_WARNING_THRESHOLD:\n            self.logger.warning(\"The dataset contains %d samples (%d time series \u00d7 %d times). Consider using get_*_dataloader() for batch loading.\", total_samples, len(ts_ids), len(time_period))\n\n        if as_single_dataframe:\n            self.logger.debug(\"Returning a single DataFrame with all features for all time series.\")\n            return dataset_loaders.create_single_df_from_dataloader(\n                dataloader,\n                ts_ids,\n                self.dataset_config.features_to_take,\n                self.dataset_config.time_format,\n                self.dataset_config.include_ts_id,\n                self.dataset_config.include_time,\n                self.dataset_config.dataset_type,\n                True\n            )\n        else:\n            self.logger.debug(\"Returning multiple DataFrames, one per time series.\")\n            return dataset_loaders.create_multiple_df_from_dataloader(\n                dataloader,\n                ts_ids,\n                self.dataset_config.features_to_take,\n                self.dataset_config.time_format,\n                self.dataset_config.include_ts_id,\n                self.dataset_config.include_time,\n                self.dataset_config.dataset_type,\n                True\n            )\n\n    def _get_numpy(self, dataloader: DataLoader, ts_ids: np.ndarray, time_period: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Returns all data from the DataLoader as a NumPy `ndarray`. \"\"\"\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before getting Numpy array.\")\n\n        total_samples = len(ts_ids) * len(time_period)\n        if total_samples &gt;= LOADING_WARNING_THRESHOLD:\n            self.logger.warning(\"The dataset contains %d samples (%d time series \u00d7 %d times). Consider using get_*_dataloader() for batch loading.\", total_samples, len(ts_ids), len(time_period))\n\n        self.logger.debug(\"Creating numpy array from dataloader.\")\n        return dataset_loaders.create_numpy_from_dataloader(\n            dataloader,\n            ts_ids,\n            self.dataset_config.time_format,\n            self.dataset_config.include_time,\n            self.dataset_config.dataset_type,\n            True\n        )\n\n    def _clear(self) -&gt; None:\n        \"\"\"Clears set data. Mainly called when initializing new config. \"\"\"\n        self.train_dataset = None\n        self.train_dataloader = None\n        self.val_dataset = None\n        self.val_dataloader = None\n        self.test_dataset = None\n        self.test_dataloader = None\n        self.all_dataset = None\n        self.all_dataloader = None\n        self.dataset_config = None\n        self.logger.debug(\"Dataset attributes had been cleared. \")\n\n    def _update_annotations_imported_status(self, on: AnnotationType, identifier: str):\n        if on == AnnotationType.TS_ID:\n            self.imported_annotations_ts_identifier = identifier\n        elif on == AnnotationType.ID_TIME:\n            self.imported_annotations_time_identifier = identifier\n        elif on == AnnotationType.BOTH:\n            self.imported_annotations_both_identifier = identifier\n\n    def _update_config_imported_status(self, identifier: str) -&gt; None:\n        self.dataset_config.import_identifier = identifier\n        self._export_config_copy.import_identifier = identifier\n\n    @abstractmethod\n    def _initialize_datasets(self) -&gt; None:\n        \"\"\" Called in [`set_dataset_config_and_initialize`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.set_dataset_config_and_initialize), initializes datasets for sets.\"\"\"\n        ...\n\n    @abstractmethod\n    def _initialize_transformers_and_details(self, workers: int) -&gt; None:\n        \"\"\" Called in [`set_dataset_config_and_initialize`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.set_dataset_config_and_initialize). Goes through data to validate time series against `nan_threshold`, fit `transformers`, fit `anomaly handlers` and prepare `fillers`\"\"\"\n        ...\n\n    def _update_export_config_copy(self) -&gt; None:\n        \"\"\" Called at the end of [`set_dataset_config_and_initialize`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.set_dataset_config_and_initialize) or when changing config values. Updates values of config used for saving config.\"\"\"\n\n        self._export_config_copy.train_batch_size = self.dataset_config.train_batch_size\n        self._export_config_copy.val_batch_size = self.dataset_config.val_batch_size\n        self._export_config_copy.test_batch_size = self.dataset_config.test_batch_size\n        self._export_config_copy.all_batch_size = self.dataset_config.all_batch_size\n\n        self._export_config_copy.train_workers = self.dataset_config.train_workers\n        self._export_config_copy.val_workers = self.dataset_config.val_workers\n        self._export_config_copy.test_workers = self.dataset_config.test_workers\n        self._export_config_copy.all_workers = self.dataset_config.all_workers\n        self._export_config_copy.init_workers = self.dataset_config.init_workers\n\n    def _validate_config_for_dataset(self, config: DatasetConfig) -&gt; bool:\n        \"\"\"Validates whether config is supposed to be used for this dataset. \"\"\"\n\n        if config.database_name != self.metadata.database_name:\n            self.logger.error(\"This config is not compatible with the current dataset. Difference in database name between config and this dataset.\")\n            raise ValueError(\"This config is not compatible with the current dataset.\", f\"config.database_name == {config.database_name} and dataset.database_name == {self.metadata.database_name}\")\n\n        if config.dataset_type != self.metadata.dataset_type:\n            self.logger.error(\"This config is not compatible with the current dataset. Difference in is_series_based between config and this dataset.\")\n            raise ValueError(\"This config is not compatible with the current dataset.\", f\"config.dataset_type == {config.dataset_type} and dataset.dataset_type == {self.metadata.dataset_type}\")\n\n        if config.aggregation != self.metadata.aggregation:\n            self.logger.error(\"This config is not compatible with the current dataset. Difference in aggregation type between config and this dataset.\")\n            raise ValueError(\"This config is not compatible with the current dataset.\", f\"config.aggregation == {config.aggregation} and dataset.aggregation == {self.metadata.aggregation}\")\n\n        if config.source_type != self.metadata.source_type:\n            self.logger.error(\"This config is not compatible with the current dataset. Difference in source type between config and this dataset.\")\n            raise ValueError(\"This config is not compatible with the current dataset.\", f\"config.source_type == {config.source_type} and dataset.source_type == {self.metadata.source_type}\")\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata: DatasetMetadata\n</code></pre> <p>Holds various metadata used in dataset for its creation, loading data and similar.</p>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.dataset_config","title":"dataset_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dataset_config: Optional[DatasetConfig] = field(default=None, init=False)\n</code></pre> <p>Configuration of the dataset.</p>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.train_dataset","title":"train_dataset  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>train_dataset: Optional[Dataset] = field(default=None, init=False)\n</code></pre> <p>Training set as a <code>BaseDataset</code> instance wrapping the PyTables database.</p>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.val_dataset","title":"val_dataset  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>val_dataset: Optional[Dataset] = field(default=None, init=False)\n</code></pre> <p>Validation set as a <code>BaseDataset</code> instance wrapping the PyTables database.</p>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.test_dataset","title":"test_dataset  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>test_dataset: Optional[Dataset] = field(default=None, init=False)\n</code></pre> <p>Test set as a <code>BaseDataset</code> instance wrapping the PyTables database.</p>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.all_dataset","title":"all_dataset  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>all_dataset: Optional[Dataset] = field(default=None, init=False)\n</code></pre> <p>All set as a <code>BaseDataset</code> instance wrapping the PyTables database.</p>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.dataloader_factory","title":"dataloader_factory  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dataloader_factory: Optional[DataloaderFactory] = field(default=None, init=False)\n</code></pre> <p>Factory used to create Dataloaders for specific CesnetDataset subclass.</p>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.train_dataloader","title":"train_dataloader  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>train_dataloader: Optional[DataLoader] = field(default=None, init=False)\n</code></pre> <p>Iterable PyTorch <code>DataLoader</code> for training set.</p>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.val_dataloader","title":"val_dataloader  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>val_dataloader: Optional[DataLoader] = field(default=None, init=False)\n</code></pre> <p>Iterable PyTorch <code>DataLoader</code> for validation set.</p>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.test_dataloader","title":"test_dataloader  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>test_dataloader: Optional[DataLoader] = field(default=None, init=False)\n</code></pre> <p>Iterable PyTorch <code>DataLoader</code> for test set.</p>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.all_dataloader","title":"all_dataloader  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>all_dataloader: Optional[DataLoader] = field(default=None, init=False)\n</code></pre> <p>Iterable PyTorch <code>DataLoader</code> for all set.</p>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.related_to","title":"related_to  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>related_to: Optional[str] = field(default=None, init=False)\n</code></pre> <p>Name of file with relevant results to used benchmark.</p>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.set_dataset_config_and_initialize","title":"set_dataset_config_and_initialize","text":"<pre><code>set_dataset_config_and_initialize(dataset_config: DatasetConfig, display_config_details: Optional[Literal['text', 'diagram']] = 'text', workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Initialize training set, validation set, test set etc.. This method must be called before any data can be accessed. It is required for the final initialization of <code>dataset_config</code>.</p> <p>The following configuration attributes are used during initialization:</p> Dataset config Description <code>init_workers</code> Specifies the number of workers to use for initialization. Applied when <code>workers</code> = \"config\". <code>partial_fit_initialized_transformers</code> Determines whether initialized transformers should be partially fitted on the training data. <code>nan_threshold</code> Filters out time series with missing values exceeding the specified threshold. <p>Parameters:</p> Name Type Description Default <code>dataset_config</code> <code>DatasetConfig</code> <p>Desired configuration of the dataset.</p> required <code>display_config_details</code> <code>Optional[Literal['text', 'diagram']]</code> <p>Flag indicating whether and how to display the configuration values after initialization. <code>Default: text</code> </p> <code>'text'</code> <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use during initialization. <code>Default: \"config\"</code></p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def set_dataset_config_and_initialize(self, dataset_config: DatasetConfig, display_config_details: Optional[Literal[\"text\", \"diagram\"]] = \"text\", workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"\n    Initialize training set, validation set, test set etc.. This method must be called before any data can be accessed. It is required for the final initialization of [`dataset_config`](reference_dataset_config.md#references.DatasetConfig).\n\n    The following configuration attributes are used during initialization:\n\n    Dataset config | Description\n    -------------- | -----------\n    `init_workers` | Specifies the number of workers to use for initialization. Applied when `workers` = \"config\".\n    `partial_fit_initialized_transformers` | Determines whether initialized transformers should be partially fitted on the training data.\n    `nan_threshold` | Filters out time series with missing values exceeding the specified threshold.\n\n    Parameters:\n        dataset_config: Desired configuration of the dataset.\n        display_config_details: Flag indicating whether and how to display the configuration values after initialization. `Default: text`  \n        workers: The number of workers to use during initialization. `Default: \"config\"`  \n    \"\"\"\n\n    if display_config_details is not None:\n        display_config_details = DisplayType(display_config_details)\n\n    self._clear()\n    self.dataset_config = dataset_config\n\n    # If the config is not initialized, set a copy of the configuration for export\n    if not self.dataset_config.is_initialized:\n        self.dataset_config._update_identifiers_from_dataset_metadata(self.metadata)\n        self._export_config_copy = deepcopy(self.dataset_config)\n        self.logger.debug(\"New export_config_copy created.\")\n\n    self._validate_config_for_dataset(self.dataset_config)\n\n    if workers == \"config\":\n        workers = self.dataset_config.init_workers\n\n    if not self.dataset_config.is_initialized:\n\n        self.dataset_config._dataset_init(self.metadata)\n        self._initialize_transformers_and_details(workers)\n\n        self.dataset_config.is_initialized = True\n        self.logger.info(\"Config initialized successfully.\")\n    else:\n        self.logger.info(\"Config already initialized. Skipping re-initialization.\")\n\n    # Initialize datasets\n    self._initialize_datasets()\n    self.logger.debug(\"Datasets have been successfully initialized.\")\n\n    self._update_export_config_copy()\n    self.logger.debug(\"Export config copy updated with the latest dataset configuration.\")\n\n    if display_config_details is not None:\n        self.summary(display_config_details)\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_train_dataloader","title":"get_train_dataloader","text":"<pre><code>get_train_dataloader(ts_id: int | None = None, workers: int | Literal['config'] = 'config', **kwargs) -&gt; DataLoader\n</code></pre> <p>Returns a PyTorch <code>DataLoader</code> for training set.</p> <p>The <code>DataLoader</code> is created on the first call and cached for subsequent use.  The cached dataloader is cleared when either <code>get_train_df</code> or <code>get_train_numpy</code> is called.</p> <p>The structure of the returned batch depends on the <code>time_format</code> and whether <code>sliding_window_size</code> is used:</p> <ul> <li>When <code>sliding_window_size</code> is used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> <li><code>np.ndarray</code> of times with shape <code>(times - 1)</code></li> <li><code>np.ndarray</code> of time with shape <code>(1)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> </ul> </li> </ul> </li> <li>When <code>sliding_window_size</code> is not used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> <li><code>np.ndarray</code> of time with shape <code>(times)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> </ul> </li> </ul> </li> </ul> <p>The <code>DataLoader</code> is configured with the following config attributes:</p> Dataset config Description <code>train_batch_size</code> Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>sliding_window_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_prediction_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_step</code> Available only for time-based datasets. Number of times to move by after each window. <code>train_workers</code> Specifies the number of workers to use for loading train data. Applied when <code>workers</code> = \"config\". <code>train_dataloader_order</code> Available only for series-based datasets. Whether to load train data in sequential or random order. <code>random_state</code> Seed for loading train data in random order. <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading train data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>ts_id</code> <code>int | None</code> <p>Specifies time series to take. If None returns all time series as normal. <code>Default: \"None\"</code></p> <code>None</code> <p>Returns:</p> Type Description <code>DataLoader</code> <p>An iterable <code>DataLoader</code> containing data from training set.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_train_dataloader(self, ts_id: int | None = None, workers: int | Literal[\"config\"] = \"config\", **kwargs) -&gt; DataLoader:\n    \"\"\"\n    Returns a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for training set.\n\n    The `DataLoader` is created on the first call and cached for subsequent use. &lt;br/&gt;\n    The cached dataloader is cleared when either [`get_train_df`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_train_df) or [`get_train_numpy`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_train_numpy) is called.\n\n    The structure of the returned batch depends on the `time_format` and whether `sliding_window_size` is used:\n\n    - When `sliding_window_size` is used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n            - `np.ndarray` of times with shape `(times - 1)`\n            - `np.ndarray` of time with shape `(1)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n    - When `sliding_window_size` is not used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n            - `np.ndarray` of time with shape `(times)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n\n    The `DataLoader` is configured with the following config attributes:\n\n    Dataset config | Description\n    -------------- | -----------\n    `train_batch_size` | Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `sliding_window_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n    `sliding_window_prediction_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n    `sliding_window_step` | Available only for time-based datasets. Number of times to move by after each window.\n    `train_workers` | Specifies the number of workers to use for loading train data. Applied when `workers` = \"config\".\n    `train_dataloader_order` | Available only for series-based datasets. Whether to load train data in sequential or random order.\n    `random_state` | Seed for loading train data in random order.\n\n    Parameters:\n        workers: The number of workers to use for loading train data. `Default: \"config\"` \n        ts_id: Specifies time series to take. If None returns all time series as normal. `Default: \"None\"`\n\n    Returns:\n        An iterable `DataLoader` containing data from training set.          \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access train_dataloader.\")\n\n    if not self.dataset_config.has_train():\n        raise ValueError(\"Dataloader for training set is not available in the dataset configuration.\")\n\n    assert self.train_dataset is not None, \"The train_dataset must be initialized before accessing data from training set.\"\n\n    default_kwargs = {'take_all': False, \"cache_loader\": True}\n    kwargs = {**default_kwargs, **kwargs}\n\n    if ts_id is not None:\n\n        if ts_id == self.dataset_config.used_singular_train_time_series and self.train_dataloader is not None:\n            self.logger.debug(\"Returning cached train_dataloader.\")\n            return self.train_dataloader\n\n        dataset = self._get_singular_time_series_dataset(self.train_dataset, ts_id)\n        self.dataset_config.used_singular_train_time_series = ts_id\n        if self.train_dataloader:\n            del self.train_dataloader\n            self.train_dataloader = None\n            self.logger.info(\"Destroyed previous cached train_dataloader.\")\n\n        self.dataset_config.used_train_workers = 0\n        self.train_dataloader = self.dataloader_factory.create_dataloader(dataset, self.dataset_config, 0, False, self.dataset_config.train_batch_size)\n        self.logger.info(\"Created new cached train_dataloader.\")\n        return self.train_dataloader\n    elif self.dataset_config.used_singular_train_time_series is not None and self.train_dataloader is not None:\n        del self.train_dataloader\n        self.train_dataloader = None\n        self.dataset_config.used_singular_train_time_series = None\n        self.logger.info(\"Destroyed previous cached train_dataloader.\")\n\n    if workers == \"config\":\n        workers = self.dataset_config.train_workers\n\n    # If the dataloader is cached and number of used workers did not change, return the cached dataloader\n    if self.train_dataloader and kwargs[\"cache_loader\"] and workers == self.dataset_config.used_train_workers:\n        self.logger.debug(\"Returning cached train_dataloader.\")\n        return self.train_dataloader\n\n    # Update the used workers count\n    self.dataset_config.used_train_workers = workers\n\n    # If there's a previously cached dataloader, destroy it\n    if self.train_dataloader:\n        del self.train_dataloader\n        self.train_dataloader = None\n        self.logger.info(\"Destroyed previous cached train_dataloader.\")\n\n    # If caching is enabled, create a new cached dataloader\n    if kwargs[\"cache_loader\"]:\n        self.train_dataloader = self.dataloader_factory.create_dataloader(self.train_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.train_batch_size, order=self.dataset_config.train_dataloader_order)\n        self.logger.info(\"Created new cached train_dataloader.\")\n        return self.train_dataloader\n\n    # If caching is disabled, create a new uncached dataloader\n    self.logger.debug(\"Created new uncached train_dataloader.\")\n    return self.dataloader_factory.create_dataloader(self.train_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.train_batch_size, order=self.dataset_config.train_dataloader_order)\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_val_dataloader","title":"get_val_dataloader","text":"<pre><code>get_val_dataloader(ts_id: int | None = None, workers: int | Literal['config'] = 'config', **kwargs) -&gt; DataLoader\n</code></pre> <p>Returns a PyTorch <code>DataLoader</code> for validation set.</p> <p>The <code>DataLoader</code> is created on the first call and cached for subsequent use.  The cached dataloader is cleared when either <code>get_val_df</code> or <code>get_val_numpy</code> is called.</p> <p>The structure of the returned batch depends on the <code>time_format</code> and whether <code>sliding_window_size</code> is used:</p> <ul> <li>When <code>sliding_window_size</code> is used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> <li><code>np.ndarray</code> of times with shape <code>(times - 1)</code></li> <li><code>np.ndarray</code> of time with shape <code>(1)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> </ul> </li> </ul> </li> <li>When <code>sliding_window_size</code> is not used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> <li><code>np.ndarray</code> of time with shape <code>(times)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> </ul> </li> </ul> </li> </ul> <p>The <code>DataLoader</code> is configured with the following config attributes:</p> Dataset config Description <code>val_batch_size</code> Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>sliding_window_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_prediction_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_step</code> Available only for time-based datasets. Number of times to move by after each window. <code>val_workers</code> Specifies the number of workers to use for loading validation data. Applied when <code>workers</code> = \"config\". <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading validation data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>ts_id</code> <code>int | None</code> <p>Specifies time series to take. If None returns all time series as normal. <code>Default: \"None\"</code></p> <code>None</code> <p>Returns:</p> Type Description <code>DataLoader</code> <p>An iterable <code>DataLoader</code> containing data from validation set.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_val_dataloader(self, ts_id: int | None = None, workers: int | Literal[\"config\"] = \"config\", **kwargs) -&gt; DataLoader:\n    \"\"\"\n    Returns a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for validation set.\n\n    The `DataLoader` is created on the first call and cached for subsequent use. &lt;br/&gt;\n    The cached dataloader is cleared when either [`get_val_df`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_val_df) or [`get_val_numpy`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_val_numpy) is called.\n\n    The structure of the returned batch depends on the `time_format` and whether `sliding_window_size` is used:\n\n    - When `sliding_window_size` is used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n            - `np.ndarray` of times with shape `(times - 1)`\n            - `np.ndarray` of time with shape `(1)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n    - When `sliding_window_size` is not used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n            - `np.ndarray` of time with shape `(times)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n\n    The `DataLoader` is configured with the following config attributes:\n\n    Dataset config | Description\n    -------------- | -----------\n    `val_batch_size` | Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `sliding_window_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n    `sliding_window_prediction_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n    `sliding_window_step` | Available only for time-based datasets. Number of times to move by after each window.\n    `val_workers` | Specifies the number of workers to use for loading validation data. Applied when `workers` = \"config\".\n\n\n    Parameters:\n        workers: The number of workers to use for loading validation data. `Default: \"config\"`  \n        ts_id: Specifies time series to take. If None returns all time series as normal. `Default: \"None\"`\n\n    Returns:\n        An iterable `DataLoader` containing data from validation set.        \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access val_dataloader.\")\n\n    if not self.dataset_config.has_val():\n        raise ValueError(\"Dataloader for validation set is not available in the dataset configuration.\")\n\n    assert self.val_dataset is not None, \"The val_dataset must be initialized before accessing data from validation set.\"\n\n    default_kwargs = {'take_all': False, \"cache_loader\": True}\n    kwargs = {**default_kwargs, **kwargs}\n\n    if ts_id is not None:\n\n        if ts_id == self.dataset_config.used_singular_val_time_series and self.val_dataloader is not None:\n            self.logger.debug(\"Returning cached val_dataloader.\")\n            return self.val_dataloader\n\n        dataset = self._get_singular_time_series_dataset(self.val_dataset, ts_id)\n        self.dataset_config.used_singular_val_time_series = ts_id\n        if self.val_dataloader:\n            del self.val_dataloader\n            self.val_dataloader = None\n            self.logger.info(\"Destroyed previous cached val_dataloader.\")\n\n        self.dataset_config.used_val_workers = 0\n        self.val_dataloader = self.dataloader_factory.create_dataloader(dataset, self.dataset_config, 0, False, self.dataset_config.val_batch_size)\n        self.logger.info(\"Created new cached val_dataloader.\")\n        return self.val_dataloader\n    elif self.dataset_config.used_singular_val_time_series is not None and self.val_dataloader is not None:\n        del self.val_dataloader\n        self.val_dataloader = None\n        self.dataset_config.used_singular_val_time_series = None\n        self.logger.info(\"Destroyed previous cached val_dataloader.\")\n\n    if workers == \"config\":\n        workers = self.dataset_config.val_workers\n\n    # If the dataloader is cached and number of used workers did not change, return the cached dataloader\n    if self.val_dataloader and kwargs[\"cache_loader\"] and workers == self.dataset_config.used_val_workers:\n        self.logger.debug(\"Returning cached val_dataloader.\")\n        return self.val_dataloader\n\n    # Update the used workers count\n    self.dataset_config.used_val_workers = workers\n\n    # If there's a previously cached dataloader, destroy it\n    if self.val_dataloader:\n        del self.val_dataloader\n        self.val_dataloader = None\n        self.logger.info(\"Destroyed previous cached val_dataloader.\")\n\n    # If caching is enabled, create a new cached dataloader\n    if kwargs[\"cache_loader\"]:\n        self.val_dataloader = self.dataloader_factory.create_dataloader(self.val_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.val_batch_size)\n        self.logger.info(\"Created new cached val_dataloader.\")\n        return self.val_dataloader\n\n    # If caching is disabled, create a new uncached dataloader\n    self.logger.debug(\"Created new uncached val_dataloader.\")\n    return self.dataloader_factory.create_dataloader(self.val_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.val_batch_size)\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_test_dataloader","title":"get_test_dataloader","text":"<pre><code>get_test_dataloader(ts_id: int | None = None, workers: int | Literal['config'] = 'config', **kwargs) -&gt; DataLoader\n</code></pre> <p>Returns a PyTorch <code>DataLoader</code> for test set.</p> <p>The <code>DataLoader</code> is created on the first call and cached for subsequent use.  The cached dataloader is cleared when either <code>get_test_df</code> or <code>get_test_numpy</code> is called.</p> <p>The structure of the returned batch depends on the <code>time_format</code> and whether <code>sliding_window_size</code> is used:</p> <ul> <li>When <code>sliding_window_size</code> is used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> <li><code>np.ndarray</code> of times with shape <code>(times - 1)</code></li> <li><code>np.ndarray</code> of time with shape <code>(1)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> </ul> </li> </ul> </li> <li>When <code>sliding_window_size</code> is not used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> <li><code>np.ndarray</code> of time with shape <code>(times)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> </ul> </li> </ul> </li> </ul> <p>The <code>DataLoader</code> is configured with the following config attributes:</p> Dataset config Description <code>test_batch_size</code> Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>sliding_window_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_prediction_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_step</code> Available only for time-based datasets. Number of times to move by after each window. <code>test_workers</code> Specifies the number of workers to use for loading test data. Applied when <code>workers</code> = \"config\". <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading test data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>ts_id</code> <code>int | None</code> <p>Specifies time series to take. If None returns all time series as normal. <code>Default: \"None\"</code></p> <code>None</code> <p>Returns:</p> Type Description <code>DataLoader</code> <p>An iterable <code>DataLoader</code> containing data from test set.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_test_dataloader(self, ts_id: int | None = None, workers: int | Literal[\"config\"] = \"config\", **kwargs) -&gt; DataLoader:\n    \"\"\"\n    Returns a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for test set.\n\n    The `DataLoader` is created on the first call and cached for subsequent use. &lt;br/&gt;\n    The cached dataloader is cleared when either [`get_test_df`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_test_df) or [`get_test_numpy`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_test_numpy) is called.\n\n    The structure of the returned batch depends on the `time_format` and whether `sliding_window_size` is used:\n\n    - When `sliding_window_size` is used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n            - `np.ndarray` of times with shape `(times - 1)`\n            - `np.ndarray` of time with shape `(1)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n    - When `sliding_window_size` is not used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n            - `np.ndarray` of time with shape `(times)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n\n    The `DataLoader` is configured with the following config attributes:\n\n    Dataset config | Description\n    -------------- | -----------\n    `test_batch_size` | Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `sliding_window_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n    `sliding_window_prediction_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n    `sliding_window_step` | Available only for time-based datasets. Number of times to move by after each window.\n    `test_workers` | Specifies the number of workers to use for loading test data. Applied when `workers` = \"config\".\n\n\n    Parameters:\n        workers: The number of workers to use for loading test data. `Default: \"config\"`  \n        ts_id: Specifies time series to take. If None returns all time series as normal. `Default: \"None\"`\n\n    Returns:\n        An iterable `DataLoader` containing data from test set.        \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access test_dataloader.\")\n\n    if not self.dataset_config.has_test():\n        raise ValueError(\"Dataloader for test set is not available in the dataset configuration.\")\n\n    assert self.test_dataset is not None, \"The test_dataset must be initialized before accessing data from test set.\"\n\n    default_kwargs = {'take_all': False, \"cache_loader\": True}\n    kwargs = {**default_kwargs, **kwargs}\n\n    if ts_id is not None:\n\n        if ts_id == self.dataset_config.used_singular_test_time_series and self.test_dataloader is not None:\n            self.logger.debug(\"Returning cached test_dataloader.\")\n            return self.test_dataloader\n\n        dataset = self._get_singular_time_series_dataset(self.test_dataset, ts_id)\n        self.dataset_config.used_singular_test_time_series = ts_id\n        if self.test_dataloader:\n            del self.test_dataloader\n            self.test_dataloader = None\n            self.logger.info(\"Destroyed previous cached test_dataloader.\")\n\n        self.dataset_config.used_test_workers = 0\n        self.test_dataloader = self.dataloader_factory.create_dataloader(dataset, self.dataset_config, 0, False, self.dataset_config.test_batch_size)\n        self.logger.info(\"Created new cached test_dataloader.\")\n        return self.test_dataloader\n    elif self.dataset_config.used_singular_test_time_series is not None and self.test_dataloader is not None:\n        del self.test_dataloader\n        self.test_dataloader = None\n        self.dataset_config.used_singular_test_time_series = None\n        self.logger.info(\"Destroyed previous cached test_dataloader.\")\n\n    if workers == \"config\":\n        workers = self.dataset_config.test_workers\n\n    # If the dataloader is cached and number of used workers did not change, return the cached dataloader\n    if self.test_dataloader and kwargs[\"cache_loader\"] and workers == self.dataset_config.used_test_workers:\n        self.logger.debug(\"Returning cached test_dataloader.\")\n        return self.test_dataloader\n\n    # Update the used workers count\n    self.dataset_config.used_test_workers = workers\n\n    # If there's a previously cached dataloader, destroy it\n    if self.test_dataloader:\n        del self.test_dataloader\n        self.test_dataloader = None\n        self.logger.info(\"Destroyed previous cached test_dataloader.\")\n\n    # If caching is enabled, create a new cached dataloader\n    if kwargs[\"cache_loader\"]:\n        self.test_dataloader = self.dataloader_factory.create_dataloader(self.test_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.test_batch_size)\n        self.logger.info(\"Created new cached test_dataloader.\")\n        return self.test_dataloader\n\n    # If caching is disabled, create a new uncached dataloader\n    self.logger.debug(\"Created new uncached test_dataloader.\")\n    return self.dataloader_factory.create_dataloader(self.test_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.test_batch_size)\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_all_dataloader","title":"get_all_dataloader","text":"<pre><code>get_all_dataloader(ts_id: int | None = None, workers: int | Literal['config'] = 'config', **kwargs) -&gt; DataLoader\n</code></pre> <p>Returns a PyTorch <code>DataLoader</code> for all set.</p> <p>The <code>DataLoader</code> is created on the first call and cached for subsequent use.  The cached dataloader is cleared when either <code>get_all_df</code> or <code>get_all_numpy</code> is called.</p> <p>The structure of the returned batch depends on the <code>time_format</code> and whether <code>sliding_window_size</code> is used:</p> <ul> <li>When <code>sliding_window_size</code> is used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> <li><code>np.ndarray</code> of times with shape <code>(times - 1)</code></li> <li><code>np.ndarray</code> of time with shape <code>(1)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> </ul> </li> </ul> </li> <li>When <code>sliding_window_size</code> is not used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> <li><code>np.ndarray</code> of time with shape <code>(times)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> </ul> </li> </ul> </li> </ul> <p>The <code>DataLoader</code> is configured with the following config attributes:</p> Dataset config Description <code>all_batch_size</code> Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>sliding_window_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_prediction_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_step</code> Available only for time-based datasets. Number of times to move by after each window. <code>all_workers</code> Specifies the number of workers to use for loading all data. Applied when <code>workers</code> = \"config\". <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading all data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>ts_id</code> <code>int | None</code> <p>Specifies time series to take. If None returns all time series as normal. <code>Default: \"None\"</code></p> <code>None</code> <p>Returns:</p> Type Description <code>DataLoader</code> <p>An iterable <code>DataLoader</code> containing data from all set.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_all_dataloader(self, ts_id: int | None = None, workers: int | Literal[\"config\"] = \"config\", **kwargs) -&gt; DataLoader:\n    \"\"\"\n    Returns a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for all set.\n\n    The `DataLoader` is created on the first call and cached for subsequent use. &lt;br/&gt;\n    The cached dataloader is cleared when either [`get_all_df`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_all_df) or [`get_all_numpy`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_all_numpy) is called.\n\n    The structure of the returned batch depends on the `time_format` and whether `sliding_window_size` is used:\n\n    - When `sliding_window_size` is used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n            - `np.ndarray` of times with shape `(times - 1)`\n            - `np.ndarray` of time with shape `(1)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n    - When `sliding_window_size` is not used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n            - `np.ndarray` of time with shape `(times)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n\n    The `DataLoader` is configured with the following config attributes:\n\n    Dataset config | Description\n    -------------- | -----------\n    `all_batch_size` | Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `sliding_window_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n    `sliding_window_prediction_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n    `sliding_window_step` | Available only for time-based datasets. Number of times to move by after each window.\n    `all_workers` | Specifies the number of workers to use for loading all data. Applied when `workers` = \"config\".\n\n\n    Parameters:\n        workers: The number of workers to use for loading all data. `Default: \"config\"`  \n        ts_id: Specifies time series to take. If None returns all time series as normal. `Default: \"None\"`\n\n    Returns:\n        An iterable `DataLoader` containing data from all set.       \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access all_dataloader.\")\n\n    if not self.dataset_config.has_all():\n        raise ValueError(\"Dataloader for all set is not available in the dataset configuration.\")\n\n    assert self.all_dataset is not None, \"The all_dataset must be initialized before accessing data from all set.\"\n\n    default_kwargs = {'take_all': False, \"cache_loader\": True}\n    kwargs = {**default_kwargs, **kwargs}\n\n    if ts_id is not None:\n\n        if ts_id == self.dataset_config.used_singular_all_time_series and self.all_dataloader is not None:\n            self.logger.debug(\"Returning cached all_dataloader.\")\n            return self.all_dataloader\n\n        dataset = self._get_singular_time_series_dataset(self.all_dataset, ts_id)\n        self.dataset_config.used_singular_all_time_series = ts_id\n        if self.all_dataloader:\n            del self.all_dataloader\n            self.all_dataloader = None\n            self.logger.info(\"Destroyed previous cached all_dataloader.\")\n\n        self.dataset_config.used_all_workers = 0\n        self.all_dataloader = self.dataloader_factory.create_dataloader(dataset, self.dataset_config, 0, False, self.dataset_config.all_batch_size)\n        self.logger.info(\"Created new cached all_dataloader.\")\n        return self.all_dataloader\n    elif self.dataset_config.used_singular_all_time_series is not None and self.all_dataloader is not None:\n        del self.all_dataloader\n        self.all_dataloader = None\n        self.dataset_config.used_singular_all_time_series = None\n        self.logger.info(\"Destroyed previous cached all_dataloader.\")\n\n    if workers == \"config\":\n        workers = self.dataset_config.all_workers\n\n    # If the dataloader is cached and number of used workers did not change, return the cached dataloader\n    if self.all_dataloader and kwargs[\"cache_loader\"] and workers == self.dataset_config.used_all_workers:\n        self.logger.debug(\"Returning cached all_dataloader.\")\n        return self.all_dataloader\n\n    # Update the used workers count\n    self.dataset_config.used_all_workers = workers\n\n    # If there's a previously cached dataloader, destroy it\n    if self.all_dataloader:\n        del self.all_dataloader\n        self.all_dataloader = None\n        self.logger.info(\"Destroyed previous cached all_dataloader.\")\n\n    # If caching is enabled, create a new cached dataloader\n    if kwargs[\"cache_loader\"]:\n        self.all_dataloader = self.dataloader_factory.create_dataloader(self.all_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.all_batch_size)\n        self.logger.info(\"Created new cached all_dataloader.\")\n        return self.all_dataloader\n\n    # If caching is disabled, create a new uncached dataloader\n    self.logger.debug(\"Creating new uncached all_dataloader.\")\n    return self.dataloader_factory.create_dataloader(self.all_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.all_batch_size)\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_train_df","title":"get_train_df","text":"<pre><code>get_train_df(workers: int | Literal['config'] = 'config', as_single_dataframe: bool = True) -&gt; pd.DataFrame\n</code></pre> <p>Creates a Pandas <code>DataFrame</code> containing all the data from training set grouped by time series.</p> <p>This method uses the <code>train_dataloader</code> with a batch size set to the total number of data in the training set. The cached <code>train_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire training set is loaded into memory, which may lead to high memory usage. If working with large training set, consider using <code>get_train_dataloader</code> instead to handle data in batches.</p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading train data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>as_single_dataframe</code> <code>bool</code> <p>Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. <code>Default: True</code> </p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A single Pandas DataFrame containing all data from training set, or a list of DataFrames (one per time series).</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_train_df(self, workers: int | Literal[\"config\"] = \"config\", as_single_dataframe: bool = True) -&gt; pd.DataFrame:\n    \"\"\"\n    Creates a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) containing all the data from training set grouped by time series.\n\n    This method uses the `train_dataloader` with a batch size set to the total number of data in the training set. The cached `train_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire training set is loaded into memory, which may lead to high memory usage. If working with large training set, consider using `get_train_dataloader` instead to handle data in batches.\n\n    Parameters:\n        workers: The number of workers to use for loading train data. `Default: \"config\"`  \n        as_single_dataframe: Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. `Default: True` \n\n    Returns:\n        A single Pandas DataFrame containing all data from training set, or a list of DataFrames (one per time series).\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access train_dataloader.\")\n\n    if not self.dataset_config.has_train():\n        raise ValueError(\"Dataloader for training set is not available in the dataset configuration.\")\n\n    assert self.train_dataset is not None, \"The train_dataset must be initialized before accessing data from training set.\"\n\n    ts_ids, time_period = self.dataset_config._get_train()\n\n    should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n    dataloader = self.get_train_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n    return self._get_df(dataloader, as_single_dataframe, ts_ids, time_period)\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_val_df","title":"get_val_df","text":"<pre><code>get_val_df(workers: int | Literal['config'] = 'config', as_single_dataframe: bool = True) -&gt; pd.DataFrame\n</code></pre> <p>Create a Pandas <code>DataFrame</code> containing all the data from validation set grouped by time series.</p> <p>This method uses the <code>val_dataloader</code> with a batch size set to the total number of data in the validation set. The cached <code>val_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire validation set is loaded into memory, which may lead to high memory usage. If working with large validation set, consider using <code>get_val_dataloader</code> instead to handle data in batches.</p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading validation data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>as_single_dataframe</code> <code>bool</code> <p>Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. <code>Default: True</code> </p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A single Pandas DataFrame containing all data from validation set, or a list of DataFrames (one per time series).</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_val_df(self, workers: int | Literal[\"config\"] = \"config\", as_single_dataframe: bool = True) -&gt; pd.DataFrame:\n    \"\"\"\n    Create a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) containing all the data from validation set grouped by time series.\n\n    This method uses the `val_dataloader` with a batch size set to the total number of data in the validation set. The cached `val_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire validation set is loaded into memory, which may lead to high memory usage. If working with large validation set, consider using `get_val_dataloader` instead to handle data in batches.\n\n    Parameters:\n        workers: The number of workers to use for loading validation data. `Default: \"config\"`  \n        as_single_dataframe: Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. `Default: True` \n\n    Returns:\n        A single Pandas DataFrame containing all data from validation set, or a list of DataFrames (one per time series).\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access val_dataloader.\")\n\n    if not self.dataset_config.has_val():\n        raise ValueError(\"Dataloader for validation set is not available in the dataset configuration.\")\n\n    assert self.val_dataset is not None, \"The val_dataset must be initialized before accessing data from validation set.\"\n\n    ts_ids, time_period = self.dataset_config._get_val()\n\n    should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n    dataloader = self.get_val_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n    return self._get_df(dataloader, as_single_dataframe, ts_ids, time_period)\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_test_df","title":"get_test_df","text":"<pre><code>get_test_df(workers: int | Literal['config'] = 'config', as_single_dataframe: bool = True) -&gt; pd.DataFrame\n</code></pre> <p>Creates a Pandas <code>DataFrame</code> containing all the data from test set grouped by time series.</p> <p>This method uses the <code>test_dataloader</code> with a batch size set to the total number of data in the test set. The cached <code>test_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire test set is loaded into memory, which may lead to high memory usage. If working with large test set, consider using <code>get_test_dataloader</code> instead to handle data in batches.</p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading test data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>as_single_dataframe</code> <code>bool</code> <p>Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. <code>Default: True</code> </p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A single Pandas DataFrame containing all data from test set, or a list of DataFrames (one per time series).</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_test_df(self, workers: int | Literal[\"config\"] = \"config\", as_single_dataframe: bool = True) -&gt; pd.DataFrame:\n    \"\"\"\n    Creates a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) containing all the data from test set grouped by time series.\n\n    This method uses the `test_dataloader` with a batch size set to the total number of data in the test set. The cached `test_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire test set is loaded into memory, which may lead to high memory usage. If working with large test set, consider using `get_test_dataloader` instead to handle data in batches.\n\n    Parameters:\n        workers: The number of workers to use for loading test data. `Default: \"config\"`  \n        as_single_dataframe: Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. `Default: True` \n\n    Returns:\n        A single Pandas DataFrame containing all data from test set, or a list of DataFrames (one per time series).\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access test_dataloader.\")\n\n    if not self.dataset_config.has_test():\n        raise ValueError(\"Dataloader for test set is not available in the dataset configuration.\")\n\n    assert self.test_dataset is not None, \"The test_dataset must be initialized before accessing data from test set.\"\n\n    ts_ids, time_period = self.dataset_config._get_test()\n\n    should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n    dataloader = self.get_test_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n    return self._get_df(dataloader, as_single_dataframe, ts_ids, time_period)\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_all_df","title":"get_all_df","text":"<pre><code>get_all_df(workers: int | Literal['config'] = 'config', as_single_dataframe: bool = True) -&gt; pd.DataFrame\n</code></pre> <p>Creates a Pandas <code>DataFrame</code> containing all the data from all set grouped by time series.</p> <p>This method uses the <code>all_dataloader</code> with a batch size set to the total number of data in the all set. The cached <code>all_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire all set is loaded into memory, which may lead to high memory usage. If working with large all set, consider using <code>get_all_dataloader</code> instead to handle data in batches.</p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading all data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>as_single_dataframe</code> <code>bool</code> <p>Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. <code>Default: True</code> </p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A single Pandas DataFrame containing all data from all set, or a list of DataFrames (one per time series).</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_all_df(self, workers: int | Literal[\"config\"] = \"config\", as_single_dataframe: bool = True) -&gt; pd.DataFrame:\n    \"\"\"\n    Creates a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) containing all the data from all set grouped by time series.\n\n    This method uses the `all_dataloader` with a batch size set to the total number of data in the all set. The cached `all_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire all set is loaded into memory, which may lead to high memory usage. If working with large all set, consider using `get_all_dataloader` instead to handle data in batches.\n\n    Parameters:\n        workers: The number of workers to use for loading all data. `Default: \"config\"`  \n        as_single_dataframe: Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. `Default: True` \n\n    Returns:\n        A single Pandas DataFrame containing all data from all set, or a list of DataFrames (one per time series).\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access all_dataloader.\")\n\n    if not self.dataset_config.has_all():\n        raise ValueError(\"Dataloader for all set is not available in the dataset configuration.\")\n\n    assert self.all_dataset is not None, \"The all_dataset must be initialized before accessing data from all set.\"\n\n    ts_ids, time_period = self.dataset_config._get_all()\n\n    should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n    dataloader = self.get_all_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n    return self._get_df(dataloader, as_single_dataframe, ts_ids, time_period)\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_train_numpy","title":"get_train_numpy","text":"<pre><code>get_train_numpy(workers: int | Literal['config'] = 'config') -&gt; np.ndarray\n</code></pre> <p>Creates a NumPy array containing all the data from training set grouped by time series, with the shape <code>(num_time_series, num_times, num_features)</code>.</p> <p>This method uses the <code>train_dataloader</code> with a batch size set to the total number of data in the training set. The cached <code>train_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire training set is loaded into memory, which may lead to high memory usage. If working with large training set, consider using <code>get_train_dataloader</code> instead to handle data in batches.        </p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading train data. <code>Default: \"config\"</code> </p> <code>'config'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A NumPy array containing all the data in training set with the shape <code>(num_time_series, num_times, num_features)</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_train_numpy(self, workers: int | Literal[\"config\"] = \"config\") -&gt; np.ndarray:\n    \"\"\"\n    Creates a NumPy array containing all the data from training set grouped by time series, with the shape `(num_time_series, num_times, num_features)`.\n\n    This method uses the `train_dataloader` with a batch size set to the total number of data in the training set. The cached `train_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire training set is loaded into memory, which may lead to high memory usage. If working with large training set, consider using `get_train_dataloader` instead to handle data in batches.        \n\n    Parameters:\n        workers: The number of workers to use for loading train data. `Default: \"config\"`  \n\n    Returns:\n        A NumPy array containing all the data in training set with the shape `(num_time_series, num_times, num_features)`.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access train_dataloader.\")\n\n    if not self.dataset_config.has_train():\n        raise ValueError(\"Dataloader for training set is not available in the dataset configuration.\")\n\n    assert self.train_dataset is not None, \"The train_dataset must be initialized before accessing data from training set.\"\n\n    ts_ids, time_period = self.dataset_config._get_train()\n\n    should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n    dataloader = self.get_train_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n    return self._get_numpy(dataloader, ts_ids, time_period)\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_val_numpy","title":"get_val_numpy","text":"<pre><code>get_val_numpy(workers: int | Literal['config'] = 'config') -&gt; np.ndarray\n</code></pre> <p>Creates a NumPy array containing all the data from validation set grouped by time series, with the shape <code>(num_time_series, num_times, num_features)</code>.</p> <p>This method uses the <code>val_dataloader</code> with a batch size set to the total number of data in the validation set. The cached <code>val_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire validation set is loaded into memory, which may lead to high memory usage. If working with large validation set, consider using <code>get_val_dataloader</code> instead to handle data in batches.        </p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading validation data. <code>Default: \"config\"</code> </p> <code>'config'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A NumPy array containing all the data in validation set with the shape <code>(num_time_series, num_times, num_features)</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_val_numpy(self, workers: int | Literal[\"config\"] = \"config\") -&gt; np.ndarray:\n    \"\"\"\n    Creates a NumPy array containing all the data from validation set grouped by time series, with the shape `(num_time_series, num_times, num_features)`.\n\n    This method uses the `val_dataloader` with a batch size set to the total number of data in the validation set. The cached `val_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire validation set is loaded into memory, which may lead to high memory usage. If working with large validation set, consider using `get_val_dataloader` instead to handle data in batches.        \n\n    Parameters:\n        workers: The number of workers to use for loading validation data. `Default: \"config\"`  \n\n    Returns:\n        A NumPy array containing all the data in validation set with the shape `(num_time_series, num_times, num_features)`.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access val_dataloader.\")\n\n    if not self.dataset_config.has_val():\n        raise ValueError(\"Dataloader for validation set is not available in the dataset configuration.\")\n\n    assert self.val_dataset is not None, \"The val_dataset must be initialized before accessing data from validation set.\"\n\n    ts_ids, time_period = self.dataset_config._get_val()\n\n    should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n    dataloader = self.get_val_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n    return self._get_numpy(dataloader, ts_ids, time_period)\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_test_numpy","title":"get_test_numpy","text":"<pre><code>get_test_numpy(workers: int | Literal['config'] = 'config') -&gt; np.ndarray\n</code></pre> <p>Creates a NumPy array containing all the data from test set grouped by time series, with the shape <code>(num_time_series, num_times, num_features)</code>.</p> <p>This method uses the <code>test_dataloader</code> with a batch size set to the total number of data in the test set. The cached <code>test_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire test set is loaded into memory, which may lead to high memory usage. If working with large test set, consider using <code>get_test_dataloader</code> instead to handle data in batches.        </p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading test data. <code>Default: \"config\"</code> </p> <code>'config'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A NumPy array containing all the data in test set with the shape <code>(num_time_series, num_times, num_features)</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_test_numpy(self, workers: int | Literal[\"config\"] = \"config\") -&gt; np.ndarray:\n    \"\"\"\n    Creates a NumPy array containing all the data from test set grouped by time series, with the shape `(num_time_series, num_times, num_features)`.\n\n    This method uses the `test_dataloader` with a batch size set to the total number of data in the test set. The cached `test_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire test set is loaded into memory, which may lead to high memory usage. If working with large test set, consider using `get_test_dataloader` instead to handle data in batches.        \n\n    Parameters:\n        workers: The number of workers to use for loading test data. `Default: \"config\"`  \n\n    Returns:\n        A NumPy array containing all the data in test set with the shape `(num_time_series, num_times, num_features)`.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access test_dataloader.\")\n\n    if not self.dataset_config.has_test():\n        raise ValueError(\"Dataloader for test set is not available in the dataset configuration.\")\n\n    assert self.test_dataset is not None, \"The test_dataset must be initialized before accessing data from test set.\"\n\n    ts_ids, time_period = self.dataset_config._get_test()\n\n    should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n    dataloader = self.get_test_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n    return self._get_numpy(dataloader, ts_ids, time_period)\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_all_numpy","title":"get_all_numpy","text":"<pre><code>get_all_numpy(workers: int | Literal['config'] = 'config') -&gt; np.ndarray\n</code></pre> <p>Creates a NumPy array containing all the data from all set grouped by time series, with the shape <code>(num_time_series, num_times, num_features)</code>.</p> <p>This method uses the <code>all_dataloader</code> with a batch size set to the total number of data in the all set. The cached <code>all_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire all set is loaded into memory, which may lead to high memory usage. If working with large all set, consider using <code>get_all_dataloader</code> instead to handle data in batches.        </p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading all data. <code>Default: \"config\"</code> </p> <code>'config'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A NumPy array containing all the data in all set with the shape <code>(num_time_series, num_times, num_features)</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_all_numpy(self, workers: int | Literal[\"config\"] = \"config\") -&gt; np.ndarray:\n    \"\"\"\n    Creates a NumPy array containing all the data from all set grouped by time series, with the shape `(num_time_series, num_times, num_features)`.\n\n    This method uses the `all_dataloader` with a batch size set to the total number of data in the all set. The cached `all_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire all set is loaded into memory, which may lead to high memory usage. If working with large all set, consider using `get_all_dataloader` instead to handle data in batches.        \n\n    Parameters:\n        workers: The number of workers to use for loading all data. `Default: \"config\"`  \n\n    Returns:\n        A NumPy array containing all the data in all set with the shape `(num_time_series, num_times, num_features)`.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access all_dataloader.\")\n\n    if not self.dataset_config.has_all():\n        raise ValueError(\"Dataloader for all set is not available in the dataset configuration.\")\n\n    assert self.all_dataset is not None, \"The all_dataset must be initialized before accessing data from all set.\"\n\n    ts_ids, time_period = self.dataset_config._get_all()\n\n    should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n    dataloader = self.get_all_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n    return self._get_numpy(dataloader, ts_ids, time_period)\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.update_dataset_config_and_initialize","title":"update_dataset_config_and_initialize  <code>abstractmethod</code>","text":"<pre><code>update_dataset_config_and_initialize(**kwargs)\n</code></pre> <p>Used to modify selected configurations set in config.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>@abstractmethod\ndef update_dataset_config_and_initialize(self, **kwargs):\n    \"\"\"Used to modify selected configurations set in config.\"\"\"\n    ...\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.apply_filler","title":"apply_filler","text":"<pre><code>apply_filler(fill_missing_with: type | FillerType | Literal['mean_filler', 'forward_filler', 'linear_interpolation_filler'] | None, workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Used for updating filler set in config. Set parameter to <code>config</code> to keep it as it is config. If exception is thrown during set, no changes are made.</p> <p>Affects following configuration: </p> Dataset config Description <code>fill_missing_with</code> Defines how to fill missing values in the dataset. <p>Parameters:</p> Name Type Description Default <code>fill_missing_with</code> <code>type | FillerType | Literal['mean_filler', 'forward_filler', 'linear_interpolation_filler'] | None</code> <p>Defines how to fill missing values in the dataset. <code>Defaults: config</code>.  </p> required <code>workers</code> <code>int | Literal['config']</code> <p>How many workers to use when setting new filler. <code>Defaults: config</code>.</p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def apply_filler(self, fill_missing_with: type | FillerType | Literal[\"mean_filler\", \"forward_filler\", \"linear_interpolation_filler\"] | None, workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"Used for updating filler set in config.\n    Set parameter to `config` to keep it as it is config.\n    If exception is thrown during set, no changes are made.\n\n    Affects following configuration: \n\n    Dataset config | Description\n    -------------- | -----------\n    `fill_missing_with` | Defines how to fill missing values in the dataset.\n\n    Parameters:\n        fill_missing_with: Defines how to fill missing values in the dataset. `Defaults: config`.  \n        workers: How many workers to use when setting new filler. `Defaults: config`.      \n    \"\"\"\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating filler.\")\n\n    self.update_dataset_config_and_initialize(fill_missing_with=fill_missing_with, workers=workers)\n    self.logger.info(\"Filler has been changed successfuly.\")\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.apply_anomaly_handler","title":"apply_anomaly_handler","text":"<pre><code>apply_anomaly_handler(handle_anomalies_with: type | AnomalyHandlerType | Literal['z-score', 'interquartile_range'] | None | Literal['config'], workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Used for updating anomaly handler set in config.</p> <p>Set parameter to <code>config</code> to keep it as it is config.</p> <p>If exception is thrown during set, no changes are made.</p> <p>Affects following configuration:</p> Dataset config Description <code>handle_anomalies_with</code> Defines the anomaly handler to handle anomalies in the dataset. <p>Parameters:</p> Name Type Description Default <code>handle_anomalies_with</code> <code>type | AnomalyHandlerType | Literal['z-score', 'interquartile_range'] | None | Literal['config']</code> <p>Defines the anomaly handler to handle anomalies in the dataset. <code>Defaults: config</code>.  </p> required <code>workers</code> <code>int | Literal['config']</code> <p>How many workers to use when setting new filler. <code>Defaults: config</code>.</p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def apply_anomaly_handler(self, handle_anomalies_with: type | AnomalyHandlerType | Literal[\"z-score\", \"interquartile_range\"] | None | Literal[\"config\"], workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"Used for updating anomaly handler set in config.\n\n    Set parameter to `config` to keep it as it is config.\n\n    If exception is thrown during set, no changes are made.\n\n    Affects following configuration:\n\n    Dataset config | Description\n    -------------- | -----------\n    `handle_anomalies_with` | Defines the anomaly handler to handle anomalies in the dataset.\n\n    Parameters:\n        handle_anomalies_with: Defines the anomaly handler to handle anomalies in the dataset. `Defaults: config`.  \n        workers: How many workers to use when setting new filler. `Defaults: config`.      \n    \"\"\"\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating anomaly handler.\")\n\n    self.update_dataset_config_and_initialize(handle_anomalies_with=handle_anomalies_with, workers=workers)\n    self.logger.info(\"Anomaly handler has been changed successfuly.\")\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.apply_transformer","title":"apply_transformer","text":"<pre><code>apply_transformer(transform_with: type | list[Transformer] | ndarray[Transformer] | TransformerType | Transformer | Literal['min_max_scaler', 'standard_scaler', 'max_abs_scaler', 'log_transformer', 'robust_scaler', 'power_transformer', 'quantile_transformer', 'l2_normalizer'] | None | Literal['config'] = 'config', create_transformer_per_time_series: bool | Literal['config'] = 'config', partial_fit_initialized_transformers: bool | Literal['config'] = 'config', workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Used for updating transformer and relevenat configurations set in config. Set parameter to <code>config</code> to keep it as it is config. If exception is thrown during set, no changes are made.</p> <p>Affects following configuration:</p> Dataset config Description <code>transform_with</code> Defines the transformer to transform the dataset. <code>create_transformer_per_time_series</code> If <code>True</code>, a separate transformer is created for each time series. Not used when using already initialized transformers. <code>partial_fit_initialized_transformers</code> If <code>True</code>, partial fitting on train set is performed when using initialized transformers. <p>Parameters:</p> Name Type Description Default <code>transform_with</code> <code>type | list[Transformer] | ndarray[Transformer] | TransformerType | Transformer | Literal['min_max_scaler', 'standard_scaler', 'max_abs_scaler', 'log_transformer', 'robust_scaler', 'power_transformer', 'quantile_transformer', 'l2_normalizer'] | None | Literal['config']</code> <p>Defines the transformer to transform the dataset. <code>Defaults: config</code>.  </p> <code>'config'</code> <code>create_transformer_per_time_series</code> <code>bool | Literal['config']</code> <p>If <code>True</code>, a separate transformer is created for each time series. Not used when using already initialized transformers. <code>Defaults: config</code>.  </p> <code>'config'</code> <code>partial_fit_initialized_transformers</code> <code>bool | Literal['config']</code> <p>If <code>True</code>, partial fitting on train set is performed when using initiliazed transformers. <code>Defaults: config</code>.  </p> <code>'config'</code> <code>workers</code> <code>int | Literal['config']</code> <p>How many workers to use when setting new transformer. <code>Defaults: config</code>.</p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def apply_transformer(self, transform_with: type | list[Transformer] | np.ndarray[Transformer] | TransformerType | Transformer | Literal[\"min_max_scaler\", \"standard_scaler\", \"max_abs_scaler\", \"log_transformer\", \"robust_scaler\", \"power_transformer\", \"quantile_transformer\", \"l2_normalizer\"] | None | Literal[\"config\"] = \"config\",\n                      create_transformer_per_time_series: bool | Literal[\"config\"] = \"config\", partial_fit_initialized_transformers: bool | Literal[\"config\"] = \"config\", workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"Used for updating transformer and relevenat configurations set in config.\n    Set parameter to `config` to keep it as it is config.\n    If exception is thrown during set, no changes are made.\n\n    Affects following configuration:\n\n    Dataset config | Description\n    -------------- | -----------\n    `transform_with` | Defines the transformer to transform the dataset.\n    `create_transformer_per_time_series` | If `True`, a separate transformer is created for each time series. Not used when using already initialized transformers.\n    `partial_fit_initialized_transformers` | If `True`, partial fitting on train set is performed when using initialized transformers.\n\n    Parameters:\n        transform_with: Defines the transformer to transform the dataset. `Defaults: config`.  \n        create_transformer_per_time_series: If `True`, a separate transformer is created for each time series. Not used when using already initialized transformers. `Defaults: config`.  \n        partial_fit_initialized_transformers: If `True`, partial fitting on train set is performed when using initiliazed transformers. `Defaults: config`.  \n        workers: How many workers to use when setting new transformer. `Defaults: config`.      \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating transformer values.\")\n\n    self.update_dataset_config_and_initialize(transform_with=transform_with, create_transformer_per_time_series=create_transformer_per_time_series, partial_fit_initialized_transformers=partial_fit_initialized_transformers, workers=workers)\n    self.logger.info(\"Transformer configuration has been changed successfuly.\")\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.set_default_values","title":"set_default_values","text":"<pre><code>set_default_values(default_values: list[Number] | NDArray[number] | dict[str, Number] | Number | Literal['default'] | None, workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Used for updating default values set in config. Set parameter to <code>config</code> to keep it as it is config. If exception is thrown during set, no changes are made.</p> <p>Affects following configuration:</p> Dataset config Description <code>default_values</code> Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature. <p>Parameters:</p> Name Type Description Default <code>default_values</code> <code>list[Number] | NDArray[number] | dict[str, Number] | Number | Literal['default'] | None</code> <p>Default values for missing data, applied before fillers. <code>Defaults: config</code>.  </p> required <code>workers</code> <code>int | Literal['config']</code> <p>How many workers to use when setting new default values. <code>Defaults: config</code>.</p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def set_default_values(self, default_values: list[Number] | npt.NDArray[np.number] | dict[str, Number] | Number | Literal[\"default\"] | None, workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"Used for updating default values set in config.\n    Set parameter to `config` to keep it as it is config.\n    If exception is thrown during set, no changes are made.\n\n    Affects following configuration:\n\n    Dataset config | Description\n    -------------- | -----------\n    `default_values` | Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature.\n\n    Parameters:\n        default_values: Default values for missing data, applied before fillers. `Defaults: config`.  \n        workers: How many workers to use when setting new default values. `Defaults: config`.      \n    \"\"\"\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating default values.\")\n\n    self.update_dataset_config_and_initialize(default_values=default_values, workers=workers)\n    self.logger.info(\"Default values has been changed successfuly.\")\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.set_preprocess_order","title":"set_preprocess_order","text":"<pre><code>set_preprocess_order(preprocess_order: list[str, type] | Literal['config'] = 'config', workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Used for updating preprocess_order set in config. Set parameter to <code>config</code> to keep it as it is config. If exception is thrown during set, no changes are made.</p> <p>Affects following configuration: </p> Dataset config Description <code>preprocess_order</code> Used order of when preprocesses are applied. Can be also used to add/remove custom handlers. <p>Parameters:</p> Name Type Description Default <code>preprocess_order</code> <code>list[str, type] | Literal['config']</code> <p>Used order of when preprocesses are applied. Can be also used to add/remove custom handlers. <code>Defaults: config</code>.  </p> <code>'config'</code> <code>workers</code> <code>int | Literal['config']</code> <p>How many workers to use when setting new default values. <code>Defaults: config</code>.</p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def set_preprocess_order(self, preprocess_order: list[str, type] | Literal[\"config\"] = \"config\", workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"Used for updating preprocess_order set in config.\n    Set parameter to `config` to keep it as it is config.\n    If exception is thrown during set, no changes are made.\n\n    Affects following configuration: \n\n    Dataset config | Description\n    -------------- | -----------\n    `preprocess_order` | Used order of when preprocesses are applied. Can be also used to add/remove custom handlers.\n\n    Parameters:\n        preprocess_order: Used order of when preprocesses are applied. Can be also used to add/remove custom handlers. `Defaults: config`.  \n        workers: How many workers to use when setting new default values. `Defaults: config`.      \n    \"\"\"\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating preprocess order.\")\n\n    self.update_dataset_config_and_initialize(preprocess_order=preprocess_order, workers=workers)\n    self.logger.info(\"Preprocess order has been changed successfuly.\")\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.set_workers","title":"set_workers","text":"<pre><code>set_workers(train_workers: int | Literal['config'] = 'config', val_workers: int | Literal['config'] = 'config', test_workers: int | Literal['config'] = 'config', all_workers: int | Literal['config'] = 'config', init_workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Used for updating workers set in config. Set parameter to <code>config</code> to keep it as it is config. If exception is thrown during set, no changes are made.</p> <p>Affects following configuration:</p> Dataset config Description <code>train_workers</code> Number of workers for loading training data. <code>val_workers</code> Number of workers for loading validation data. <code>test_workers</code> Number of workers for loading test data. <code>all_workers</code> Number of workers for loading all data. <code>init_workers</code> Number of workers for dataset configuration. <p>Parameters:</p> Name Type Description Default <code>train_workers</code> <code>int | Literal['config']</code> <p>Number of workers for loading training data. <code>Defaults: config</code>.</p> <code>'config'</code> <code>val_workers</code> <code>int | Literal['config']</code> <p>Number of workers for loading validation data. <code>Defaults: config</code>.</p> <code>'config'</code> <code>test_workers</code> <code>int | Literal['config']</code> <p>Number of workers for loading test data. <code>Defaults: config</code>.</p> <code>'config'</code> <code>all_workers</code> <code>int | Literal['config']</code> <p>Number of workers for loading all data.  <code>Defaults: config</code>.</p> <code>'config'</code> <code>init_workers</code> <code>int | Literal['config']</code> <p>Number of workers for dataset configuration. <code>Defaults: config</code>.</p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def set_workers(self, train_workers: int | Literal[\"config\"] = \"config\", val_workers: int | Literal[\"config\"] = \"config\",\n                test_workers: int | Literal[\"config\"] = \"config\", all_workers: int | Literal[\"config\"] = \"config\", init_workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"Used for updating workers set in config.\n    Set parameter to `config` to keep it as it is config.\n    If exception is thrown during set, no changes are made.\n\n    Affects following configuration:\n\n    Dataset config | Description\n    -------------- | -----------\n    `train_workers` | Number of workers for loading training data.\n    `val_workers` | Number of workers for loading validation data.\n    `test_workers` | Number of workers for loading test data.\n    `all_workers` | Number of workers for loading all data.\n    `init_workers` | Number of workers for dataset configuration.\n\n    Parameters:\n        train_workers: Number of workers for loading training data. `Defaults: config`.\n        val_workers: Number of workers for loading validation data. `Defaults: config`.\n        test_workers: Number of workers for loading test data. `Defaults: config`.\n        all_workers: Number of workers for loading all data.  `Defaults: config`.\n        init_workers: Number of workers for dataset configuration. `Defaults: config`.            \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating workers.\")\n\n    self.update_dataset_config_and_initialize(train_workers=train_workers, val_workers=val_workers, test_workers=test_workers, all_workers=all_workers, init_workers=init_workers, workers=\"config\")\n    self.logger.info(\"Workers has been changed successfuly.\")\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.set_batch_sizes","title":"set_batch_sizes","text":"<pre><code>set_batch_sizes(train_batch_size: int | Literal['config'] = 'config', val_batch_size: int | Literal['config'] = 'config', test_batch_size: int | Literal['config'] = 'config', all_batch_size: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Used for updating batch sizes set in config. Set parameter to <code>config</code> to keep it as it is config. If exception is thrown during set, no changes are made.</p> <p>Affects following configuration:</p> Dataset config Description <code>train_batch_size</code> Number of samples per batch for train set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>val_batch_size</code> Number of samples per batch for val set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>test_batch_size</code> Number of samples per batch for test set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>all_batch_size</code> Number of samples per batch for all set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <p>Parameters:</p> Name Type Description Default <code>train_batch_size</code> <code>int | Literal['config']</code> <p>Number of samples per batch for train set. <code>Defaults: config</code>.</p> <code>'config'</code> <code>val_batch_size</code> <code>int | Literal['config']</code> <p>Number of samples per batch for val set. <code>Defaults: config</code>.</p> <code>'config'</code> <code>test_batch_size</code> <code>int | Literal['config']</code> <p>Number of samples per batch for test set. <code>Defaults: config</code>.</p> <code>'config'</code> <code>all_batch_size</code> <code>int | Literal['config']</code> <p>Number of samples per batch for all set. <code>Defaults: config</code>.</p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def set_batch_sizes(self, train_batch_size: int | Literal[\"config\"] = \"config\", val_batch_size: int | Literal[\"config\"] = \"config\",\n                    test_batch_size: int | Literal[\"config\"] = \"config\", all_batch_size: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"Used for updating batch sizes set in config.\n    Set parameter to `config` to keep it as it is config.\n    If exception is thrown during set, no changes are made.\n\n    Affects following configuration:\n\n    Dataset config | Description\n    -------------- | -----------\n    `train_batch_size` | Number of samples per batch for train set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `val_batch_size` | Number of samples per batch for val set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `test_batch_size` | Number of samples per batch for test set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `all_batch_size` | Number of samples per batch for all set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n\n    Parameters:\n        train_batch_size: Number of samples per batch for train set. `Defaults: config`.\n        val_batch_size: Number of samples per batch for val set. `Defaults: config`.\n        test_batch_size: Number of samples per batch for test set. `Defaults: config`.\n        all_batch_size: Number of samples per batch for all set. `Defaults: config`.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating batch sizes.\")\n\n    self.update_dataset_config_and_initialize(train_batch_size=train_batch_size, val_batch_size=val_batch_size, test_batch_size=test_batch_size, all_batch_size=all_batch_size, workers=\"config\")\n    self.logger.info(\"Batch sizes has been changed successfuly.\")\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.display_dataset_details","title":"display_dataset_details","text":"<pre><code>display_dataset_details() -&gt; None\n</code></pre> <p>Display information about the contents of the dataset.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>    def display_dataset_details(self) -&gt; None:\n        \"\"\"Display information about the contents of the dataset.  \"\"\"\n\n        to_display = f'''\nDataset details:\n\n    {self.metadata.aggregation}\n        Time indices: {range(self.metadata.time_indices[ID_TIME_COLUMN_NAME][0], self.metadata.time_indices[ID_TIME_COLUMN_NAME][-1])}\n        Datetime: {(datetime.fromtimestamp(self.metadata.time_indices['time'][0], tz=timezone.utc), datetime.fromtimestamp(self.metadata.time_indices['time'][-1], timezone.utc))}\n\n    {self.metadata.source_type}\n        Time series indices: {get_abbreviated_list_string(self.metadata.ts_indices[self.metadata.ts_id_name])}; use 'get_available_ts_indices' for full list\n        Features with default values: {self.metadata.default_values}\n\n        Additional data: {list(self.metadata.additional_data.keys())}\n        '''\n\n        print(to_display)\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.summary","title":"summary","text":"<pre><code>summary(display_type: Literal['text', 'diagram']) -&gt; None\n</code></pre> <p>Used to display used configurations. Can be displayed as interactive html diagram or text summary.</p> <p>Parameters:</p> Name Type Description Default <code>display_type</code> <code>Literal['text', 'diagram']</code> <p>Whether configuration should be display as diagram or text summary.</p> required Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def summary(self, display_type: Literal[\"text\", \"diagram\"]) -&gt; None:\n    \"\"\"Used to display used configurations. Can be displayed as interactive html diagram or text summary.\n\n    Parameters:\n        display_type: Whether configuration should be display as diagram or text summary.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to display summary.\")\n\n    display_type = DisplayType(display_type)\n\n    if display_type == DisplayType.TEXT:\n        print(self.dataset_config)\n    elif display_type == DisplayType.DIAGRAM:\n        steps = self.dataset_config._get_summary_steps()\n        return css_utils.display_summary_diagram(steps)\n    else:\n        raise NotImplementedError()\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.save_summary_diagram_as_html","title":"save_summary_diagram_as_html","text":"<pre><code>save_summary_diagram_as_html(path: str)\n</code></pre> <p>Saves diagram produces from <code>summary</code> method as html file to specified path.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def save_summary_diagram_as_html(self, path: str):\n    \"\"\"Saves diagram produces from `summary` method as html file to specified path. \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to save summary diagram.\")\n\n    steps = self.dataset_config._get_summary_steps()\n    html = css_utils.get_summary_diagram(steps)\n\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        f.write(html)\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_feature_names","title":"get_feature_names","text":"<pre><code>get_feature_names() -&gt; list[str]\n</code></pre> <p>Returns a list of all available feature names in the dataset.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_feature_names(self) -&gt; list[str]:\n    \"\"\"Returns a list of all available feature names in the dataset. \"\"\"\n\n    return list(self.metadata.features.keys())\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_data_about_set","title":"get_data_about_set  <code>abstractmethod</code>","text":"<pre><code>get_data_about_set(about: SplitType | Literal['train', 'val', 'test', 'all']) -&gt; dict\n</code></pre> <p>Retrieves data related to the specified set.</p> <p>Parameters:</p> Name Type Description Default <code>about</code> <code>SplitType | Literal['train', 'val', 'test', 'all']</code> <p>Specifies the set to retrieve data about.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the requested data for the set.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>@abstractmethod\ndef get_data_about_set(self, about: SplitType | Literal[\"train\", \"val\", \"test\", \"all\"]) -&gt; dict:\n    \"\"\"\n    Retrieves data related to the specified set.\n\n    Parameters:\n        about: Specifies the set to retrieve data about.\n\n    Returns:\n        A dictionary containing the requested data for the set.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_available_ts_indices","title":"get_available_ts_indices","text":"<pre><code>get_available_ts_indices() -&gt; np.ndarray\n</code></pre> <p>Returns the available time series indices in this dataset.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_available_ts_indices(self) -&gt; np.ndarray:\n    \"\"\"Returns the available time series indices in this dataset. \"\"\"\n    return self.metadata.ts_indices\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_additional_data","title":"get_additional_data","text":"<pre><code>get_additional_data(data_name: str) -&gt; pd.DataFrame\n</code></pre> <p>Create a Pandas <code>DataFrame</code> of additional data of <code>data_name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data_name</code> <code>str</code> <p>Name of additional data to return.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Dataframe of additional data of <code>data_name</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_additional_data(self, data_name: str) -&gt; pd.DataFrame:\n    \"\"\"Create a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) of additional data of `data_name`.\n\n    Parameters:\n        data_name: Name of additional data to return.\n\n    Returns:\n        Dataframe of additional data of `data_name`.\n    \"\"\"\n\n    if data_name not in self.metadata.additional_data:\n        self.logger.error(\"%s is not available for this dataset.\", data_name)\n        raise ValueError(f\"{data_name} is not available for this dataset.\", f\"Possible options are: {self.metadata.additional_data}\")\n\n    data = get_additional_data(self.metadata.dataset_path, data_name)\n    data_df = pd.DataFrame(data)\n\n    for column, column_type in self.metadata.additional_data[data_name]:\n        if column_type == datetime:\n            data_df[column] = data_df[column].apply(lambda x: datetime.fromtimestamp(x, tz=timezone.utc))\n        else:\n            data_df[column] = data_df[column].astype(column_type)\n\n    return data_df\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.plot","title":"plot","text":"<pre><code>plot(ts_id: int, plot_type: Literal['scatter', 'line'], features: list[str] | str | Literal['config'] = 'config', feature_per_plot: bool = True, time_format: TimeFormat | Literal['config', 'id_time', 'datetime', 'unix_time', 'shifted_unix_time'] = 'config', is_interactive: bool = True) -&gt; None\n</code></pre> <p>Displays a graph for the selected <code>ts_id</code> and its <code>features</code>.</p> <p>The plotting is done using the <code>Plotly</code> library, which provides interactive graphs.</p> <p>Parameters:</p> Name Type Description Default <code>ts_id</code> <code>int</code> <p>The ID of the time series to display.</p> required <code>plot_type</code> <code>Literal['scatter', 'line']</code> <p>The type of graph to plot.</p> required <code>features</code> <code>list[str] | str | Literal['config']</code> <p>The features to display in the plot. <code>Defaults: \"config\"</code>.</p> <code>'config'</code> <code>feature_per_plot</code> <code>bool</code> <p>Whether each feature should be displayed in a separate plot or combined into one. <code>Defaults: True</code>.</p> <code>True</code> <code>time_format</code> <code>TimeFormat | Literal['config', 'id_time', 'datetime', 'unix_time', 'shifted_unix_time']</code> <p>The time format to use for the x-axis. <code>Defaults: \"config\"</code>.</p> <code>'config'</code> <code>is_interactive</code> <code>bool</code> <p>Whether the plot should be interactive (e.g., zoom, hover). <code>Defaults: True</code>.</p> <code>True</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def plot(self, ts_id: int, plot_type: Literal[\"scatter\", \"line\"], features: list[str] | str | Literal[\"config\"] = \"config\", feature_per_plot: bool = True,\n         time_format: TimeFormat | Literal[\"config\", \"id_time\", \"datetime\", \"unix_time\", \"shifted_unix_time\"] = \"config\", is_interactive: bool = True) -&gt; None:\n    \"\"\"\n    Displays a graph for the selected `ts_id` and its `features`.\n\n    The plotting is done using the [`Plotly`](https://plotly.com/python/) library, which provides interactive graphs.\n\n    Parameters:\n        ts_id: The ID of the time series to display.\n        plot_type: The type of graph to plot.\n        features: The features to display in the plot. `Defaults: \"config\"`.\n        feature_per_plot: Whether each feature should be displayed in a separate plot or combined into one. `Defaults: True`.\n        time_format: The time format to use for the x-axis. `Defaults: \"config\"`.\n        is_interactive: Whether the plot should be interactive (e.g., zoom, hover). `Defaults: True`.\n    \"\"\"\n\n    if time_format == \"config\":\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to plot.\")\n\n        time_format = self.dataset_config.time_format\n        self.logger.debug(\"Using time format from dataset configuration: %s\", time_format)\n    else:\n        time_format = TimeFormat(time_format)\n        self.logger.debug(\"Using specified time format: %s\", time_format)\n\n    time_series, times, features = self.__get_data_for_plot(ts_id, features, time_format)\n    self.logger.debug(\"Received data for plotting. Time series, times, and features are ready.\")\n\n    plots = []\n\n    if feature_per_plot:\n        self.logger.debug(\"Creating individual plots for each feature.\")\n        fig = make_subplots(rows=len(features), cols=1, shared_xaxes=False, x_title=time_format.value)\n\n        for i, feature in enumerate(features):\n            if plot_type == \"scatter\":\n                plot = go.Scatter(x=times, y=time_series[:, i], mode=\"markers\", name=feature, legendgroup=feature)\n                self.logger.debug(\"Creating scatter plot for feature: %s\", feature)\n            elif plot_type == \"line\":\n                plot = go.Scatter(x=times, y=time_series[:, i], mode=\"lines\", name=feature)\n                self.logger.debug(\"Creating line plot for feature: %s\", feature)\n            else:\n                raise ValueError(\"Invalid plot type.\")\n\n            fig.add_traces(plot, rows=i + 1, cols=1)\n\n        fig.update_layout(height=200 + 120 * len(features), width=2000, autosize=len(features) == 1, showlegend=True)\n        self.logger.debug(\"Created subplots for features: %s.\", features)\n    else:\n        self.logger.debug(\"Creating a combined plot for all features.\")\n        for i, feature in enumerate(features):\n            if plot_type == \"scatter\":\n                plot = go.Scatter(x=times, y=time_series[:, i], mode=\"markers\", name=feature)\n                self.logger.debug(\"Creating scatter plot for feature: %s\", feature)\n            elif plot_type == \"line\":\n                plot = go.Scatter(x=times, y=time_series[:, i], mode=\"lines\", name=feature)\n                self.logger.debug(\"Creating line plot for feature: %s\", feature)\n            else:\n                raise ValueError(\"Invalid plot type.\")\n            plots.append(plot)\n\n        fig = go.Figure(data=plots)\n        fig.update_layout(xaxis_title=time_format.value, showlegend=True, height=200 + 120 * 2)\n        self.logger.debug(\"Created combined plot for features: %s.\", features)\n\n    if not is_interactive:\n        self.logger.debug(\"Disabling interactivity for the plot.\")\n        fig.update_layout(updatemenus=[], dragmode=False, hovermode=False)\n\n    self.logger.debug(\"Displaying the plot.\")\n    fig.show()\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.add_annotation","title":"add_annotation","text":"<pre><code>add_annotation(annotation: str, annotation_group: str, ts_id: int | None, id_time: int | None, enforce_ids: bool = True) -&gt; None\n</code></pre> <p>Adds an annotation to the specified <code>annotation_group</code>.</p> <ul> <li>If the provided <code>annotation_group</code> does not exist, it will be created.</li> <li>At least one of <code>ts_id</code> or <code>id_time</code> must be provided to associate the annotation with time series or/and time point.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>annotation</code> <code>str</code> <p>The annotation to be added.</p> required <code>annotation_group</code> <code>str</code> <p>The group to which the annotation should be added.</p> required <code>ts_id</code> <code>int | None</code> <p>The time series ID to which the annotation should be added.</p> required <code>id_time</code> <code>int | None</code> <p>The time ID to which the annotation should be added.</p> required <code>enforce_ids</code> <code>bool</code> <p>Flag indicating whether the <code>ts_id</code> and <code>id_time</code> must belong to this dataset. <code>Default: True</code></p> <code>True</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def add_annotation(self, annotation: str, annotation_group: str, ts_id: int | None, id_time: int | None, enforce_ids: bool = True) -&gt; None:\n    \"\"\" \n    Adds an annotation to the specified `annotation_group`.\n\n    - If the provided `annotation_group` does not exist, it will be created.\n    - At least one of `ts_id` or `id_time` must be provided to associate the annotation with time series or/and time point.\n\n    Parameters:\n        annotation: The annotation to be added.\n        annotation_group: The group to which the annotation should be added.\n        ts_id: The time series ID to which the annotation should be added.\n        id_time: The time ID to which the annotation should be added.\n        enforce_ids: Flag indicating whether the `ts_id` and `id_time` must belong to this dataset. `Default: True`  \n    \"\"\"\n\n    if enforce_ids:\n        self._validate_annotation_ids(ts_id, id_time)\n    self.annotations.add_annotation(annotation, annotation_group, ts_id, id_time)\n\n    if ts_id is not None and id_time is not None:\n        self._update_annotations_imported_status(AnnotationType.BOTH, None)\n    elif ts_id is not None and id_time is None:\n        self._update_annotations_imported_status(AnnotationType.TS_ID, None)\n    elif ts_id is None and id_time is not None:\n        self._update_annotations_imported_status(AnnotationType.ID_TIME, None)\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.remove_annotation","title":"remove_annotation","text":"<pre><code>remove_annotation(annotation_group: str, ts_id: int | None, id_time: int | None) -&gt; None\n</code></pre> <p>Removes an annotation from the specified <code>annotation_group</code>.</p> <ul> <li>At least one of <code>ts_id</code> or <code>id_time</code> must be provided to associate the annotation with time series or/and time point.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>annotation_group</code> <code>str</code> <p>The annotation group from which the annotation should be removed.</p> required <code>ts_id</code> <code>int | None</code> <p>The time series ID from which the annotation should be removed.</p> required <code>id_time</code> <code>int | None</code> <p>The time ID from which the annotation should be removed.</p> required Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def remove_annotation(self, annotation_group: str, ts_id: int | None, id_time: int | None) -&gt; None:\n    \"\"\"  \n    Removes an annotation from the specified `annotation_group`.\n\n    - At least one of `ts_id` or `id_time` must be provided to associate the annotation with time series or/and time point.\n\n    Parameters:\n        annotation_group: The annotation group from which the annotation should be removed.\n        ts_id: The time series ID from which the annotation should be removed.\n        id_time: The time ID from which the annotation should be removed. \n    \"\"\"\n\n    self.annotations.remove_annotation(annotation_group, ts_id, id_time, False)\n\n    if ts_id is not None and id_time is not None:\n        self._update_annotations_imported_status(AnnotationType.BOTH, None)\n    elif ts_id is not None and id_time is None:\n        self._update_annotations_imported_status(AnnotationType.TS_ID, None)\n    elif ts_id is None and id_time is not None:\n        self._update_annotations_imported_status(AnnotationType.ID_TIME, None)\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.add_annotation_group","title":"add_annotation_group","text":"<pre><code>add_annotation_group(annotation_group: str, on: AnnotationType | Literal['id_time', 'ts_id', 'both'])\n</code></pre> <p>Adds a new <code>annotation_group</code>.</p> <p>Parameters:</p> Name Type Description Default <code>annotation_group</code> <code>str</code> <p>The name of the annotation group to be added.</p> required <code>on</code> <code>AnnotationType | Literal['id_time', 'ts_id', 'both']</code> <p>Specifies which part of the data should be annotated. If set to <code>\"both\"</code>, annotations will be applied as if <code>id_time</code> and <code>ts_id</code> were both set.</p> required Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def add_annotation_group(self, annotation_group: str, on: AnnotationType | Literal[\"id_time\", \"ts_id\", \"both\"]):\n    \"\"\" \n    Adds a new `annotation_group`.\n\n    Parameters:\n        annotation_group: The name of the annotation group to be added.\n        on: Specifies which part of the data should be annotated. If set to `\"both\"`, annotations will be applied as if `id_time` and `ts_id` were both set.\n    \"\"\"\n    on = AnnotationType(on)\n\n    self.annotations.add_annotation_group(annotation_group, on, False)\n\n    self._update_annotations_imported_status(on, None)\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.remove_annotation_group","title":"remove_annotation_group","text":"<pre><code>remove_annotation_group(annotation_group: str, on: AnnotationType | Literal['id_time', 'ts_id', 'both'])\n</code></pre> <p>Removes the specified <code>annotation_group</code>.</p> <p>Parameters:</p> Name Type Description Default <code>annotation_group</code> <code>str</code> <p>The name of the annotation group to be removed.</p> required <code>on</code> <code>AnnotationType | Literal['id_time', 'ts_id', 'both']</code> <p>Specifies which part of the data the <code>annotation_group</code> should be removed from. If set to <code>\"both\"</code>, annotations will be applied as if <code>id_time</code> and <code>ts_id</code> were both set.</p> required Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def remove_annotation_group(self, annotation_group: str, on: AnnotationType | Literal[\"id_time\", \"ts_id\", \"both\"]):\n    \"\"\" \n    Removes the specified `annotation_group`.\n\n    Parameters:\n        annotation_group: The name of the annotation group to be removed.\n        on: Specifies which part of the data the `annotation_group` should be removed from. If set to `\"both\"`, annotations will be applied as if `id_time` and `ts_id` were both set.        \n    \"\"\"\n    on = AnnotationType(on)\n\n    self.annotations.remove_annotation_group(annotation_group, on, False)\n\n    self._update_annotations_imported_status(on, None)\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_annotations","title":"get_annotations","text":"<pre><code>get_annotations(on: AnnotationType | Literal['id_time', 'ts_id', 'both']) -&gt; pd.DataFrame\n</code></pre> <p>Returns the annotations as a Pandas <code>DataFrame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>on</code> <code>AnnotationType | Literal['id_time', 'ts_id', 'both']</code> <p>Specifies which annotations to return. If set to <code>\"both\"</code>, annotations will be applied as if <code>id_time</code> and <code>ts_id</code> were both set.         </p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A Pandas DataFrame containing the selected annotations.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_annotations(self, on: AnnotationType | Literal[\"id_time\", \"ts_id\", \"both\"]) -&gt; pd.DataFrame:\n    \"\"\" \n    Returns the annotations as a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html).\n\n    Parameters:\n        on: Specifies which annotations to return. If set to `\"both\"`, annotations will be applied as if `id_time` and `ts_id` were both set.         \n\n    Returns:\n        A Pandas DataFrame containing the selected annotations.      \n    \"\"\"\n    on = AnnotationType(on)\n\n    return self.annotations.get_annotations(on, self.metadata.ts_id_name)\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.import_annotations","title":"import_annotations","text":"<pre><code>import_annotations(identifier: str, enforce_ids: bool = True) -&gt; None\n</code></pre> <p>Imports annotations from a CSV file.</p> <p>First, it attempts to load the built-in annotations, if no built-in annotations with such an identifier exists, it attempts to load a custom annotations from the <code>\"data_root\"/tszoo/annotations/</code> directory.</p> <p><code>data_root</code> is specified when the dataset is created.     </p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The name of the CSV file.     </p> required <code>enforce_ids</code> <code>bool</code> <p>Flag indicating whether the <code>ts_id</code> and <code>id_time</code> must belong to this dataset. <code>Default: True</code></p> <code>True</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def import_annotations(self, identifier: str, enforce_ids: bool = True) -&gt; None:\n    \"\"\" \n    Imports annotations from a CSV file.\n\n    First, it attempts to load the built-in annotations, if no built-in annotations with such an identifier exists, it attempts to load a custom annotations from the `\"data_root\"/tszoo/annotations/` directory.\n\n    `data_root` is specified when the dataset is created.     \n\n    Parameters:\n        identifier: The name of the CSV file.     \n        enforce_ids: Flag indicating whether the `ts_id` and `id_time` must belong to this dataset. `Default: True`                \n    \"\"\"\n\n    annotations_file_path, is_built_in = get_annotations_path_and_whether_it_is_built_in(identifier, self.metadata.annotations_root, self.logger)\n\n    if is_built_in:\n        self.logger.info(\"Built-in annotations found: %s.\", identifier)\n        if not os.path.exists(annotations_file_path):\n            self.logger.info(\"Downloading annotations with identifier: %s\", identifier)\n            annotations_url = f\"{ANNOTATIONS_DOWNLOAD_BUCKET}&amp;file={identifier}\"  # probably will change annotations bucket... placeholder\n            resumable_download(url=annotations_url, file_path=annotations_file_path, silent=False)\n\n        self.logger.debug(\"Loading annotations from %s\", annotations_file_path)\n        temp_df = pd.read_csv(annotations_file_path)\n        self.logger.debug(\"Created DataFrame from file: %s\", annotations_file_path)\n    else:\n        self.logger.info(\"Custom annotations found: %s.\", identifier)\n        self.logger.debug(\"Loading annotations from %s\", annotations_file_path)\n        temp_df = pd.read_csv(annotations_file_path)\n        self.logger.debug(\"Created DataFrame from file: %s\", annotations_file_path)\n\n    ts_id_index = None\n    time_id_index = None\n    on = None\n\n    # Check the columns of the DataFrame to identify the type of annotation\n    if self.metadata.ts_id_name in temp_df.columns and ID_TIME_COLUMN_NAME in temp_df.columns:\n        self.annotations.clear_time_in_time_series()\n        time_id_index = temp_df.columns.tolist().index(ID_TIME_COLUMN_NAME)\n        ts_id_index = temp_df.columns.tolist().index(self.metadata.ts_id_name)\n        on = AnnotationType.BOTH\n        self.logger.info(\"Annotations detected as %s (both %s and id_time)\", AnnotationType.BOTH, self.metadata.ts_id_name)\n\n    elif self.metadata.ts_id_name in temp_df.columns:\n        self.annotations.clear_time_series()\n        ts_id_index = temp_df.columns.tolist().index(self.metadata.ts_id_name)\n        on = AnnotationType.TS_ID\n        self.logger.info(\"Annotations detected as %s (%s only)\", AnnotationType.TS_ID, self.metadata.ts_id_name)\n\n    elif ID_TIME_COLUMN_NAME in temp_df.columns:\n        self.annotations.clear_time()\n        time_id_index = temp_df.columns.tolist().index(ID_TIME_COLUMN_NAME)\n        on = AnnotationType.ID_TIME\n        self.logger.info(\"Annotations detected as %s (%s only)\", AnnotationType.ID_TIME, ID_TIME_COLUMN_NAME)\n\n    else:\n        raise ValueError(f\"Could not find {self.metadata.ts_id_name} and {ID_TIME_COLUMN_NAME} in the imported CSV.\")\n\n    # Process each row in the DataFrame and add annotations\n    for row in temp_df.itertuples(False):\n        for i, _ in enumerate(temp_df.columns):\n            if i == time_id_index or i == ts_id_index:\n                continue\n\n            ts_id = None\n            if ts_id_index is not None:\n                ts_id = row[ts_id_index]\n\n            id_time = None\n            if time_id_index is not None:\n                id_time = row[time_id_index]\n\n            self.add_annotation(row[i], temp_df.columns[i], ts_id, id_time, enforce_ids)\n\n    self._update_annotations_imported_status(on, identifier)\n    self.logger.info(\"Successfully imported annotations from %s\", annotations_file_path)\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.import_config","title":"import_config","text":"<pre><code>import_config(identifier: str, display_config_details: Optional[Literal['text', 'diagram']] = 'text', workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Import the dataset_config from a pickle file and initializes the dataset. Config type must correspond to dataset type.</p> <p>First, it attempts to load the built-in config, if no built-in config with such an identifier exists, it attempts to load a custom config from the <code>\"data_root\"/tszoo/configs/</code> directory.</p> <p><code>data_root</code> is specified when the dataset is created.       </p> <p>The following configuration attributes are used during initialization:</p> Dataset config Description <code>init_workers</code> Specifies the number of workers to use for initialization. Applied when <code>workers</code> = \"config\". <code>partial_fit_initialized_transformers</code> Determines whether initialized transformers should be partially fitted on the training data. <code>nan_threshold</code> Filters out time series with missing values exceeding the specified threshold. <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>Name of the pickle file.</p> required <code>display_config_details</code> <code>Optional[Literal['text', 'diagram']]</code> <p>Flag indicating whether to display the configuration values after initialization. <code>Default: True</code> </p> <code>'text'</code> <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use during initialization. <code>Default: \"config\"</code></p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def import_config(self, identifier: str, display_config_details: Optional[Literal[\"text\", \"diagram\"]] = \"text\", workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\" \n    Import the dataset_config from a pickle file and initializes the dataset. Config type must correspond to dataset type.\n\n    First, it attempts to load the built-in config, if no built-in config with such an identifier exists, it attempts to load a custom config from the `\"data_root\"/tszoo/configs/` directory.\n\n    `data_root` is specified when the dataset is created.       \n\n    The following configuration attributes are used during initialization:\n\n    Dataset config | Description\n    -------------- | -----------\n    `init_workers` | Specifies the number of workers to use for initialization. Applied when `workers` = \"config\".\n    `partial_fit_initialized_transformers` | Determines whether initialized transformers should be partially fitted on the training data.\n    `nan_threshold` | Filters out time series with missing values exceeding the specified threshold.\n\n    Parameters:\n        identifier: Name of the pickle file.\n        display_config_details: Flag indicating whether to display the configuration values after initialization. `Default: True` \n        workers: The number of workers to use during initialization. `Default: \"config\"`  \n    \"\"\"\n\n    if display_config_details is not None:\n        display_config_details = DisplayType(display_config_details)\n\n    # Load config\n    config = load_config(identifier, self.metadata.configs_root, self.metadata.database_name, self.metadata.source_type, self.metadata.aggregation, self.logger)\n\n    self.logger.info(\"Initializing dataset configuration with the imported config.\")\n    self.set_dataset_config_and_initialize(config, display_config_details, workers)\n\n    self._update_config_imported_status(identifier)\n    self.logger.info(\"Successfully used config with identifier %s\", identifier)\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.save_annotations","title":"save_annotations","text":"<pre><code>save_annotations(identifier: str, on: AnnotationType | Literal['id_time', 'ts_id', 'both'], force_write: bool = False) -&gt; None\n</code></pre> <p>Saves the annotations as a CSV file.</p> <p>The file will be saved to a path determined by the <code>data_root</code> specified when the dataset was created.</p> <p>The annotations will be saved under the directory <code>data_root/tszoo/annotations/</code>.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The name of the CSV file.</p> required <code>on</code> <code>AnnotationType | Literal['id_time', 'ts_id', 'both']</code> <p>What annotation type should be saved. If set to <code>\"both\"</code>, annotations will be applied as if <code>id_time</code> and <code>ts_id</code> were both set.   </p> required <code>force_write</code> <code>bool</code> <p>If set to <code>True</code>, will overwrite any existing files with the same name. <code>Default: False</code></p> <code>False</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def save_annotations(self, identifier: str, on: AnnotationType | Literal[\"id_time\", \"ts_id\", \"both\"], force_write: bool = False) -&gt; None:\n    \"\"\" \n    Saves the annotations as a CSV file.\n\n    The file will be saved to a path determined by the `data_root` specified when the dataset was created.\n\n    The annotations will be saved under the directory `data_root/tszoo/annotations/`.\n\n    Parameters:\n        identifier: The name of the CSV file.\n        on: What annotation type should be saved. If set to `\"both\"`, annotations will be applied as if `id_time` and `ts_id` were both set.   \n        force_write: If set to `True`, will overwrite any existing files with the same name. `Default: False`               \n    \"\"\"\n\n    if exists_built_in_annotations(identifier):\n        raise ValueError(\"Built-in annotations with this identifier already exists. Choose another identifier.\")\n\n    on = AnnotationType(on)\n\n    temp_df = self.get_annotations(on)\n\n    # Ensure the annotations root directory exists, creating it if necessary\n    if not os.path.exists(self.metadata.annotations_root):\n        os.makedirs(self.metadata.annotations_root)\n        self.logger.info(\"Created annotations directory at %s\", self.metadata.annotations_root)\n\n    path = os.path.join(self.metadata.annotations_root, f\"{identifier}.csv\")\n\n    if os.path.exists(path) and not force_write:\n        raise ValueError(f\"Annotations already exist at {path}. Set force_write=True to overwrite.\")\n    self.logger.debug(\"Annotations CSV file path: %s\", path)\n\n    temp_df.to_csv(path, index=False)\n\n    self._update_annotations_imported_status(on, identifier)\n    self.logger.info(\"Annotations successfully saved to %s\", path)\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.save_config","title":"save_config","text":"<pre><code>save_config(identifier: str, create_with_details_file: bool = True, force_write: bool = False, **kwargs) -&gt; None\n</code></pre> <p>Saves the config as a pickle file.</p> <p>The file will be saved to a path determined by the <code>data_root</code> specified when the dataset was created.  The config will be saved under the directory <code>data_root/tszoo/configs/</code>.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The name of the pickle file.</p> required <code>create_with_details_file</code> <code>bool</code> <p>Whether to export the config along with a readable text file that provides details. <code>Defaults: True</code>. </p> <code>True</code> <code>force_write</code> <code>bool</code> <p>If set to <code>True</code>, will overwrite any existing files with the same name. <code>Default: False</code></p> <code>False</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def save_config(self, identifier: str, create_with_details_file: bool = True, force_write: bool = False, **kwargs) -&gt; None:\n    \"\"\" \n    Saves the config as a pickle file.\n\n    The file will be saved to a path determined by the `data_root` specified when the dataset was created. \n    The config will be saved under the directory `data_root/tszoo/configs/`.\n\n    Parameters:\n        identifier: The name of the pickle file.\n        create_with_details_file: Whether to export the config along with a readable text file that provides details. `Defaults: True`. \n        force_write: If set to `True`, will overwrite any existing files with the same name. `Default: False`            \n    \"\"\"\n\n    default_kwargs = {'hard_force': False}\n    kwargs = {**default_kwargs, **kwargs}\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to save config.\")\n\n    if not kwargs[\"hard_force\"] and exists_built_in_config(identifier):\n        raise ValueError(\"Built-in config with this identifier already exists. Choose another identifier.\")\n\n    # Ensure the config directory exists\n    if not os.path.exists(self.metadata.configs_root):\n        os.makedirs(self.metadata.configs_root)\n        self.logger.info(\"Created config directory at %s\", self.metadata.configs_root)\n\n    path_pickle = os.path.join(self.metadata.configs_root, f\"{identifier}.pickle\")\n    path_details = os.path.join(self.metadata.configs_root, f\"{identifier}.txt\")\n\n    if os.path.exists(path_pickle) and not force_write:\n        raise ValueError(f\"Config at path {path_pickle} already exists. Set force_write=True to overwrite.\")\n    self.logger.debug(\"Config pickle path: %s\", path_pickle)\n\n    if create_with_details_file:\n        if os.path.exists(path_details) and not force_write:\n            raise ValueError(f\"Config details at path {path_details} already exists. Set force_write=True to overwrite.\")\n        self.logger.debug(\"Config details path: %s\", path_details)\n\n    if not self.dataset_config.filler_factory.creates_built_in:\n        self.logger.warning(\"You are using a custom filler. Ensure the config is distributed with the source code of the filler.\")\n\n    if not self.dataset_config.anomaly_handler_factory.creates_built_in:\n        self.logger.warning(\"You are using a custom anomaly handler. Ensure the config is distributed with the source code of the anomaly handler.\")\n\n    if not self.dataset_config.transformer_factory.creates_built_in:\n        self.logger.warning(\"You are using a custom transformer. Ensure the config is distributed with the source code of the transformer.\")\n\n    if len(self.dataset_config.preprocess_order) != len(MANDATORY_PREPROCESSES_ORDER):\n        self.logger.warning(\"You are using at least one custom handler. Ensure the config is distributed with the source code of every custom handler.\")\n\n    pickle_dump(self._export_config_copy, path_pickle)\n    self.logger.info(\"Config pickle saved to %s\", path_pickle)\n\n    if create_with_details_file:\n        with open(path_details, \"w\", encoding=\"utf-8\") as file:\n            file.write(str(self.dataset_config))\n        self.logger.info(\"Config details saved to %s\", path_details)\n\n    self._update_config_imported_status(identifier)\n    self.dataset_config.export_update_needed = False\n    self.logger.info(\"Config successfully saved\")\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.save_benchmark","title":"save_benchmark","text":"<pre><code>save_benchmark(identifier: str, force_write: bool = False, **kwargs) -&gt; None\n</code></pre> <p>Saves the benchmark as a YAML file.</p> <p>The benchmark, along with any associated annotations and config files, will be saved in a path determined by the <code>data_root</code> specified when creating the dataset.  The default save path for benchmark is <code>\"data_root/tszoo/benchmarks/\"</code>.</p> <p>If you are using imported <code>annotations</code> or <code>config</code> (whether custom or built-in), their file names will be set in the <code>benchmark</code> file.  If new <code>annotations</code> or <code>config</code> are created during the process, their filenames will be derived from the provided <code>identifier</code> and set in the <code>benchmark</code> file.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The name of the YAML file.</p> required <code>force_write</code> <code>bool</code> <p>If set to <code>True</code>, will overwrite any existing files with the same name. <code>Default: False</code></p> <code>False</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def save_benchmark(self, identifier: str, force_write: bool = False, **kwargs) -&gt; None:\n    \"\"\" \n    Saves the benchmark as a YAML file.\n\n    The benchmark, along with any associated annotations and config files, will be saved in a path determined by the `data_root` specified when creating the dataset. \n    The default save path for benchmark is `\"data_root/tszoo/benchmarks/\"`.\n\n    If you are using imported `annotations` or `config` (whether custom or built-in), their file names will be set in the `benchmark` file. \n    If new `annotations` or `config` are created during the process, their filenames will be derived from the provided `identifier` and set in the `benchmark` file.\n\n    Parameters:\n        identifier: The name of the YAML file.\n        force_write: If set to `True`, will overwrite any existing files with the same name. `Default: False`            \n    \"\"\"\n\n    default_kwargs = {'hard_force': False}\n    kwargs = {**default_kwargs, **kwargs}\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to save benchmark.\")\n\n    if not kwargs[\"hard_force\"] and exists_built_in_benchmark(identifier):\n        raise ValueError(\"Built-in benchmark with this identifier already exists. Choose another identifier.\")\n\n    # Determine annotation names based on the available annotations and whether the annotations were imported\n    if len(self.annotations.time_series_annotations) &gt; 0:\n        annotations_ts_name = self.imported_annotations_ts_identifier if self.imported_annotations_ts_identifier is not None else f\"{identifier}_{AnnotationType.TS_ID.value}\"\n    else:\n        annotations_ts_name = None\n\n    if len(self.annotations.time_annotations) &gt; 0:\n        annotations_time_name = self.imported_annotations_time_identifier if self.imported_annotations_time_identifier is not None else f\"{identifier}_{AnnotationType.ID_TIME.value}\"\n    else:\n        annotations_time_name = None\n\n    if len(self.annotations.time_in_series_annotations) &gt; 0:\n        annotations_both_name = self.imported_annotations_both_identifier if self.imported_annotations_both_identifier is not None else f\"{identifier}_{AnnotationType.BOTH.value}\"\n    else:\n        annotations_both_name = None\n\n    # Use the imported identifier if available and update is not necessary, otherwise default to the current identifier\n    config_name = self.dataset_config.import_identifier if (self.dataset_config.import_identifier is not None and not self.dataset_config.export_update_needed) else identifier\n\n    export_benchmark = ExportBenchmark(self.metadata.database_name,\n                                       self.metadata.source_type.value,\n                                       self.metadata.aggregation.value,\n                                       self.metadata.dataset_type.value,\n                                       config_name,\n                                       annotations_ts_name,\n                                       annotations_time_name,\n                                       annotations_both_name,\n                                       related_results_identifier=self.related_to,\n                                       version=version.config_and_benchmarks_current_version)\n\n    # If the config was not imported, save it\n    if self.dataset_config.import_identifier is None or self.dataset_config.export_update_needed:\n        self.save_config(export_benchmark.config_identifier, force_write=force_write, hard_force=kwargs[\"hard_force\"])\n    else:\n        self.logger.info(\"Using already existing config with identifier: %s\", self.dataset_config.import_identifier)\n\n    # Save ts_id annotations if available and not previously imported\n    if self.imported_annotations_ts_identifier is None and len(self.annotations.time_series_annotations) &gt; 0:\n        self.save_annotations(export_benchmark.annotations_ts_identifier, AnnotationType.TS_ID, force_write=force_write)\n    elif self.imported_annotations_ts_identifier is not None:\n        self.logger.info(\"Using already existing annotations with identifier: %s; type: %s\", self.imported_annotations_ts_identifier, AnnotationType.TS_ID)\n\n    # Save id_time annotations if available and not previously imported\n    if self.imported_annotations_time_identifier is None and len(self.annotations.time_annotations) &gt; 0:\n        self.save_annotations(export_benchmark.annotations_time_identifier, AnnotationType.ID_TIME, force_write=force_write)\n    elif self.imported_annotations_time_identifier is not None:\n        self.logger.info(\"Using already existing annotations with identifier: %s; type: %s\", self.imported_annotations_time_identifier, AnnotationType.ID_TIME)\n\n    # Save both annotations if available and not previously imported\n    if self.imported_annotations_both_identifier is None and len(self.annotations.time_in_series_annotations) &gt; 0:\n        self.save_annotations(export_benchmark.annotations_both_identifier, AnnotationType.BOTH, force_write=force_write)\n    elif self.imported_annotations_both_identifier is not None:\n        self.logger.info(\"Using already existing annotations with identifier: %s; type: %s\", self.imported_annotations_both_identifier, AnnotationType.BOTH)\n\n    # Ensure the benchmark directory exists\n    if not os.path.exists(self.metadata.benchmarks_root):\n        os.makedirs(self.metadata.benchmarks_root)\n        self.logger.info(\"Created benchmarks directory at %s\", self.metadata.benchmarks_root)\n\n    benchmark_path = os.path.join(self.metadata.benchmarks_root, f\"{identifier}.yaml\")\n\n    if os.path.exists(benchmark_path) and not force_write:\n        self.logger.error(\"Benchmark file already exists at %s\", benchmark_path)\n        raise ValueError(f\"Benchmark at path {benchmark_path} already exists. Set force_write=True to overwrite.\")\n    self.logger.debug(\"Benchmark YAML file path: %s\", benchmark_path)\n\n    yaml_dump(export_benchmark.to_dict(), benchmark_path)\n    self.logger.info(\"Benchmark successfully saved to %s\", benchmark_path)\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_transformers","title":"get_transformers","text":"<pre><code>get_transformers() -&gt; np.ndarray[Transformer] | Transformer | None\n</code></pre> <p>Returns used transformers from config.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_transformers(self) -&gt; np.ndarray[Transformer] | Transformer | None:\n    \"\"\"Returns used transformers from config. \"\"\"\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting get transformers.\")\n\n    for i, preprocess_type in enumerate(self.dataset_config.preprocess_order):\n        if preprocess_type == PreprocessType.TRANSFORMING:\n            holder: TransformerHolder = self.dataset_config.train_preprocess_order[i].holder\n            return holder.transformers\n\n    return None\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.check_errors","title":"check_errors","text":"<pre><code>check_errors() -&gt; None\n</code></pre> <p>Validates whether the dataset is corrupted. </p> <p>Raises an exception if corrupted.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def check_errors(self) -&gt; None:\n    \"\"\"\n    Validates whether the dataset is corrupted. \n\n    Raises an exception if corrupted.\n    \"\"\"\n\n    dataset, _ = load_database(self.metadata.dataset_path)\n\n    try:\n        node_iter = dataset.walk_nodes()\n\n        # Process each node in the dataset\n        for node in node_iter:\n            if isinstance(node, tb.Table):\n\n                iter_by = min(LOADING_WARNING_THRESHOLD, len(node))\n                iters_done = 0\n\n                # Process the node in chunks to avoid memory issues\n                while iters_done &lt; len(node):\n                    iter_by = min(LOADING_WARNING_THRESHOLD, len(node) - iters_done)\n                    _ = node[iters_done: iters_done + iter_by]  # Fetch the data in chunks\n                    iters_done += iter_by\n\n                self.logger.info(\"Table '%s' checked successfully. (%d rows processed)\", node._v_pathname, len(node))\n\n        self.logger.info(\"Dataset check completed with no errors found.\")\n\n    except Exception as e:\n        self.logger.error(\"Error encountered during dataset check: %s\", str(e))\n\n    finally:\n        dataset.close()\n        self.logger.debug(\"Dataset connection closed.\")\n</code></pre>"},{"location":"reference_cesnet_dataset/#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.__get_data_for_plot","title":"__get_data_for_plot","text":"<pre><code>__get_data_for_plot(ts_id: int, features: list[str] | str, time_format: TimeFormat) -&gt; tuple[np.ndarray, np.ndarray, list[str]]\n</code></pre> <p>Returns prepared data for plotting.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def __get_data_for_plot(self, ts_id: int, features: list[str] | str, time_format: TimeFormat) -&gt; tuple[np.ndarray, np.ndarray, list[str]]:\n    \"\"\"Returns prepared data for plotting. \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before getting data for plotting.\")\n\n    features_indices = []\n\n    if features == \"config\":\n        features = deepcopy(self.dataset_config.features_to_take_without_ids)\n        features_indices = np.arange(len(features))\n        self.logger.debug(\"Features set from dataset config: %s\", features)\n    else:\n        if isinstance(features, str):\n            features = [features]\n\n        if len(features) == 0:\n            raise ValueError(\"No features specified to plot. Please provide valid features.\")\n        if len(set(features)) != len(features):\n            raise ValueError(\"Duplicate features detected. All features must be unique.\")\n\n        for feature in features:\n            if feature not in self.dataset_config.features_to_take_without_ids:\n                raise ValueError(f\"Feature '{feature}' is not valid. It is not present in the dataset configuration.\", self.dataset_config.features_to_take_without_ids)\n\n            index_in_config_features = self.dataset_config.features_to_take_without_ids.index(feature)\n            features_indices.append(index_in_config_features)\n\n    real_feature_indices = np.array(self.dataset_config.indices_of_features_to_take_no_ids)[features_indices]\n    real_feature_indices = real_feature_indices.astype(int)\n\n    time_series, time_period = self._get_data_for_plot(ts_id, real_feature_indices, time_format)\n    self.logger.debug(\"Time series data and corresponding time values retrieved.\")\n\n    return time_series, time_period, features\n</code></pre>"},{"location":"reference_custom_handlers/","title":"Custom handlers","text":""},{"location":"reference_custom_handlers/#cesnet_tszoo.utils.custom_handler.custom_handler","title":"cesnet_tszoo.utils.custom_handler.custom_handler","text":""},{"location":"reference_custom_handlers/#cesnet_tszoo.utils.custom_handler.custom_handler.PerSeriesCustomHandler","title":"PerSeriesCustomHandler","text":"<p>               Bases: <code>CustomHandler</code></p> <p>Base class for PerSeriesCustomHandler. Used for custom handlers that are fitted on single train time series and then applied to that time series parts from target sets.</p> <p>This class serves as the foundation for creating PerSeriesCustomHandler handlers and should be subclassed from.</p> <p>Example:</p> <pre><code>import numpy as np\n\nclass PerFitTest(PerSeriesCustomHandler):\n\n    def __init__(self):\n        self.count = 0\n        super().__init__()\n\n    def fit(self, data: np.ndarray) -&gt; None:\n        self.count += 1\n\n    def apply(self, data: np.ndarray) -&gt; np.ndarray:\n        data[:, :] = self.count\n        return data\n\n    @staticmethod\n    def get_target_sets():\n        return [\"val\"]\n</code></pre> Source code in <code>cesnet_tszoo\\utils\\custom_handler\\custom_handler.py</code> <pre><code>class PerSeriesCustomHandler(CustomHandler):\n    \"\"\"\n    Base class for PerSeriesCustomHandler. Used for custom handlers that are fitted on single train time series and then applied to that time series parts from target sets.\n\n    This class serves as the foundation for creating PerSeriesCustomHandler handlers and should be subclassed from.\n\n    Example:\n\n        import numpy as np\n\n        class PerFitTest(PerSeriesCustomHandler):\n\n            def __init__(self):\n                self.count = 0\n                super().__init__()\n\n            def fit(self, data: np.ndarray) -&gt; None:\n                self.count += 1\n\n            def apply(self, data: np.ndarray) -&gt; np.ndarray:\n                data[:, :] = self.count\n                return data\n\n            @staticmethod\n            def get_target_sets():\n                return [\"val\"]             \n    \"\"\"\n\n    @abstractmethod\n    def fit(self, data: np.ndarray) -&gt; None:\n        \"\"\"\n        Sets the PerSeriesCustomHandler values for a given time series data. Usually train set part of the time series.\n\n        Parameters:\n            data: A numpy array representing data for a single time series with shape `(times, features)` excluding any identifiers.  \n        \"\"\"\n        ...\n\n    @abstractmethod\n    def apply(self, data: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Applies on the input data for a given time series part.\n\n        Parameters:\n            data: A numpy array representing data for a single time series with shape `(times, features)` excluding any identifiers.  \n\n        Returns:\n            The changed data, with the same shape as the input `(times, features)`.            \n        \"\"\"\n        ...\n\n    @staticmethod\n    @abstractmethod\n    def get_target_sets() -&gt; set[SplitType] | set[Literal[\"train\", \"val\", \"test\", \"all\"]]:\n        \"\"\"Specifies on which sets this handler should be used. \"\"\"\n        ...\n</code></pre>"},{"location":"reference_custom_handlers/#cesnet_tszoo.utils.custom_handler.custom_handler.PerSeriesCustomHandler.apply","title":"apply  <code>abstractmethod</code>","text":"<pre><code>apply(data: ndarray) -&gt; np.ndarray\n</code></pre> <p>Applies on the input data for a given time series part.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>A numpy array representing data for a single time series with shape <code>(times, features)</code> excluding any identifiers.  </p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The changed data, with the same shape as the input <code>(times, features)</code>.</p> Source code in <code>cesnet_tszoo\\utils\\custom_handler\\custom_handler.py</code> <pre><code>@abstractmethod\ndef apply(self, data: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Applies on the input data for a given time series part.\n\n    Parameters:\n        data: A numpy array representing data for a single time series with shape `(times, features)` excluding any identifiers.  \n\n    Returns:\n        The changed data, with the same shape as the input `(times, features)`.            \n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference_custom_handlers/#cesnet_tszoo.utils.custom_handler.custom_handler.PerSeriesCustomHandler.fit","title":"fit  <code>abstractmethod</code>","text":"<pre><code>fit(data: ndarray) -&gt; None\n</code></pre> <p>Sets the PerSeriesCustomHandler values for a given time series data. Usually train set part of the time series.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>A numpy array representing data for a single time series with shape <code>(times, features)</code> excluding any identifiers.</p> required Source code in <code>cesnet_tszoo\\utils\\custom_handler\\custom_handler.py</code> <pre><code>@abstractmethod\ndef fit(self, data: np.ndarray) -&gt; None:\n    \"\"\"\n    Sets the PerSeriesCustomHandler values for a given time series data. Usually train set part of the time series.\n\n    Parameters:\n        data: A numpy array representing data for a single time series with shape `(times, features)` excluding any identifiers.  \n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference_custom_handlers/#cesnet_tszoo.utils.custom_handler.custom_handler.PerSeriesCustomHandler.get_target_sets","title":"get_target_sets  <code>abstractmethod</code> <code>staticmethod</code>","text":"<pre><code>get_target_sets() -&gt; set[SplitType] | set[Literal['train', 'val', 'test', 'all']]\n</code></pre> <p>Specifies on which sets this handler should be used.</p> Source code in <code>cesnet_tszoo\\utils\\custom_handler\\custom_handler.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef get_target_sets() -&gt; set[SplitType] | set[Literal[\"train\", \"val\", \"test\", \"all\"]]:\n    \"\"\"Specifies on which sets this handler should be used. \"\"\"\n    ...\n</code></pre>"},{"location":"reference_custom_handlers/#cesnet_tszoo.utils.custom_handler.custom_handler.AllSeriesCustomHandler","title":"AllSeriesCustomHandler","text":"<p>               Bases: <code>CustomHandler</code></p> <p>Base class for AllSeriesCustomHandler. Used for custom handlers that are fitted on all train time series and then applied to all (from target sets) time series.</p> <p>This class serves as the foundation for creating AllSeriesCustomHandler handlers and should be subclassed from.</p> <p>Example:</p> <pre><code>import numpy as np\n\nclass AllFitTest(AllSeriesCustomHandler):\n\n    def __init__(self):\n        self.count = 0\n        super().__init__()\n\n    def partial_fit(self, data: np.ndarray) -&gt; None:\n        self.count += 1\n\n    def apply(self, data: np.ndarray) -&gt; np.ndarray:\n        data[:, :] = self.count\n        return data\n\n    @staticmethod\n    def get_target_sets():\n        return [\"train\"]\n</code></pre> Source code in <code>cesnet_tszoo\\utils\\custom_handler\\custom_handler.py</code> <pre><code>class AllSeriesCustomHandler(CustomHandler):\n    \"\"\"\n    Base class for AllSeriesCustomHandler. Used for custom handlers that are fitted on all train time series and then applied to all (from target sets) time series.\n\n    This class serves as the foundation for creating AllSeriesCustomHandler handlers and should be subclassed from.\n\n    Example:\n\n        import numpy as np\n\n        class AllFitTest(AllSeriesCustomHandler):\n\n            def __init__(self):\n                self.count = 0\n                super().__init__()\n\n            def partial_fit(self, data: np.ndarray) -&gt; None:\n                self.count += 1\n\n            def apply(self, data: np.ndarray) -&gt; np.ndarray:\n                data[:, :] = self.count\n                return data\n\n            @staticmethod\n            def get_target_sets():\n                return [\"train\"]\n\n    \"\"\"\n\n    @abstractmethod\n    def partial_fit(self, data: np.ndarray) -&gt; None:\n        \"\"\"\n        Sets the AllSeriesCustomHandler values for a given time series data. Usually train set part of some time series.\n\n        Parameters:\n            data: A numpy array representing data for a time series with shape `(times, features)` excluding any identifiers.  \n        \"\"\"\n        ...\n\n    @abstractmethod\n    def apply(self, data: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Applies on the input data for a given time series part.\n\n        Parameters:\n            data: A numpy array representing data for a time series with shape `(times, features)` excluding any identifiers.  \n\n        Returns:\n            The changed data, with the same shape as the input `(times, features)`.            \n        \"\"\"\n        ...\n\n    @staticmethod\n    @abstractmethod\n    def get_target_sets() -&gt; set[SplitType] | set[Literal[\"train\", \"val\", \"test\", \"all\"]]:\n        \"\"\"Specifies on which sets this handler should be used. \"\"\"\n        ...\n</code></pre>"},{"location":"reference_custom_handlers/#cesnet_tszoo.utils.custom_handler.custom_handler.AllSeriesCustomHandler.apply","title":"apply  <code>abstractmethod</code>","text":"<pre><code>apply(data: ndarray) -&gt; np.ndarray\n</code></pre> <p>Applies on the input data for a given time series part.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>A numpy array representing data for a time series with shape <code>(times, features)</code> excluding any identifiers.  </p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The changed data, with the same shape as the input <code>(times, features)</code>.</p> Source code in <code>cesnet_tszoo\\utils\\custom_handler\\custom_handler.py</code> <pre><code>@abstractmethod\ndef apply(self, data: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Applies on the input data for a given time series part.\n\n    Parameters:\n        data: A numpy array representing data for a time series with shape `(times, features)` excluding any identifiers.  \n\n    Returns:\n        The changed data, with the same shape as the input `(times, features)`.            \n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference_custom_handlers/#cesnet_tszoo.utils.custom_handler.custom_handler.AllSeriesCustomHandler.get_target_sets","title":"get_target_sets  <code>abstractmethod</code> <code>staticmethod</code>","text":"<pre><code>get_target_sets() -&gt; set[SplitType] | set[Literal['train', 'val', 'test', 'all']]\n</code></pre> <p>Specifies on which sets this handler should be used.</p> Source code in <code>cesnet_tszoo\\utils\\custom_handler\\custom_handler.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef get_target_sets() -&gt; set[SplitType] | set[Literal[\"train\", \"val\", \"test\", \"all\"]]:\n    \"\"\"Specifies on which sets this handler should be used. \"\"\"\n    ...\n</code></pre>"},{"location":"reference_custom_handlers/#cesnet_tszoo.utils.custom_handler.custom_handler.AllSeriesCustomHandler.partial_fit","title":"partial_fit  <code>abstractmethod</code>","text":"<pre><code>partial_fit(data: ndarray) -&gt; None\n</code></pre> <p>Sets the AllSeriesCustomHandler values for a given time series data. Usually train set part of some time series.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>A numpy array representing data for a time series with shape <code>(times, features)</code> excluding any identifiers.</p> required Source code in <code>cesnet_tszoo\\utils\\custom_handler\\custom_handler.py</code> <pre><code>@abstractmethod\ndef partial_fit(self, data: np.ndarray) -&gt; None:\n    \"\"\"\n    Sets the AllSeriesCustomHandler values for a given time series data. Usually train set part of some time series.\n\n    Parameters:\n        data: A numpy array representing data for a time series with shape `(times, features)` excluding any identifiers.  \n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference_custom_handlers/#cesnet_tszoo.utils.custom_handler.custom_handler.NoFitCustomHandler","title":"NoFitCustomHandler","text":"<p>               Bases: <code>CustomHandler</code></p> <p>Base class for NoFitCustomHandler. Used for custom handlers that are not fitted and are applied to (from target sets) time series.</p> <p>This class serves as the foundation for creating NoFitCustomHandler handlers and should be subclassed from.</p> <p>Example:</p> <pre><code>import numpy as np\n\nclass NoFitTest(NoFitCustomHandler):\n    def apply(self, data: np.ndarray) -&gt; np.ndarray:\n        data[:, :] = -1\n        return data\n\n    @staticmethod\n    def get_target_sets():\n        return [\"test\"]\n</code></pre> Source code in <code>cesnet_tszoo\\utils\\custom_handler\\custom_handler.py</code> <pre><code>class NoFitCustomHandler(CustomHandler):\n    \"\"\"\n    Base class for NoFitCustomHandler. Used for custom handlers that are not fitted and are applied to (from target sets) time series.\n\n    This class serves as the foundation for creating NoFitCustomHandler handlers and should be subclassed from.\n\n    Example:\n\n        import numpy as np\n\n        class NoFitTest(NoFitCustomHandler):\n            def apply(self, data: np.ndarray) -&gt; np.ndarray:\n                data[:, :] = -1\n                return data\n\n            @staticmethod\n            def get_target_sets():\n                return [\"test\"]\n\n    \"\"\"\n\n    @abstractmethod\n    def apply(self, data: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Applies on the input data for a given time series part.\n\n        Parameters:\n            data: A numpy array representing data for a time series with shape `(times, features)` excluding any identifiers.  \n\n        Returns:\n            The changed data, with the same shape as the input `(times, features)`.            \n        \"\"\"\n        ...\n\n    @staticmethod\n    @abstractmethod\n    def get_target_sets() -&gt; set[SplitType] | set[Literal[\"train\", \"val\", \"test\", \"all\"]]:\n        \"\"\"Specifies on which sets this handler should be used. \"\"\"\n        ...\n</code></pre>"},{"location":"reference_custom_handlers/#cesnet_tszoo.utils.custom_handler.custom_handler.NoFitCustomHandler.apply","title":"apply  <code>abstractmethod</code>","text":"<pre><code>apply(data: ndarray) -&gt; np.ndarray\n</code></pre> <p>Applies on the input data for a given time series part.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>A numpy array representing data for a time series with shape <code>(times, features)</code> excluding any identifiers.  </p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The changed data, with the same shape as the input <code>(times, features)</code>.</p> Source code in <code>cesnet_tszoo\\utils\\custom_handler\\custom_handler.py</code> <pre><code>@abstractmethod\ndef apply(self, data: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Applies on the input data for a given time series part.\n\n    Parameters:\n        data: A numpy array representing data for a time series with shape `(times, features)` excluding any identifiers.  \n\n    Returns:\n        The changed data, with the same shape as the input `(times, features)`.            \n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference_custom_handlers/#cesnet_tszoo.utils.custom_handler.custom_handler.NoFitCustomHandler.get_target_sets","title":"get_target_sets  <code>abstractmethod</code> <code>staticmethod</code>","text":"<pre><code>get_target_sets() -&gt; set[SplitType] | set[Literal['train', 'val', 'test', 'all']]\n</code></pre> <p>Specifies on which sets this handler should be used.</p> Source code in <code>cesnet_tszoo\\utils\\custom_handler\\custom_handler.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef get_target_sets() -&gt; set[SplitType] | set[Literal[\"train\", \"val\", \"test\", \"all\"]]:\n    \"\"\"Specifies on which sets this handler should be used. \"\"\"\n    ...\n</code></pre>"},{"location":"reference_dataset_config/","title":"Dataset config class","text":""},{"location":"reference_dataset_config/#cesnet_tszoo.configs.base_config.DatasetConfig","title":"<code>cesnet_tszoo.configs.base_config.DatasetConfig</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for configuration management. This class should not be used directly. Instead, use one of its derived classes, such as <code>TimeBasedConfig</code>, <code>DisjointTimeBasedConfig</code> or <code>SeriesBasedConfig</code>.</p> Source code in <code>cesnet_tszoo\\configs\\base_config.py</code> <pre><code>class DatasetConfig(ABC):\n    \"\"\"\n    Base class for configuration management. This class should **not** be used directly. Instead, use one of its derived classes, such as [`TimeBasedConfig`](reference_time_based_config.md#references.TimeBasedConfig), [`DisjointTimeBasedConfig`](reference_disjoint_time_based_config.md#references.DisjointTimeBasedConfig) or [`SeriesBasedConfig`](reference_series_based_config.md#references.SeriesBasedConfig).\n\n    Attributes:\n        used_train_workers: Tracks the number of train workers in use. Helps determine if the train dataloader should be recreated based on worker changes.\n        used_val_workers: Tracks the number of validation workers in use. Helps determine if the validation dataloader should be recreated based on worker changes.\n        used_test_workers: Tracks the number of test workers in use. Helps determine if the test dataloader should be recreated based on worker changes.\n        used_all_workers: Tracks the total number of all workers in use. Helps determine if the all dataloader should be recreated based on worker changes.\n        import_identifier: Tracks the name of the config upon import. None if not imported.\n        filler_factory: Represents factory used to create passed Filler type.\n        anomaly_handler_factory: Represents factory used to create passed Anomaly Handler type.\n        transformer_factory: Represents factory used to create passed Transformer type.\n        can_fit_fillers: Whether fillers in this config, can be fitted.\n        logger: Logger for displaying information. \n        aggregation: The aggregation period used for the data.\n        source_type: The source type of the data.\n        database_name: Specifies which database this config applies to.\n        features_to_take_without_ids: Features to be returned, excluding time or time series IDs.\n        indices_of_features_to_take_no_ids: Indices of non-ID features in `features_to_take`.\n        ts_id_name: Name of the time series ID, dependent on `source_type`.\n        used_singular_train_time_series: Currently used singular train set time series for dataloader.\n        used_singular_val_time_series: Currently used singular validation set time series for dataloader.\n        used_singular_test_time_series: Currently used singular test set time series for dataloader.\n        used_singular_all_time_series: Currently used singular all set time series for dataloader.       \n        train_preprocess_order: All preprocesses used for train set. \n        val_preprocess_order: All preprocesses used for val set. \n        test_preprocess_order: All preprocesses used for test set. \n        all_preprocess_order: All preprocesses used for all set. \n        is_initialized: Flag indicating if the configuration has already been initialized. If true, config initialization will be skipped.  \n        version: Version of cesnet-tszoo this config was made in.\n        export_update_needed: Whether config was updated to newer version and should be exported.\n        features_to_take: Defines which features are used.\n        default_values: Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature.\n        train_batch_size: Batch size for the train dataloader, when window size is None.\n        val_batch_size: Batch size for the validation dataloader, when window size is None.\n        test_batch_size: Batch size for the test dataloader, when window size is None.\n        all_batch_size: Batch size for the all dataloader, when window size is None.\n        preprocess_order: Defines in which order preprocesses are used. Also can add to order a type of PerSeriesCustomHandler, AllSeriesCustomHandler or NoFitCustomHandler.\n        partial_fit_initialized_transformers: If `True`, partial fitting on train set is performed when using initiliazed transformers.\n        include_time: If `True`, time data is included in the returned values.\n        include_ts_id: If `True`, time series IDs are included in the returned values.\n        time_format: Format for the returned time data. When using TimeFormat.DATETIME, time will be returned as separate list along rest of the values.\n        train_workers: Number of workers for loading training data. `0` means that the data will be loaded in the main process.\n        val_workers: Number of workers for loading validation data. `0` means that the data will be loaded in the main process.\n        test_workers: Number of workers for loading test data. `0` means that the data will be loaded in the main process.\n        all_workers: Number of workers for loading all data. `0` means that the data will be loaded in the main process.\n        init_workers: Number of workers for initial dataset processing during configuration. `0` means that the data will be loaded in the main process.\n        nan_threshold: Maximum allowable percentage of missing data. Time series exceeding this threshold are excluded. Time series over the threshold will not be used. Used for `train/val/test/all` separately.\n        create_transformer_per_time_series: If `True`, a separate transformer is created for each time series. Not used when using already initialized transformers. \n        dataset_type: Type of a dataset this config is used for.\n        train_dataloader_order: Defines the order of data returned by the training dataloader.\n        random_state: Fixes randomness for reproducibility during configuration and dataset initialization.            \n\n    \"\"\"\n\n    def __init__(self,\n                 features_to_take: list[str] | Literal[\"all\"],\n                 default_values: list[Number] | npt.NDArray[np.number] | dict[str, Number] | Number | Literal[\"default\"] | None,\n                 train_batch_size: int,\n                 val_batch_size: int,\n                 test_batch_size: int,\n                 all_batch_size: int,\n                 preprocess_order: list[str, type],\n                 fill_missing_with: type | FillerType | Literal[\"mean_filler\", \"forward_filler\", \"linear_interpolation_filler\"] | None,\n                 transform_with: type | TransformerType | list[Transformer] | np.ndarray[Transformer] | Transformer | Literal[\"min_max_scaler\", \"standard_scaler\", \"max_abs_scaler\", \"log_transformer\", \"robust_scaler\", \"power_transformer\", \"quantile_transformer\", \"l2_normalizer\"] | None,\n                 handle_anomalies_with: type | AnomalyHandlerType | Literal[\"z-score\", \"interquartile_range\"] | None,\n                 partial_fit_initialized_transformers: bool,\n                 include_time: bool,\n                 include_ts_id: bool,\n                 time_format: TimeFormat | Literal[\"id_time\", \"datetime\", \"unix_time\", \"shifted_unix_time\"],\n                 train_workers: int,\n                 val_workers: int,\n                 test_workers: int,\n                 all_workers: int,\n                 init_workers: int,\n                 nan_threshold: float,\n                 create_transformer_per_time_series: bool,\n                 dataset_type: DatasetType,\n                 train_dataloader_order: DataloaderOrder | Literal[\"random\", \"sequential\"],\n                 random_state: int | None,\n                 can_fit_fillers: bool,\n                 logger: logging.Logger):\n        \"\"\"           \n        Parameters:\n            features_to_take: Defines which features are used.\n            default_values: Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature.\n            train_batch_size: Batch size for the train dataloader, when window size is None.\n            val_batch_size: Batch size for the validation dataloader, when window size is None.\n            test_batch_size: Batch size for the test dataloader, when window size is None.\n            all_batch_size: Batch size for the all dataloader, when window size is None.\n            preprocess_order: Defines in which order preprocesses are used. Also can add to order a type of PerSeriesCustomHandler, AllSeriesCustomHandler or NoFitCustomHandler.\n            fill_missing_with: Defines how to fill missing values in the dataset. Can pass enum `FillerType` for built-in filler or pass a type of custom filler that must derive from `Filler` base class.\n            transform_with: Defines the transformer to transform the dataset. Can pass enum `TransformerType` for built-in transformer, pass a type of custom transformer or instance of already fitted transformer(s).\n            handle_anomalies_with: Defines the anomaly handler for handling anomalies in the dataset. Can pass enum `AnomalyHandlerType` for built-in anomaly handler or a type of custom anomaly handler.\n            partial_fit_initialized_transformers: If `True`, partial fitting on train set is performed when using initiliazed transformers.\n            include_time: If `True`, time data is included in the returned values.\n            include_ts_id: If `True`, time series IDs are included in the returned values.\n            time_format: Format for the returned time data. When using TimeFormat.DATETIME, time will be returned as separate list along rest of the values.\n            train_workers: Number of workers for loading training data. `0` means that the data will be loaded in the main process.\n            val_workers: Number of workers for loading validation data. `0` means that the data will be loaded in the main process.\n            test_workers: Number of workers for loading test data. `0` means that the data will be loaded in the main process.\n            all_workers: Number of workers for loading all data. `0` means that the data will be loaded in the main process.\n            init_workers: Number of workers for initial dataset processing during configuration. `0` means that the data will be loaded in the main process.\n            nan_threshold: Maximum allowable percentage of missing data. Time series exceeding this threshold are excluded. Time series over the threshold will not be used. Used for `train/val/test/all` separately.\n            create_transformer_per_time_series: If `True`, a separate transformer is created for each time series. Not used when using already initialized transformers. \n            dataset_type: Type of a dataset this config is used for.\n            train_dataloader_order: Defines the order of data returned by the training dataloader.\n            random_state: Fixes randomness for reproducibility during configuration and dataset initialization.              \n        \"\"\"\n\n        self.used_train_workers: Optional[int] = None\n        self.used_val_workers: Optional[int] = None\n        self.used_test_workers: Optional[int] = None\n        self.used_all_workers: Optional[int] = None\n        self.import_identifier: Optional[str] = None\n        self.filler_factory: filler_factories.FillerFactory = filler_factories.get_filler_factory(fill_missing_with)\n        self.anomaly_handler_factory: anomaly_handler_factories.AnomalyHandlerFactory = anomaly_handler_factories.get_anomaly_handler_factory(handle_anomalies_with)\n        self.transformer_factory: transformer_factories.TransformerFactory = transformer_factories.get_transformer_factory(transform_with, create_transformer_per_time_series, partial_fit_initialized_transformers)\n        self.can_fit_fillers: bool = can_fit_fillers\n        self.logger: logging.Logger = logger\n\n        self.aggregation: Optional[AgreggationType] = None\n        self.source_type: Optional[SourceType] = None\n        self.database_name: Optional[str] = None\n        self.features_to_take_without_ids: Optional[np.ndarray] = None\n        self.indices_of_features_to_take_no_ids: Optional[np.ndarray] = None\n        self.ts_id_name: Optional[str] = None\n        self.used_singular_train_time_series: Optional[int] = None\n        self.used_singular_val_time_series: Optional[int] = None\n        self.used_singular_test_time_series: Optional[int] = None\n        self.used_singular_all_time_series: Optional[int] = None\n        self.train_preprocess_order: list[PreprocessNote] = []\n        self.val_preprocess_order: list[PreprocessNote] = []\n        self.test_preprocess_order: list[PreprocessNote] = []\n        self.all_preprocess_order: list[PreprocessNote] = []\n        self.is_initialized: bool = False\n        self.version: str = version.config_and_benchmarks_current_version\n        self.export_update_needed: bool = False\n\n        self.features_to_take: list[str] = features_to_take\n        self.default_values: np.ndarray = default_values\n        self.train_batch_size: int = train_batch_size\n        self.val_batch_size: int = val_batch_size\n        self.test_batch_size: int = test_batch_size\n        self.all_batch_size: int = all_batch_size\n        self.preprocess_order: list[PreprocessType] = list(preprocess_order)\n        self.partial_fit_initialized_transformers: bool = partial_fit_initialized_transformers\n        self.include_time: bool = include_time\n        self.include_ts_id: bool = include_ts_id\n        self.time_format: TimeFormat = time_format\n        self.train_workers: int = train_workers\n        self.val_workers: int = val_workers\n        self.test_workers: int = test_workers\n        self.all_workers: int = all_workers\n        self.init_workers: int = init_workers\n        self.nan_threshold: float = nan_threshold\n        self.create_transformer_per_time_series: bool = create_transformer_per_time_series\n        self.dataset_type: DatasetType = dataset_type\n        self.train_dataloader_order: DataloaderOrder = train_dataloader_order\n        self.random_state: Optional[int] = random_state\n\n        self._validate_construction()\n\n        self.logger.info(\"Quick validation succeeded.\")\n\n    def _validate_construction(self) -&gt; None:\n        \"\"\"Performs basic parameter validation to ensure correct configuration. More comprehensive validation, which requires dataset-specific data, is handled in [`_dataset_init`](reference_dataset_config.md#references.DatasetConfig._dataset_init). \"\"\"\n\n        # Ensuring boolean flags are correctly set\n        assert isinstance(self.partial_fit_initialized_transformers, bool), \"partial_fit_initialized_transformers must be a boolean value.\"\n        assert isinstance(self.include_time, bool), \"include_time must be a boolean value.\"\n        assert isinstance(self.include_ts_id, bool), \"include_ts_id must be a boolean value.\"\n        assert isinstance(self.create_transformer_per_time_series, bool), \"create_transformer_per_time_series must be a boolean value.\"\n\n        # Ensuring worker count values are non-negative integers\n        assert isinstance(self.train_workers, int) and self.train_workers &gt;= 0, \"train_workers must be a non-negative integer.\"\n        assert isinstance(self.val_workers, int) and self.val_workers &gt;= 0, \"val_workers must be a non-negative integer.\"\n        assert isinstance(self.test_workers, int) and self.test_workers &gt;= 0, \"test_workers must be a non-negative integer.\"\n        assert isinstance(self.all_workers, int) and self.all_workers &gt;= 0, \"all_workers must be a non-negative integer.\"\n        assert isinstance(self.init_workers, int) and self.init_workers &gt;= 0, \"init_workers must be a non-negative integer.\"\n\n        # Ensuring batch size values are positive integers\n        assert isinstance(self.train_batch_size, int) and self.train_batch_size &gt; 0, \"train_batch_size must be a positive integer.\"\n        assert isinstance(self.val_batch_size, int) and self.val_batch_size &gt; 0, \"val_batch_size must be a positive integer.\"\n        assert isinstance(self.test_batch_size, int) and self.test_batch_size &gt; 0, \"test_batch_size must be a positive integer.\"\n        assert isinstance(self.all_batch_size, int) and self.all_batch_size &gt; 0, \"all_batch_size must be a positive integer.\"\n\n        # Ensuring that preprocess order contains all required preprocesses\n        assert self.preprocess_order is not None, \"preprocess_order must be set.\"\n        assert isinstance(self.preprocess_order, list), \"preprocess_order must be list\"\n        assert MANDATORY_PREPROCESSES_ORDER.issubset(self.preprocess_order) or MANDATORY_PREPROCESSES_ORDER_ENUM.issubset(self.preprocess_order), f\"preprocess_order must at least contain order for {list(MANDATORY_PREPROCESSES_ORDER)}\"\n\n        mandatory_count = 0\n        for preprocess in self.preprocess_order:\n            if isinstance(preprocess, (str, PreprocessType)):\n                PreprocessType(preprocess)\n                mandatory_count += 1\n            elif not isinstance(preprocess, type):\n                raise ValueError(f\"Values in preprocess_order must be either from {list(MANDATORY_PREPROCESSES_ORDER)} or a type.\")\n\n        if mandatory_count != len(MANDATORY_PREPROCESSES_ORDER):\n            raise ValueError(f\"preprocess_order must not contain duplicate mandatory preprocesses ({MANDATORY_PREPROCESSES_ORDER}).\")\n\n        # Validate nan_threshold value\n        assert isinstance(self.nan_threshold, Number) and 0 &lt;= self.nan_threshold &lt;= 1, \"nan_threshold must be a number between 0 and 1.\"\n        self.nan_threshold = float(self.nan_threshold)\n\n        # Convert time_format and train_dataloader_order to their respective enum types\n        self.time_format = TimeFormat(self.time_format)\n        self.train_dataloader_order = DataloaderOrder(self.train_dataloader_order)\n\n    def _update_batch_sizes(self, train_batch_size: int, val_batch_size: int, test_batch_size: int, all_batch_size: int) -&gt; None:\n\n        # Ensuring batch size values are positive integers\n        assert isinstance(train_batch_size, int) and train_batch_size &gt; 0, \"train_batch_size must be a positive integer.\"\n        assert isinstance(val_batch_size, int) and val_batch_size &gt; 0, \"val_batch_size must be a positive integer.\"\n        assert isinstance(test_batch_size, int) and test_batch_size &gt; 0, \"test_batch_size must be a positive integer.\"\n        assert isinstance(all_batch_size, int) and all_batch_size &gt; 0, \"all_batch_size must be a positive integer.\"\n\n        self.train_batch_size = train_batch_size\n        self.val_batch_size = val_batch_size\n        self.test_batch_size = test_batch_size\n        self.all_batch_size = all_batch_size\n\n        self.logger.debug(\"Updated batch sizes.\")\n\n    def _update_workers(self, train_workers: int, val_workers: int, test_workers: int, all_workers: int, init_workers: int) -&gt; None:\n\n        # Ensuring worker count values are non-negative integers\n        assert isinstance(self.train_workers, int) and self.train_workers &gt;= 0, \"train_workers must be a non-negative integer.\"\n        assert isinstance(self.val_workers, int) and self.val_workers &gt;= 0, \"val_workers must be a non-negative integer.\"\n        assert isinstance(self.test_workers, int) and self.test_workers &gt;= 0, \"test_workers must be a non-negative integer.\"\n        assert isinstance(self.all_workers, int) and self.all_workers &gt;= 0, \"all_workers must be a non-negative integer.\"\n        assert isinstance(self.init_workers, int) and self.init_workers &gt;= 0, \"init_workers must be a non-negative integer.\"\n\n        self.train_workers = train_workers\n        self.val_workers = val_workers\n        self.test_workers = test_workers\n        self.all_workers = all_workers\n        self.init_workers = init_workers\n\n        self.logger.debug(\"Updated workers.\")\n\n    @abstractmethod\n    def _get_train(self) -&gt; tuple[np.ndarray, np.ndarray] | tuple[None, None]:\n        \"\"\"Returns the indices corresponding to the training set. \"\"\"\n        ...\n\n    @abstractmethod\n    def _get_val(self) -&gt; tuple[np.ndarray, np.ndarray] | tuple[None, None]:\n        \"\"\"Returns the indices corresponding to the validation set. \"\"\"\n        ...\n\n    @abstractmethod\n    def _get_test(self) -&gt; tuple[np.ndarray, np.ndarray] | tuple[None, None]:\n        \"\"\"Returns the indices corresponding to the test set. \"\"\"\n        ...\n\n    @abstractmethod\n    def _get_all(self) -&gt; tuple[np.ndarray, np.ndarray] | tuple[None, None]:\n        \"\"\"Returns the indices corresponding to the all set. \"\"\"\n        ...\n\n    @abstractmethod\n    def has_train(self) -&gt; bool:\n        \"\"\"Returns whether training set is used. \"\"\"\n        ...\n\n    @abstractmethod\n    def has_val(self) -&gt; bool:\n        \"\"\"Returns whether validation set is used. \"\"\"\n        ...\n\n    @abstractmethod\n    def has_test(self) -&gt; bool:\n        \"\"\"Returns whether test set is used. \"\"\"\n        ...\n\n    @abstractmethod\n    def has_all(self) -&gt; bool:\n        \"\"\"Returns whether all set is used. \"\"\"\n        ...\n\n    def _get_train_preprocess_init_order_groups(self) -&gt; list[PreprocessOrderGroup]:\n        return self.__get_preprocess_init_order_groups(self.train_preprocess_order)\n\n    def _get_val_preprocess_init_order_groups(self) -&gt; list[PreprocessOrderGroup]:\n        return self.__get_preprocess_init_order_groups(self.val_preprocess_order)\n\n    def _get_test_preprocess_init_order_groups(self) -&gt; list[PreprocessOrderGroup]:\n        return self.__get_preprocess_init_order_groups(self.test_preprocess_order)\n\n    def __get_preprocess_init_order_groups(self, preprocess_order) -&gt; list[PreprocessOrderGroup]:\n        \"\"\"Returns preprocess grouped orders used when initializing config. \"\"\"\n\n        groups = []\n\n        outers = []\n        inners = []\n\n        preprocess_note: PreprocessNote\n        for preprocess_note in preprocess_order:\n\n            if preprocess_note.is_inner_preprocess:\n\n                if len(outers) &gt; 0:\n                    group = PreprocessOrderGroup(inners + outers)\n                    groups.append(group)\n\n                    inners = group.get_preprocess_orders_for_inner_transform()\n                    outers.clear()\n\n                inners.append(preprocess_note)\n\n            if not preprocess_note.is_inner_preprocess:\n                outers.append(preprocess_note)\n\n        group = PreprocessOrderGroup(inners + outers)\n\n        if group.any_preprocess_needs_fitting or group.any_preprocess_is_dummy_fitting:\n            groups.append(group)\n\n        if len(groups) == 0:\n            groups.append(PreprocessOrderGroup([]))\n\n        return groups\n\n    def _update_identifiers_from_dataset_metadata(self, dataset_metadata: DatasetMetadata) -&gt; None:\n        \"\"\"Updates identifying attributes from dataset metadata. \"\"\"\n\n        self.aggregation = dataset_metadata.aggregation\n        self.source_type = dataset_metadata.source_type\n        self.database_name = dataset_metadata.database_name\n\n    def _dataset_init(self, dataset_metadata: DatasetMetadata) -&gt; None:\n        \"\"\"Performs deeper parameter validation and updates values based on data from the dataset. \"\"\"\n\n        rd = np.random.RandomState(self.random_state)\n\n        self.ts_id_name = dataset_metadata.ts_id_name\n\n        self._set_features_to_take(dataset_metadata.features)\n        self.logger.debug(\"Features to take have been successfully set.\")\n\n        self._set_ts(dataset_metadata.ts_indices, dataset_metadata.ts_row_ranges, rd)\n        self.logger.debug(\"Time series IDs have been successfully set.\")\n\n        self._set_time_period(dataset_metadata.time_indices)\n        self.logger.debug(\"Time period have been successfully set.\")\n\n        self._set_default_values(dataset_metadata.default_values)\n        self.logger.debug(\"Default values have been successfully set.\")\n\n        self._set_preprocess_order()\n        self.logger.debug(\"Preprocess order have been successfully set.\")\n\n        self._validate_finalization()\n        self.logger.debug(\"Finalization and validation completed successfully.\")\n\n    def _set_features_to_take(self, all_dataset_features: dict[str, np.dtype]) -&gt; None:\n        \"\"\"Validates and filters the input `features_to_take` based on the `dataset`, `source_type`, and `aggregation`. \"\"\"\n\n        if self.features_to_take == \"all\":\n            self.features_to_take = list(all_dataset_features.keys())\n            self.logger.debug(\"All features used because 'features_to_take' is set to 'all'.\")\n\n        # Handling the inclusion of time ID in features\n        if self.include_time and self.features_to_take.count(ID_TIME_COLUMN_NAME) == 0 and self.time_format != TimeFormat.DATETIME:\n            self.features_to_take.insert(0, ID_TIME_COLUMN_NAME)\n            self.logger.debug(\"Added '%s' to the features as 'include_time' is true and 'time_format' is not datetime.\", ID_TIME_COLUMN_NAME)\n        elif self.include_time and self.features_to_take.count(ID_TIME_COLUMN_NAME) &gt; 0 and self.time_format == TimeFormat.DATETIME:\n            self.features_to_take.remove(ID_TIME_COLUMN_NAME)\n            self.logger.debug(\"Removed '%s' from the features because 'time_format' is datetime.\", ID_TIME_COLUMN_NAME)\n        elif not self.include_time and self.features_to_take.count(ID_TIME_COLUMN_NAME) &gt; 0:\n            self.features_to_take.remove(ID_TIME_COLUMN_NAME)\n            self.logger.debug(\"Removed '%s' from the features as 'include_time' is false.\", ID_TIME_COLUMN_NAME)\n\n        # Handling the inclusion of time series ID feature\n        if self.include_ts_id and self.features_to_take.count(self.ts_id_name) &lt;= 0:\n            self.features_to_take.insert(0, self.ts_id_name)\n            self.logger.debug(\"Added '%s' to the features as 'include_ts_id' is true.\", self.ts_id_name)\n        elif not self.include_ts_id and self.features_to_take.count(self.ts_id_name) &gt; 0:\n            self.features_to_take.remove(self.ts_id_name)\n            self.logger.debug(\"Removed '%s' from the features as 'include_ts_id' is false.\", self.ts_id_name)\n\n        # Filtering features based on available dataset features\n        temp = list(self.features_to_take)\n        self.features_to_take = [feature for feature in self.features_to_take if feature in all_dataset_features or feature == ID_TIME_COLUMN_NAME or feature == self.ts_id_name]\n\n        if len(temp) != len(self.features_to_take):\n            self.logger.warning(\"Some features were removed as they are not available in the dataset.\")\n\n        # Preparing indices and features without time and time series ID\n        self.indices_of_features_to_take_no_ids = [idx for idx, feature in enumerate(self.features_to_take) if feature != ID_TIME_COLUMN_NAME and feature != self.ts_id_name]\n        self.features_to_take_without_ids = [feature for feature in self.features_to_take if feature != ID_TIME_COLUMN_NAME and feature != self.ts_id_name]\n\n        # Assert that at least one feature is used\n        assert len(self.features_to_take_without_ids) &gt; 0, \"At least one non-ID feature must be used.\"\n\n    def _set_default_values(self, default_values: dict[str, Number]) -&gt; None:\n        \"\"\"Validates and filters the input `default_values` based on the `dataset`, `source_type`, `aggregation`, and `features_to_take`. \"\"\"\n\n        if self.default_values == \"default\":\n            self.default_values = dict(default_values)\n            self.logger.debug(\"Using default dataset values for default values because 'default_values' is set to 'default'.\")\n\n        elif isinstance(self.default_values, Number):\n            # If default_values is a single number, assign it to all features\n\n            orig_default_value = self.default_values\n            self.default_values = {feature: float(self.default_values) for feature in self.features_to_take_without_ids}\n            self.logger.debug(\"Assigned the default value %s to all features as 'default_values' is a single number.\", float(orig_default_value))\n\n        elif isinstance(self.default_values, (list, np.ndarray)):\n            # If default_values is a list or ndarray, ensure the length matches with features_to_take_without_ids\n            if len(self.default_values) != len(self.features_to_take_without_ids):\n                raise ValueError(\"The number of values in 'default_values' does not match the number of features in 'features_to_take'.\")\n            self.default_values = {feature: value for feature, value in zip(self.features_to_take_without_ids, self.default_values) if feature != ID_TIME_COLUMN_NAME and feature != self.ts_id_name}\n            self.logger.debug(\"Mapped default values to features, skipping IDs features: %s\", self.default_values)\n\n        elif isinstance(self.default_values, dict):\n            # If default_values is a dictionary, ensure its keys match the features\n            if set(self.default_values.keys()) != set(self.features_to_take_without_ids):\n                raise ValueError(\"The keys in 'default_values' do not match the features in 'features_to_take'.\")\n            self.logger.debug(\"Using provided default values for features: %s\", self.default_values)\n\n        elif self.default_values is None or math.isnan(self.default_values) or np.isnan(self.default_values):\n            # If default_values is None or NaN, assign NaN to each feature\n            self.default_values = {feature: np.nan for feature in self.features_to_take_without_ids}\n            self.logger.debug(\"Assigned NaN as the default value for all features because 'default_values' is None or NaN.\")\n\n        # Convert the default values into a NumPy array for consistent data handling\n        temp_default_values = np.ndarray(len(self.features_to_take_without_ids), np.float64)\n        for i, feature in enumerate(self.features_to_take_without_ids):\n            temp_default_values[i] = self.default_values[feature]\n\n        self.default_values = temp_default_values\n\n    def _set_preprocess_order(self):\n        \"\"\"Validates and converts preprocess order to their enum variant. Also initializes preprocess_orders for all sets. \"\"\"\n\n        for i, order in enumerate(self.preprocess_order):\n            if isinstance(order, (str, PreprocessType)):\n                self.preprocess_order[i] = PreprocessType(order)\n            elif not isinstance(order, type):\n                raise NotImplementedError(\"Currenty preprocess order supports only string names or types\")\n\n        self._init_preprocess_order()\n\n    def _init_preprocess_order(self):\n        self.train_preprocess_order = []\n        self.val_preprocess_order = []\n        self.test_preprocess_order = []\n        self.all_preprocess_order = []\n\n        for preprocess_type in self.preprocess_order:\n\n            if preprocess_type == PreprocessType.TRANSFORMING:\n                self.__set_transform_order(preprocess_type)\n            elif preprocess_type == PreprocessType.FILLING_GAPS:\n                self.__set_filling_order(preprocess_type)\n            elif preprocess_type == PreprocessType.HANDLING_ANOMALIES:\n                self.__set_anomaly_handler_order(preprocess_type)\n            elif isinstance(preprocess_type, type):\n                self.__set_custom_handler(preprocess_type)\n            else:\n                raise NotImplementedError()\n\n    def __set_transform_order(self, preprocess_type: PreprocessType):\n        needs_fitting = (self.partial_fit_initialized_transformers or not self.transformer_factory.has_already_initialized) and not self.transformer_factory.is_empty_factory\n        should_partial_fit = (self.transformer_factory.has_already_initialized and self.partial_fit_initialized_transformers) or (not self.transformer_factory.has_already_initialized and not self.create_transformer_per_time_series)\n        is_outer = not self.create_transformer_per_time_series and needs_fitting\n        transformers = self._get_feature_transformers()\n\n        self.train_preprocess_order.append(PreprocessNote(preprocess_type, False, needs_fitting, self.has_train(), not is_outer, TransformerHolder(transformers, self.create_transformer_per_time_series, should_partial_fit)))\n        self.val_preprocess_order.append(PreprocessNote(preprocess_type, needs_fitting, False, self.has_val(), not is_outer, TransformerHolder(transformers, self.create_transformer_per_time_series, False)))\n        self.test_preprocess_order.append(PreprocessNote(preprocess_type, needs_fitting, False, self.has_test(), not is_outer, TransformerHolder(transformers, self.create_transformer_per_time_series, False)))\n        self.all_preprocess_order.append(PreprocessNote(preprocess_type, needs_fitting, False, self.has_all(), not is_outer, TransformerHolder(transformers, self.create_transformer_per_time_series, False)))\n\n    def __set_filling_order(self, preprocess_type: PreprocessType):\n        needs_fitting = self.can_fit_fillers and not self.filler_factory.is_empty_factory\n        train_fillers, val_fillers, test_fillers, all_fillers = self._get_fillers()\n\n        self.train_preprocess_order.append(PreprocessNote(preprocess_type, needs_fitting, False, self.has_train(), True, FillingHolder(train_fillers, self.default_values)))\n        self.val_preprocess_order.append(PreprocessNote(preprocess_type, False, needs_fitting, self.has_val(), True, FillingHolder(val_fillers, self.default_values)))\n        self.test_preprocess_order.append(PreprocessNote(preprocess_type, False, needs_fitting, self.has_test(), True, FillingHolder(test_fillers, self.default_values)))\n        self.all_preprocess_order.append(PreprocessNote(preprocess_type, needs_fitting, False, self.has_all(), True, FillingHolder(all_fillers, self.default_values)))\n\n    def __set_anomaly_handler_order(self, preprocess_type: PreprocessType):\n        anomaly_handlers = self._get_anomaly_handlers()\n\n        self.train_preprocess_order.append(PreprocessNote(preprocess_type, False, not self.anomaly_handler_factory.is_empty_factory, self.has_train(), True, AnomalyHandlerHolder(anomaly_handlers)))\n        self.val_preprocess_order.append(PreprocessNote(preprocess_type, not self.anomaly_handler_factory.is_empty_factory, False, self.has_val(), True, AnomalyHandlerHolder(None)))\n        self.test_preprocess_order.append(PreprocessNote(preprocess_type, not self.anomaly_handler_factory.is_empty_factory, False, self.has_test(), True, AnomalyHandlerHolder(None)))\n        self.all_preprocess_order.append(PreprocessNote(preprocess_type, not self.anomaly_handler_factory.is_empty_factory, False, self.has_all(), True, AnomalyHandlerHolder(None)))\n\n    def _update_preprocess_order_supported_ids(self, preprocess_order: list[PreprocessNote], supported_ts_ids: np.ndarray | list):\n        for preprocess in preprocess_order:\n            preprocess.holder.supported_ts_updated(supported_ts_ids)\n\n    def __set_custom_handler(self, preprocess_type: type):\n        factory = custom_handler_factories.get_custom_handler_factory(preprocess_type)\n\n        if factory.preprocess_enum_type == PreprocessType.PER_SERIES_CUSTOM:\n            self._set_per_series_custom_handler(factory)\n        elif factory.preprocess_enum_type == PreprocessType.ALL_SERIES_CUSTOM:\n            self.__set_all_series_custom_handler(factory)\n        elif factory.preprocess_enum_type == PreprocessType.NO_FIT_CUSTOM:\n            self._set_no_fit_custom_handler(factory)\n        else:\n            raise NotImplementedError()\n\n    @abstractmethod\n    def _set_per_series_custom_handler(self, factory: PerSeriesCustomHandlerFactory):\n        ...\n\n    def __set_all_series_custom_handler(self, factory: AllSeriesCustomHandlerFactory):\n\n        if not self.has_train():\n            raise ValueError(\"To use AllSeriesCustomHandlerFactory you need to use train set.\")\n\n        handler = factory.create_handler()\n\n        self.train_preprocess_order.append(PreprocessNote(factory.preprocess_enum_type, False, True, factory.can_apply_to_train, False, AllSeriesCustomHandlerHolder(handler)))\n        self.val_preprocess_order.append(PreprocessNote(factory.preprocess_enum_type, True, False, factory.can_apply_to_val and self.has_val(), False, AllSeriesCustomHandlerHolder(handler)))\n        self.test_preprocess_order.append(PreprocessNote(factory.preprocess_enum_type, True, False, factory.can_apply_to_test and self.has_test(), False, AllSeriesCustomHandlerHolder(handler)))\n        self.all_preprocess_order.append(PreprocessNote(factory.preprocess_enum_type, True, False, factory.can_apply_to_all and self.has_all(), False, AllSeriesCustomHandlerHolder(handler)))\n\n    def _get_summary_steps(self) -&gt; list[css_utils.SummaryDiagramStep]:\n        steps = []\n\n        steps.append(self._get_summary_dataset())\n        steps.append(self._get_summary_filter_time_series())\n        steps.append(self._get_summary_filter_features())\n        steps += self._get_summary_preprocessing()\n        steps += self._get_summary_loader()\n\n        return steps\n\n    def _get_summary_dataset(self) -&gt; css_utils.SummaryDiagramStep:\n        attributes = [css_utils.StepAttribute(\"Database\", self.database_name),\n                      css_utils.StepAttribute(\"Aggregation\", self.aggregation),\n                      css_utils.StepAttribute(\"Source\", self.source_type)]\n\n        return css_utils.SummaryDiagramStep(\"Load from dataset\", attributes)\n\n    @abstractmethod\n    def _get_summary_filter_time_series(self) -&gt; css_utils.SummaryDiagramStep:\n        ...\n\n    def _get_summary_filter_features(self) -&gt; css_utils.SummaryDiagramStep:\n        attributes = [css_utils.StepAttribute(\"Taken features\", self.features_to_take_without_ids),\n                      css_utils.StepAttribute(\"Time series ID included\", self.include_ts_id),\n                      css_utils.StepAttribute(\"Time included\", self.include_time),\n                      css_utils.StepAttribute(\"Time format\", self.time_format)]\n\n        return css_utils.SummaryDiagramStep(\"Filter features\", attributes)\n\n    def _get_summary_preprocessing(self) -&gt; list[css_utils.SummaryDiagramStep]:\n        steps = []\n\n        for preprocess_type, train_pr, val_pr, test_pr, all_pr in list(zip(self.preprocess_order, self.train_preprocess_order, self.val_preprocess_order, self.test_preprocess_order, self.all_preprocess_order)):\n            preprocess_title = None\n            preprocess_type_name = None\n            is_per_time_series = train_pr.is_inner_preprocess\n            target_sets = []\n            requires_fitting = False\n            if train_pr.can_be_applied:\n                target_sets.append(\"train\")\n                requires_fitting = train_pr.should_be_fitted\n            if val_pr.can_be_applied:\n                target_sets.append(\"val\")\n            if test_pr.can_be_applied:\n                target_sets.append(\"test\")\n            if all_pr.can_be_applied:\n                target_sets.append(\"all\")\n\n            if len(target_sets) == 0:\n                continue\n\n            if train_pr.preprocess_type == PreprocessType.HANDLING_ANOMALIES:\n                preprocess_title = \"Handle anomalies\"\n                preprocess_type_name = self.anomaly_handler_factory.anomaly_handler_type.__name__\n\n                if self.anomaly_handler_factory.is_empty_factory:\n                    continue\n\n            elif train_pr.preprocess_type == PreprocessType.FILLING_GAPS:\n                preprocess_title = \"Handle missing values\"\n                preprocess_type_name = f\"{self.filler_factory.filler_type.__name__}\"\n\n                steps.append(css_utils.SummaryDiagramStep(\"Pre-fill with default values\", [css_utils.StepAttribute(\"Default values\", self.default_values)]))\n\n                if self.filler_factory.is_empty_factory:\n                    continue\n\n            elif train_pr.preprocess_type == PreprocessType.TRANSFORMING:\n                preprocess_title = \"Apply transformer\"\n                preprocess_type_name = self.transformer_factory.transformer_type.__name__\n\n                if self.transformer_factory.is_empty_factory:\n                    continue\n\n                is_per_time_series = self.create_transformer_per_time_series\n            elif train_pr.preprocess_type == PreprocessType.PER_SERIES_CUSTOM:\n                preprocess_title = f\"Apply {preprocess_type.__name__}\"\n                preprocess_type_name = preprocess_type.__name__\n            elif train_pr.preprocess_type == PreprocessType.ALL_SERIES_CUSTOM:\n                preprocess_title = f\"Apply {preprocess_type.__name__}\"\n                preprocess_type_name = preprocess_type.__name__\n            elif train_pr.preprocess_type == PreprocessType.NO_FIT_CUSTOM:\n                preprocess_title = f\"Apply {preprocess_type.__name__}\"\n                preprocess_type_name = preprocess_type.__name__\n\n            step = css_utils.SummaryDiagramStep(preprocess_title, [css_utils.StepAttribute(\"Type\", preprocess_type_name),\n                                                                   css_utils.StepAttribute(\"Requires fitting\", requires_fitting),\n                                                                   css_utils.StepAttribute(\"Is per time series\", is_per_time_series),\n                                                                   css_utils.StepAttribute(\"Target sets\", target_sets)])\n            steps.append(step)\n\n        return steps\n\n    @abstractmethod\n    def _get_summary_loader(self) -&gt; list[css_utils.SummaryDiagramStep]:\n        ...\n\n    @abstractmethod\n    def _set_no_fit_custom_handler(self, factory: NoFitCustomHandlerFactory):\n        ...\n\n    @abstractmethod\n    def _set_time_period(self, all_time_ids: np.ndarray) -&gt; None:\n        \"\"\"Validates and filters the input time periods based on the dataset and aggregation. This typically calls [`_process_time_period`](reference_dataset_config.md#references.DatasetConfig._process_time_period) for each time period. \"\"\"\n        ...\n\n    @abstractmethod\n    def _set_ts(self, all_ts_ids: np.ndarray, all_ts_row_ranges: np.ndarray, rd: np.random.RandomState) -&gt; None:\n        \"\"\"Validates and filters the input time series IDs based on the `dataset` and `source_type`. This typically calls [`_process_ts_ids`](reference_dataset_config.md#references.DatasetConfig._process_ts_ids) for each time series ID filter. \"\"\"\n        ...\n\n    @abstractmethod\n    def _get_feature_transformers(self) -&gt; np.ndarray[Transformer] | Transformer:\n        \"\"\"Creates transformers with `transformer_factory`. \"\"\"\n        ...\n\n    @abstractmethod\n    def _get_fillers(self) -&gt; tuple:\n        \"\"\"Creates fillers with `filler_factory`. \"\"\"\n        ...\n\n    @abstractmethod\n    def _get_anomaly_handlers(self) -&gt; np.ndarray:\n        \"\"\"Creates anomaly handlers with `anomaly_handler_factory`. \"\"\"\n        ...\n\n    @abstractmethod\n    def _validate_finalization(self) -&gt; None:\n        \"\"\"Performs final validation of the configuration. \"\"\"\n        ...\n</code></pre>"},{"location":"reference_dataset_config/#configuration-options","title":"Configuration options","text":"<p>Parameters:</p> Name Type Description Default <code>features_to_take</code> <code>list[str] | Literal['all']</code> <p>Defines which features are used.</p> required <code>default_values</code> <code>list[Number] | NDArray[number] | dict[str, Number] | Number | Literal['default'] | None</code> <p>Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature.</p> required <code>train_batch_size</code> <code>int</code> <p>Batch size for the train dataloader, when window size is None.</p> required <code>val_batch_size</code> <code>int</code> <p>Batch size for the validation dataloader, when window size is None.</p> required <code>test_batch_size</code> <code>int</code> <p>Batch size for the test dataloader, when window size is None.</p> required <code>all_batch_size</code> <code>int</code> <p>Batch size for the all dataloader, when window size is None.</p> required <code>preprocess_order</code> <code>list[str, type]</code> <p>Defines in which order preprocesses are used. Also can add to order a type of PerSeriesCustomHandler, AllSeriesCustomHandler or NoFitCustomHandler.</p> required <code>fill_missing_with</code> <code>type | FillerType | Literal['mean_filler', 'forward_filler', 'linear_interpolation_filler'] | None</code> <p>Defines how to fill missing values in the dataset. Can pass enum <code>FillerType</code> for built-in filler or pass a type of custom filler that must derive from <code>Filler</code> base class.</p> required <code>transform_with</code> <code>type | TransformerType | list[Transformer] | ndarray[Transformer] | Transformer | Literal['min_max_scaler', 'standard_scaler', 'max_abs_scaler', 'log_transformer', 'robust_scaler', 'power_transformer', 'quantile_transformer', 'l2_normalizer'] | None</code> <p>Defines the transformer to transform the dataset. Can pass enum <code>TransformerType</code> for built-in transformer, pass a type of custom transformer or instance of already fitted transformer(s).</p> required <code>handle_anomalies_with</code> <code>type | AnomalyHandlerType | Literal['z-score', 'interquartile_range'] | None</code> <p>Defines the anomaly handler for handling anomalies in the dataset. Can pass enum <code>AnomalyHandlerType</code> for built-in anomaly handler or a type of custom anomaly handler.</p> required <code>partial_fit_initialized_transformers</code> <code>bool</code> <p>If <code>True</code>, partial fitting on train set is performed when using initiliazed transformers.</p> required <code>include_time</code> <code>bool</code> <p>If <code>True</code>, time data is included in the returned values.</p> required <code>include_ts_id</code> <code>bool</code> <p>If <code>True</code>, time series IDs are included in the returned values.</p> required <code>time_format</code> <code>TimeFormat | Literal['id_time', 'datetime', 'unix_time', 'shifted_unix_time']</code> <p>Format for the returned time data. When using TimeFormat.DATETIME, time will be returned as separate list along rest of the values.</p> required <code>train_workers</code> <code>int</code> <p>Number of workers for loading training data. <code>0</code> means that the data will be loaded in the main process.</p> required <code>val_workers</code> <code>int</code> <p>Number of workers for loading validation data. <code>0</code> means that the data will be loaded in the main process.</p> required <code>test_workers</code> <code>int</code> <p>Number of workers for loading test data. <code>0</code> means that the data will be loaded in the main process.</p> required <code>all_workers</code> <code>int</code> <p>Number of workers for loading all data. <code>0</code> means that the data will be loaded in the main process.</p> required <code>init_workers</code> <code>int</code> <p>Number of workers for initial dataset processing during configuration. <code>0</code> means that the data will be loaded in the main process.</p> required <code>nan_threshold</code> <code>float</code> <p>Maximum allowable percentage of missing data. Time series exceeding this threshold are excluded. Time series over the threshold will not be used. Used for <code>train/val/test/all</code> separately.</p> required <code>create_transformer_per_time_series</code> <code>bool</code> <p>If <code>True</code>, a separate transformer is created for each time series. Not used when using already initialized transformers. </p> required <code>dataset_type</code> <code>DatasetType</code> <p>Type of a dataset this config is used for.</p> required <code>train_dataloader_order</code> <code>DataloaderOrder | Literal['random', 'sequential']</code> <p>Defines the order of data returned by the training dataloader.</p> required <code>random_state</code> <code>int | None</code> <p>Fixes randomness for reproducibility during configuration and dataset initialization.</p> required"},{"location":"reference_dataset_config/#config-attributes","title":"Config attributes","text":"<p>Attributes:</p> Name Type Description <code>used_train_workers</code> <code>Optional[int]</code> <p>Tracks the number of train workers in use. Helps determine if the train dataloader should be recreated based on worker changes.</p> <code>used_val_workers</code> <code>Optional[int]</code> <p>Tracks the number of validation workers in use. Helps determine if the validation dataloader should be recreated based on worker changes.</p> <code>used_test_workers</code> <code>Optional[int]</code> <p>Tracks the number of test workers in use. Helps determine if the test dataloader should be recreated based on worker changes.</p> <code>used_all_workers</code> <code>Optional[int]</code> <p>Tracks the total number of all workers in use. Helps determine if the all dataloader should be recreated based on worker changes.</p> <code>import_identifier</code> <code>Optional[str]</code> <p>Tracks the name of the config upon import. None if not imported.</p> <code>filler_factory</code> <code>FillerFactory</code> <p>Represents factory used to create passed Filler type.</p> <code>anomaly_handler_factory</code> <code>AnomalyHandlerFactory</code> <p>Represents factory used to create passed Anomaly Handler type.</p> <code>transformer_factory</code> <code>TransformerFactory</code> <p>Represents factory used to create passed Transformer type.</p> <code>can_fit_fillers</code> <code>bool</code> <p>Whether fillers in this config, can be fitted.</p> <code>logger</code> <code>Logger</code> <p>Logger for displaying information. </p> <code>aggregation</code> <code>Optional[AgreggationType]</code> <p>The aggregation period used for the data.</p> <code>source_type</code> <code>Optional[SourceType]</code> <p>The source type of the data.</p> <code>database_name</code> <code>Optional[str]</code> <p>Specifies which database this config applies to.</p> <code>features_to_take_without_ids</code> <code>Optional[ndarray]</code> <p>Features to be returned, excluding time or time series IDs.</p> <code>indices_of_features_to_take_no_ids</code> <code>Optional[ndarray]</code> <p>Indices of non-ID features in <code>features_to_take</code>.</p> <code>ts_id_name</code> <code>Optional[str]</code> <p>Name of the time series ID, dependent on <code>source_type</code>.</p> <code>used_singular_train_time_series</code> <code>Optional[int]</code> <p>Currently used singular train set time series for dataloader.</p> <code>used_singular_val_time_series</code> <code>Optional[int]</code> <p>Currently used singular validation set time series for dataloader.</p> <code>used_singular_test_time_series</code> <code>Optional[int]</code> <p>Currently used singular test set time series for dataloader.</p> <code>used_singular_all_time_series</code> <code>Optional[int]</code> <p>Currently used singular all set time series for dataloader.       </p> <code>train_preprocess_order</code> <code>list[PreprocessNote]</code> <p>All preprocesses used for train set. </p> <code>val_preprocess_order</code> <code>list[PreprocessNote]</code> <p>All preprocesses used for val set. </p> <code>test_preprocess_order</code> <code>list[PreprocessNote]</code> <p>All preprocesses used for test set. </p> <code>all_preprocess_order</code> <code>list[PreprocessNote]</code> <p>All preprocesses used for all set. </p> <code>is_initialized</code> <code>bool</code> <p>Flag indicating if the configuration has already been initialized. If true, config initialization will be skipped.  </p> <code>version</code> <code>str</code> <p>Version of cesnet-tszoo this config was made in.</p> <code>export_update_needed</code> <code>bool</code> <p>Whether config was updated to newer version and should be exported.</p> <code>features_to_take</code> <code>list[str]</code> <p>Defines which features are used.</p> <code>default_values</code> <code>ndarray</code> <p>Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature.</p> <code>train_batch_size</code> <code>int</code> <p>Batch size for the train dataloader, when window size is None.</p> <code>val_batch_size</code> <code>int</code> <p>Batch size for the validation dataloader, when window size is None.</p> <code>test_batch_size</code> <code>int</code> <p>Batch size for the test dataloader, when window size is None.</p> <code>all_batch_size</code> <code>int</code> <p>Batch size for the all dataloader, when window size is None.</p> <code>preprocess_order</code> <code>list[PreprocessType]</code> <p>Defines in which order preprocesses are used. Also can add to order a type of PerSeriesCustomHandler, AllSeriesCustomHandler or NoFitCustomHandler.</p> <code>partial_fit_initialized_transformers</code> <code>bool</code> <p>If <code>True</code>, partial fitting on train set is performed when using initiliazed transformers.</p> <code>include_time</code> <code>bool</code> <p>If <code>True</code>, time data is included in the returned values.</p> <code>include_ts_id</code> <code>bool</code> <p>If <code>True</code>, time series IDs are included in the returned values.</p> <code>time_format</code> <code>TimeFormat</code> <p>Format for the returned time data. When using TimeFormat.DATETIME, time will be returned as separate list along rest of the values.</p> <code>train_workers</code> <code>int</code> <p>Number of workers for loading training data. <code>0</code> means that the data will be loaded in the main process.</p> <code>val_workers</code> <code>int</code> <p>Number of workers for loading validation data. <code>0</code> means that the data will be loaded in the main process.</p> <code>test_workers</code> <code>int</code> <p>Number of workers for loading test data. <code>0</code> means that the data will be loaded in the main process.</p> <code>all_workers</code> <code>int</code> <p>Number of workers for loading all data. <code>0</code> means that the data will be loaded in the main process.</p> <code>init_workers</code> <code>int</code> <p>Number of workers for initial dataset processing during configuration. <code>0</code> means that the data will be loaded in the main process.</p> <code>nan_threshold</code> <code>float</code> <p>Maximum allowable percentage of missing data. Time series exceeding this threshold are excluded. Time series over the threshold will not be used. Used for <code>train/val/test/all</code> separately.</p> <code>create_transformer_per_time_series</code> <code>bool</code> <p>If <code>True</code>, a separate transformer is created for each time series. Not used when using already initialized transformers. </p> <code>dataset_type</code> <code>DatasetType</code> <p>Type of a dataset this config is used for.</p> <code>train_dataloader_order</code> <code>DataloaderOrder</code> <p>Defines the order of data returned by the training dataloader.</p> <code>random_state</code> <code>Optional[int]</code> <p>Fixes randomness for reproducibility during configuration and dataset initialization.</p>"},{"location":"reference_disjoint_time_based_cesnet_dataset/","title":"Disjoint-time-based dataset class","text":""},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset","title":"cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset  <code>dataclass</code>","text":"<p>               Bases: <code>CesnetDataset</code></p> <p>This class is used for disjoint-time-based returning of data. Can be created by using <code>get_dataset</code> with parameter <code>dataset_type</code> = <code>DatasetType.DISJOINT_TIME_BASED</code>.</p> <p>Disjoint-time-based means batch size affects number of returned times in one batch and each set can have different time series. Which time series are returned does not change. Additionally it supports sliding window.</p> <p>The dataset provides multiple ways to access the data:</p> <ul> <li>Iterable PyTorch DataLoader: For batch processing.</li> <li>Pandas DataFrame: For loading the entire training, validation or test set at once.</li> <li>Numpy array: For loading the entire training, validation or test set at once. </li> <li>See loading data for more details.</li> </ul> <p>The dataset is stored in a PyTables database. The internal <code>TimeBasedDataset</code>, <code>SplittedDataset</code>, <code>TimeBasedInitializerDataset</code> classes (used only when calling <code>set_dataset_config_and_initialize</code>) act as wrappers that implement the PyTorch <code>Dataset</code>  interface. These wrappers are compatible with PyTorch\u2019s <code>DataLoader</code>, providing efficient parallel data loading. </p> <p>The dataset configuration is done through the <code>DisjointTimeBasedConfig</code> class.       </p> <p>Intended usage:</p> <ol> <li>Create an instance of the dataset with the desired data root by calling <code>get_dataset</code>. This will download the dataset if it has not been previously downloaded and return instance of dataset.</li> <li>Create an instance of <code>DisjointTimeBasedConfig</code> and set it using <code>set_dataset_config_and_initialize</code>.     This initializes the dataset, including data splitting (train/validation/test), fitting transformers (if needed), selecting features, and more. This is cached for later use.</li> <li>Use <code>get_train_dataloader</code>/<code>get_train_df</code>/<code>get_train_numpy</code> to get training data for chosen model.</li> <li>Validate the model and perform the hyperparameter optimalization on <code>get_val_dataloader</code>/<code>get_val_df</code>/<code>get_val_numpy</code>.</li> <li>Evaluate the model on <code>get_test_dataloader</code>/<code>get_test_df</code>/<code>get_test_numpy</code>.  </li> </ol> <p>Alternatively you can use <code>load_benchmark</code></p> <ol> <li>Call <code>load_benchmark</code> with the desired benchmark. You can use your own saved benchmark or you can use already built-in one. This will download the dataset and annotations (if available) if they have not been previously downloaded.</li> <li>Retrieve the initialized dataset using <code>get_initialized_dataset</code>. This will provide a dataset that is ready to use.</li> <li>Use <code>get_train_dataloader</code>/<code>get_train_df</code>/<code>get_train_numpy</code> to get training data for chosen model.</li> <li>Validate the model and perform the hyperparameter optimalization on <code>get_val_dataloader</code>/<code>get_val_df</code>/<code>get_val_numpy</code>.</li> <li>Evaluate the model on <code>get_test_dataloader</code>/<code>get_test_df</code>/<code>get_test_numpy</code>.</li> </ol> Source code in <code>cesnet_tszoo\\datasets\\disjoint_time_based_cesnet_dataset.py</code> <pre><code>@dataclass\nclass DisjointTimeBasedCesnetDataset(CesnetDataset):\n    \"\"\"This class is used for disjoint-time-based returning of data. Can be created by using [`get_dataset`](reference_cesnet_database.md#cesnet_tszoo.datasets.databases.cesnet_database.CesnetDatabase.get_dataset) with parameter `dataset_type` = `DatasetType.DISJOINT_TIME_BASED`.\n\n    Disjoint-time-based means batch size affects number of returned times in one batch and each set can have different time series. Which time series are returned does not change. Additionally it supports sliding window.\n\n    The dataset provides multiple ways to access the data:\n\n    - **Iterable PyTorch DataLoader**: For batch processing.\n    - **Pandas DataFrame**: For loading the entire training, validation or test set at once.\n    - **Numpy array**: For loading the entire training, validation or test set at once. \n    - See [loading data][loading-data] for more details.\n\n    The dataset is stored in a [PyTables](https://www.pytables.org/) database. The internal `TimeBasedDataset`, `SplittedDataset`, `TimeBasedInitializerDataset` classes (used only when calling [`set_dataset_config_and_initialize`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.set_dataset_config_and_initialize)) act as wrappers that implement the PyTorch [`Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) \n    interface. These wrappers are compatible with PyTorch\u2019s [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader), providing efficient parallel data loading. \n\n    The dataset configuration is done through the [`DisjointTimeBasedConfig`](reference_disjoint_time_based_config.md#references.DisjointTimeBasedConfig) class.       \n\n    **Intended usage:**\n\n    1. Create an instance of the dataset with the desired data root by calling [`get_dataset`](reference_cesnet_database.md#cesnet_tszoo.datasets.databases.cesnet_database.CesnetDatabase.get_dataset). This will download the dataset if it has not been previously downloaded and return instance of dataset.\n    2. Create an instance of [`DisjointTimeBasedConfig`](reference_disjoint_time_based_config.md#references.DisjointTimeBasedConfig) and set it using [`set_dataset_config_and_initialize`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.set_dataset_config_and_initialize). \n       This initializes the dataset, including data splitting (train/validation/test), fitting transformers (if needed), selecting features, and more. This is cached for later use.\n    3. Use [`get_train_dataloader`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_train_dataloader)/[`get_train_df`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_train_df)/[`get_train_numpy`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_train_numpy) to get training data for chosen model.\n    4. Validate the model and perform the hyperparameter optimalization on [`get_val_dataloader`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_val_dataloader)/[`get_val_df`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_val_df)/[`get_val_numpy`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_val_numpy).\n    5. Evaluate the model on [`get_test_dataloader`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_test_dataloader)/[`get_test_df`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_test_df)/[`get_test_numpy`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_test_numpy).  \n\n    Alternatively you can use [`load_benchmark`][cesnet_tszoo.benchmarks.load_benchmark]\n\n    1. Call [`load_benchmark`][cesnet_tszoo.benchmarks.load_benchmark] with the desired benchmark. You can use your own saved benchmark or you can use already built-in one. This will download the dataset and annotations (if available) if they have not been previously downloaded.\n    2. Retrieve the initialized dataset using [`get_initialized_dataset`](reference_benchmarks.md#cesnet_tszoo.benchmarks.Benchmark.get_initialized_dataset). This will provide a dataset that is ready to use.\n    3. Use [`get_train_dataloader`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_train_dataloader)/[`get_train_df`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_train_df)/[`get_train_numpy`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_train_numpy) to get training data for chosen model.\n    4. Validate the model and perform the hyperparameter optimalization on [`get_val_dataloader`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_val_dataloader)/[`get_val_df`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_val_df)/[`get_val_numpy`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_val_numpy).\n    5. Evaluate the model on [`get_test_dataloader`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_test_dataloader)/[`get_test_df`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_test_df)/[`get_test_numpy`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_test_numpy).          \n    \"\"\"\n\n    dataset_config: Optional[DisjointTimeBasedConfig] = field(default=None, init=False)\n    \"\"\"Configuration of the dataset.\"\"\"\n\n    train_dataset: Optional[DisjointTimeBasedSplittedDataset] = field(default=None, init=False)\n    \"\"\"Training set as a `SplittedDataset` instance wrapping multiple `TimeBasedDataset` that wrap the PyTables database.\"\"\"\n    val_dataset: Optional[DisjointTimeBasedSplittedDataset] = field(default=None, init=False)\n    \"\"\"Validation set as a `SplittedDataset` instance wrapping multiple `TimeBasedDataset` that wrap the PyTables database.\"\"\"\n    test_dataset: Optional[DisjointTimeBasedSplittedDataset] = field(default=None, init=False)\n    \"\"\"Test set as a `SplittedDataset` instance wrapping multiple `TimeBasedDataset` that wrap the PyTables database. \"\"\"\n\n    train_dataloader: Optional[DisjointTimeBasedDataloader] = field(default=None, init=False)\n    \"\"\"Iterable PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for training set.\"\"\"\n    val_dataloader: Optional[DisjointTimeBasedDataloader] = field(default=None, init=False)\n    \"\"\"Iterable PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for validation set.\"\"\"\n    test_dataloader: Optional[DisjointTimeBasedDataloader] = field(default=None, init=False)\n    \"\"\"Iterable PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for test set. \"\"\"\n\n    dataloader_factory: DisjointTimeBasedDataloaderFactory = field(default=DisjointTimeBasedDataloaderFactory(), init=False)\n    \"\"\"Factory used to create DisjointTimeBasedDataloader.  \"\"\"\n\n    dataset_type: DatasetType = field(default=DatasetType.DISJOINT_TIME_BASED, init=False)\n\n    _export_config_copy: Optional[DisjointTimeBasedConfig] = field(default=None, init=False)\n\n    def __post_init__(self):\n        super().__post_init__()\n\n        self.logger.info(\"Dataset is disjoint_time_based. Use cesnet_tszoo.configs.DisjointTimeBasedConfig\")\n\n    def set_dataset_config_and_initialize(self, dataset_config: DisjointTimeBasedConfig, display_config_details: Optional[Literal[\"text\", \"diagram\"]] = \"text\", workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n        \"\"\"\n        Initialize training set, validation set, test set etc.. This method must be called before any data can be accessed. It is required for the final initialization of [`dataset_config`](reference_disjoint_time_based_config.md#references.DisjointTimeBasedConfig).\n\n        The following configuration attributes are used during initialization:\n\n        Dataset config | Description\n        -------------- | -----------\n        `init_workers` | Specifies the number of workers to use for initialization. Applied when `workers` = \"config\".\n        `partial_fit_initialized_transformers` | Determines whether initialized transformers should be partially fitted on the training data.\n        `nan_threshold` | Filters out time series with missing values exceeding the specified threshold.\n\n        Parameters:\n            dataset_config: Desired configuration of the dataset.\n            display_config_details: Flag indicating whether and how to display the configuration values after initialization. `Default: text`  \n            workers: The number of workers to use during initialization. `Default: \"config\"`  \n        \"\"\"\n\n        assert dataset_config is not None, \"Used dataset_config cannot be None.\"\n        assert isinstance(dataset_config, DisjointTimeBasedConfig), f\"This config is used for dataset of type '{dataset_config.dataset_type}'. Meanwhile this dataset is of type '{self.metadata.dataset_type}'.\"\n\n        super(DisjointTimeBasedCesnetDataset, self).set_dataset_config_and_initialize(dataset_config, display_config_details, workers)\n\n    def apply_transformer(self, transform_with: type | list[Transformer] | np.ndarray[Transformer] | TransformerType | Transformer | Literal[\"min_max_scaler\", \"standard_scaler\", \"max_abs_scaler\", \"log_transformer\", \"l2_normalizer\"] | None | Literal[\"config\"] = \"config\",\n                          partial_fit_initialized_transformers: bool | Literal[\"config\"] = \"config\", workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n        \"\"\"Used for updating transformer and relevant configurations set in config.\n        Set parameter to `config` to keep it as it is config.\n        If exception is thrown during set, no changes are made.\n\n        Affects following configuration:\n\n        Dataset config | Description\n        -------------- | -----------\n        `transform_with` | Defines the transformer to transform the dataset.\n        `partial_fit_initialized_transformers` | If `True`, partial fitting on train set is performed when using initialized transformers.\n\n        Parameters:\n            transform_with: Defines the transformer to transform the dataset. `Defaults: config`.  \n            partial_fit_initialized_transformers: If `True`, partial fitting on train set is performed when using initiliazed transformers. `Defaults: config`.  \n            workers: How many workers to use when setting new transformer. `Defaults: config`.      \n        \"\"\"\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating transformer values.\")\n\n        self.update_dataset_config_and_initialize(transform_with=transform_with, partial_fit_initialized_transformers=partial_fit_initialized_transformers, workers=workers)\n\n    def update_dataset_config_and_initialize(self,\n                                             default_values: list[Number] | npt.NDArray[np.number] | dict[str, Number] | Number | Literal[\"default\"] | None | Literal[\"config\"] = \"config\",\n                                             sliding_window_size: int | None | Literal[\"config\"] = \"config\",\n                                             sliding_window_prediction_size: int | None | Literal[\"config\"] = \"config\",\n                                             sliding_window_step: int | Literal[\"config\"] = \"config\",\n                                             set_shared_size: float | int | Literal[\"config\"] = \"config\",\n                                             train_batch_size: int | Literal[\"config\"] = \"config\",\n                                             val_batch_size: int | Literal[\"config\"] = \"config\",\n                                             test_batch_size: int | Literal[\"config\"] = \"config\",\n                                             preprocess_order: list[str, type] | Literal[\"config\"] = \"config\",\n                                             fill_missing_with: type | FillerType | Literal[\"mean_filler\", \"forward_filler\", \"linear_interpolation_filler\"] | None | Literal[\"config\"] = \"config\",\n                                             transform_with: type | list[Transformer] | np.ndarray[Transformer] | TransformerType | Transformer | Literal[\"min_max_scaler\", \"standard_scaler\", \"max_abs_scaler\", \"log_transformer\", \"l2_normalizer\"] | None | Literal[\"config\"] = \"config\",\n                                             handle_anomalies_with: type | AnomalyHandlerType | Literal[\"z-score\", \"interquartile_range\"] | None | Literal[\"config\"] = \"config\",\n                                             partial_fit_initialized_transformers: bool | Literal[\"config\"] = \"config\",\n                                             train_workers: int | Literal[\"config\"] = \"config\",\n                                             val_workers: int | Literal[\"config\"] = \"config\",\n                                             test_workers: int | Literal[\"config\"] = \"config\",\n                                             init_workers: int | Literal[\"config\"] = \"config\",\n                                             workers: int | Literal[\"config\"] = \"config\",\n                                             display_config_details: Optional[Literal[\"text\", \"diagram\"]] = None):\n        \"\"\"Used for updating selected configurations set in config.\n        Set parameter to `config` to keep it as it is config.\n        If exception is thrown during set, no changes are made.\n\n        Can affect following configuration:\n\n        Dataset config | Description\n        -------------- | -----------\n        `default_values` | Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature.\n        `sliding_window_size` | Number of times in one window. Impacts dataloader behavior. Refer to relevant config for details.\n        `sliding_window_prediction_size` | Number of times to predict from sliding_window_size. Refer to relevant config for details.\n        `sliding_window_step` | Number of times to move by after each window. Refer to relevant config for details.\n        `set_shared_size` | How much times should time periods share. Order of sharing is training set &lt; validation set &lt; test set. Refer to relevant config for details.\n        `train_batch_size` | Number of samples per batch for train set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n        `val_batch_size` | Number of samples per batch for val set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n        `test_batch_size` | Number of samples per batch for test set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n        `preprocess_order` | Used order of when preprocesses are applied. Can be also used to add/remove custom handlers.\n        `fill_missing_with` | Defines how to fill missing values in the dataset.\n        `transform_with` | Defines the transformer to transform the dataset.\n        `handle_anomalies_with` | Defines the anomaly handler to handle anomalies in the train set.\n        `partial_fit_initialized_transformers` | If `True`, partial fitting on train set is performed when using initialized transformers.\n        `train_workers` | Number of workers for loading training data.\n        `val_workers` | Number of workers for loading validation data.\n        `test_workers` | Number of workers for loading test data.\n        `init_workers` | Number of workers for dataset configuration.\n\n\n        Parameters:\n            default_values: Default values for missing data, applied before fillers. `Defaults: config`.  \n            sliding_window_size: Number of times in one window. `Defaults: config`.\n            sliding_window_prediction_size: Number of times to predict from sliding_window_size. `Defaults: config`.\n            sliding_window_step: Number of times to move by after each window. `Defaults: config`.\n            set_shared_size: How much times should time periods share. `Defaults: config`.            \n            train_batch_size: Number of samples per batch for train set. `Defaults: config`.\n            val_batch_size: Number of samples per batch for val set. `Defaults: config`.\n            test_batch_size: Number of samples per batch for test set. `Defaults: config`. \n            preprocess_order: Used order of when preprocesses are applied. Can be also used to add/remove custom handlers. `Defaults: config`.                  \n            fill_missing_with: Defines how to fill missing values in the dataset. `Defaults: config`. \n            transform_with: Defines the transformer to transform the dataset. `Defaults: config`. \n            handle_anomalies_with: Defines the anomaly handler to handle anomalies in the train set. `Defaults: config`. \n            partial_fit_initialized_transformers: If `True`, partial fitting on train set is performed when using initiliazed transformers. `Defaults: config`.    \n            train_workers: Number of workers for loading training data. `Defaults: config`.\n            val_workers: Number of workers for loading validation data. `Defaults: config`.\n            test_workers: Number of workers for loading test data. `Defaults: config`.\n            init_workers: Number of workers for dataset configuration. `Defaults: config`.                          \n            workers: How many workers to use when updating configuration. `Defaults: config`.  \n            display_config_details: Whether config details should be displayed after configuration. `Defaults: False`. \n        \"\"\"\n\n        config_editor = DisjointTimeBasedConfigEditor(self._export_config_copy,\n                                                      default_values,\n                                                      train_batch_size,\n                                                      val_batch_size,\n                                                      test_batch_size,\n                                                      preprocess_order,\n                                                      fill_missing_with,\n                                                      transform_with,\n                                                      handle_anomalies_with,\n                                                      \"config\",\n                                                      partial_fit_initialized_transformers,\n                                                      train_workers,\n                                                      val_workers,\n                                                      test_workers,\n                                                      init_workers,\n                                                      sliding_window_size,\n                                                      sliding_window_prediction_size,\n                                                      sliding_window_step,\n                                                      set_shared_size\n                                                      )\n\n        self._update_dataset_config_and_initialize(config_editor, workers, display_config_details)\n\n    def get_data_about_set(self, about: SplitType | Literal[\"train\", \"val\", \"test\"]) -&gt; dict:\n        \"\"\"\n        Retrieve data related to the specified set.\n\n        Parameters:\n            about: Specifies the set to retrieve data about.\n\n        Returned dictionary contains:\n\n        - **ts_ids:** Ids of time series in `about` set.\n        - **TimeFormat.ID_TIME:** Times in `about` set, where time format is `TimeFormat.ID_TIME`.\n        - **TimeFormat.DATETIME:** Times in `about` set, where time format is `TimeFormat.DATETIME`.\n        - **TimeFormat.UNIX_TIME:** Times in `about` set, where time format is `TimeFormat.UNIX_TIME`.\n        - **TimeFormat.SHIFTED_UNIX_TIME:** Times in `about` set, where time format is `TimeFormat.SHIFTED_UNIX_TIME`.\n\n        Returns:\n            Returns dictionary with details about set.\n        \"\"\"\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before getting data about set.\")\n\n        about = SplitType(about)\n\n        time_period = None\n        time_series = None\n\n        result = {}\n\n        if about == SplitType.TRAIN:\n            if not self.dataset_config.has_train():\n                raise ValueError(\"Train split is not used.\")\n            time_period = self.dataset_config.train_time_period\n            time_series = self.dataset_config.train_ts\n        elif about == SplitType.VAL:\n            if not self.dataset_config.has_val():\n                raise ValueError(\"Val split is not used.\")\n            time_period = self.dataset_config.val_time_period\n            time_series = self.dataset_config.val_ts\n        elif about == SplitType.TEST:\n            if not self.dataset_config.has_test():\n                raise ValueError(\"Test split is not used.\")\n            time_period = self.dataset_config.test_time_period\n            time_series = self.dataset_config.test_ts\n        else:\n            raise ValueError(\"Specified about parameter is not supported.\")\n\n        datetime_temp = np.array([datetime.fromtimestamp(time, timezone.utc) for time in self.metadata.time_indices[TIME_COLUMN_NAME][time_period[ID_TIME_COLUMN_NAME]]])\n\n        result[\"ts_ids\"] = time_series.copy()\n        result[TimeFormat.ID_TIME] = time_period[ID_TIME_COLUMN_NAME].copy()\n        result[TimeFormat.DATETIME] = datetime_temp.copy()\n        result[TimeFormat.UNIX_TIME] = self.metadata.time_indices[TIME_COLUMN_NAME][time_period[ID_TIME_COLUMN_NAME]].copy()\n        result[TimeFormat.SHIFTED_UNIX_TIME] = self.metadata.time_indices[TIME_COLUMN_NAME][time_period[ID_TIME_COLUMN_NAME]] - self.metadata.time_indices[TIME_COLUMN_NAME][0]\n\n        return result\n\n    def set_sliding_window(self, sliding_window_size: int | None | Literal[\"config\"] = \"config\", sliding_window_prediction_size: int | None | Literal[\"config\"] = \"config\",\n                           sliding_window_step: int | None | Literal[\"config\"] = \"config\", set_shared_size: float | int | Literal[\"config\"] = \"config\", workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n        \"\"\"Used for updating sliding window related values set in config.\n        Set parameter to `config` to keep it as it is config.\n        If exception is thrown during set, no changes are made.\n\n        Affects following configuration: \n\n        Dataset config | Description\n        -------------- | -----------\n        `sliding_window_size` | Number of times in one window. Impacts dataloader behavior. Refer to relevant config for details.\n        `sliding_window_prediction_size` | Number of times to predict from sliding_window_size. Refer to relevant config for details.\n        `sliding_window_step` | Number of times to move by after each window. Refer to relevant config for details.\n        `set_shared_size` | How much times should time periods share. Order of sharing is training set &lt; validation set &lt; test set. Refer to relevant config for details.\n\n        Parameters:\n            sliding_window_size: Number of times in one window. `Defaults: config`.\n            sliding_window_prediction_size: Number of times to predict from sliding_window_size. `Defaults: config`.\n            sliding_window_step: Number of times to move by after each window. `Defaults: config`.\n            set_shared_size: How much times should time periods share. `Defaults: config`.\n            workers: How many workers to use when setting new sliding window values. `Defaults: config`.  \n        \"\"\"\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating sliding window values.\")\n\n        self.update_dataset_config_and_initialize(sliding_window_size=sliding_window_size, sliding_window_prediction_size=sliding_window_prediction_size, sliding_window_step=sliding_window_step, set_shared_size=set_shared_size, workers=workers)\n        self.logger.info(\"Sliding window values has been changed successfuly.\")\n\n    def set_batch_sizes(self, train_batch_size: int | Literal[\"config\"] = \"config\", val_batch_size: int | Literal[\"config\"] = \"config\", test_batch_size: int | Literal[\"config\"] = \"config\") -&gt; None:\n        \"\"\"Used for updating batch sizes set in config.\n        Set parameter to `config` to keep it as it is config.\n        If exception is thrown during set, no changes are made.\n\n        Affects following configuration: \n\n        Dataset config | Description\n        -------------- | -----------\n        `train_batch_size` | Number of samples per batch for train set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n        `val_batch_size` | Number of samples per batch for val set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n        `test_batch_size` | Number of samples per batch for test set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n\n        Parameters:\n            train_batch_size: Number of samples per batch for train set. `Defaults: config`.\n            val_batch_size: Number of samples per batch for val set. `Defaults: config`.\n            test_batch_size: Number of samples per batch for test set. `Defaults: config`.\n        \"\"\"\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating batch sizes.\")\n\n        self.update_dataset_config_and_initialize(train_batch_size=train_batch_size, val_batch_size=val_batch_size, test_batch_size=test_batch_size, workers=\"config\")\n        self.logger.info(\"Batch sizes has been changed successfuly.\")\n\n    def set_workers(self, train_workers: int | Literal[\"config\"] = \"config\", val_workers: int | Literal[\"config\"] = \"config\",\n                    test_workers: int | Literal[\"config\"] = \"config\", init_workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n        \"\"\"Used for updating workers set in config.\n        Set parameter to `config` to keep it as it is config.\n        If exception is thrown during set, no changes are made.\n\n        Affects following configuration: \n\n        Dataset config | Description\n        -------------- | -----------\n        `train_workers` | Number of workers for loading training data.\n        `val_workers` | Number of workers for loading validation data.\n        `test_workers` | Number of workers for loading test data.\n        `init_workers` | Number of workers for dataset configuration.\n\n        Parameters:\n            train_workers: Number of workers for loading training data. `Defaults: config`.\n            val_workers: Number of workers for loading validation data. `Defaults: config`.\n            test_workers: Number of workers for loading test data. `Defaults: config`.\n            init_workers: Number of workers for dataset configuration. `Defaults: config`.            \n        \"\"\"\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating workers.\")\n\n        self.update_dataset_config_and_initialize(train_workers=train_workers, val_workers=val_workers, test_workers=test_workers, init_workers=init_workers, workers=\"config\")\n        self.logger.info(\"Workers has been changed successfuly.\")\n\n    def _initialize_datasets(self) -&gt; None:\n        \"\"\"Called in [`set_dataset_config_and_initialize`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.set_dataset_config_and_initialize), this method initializes the set datasets (train, validation, test and all). \"\"\"\n\n        if self.dataset_config.has_train():\n            load_config = DisjointTimeLoadConfig(self.dataset_config, SplitType.TRAIN)\n            self.train_dataset = DisjointTimeBasedSplittedDataset(self.metadata.dataset_path, self.metadata.data_table_path, load_config, self.dataset_config.train_workers)\n\n            self.logger.debug(\"train_dataset initiliazed.\")\n\n        if self.dataset_config.has_val():\n            load_config = DisjointTimeLoadConfig(self.dataset_config, SplitType.VAL)\n            self.val_dataset = DisjointTimeBasedSplittedDataset(self.metadata.dataset_path, self.metadata.data_table_path, load_config, self.dataset_config.val_workers)\n\n            self.logger.debug(\"val_dataset initiliazed.\")\n\n        if self.dataset_config.has_test():\n            load_config = DisjointTimeLoadConfig(self.dataset_config, SplitType.TEST)\n            self.test_dataset = DisjointTimeBasedSplittedDataset(self.metadata.dataset_path, self.metadata.data_table_path, load_config, self.dataset_config.test_workers)\n            self.logger.debug(\"test_dataset initiliazed.\")\n\n    def _initialize_transformers_and_details(self, workers: int) -&gt; None:\n        \"\"\"\n        Called in [`set_dataset_config_and_initialize`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.set_dataset_config_and_initialize). \n\n        Goes through data to validate time series against `nan_threshold`, fit/partial fit `transformers`, fit `anomaly handlers` and prepare `fillers`.\n        \"\"\"\n\n        if self.dataset_config.has_train():\n\n            self.__initialize_config_for_train_set(workers)\n\n            self.logger.debug(\"Train set updated: %s time series left.\", len(self.dataset_config.train_ts))\n\n        if self.dataset_config.has_val():\n            init_config = DisjointTimeDatasetInitConfig(self.dataset_config, SplitType.VAL, PreprocessOrderGroup([]))\n\n            ts_ids_to_take = self.__initialize_config_for_non_fit_sets(init_config, workers, \"val\")\n            self.dataset_config.val_ts = self.dataset_config.val_ts[ts_ids_to_take]\n            self.dataset_config.val_ts_row_ranges = self.dataset_config.val_ts_row_ranges[ts_ids_to_take]\n            self.dataset_config._update_preprocess_order_supported_ids(self.dataset_config.val_preprocess_order, ts_ids_to_take)\n\n            self.logger.debug(\"Val set updated: %s time series left.\", len(self.dataset_config.val_ts))\n\n        if self.dataset_config.has_test():\n            init_config = DisjointTimeDatasetInitConfig(self.dataset_config, SplitType.TEST, PreprocessOrderGroup([]))\n\n            ts_ids_to_take = self.__initialize_config_for_non_fit_sets(init_config, workers, \"test\")\n            self.dataset_config.test_ts = self.dataset_config.test_ts[ts_ids_to_take]\n            self.dataset_config.test_ts_row_ranges = self.dataset_config.test_ts_row_ranges[ts_ids_to_take]\n            self.dataset_config._update_preprocess_order_supported_ids(self.dataset_config.test_preprocess_order, ts_ids_to_take)\n\n            self.logger.debug(\"Test set updated: %s time series left.\", len(self.dataset_config.test_ts))\n\n        self.logger.info(\"Dataset initialization complete. Configuration updated.\")\n\n    def __initialize_config_for_train_set(self, workers: int) -&gt; None:\n        \"\"\"Initializes config for provided time series. \"\"\"\n\n        self.logger.info(\"Updating config for train set and fitting values.\")\n\n        is_first_cycle = True\n\n        ts_ids_to_take = []\n\n        groups = self.dataset_config._get_train_preprocess_init_order_groups()\n        for i, group in enumerate(groups):\n            ts_ids_to_take = []\n            self.logger.info(\"Starting fitting cycle %s/%s.\", i + 1, len(groups))\n\n            init_config = DisjointTimeDatasetInitConfig(self.dataset_config, SplitType.TRAIN, group)\n            init_dataset = DisjointTimeBasedInitializerDataset(self.metadata.dataset_path, self.metadata.data_table_path, init_config)\n\n            sampler = SequentialSampler(init_dataset)\n            dataloader = DataLoader(init_dataset, num_workers=workers, collate_fn=self._collate_fn, worker_init_fn=DisjointTimeBasedInitializerDataset.worker_init_fn, persistent_workers=False, sampler=sampler)\n\n            if workers == 0:\n                init_dataset.pytables_worker_init()\n\n            for ts_id, data in enumerate(tqdm(dataloader, total=len(init_config.ts_row_ranges))):\n\n                init_dataset_return: InitDatasetReturn = data[0]\n\n                if init_dataset_return.is_under_nan_threshold:\n                    ts_ids_to_take.append(ts_id)\n\n                    # updates inner preprocessors passed from InitDataset\n                    fitted_inner_index = 0\n                    for inner_preprocess_order in group.preprocess_inner_orders:\n                        if inner_preprocess_order.should_be_fitted:\n                            inner_preprocess_order.holder.update_instance(init_dataset_return.preprocess_fitted_instances[fitted_inner_index].instance, ts_id)\n                            fitted_inner_index += 1\n\n                    # updates outer preprocessors based on passed train data from InitDataset\n                    for outer_preprocess_order in group.preprocess_outer_orders:\n                        if outer_preprocess_order.should_be_fitted:\n                            outer_preprocess_order.holder.fit(init_dataset_return.train_data, ts_id)\n\n                        if outer_preprocess_order.can_be_applied:\n                            init_dataset_return.train_data = outer_preprocess_order.holder.apply(init_dataset_return.train_data, ts_id)\n\n            if workers == 0:\n                init_dataset.cleanup()\n\n            # Update config based on filtered time series\n            if is_first_cycle:\n\n                if len(ts_ids_to_take) == 0:\n                    raise ValueError(\"No valid time series left in train set after applying nan_threshold.\")\n\n                self.dataset_config.train_ts_row_ranges = self.dataset_config.train_ts_row_ranges[ts_ids_to_take]\n                self.dataset_config.train_ts = self.dataset_config.train_ts[ts_ids_to_take]\n                self.dataset_config._update_preprocess_order_supported_ids(self.dataset_config.train_preprocess_order, ts_ids_to_take)\n                self.logger.debug(\"invalid ts_ids removed: %s time series left.\", len(ts_ids_to_take))\n\n                is_first_cycle = False\n\n    def __initialize_config_for_non_fit_sets(self, init_config: DisjointTimeDatasetInitConfig, workers: int, set_name: str) -&gt; np.ndarray:\n        \"\"\"Initializes config for provided time series without fitting. \"\"\"\n        init_dataset = DisjointTimeBasedInitializerDataset(self.metadata.dataset_path, self.metadata.data_table_path, init_config)\n\n        sampler = SequentialSampler(init_dataset)\n        dataloader = DataLoader(init_dataset, num_workers=workers, collate_fn=self._collate_fn, worker_init_fn=DisjointTimeBasedInitializerDataset.worker_init_fn, persistent_workers=False, sampler=sampler)\n\n        if workers == 0:\n            init_dataset.pytables_worker_init()\n\n        ts_ids_to_take = []\n\n        self.logger.info(\"Updating config for %s set.\", set_name)\n        for i, data in enumerate(tqdm(dataloader, total=len(init_config.ts_row_ranges))):\n            init_dataset_return: InitDatasetReturn = data[0]\n\n            if init_dataset_return.is_under_nan_threshold:\n                ts_ids_to_take.append(i)\n\n        if workers == 0:\n            init_dataset.cleanup()\n\n        if len(ts_ids_to_take) == 0:\n            raise ValueError(f\"No valid time series left in {set_name} set after applying nan_threshold.\")\n\n        return ts_ids_to_take\n\n    def _update_export_config_copy(self) -&gt; None:\n        \"\"\"\n        Called at the end of [`set_dataset_config_and_initialize`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.set_dataset_config_and_initialize) or when changing config values. \n\n        Updates values of config used for saving config.\n        \"\"\"\n        self._export_config_copy.database_name = self.metadata.database_name\n\n        self._export_config_copy.train_ts = self.dataset_config.train_ts.copy() if self.dataset_config.has_train() else None\n        self._export_config_copy.val_ts = self.dataset_config.val_ts.copy() if self.dataset_config.has_val() else None\n        self._export_config_copy.test_ts = self.dataset_config.test_ts.copy() if self.dataset_config.has_test() else None\n\n        self._export_config_copy.sliding_window_size = self.dataset_config.sliding_window_size\n        self._export_config_copy.sliding_window_prediction_size = self.dataset_config.sliding_window_prediction_size\n        self._export_config_copy.sliding_window_step = self.dataset_config.sliding_window_step\n        self._export_config_copy.set_shared_size = self.dataset_config.set_shared_size\n\n        super(DisjointTimeBasedCesnetDataset, self)._update_export_config_copy()\n\n    def _get_singular_time_series_dataset(self, parent_dataset: DisjointTimeBasedSplittedDataset, ts_id: int) -&gt; DisjointTimeBasedSplittedDataset:\n        \"\"\"Returns dataset for single time series \"\"\"\n\n        temp = np.where(np.isin(parent_dataset.load_config.ts_row_ranges[self.metadata.ts_id_name], [ts_id]))[0]\n\n        if len(temp) == 0:\n            raise ValueError(f\"ts_id {ts_id} was not found in valid time series for this set. Available time series are: {parent_dataset.load_config.ts_row_ranges[self.metadata.ts_id_name]}\")\n\n        time_series_position = temp[0]\n\n        split_load_config = parent_dataset.load_config.create_split_copy(slice(time_series_position, time_series_position + 1))\n\n        dataset = DisjointTimeBasedSplittedDataset(self.metadata.dataset_path, self.metadata.data_table_path, split_load_config, 0)\n        self.logger.debug(\"Singular time series dataset initiliazed.\")\n\n        return dataset\n\n    def _get_data_for_plot(self, ts_id: int, feature_indices: np.ndarray[int], time_format: TimeFormat) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"Dataset type specific retrieval of data. \"\"\"\n\n        train_id_result, val_id_result, test_id_result = None, None, None\n\n        if (self.dataset_config.has_train()):\n            train_id_result = np.argwhere(np.isin(self.dataset_config.train_ts, ts_id)).ravel()\n        if (self.dataset_config.has_val()):\n            val_id_result = np.argwhere(np.isin(self.dataset_config.val_ts, ts_id)).ravel()\n        if (self.dataset_config.has_test()):\n            test_id_result = np.argwhere(np.isin(self.dataset_config.test_ts, ts_id)).ravel()\n\n        data = None\n        time_period = None\n\n        if self.dataset_config.has_train() and len(train_id_result) &gt; 0:\n            data = self.__get_ts_data_for_plot(self.train_dataset, ts_id, feature_indices)\n            time_period = self.get_data_about_set(SplitType.TRAIN)[time_format]\n            self.logger.debug(\"Valid ts_id found: %d\", train_id_result[0])\n\n        elif self.dataset_config.has_val() and len(val_id_result) &gt; 0:\n            data = self.__get_ts_data_for_plot(self.val_dataset, ts_id, feature_indices)\n            time_period = self.get_data_about_set(SplitType.VAL)[time_format]\n            self.logger.debug(\"Valid ts_id found: %d\", val_id_result[0])\n\n        elif self.dataset_config.has_test() and len(test_id_result) &gt; 0:\n            data = self.__get_ts_data_for_plot(self.test_dataset, ts_id, feature_indices)\n            time_period = self.get_data_about_set(SplitType.TEST)[time_format]\n            self.logger.debug(\"Valid ts_id found: %d\", test_id_result[0])\n        else:\n            raise ValueError(f\"Invalid ts_id '{ts_id}'. The provided ts_id is not found in the available time series IDs.\", self.dataset_config.train_ts, self.dataset_config.val_ts, self.dataset_config.test_ts)\n\n        return data, time_period\n\n    def __get_ts_data_for_plot(self, dataset: DisjointTimeBasedSplittedDataset, ts_id: int, feature_indices: list[int]):\n        dataset = self._get_singular_time_series_dataset(dataset, ts_id)\n\n        dataloader = self.dataloader_factory.create_dataloader(dataset, self.dataset_config, 0, True, None)\n\n        temp_data = dataset_loaders.create_numpy_from_dataloader(dataloader, np.array([ts_id]), dataset.load_config.time_format, dataset.load_config.include_time, DatasetType.TIME_BASED, True)\n\n        if (dataset.load_config.time_format == TimeFormat.DATETIME and dataset.load_config.include_time):\n            temp_data = temp_data[0]\n\n        temp_data = temp_data[0][:, feature_indices]\n\n        return temp_data\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.dataset_config","title":"dataset_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dataset_config: Optional[DisjointTimeBasedConfig] = field(default=None, init=False)\n</code></pre> <p>Configuration of the dataset.</p>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.train_dataset","title":"train_dataset  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>train_dataset: Optional[DisjointTimeBasedSplittedDataset] = field(default=None, init=False)\n</code></pre> <p>Training set as a <code>SplittedDataset</code> instance wrapping multiple <code>TimeBasedDataset</code> that wrap the PyTables database.</p>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.val_dataset","title":"val_dataset  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>val_dataset: Optional[DisjointTimeBasedSplittedDataset] = field(default=None, init=False)\n</code></pre> <p>Validation set as a <code>SplittedDataset</code> instance wrapping multiple <code>TimeBasedDataset</code> that wrap the PyTables database.</p>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.test_dataset","title":"test_dataset  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>test_dataset: Optional[DisjointTimeBasedSplittedDataset] = field(default=None, init=False)\n</code></pre> <p>Test set as a <code>SplittedDataset</code> instance wrapping multiple <code>TimeBasedDataset</code> that wrap the PyTables database.</p>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.train_dataloader","title":"train_dataloader  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>train_dataloader: Optional[DisjointTimeBasedDataloader] = field(default=None, init=False)\n</code></pre> <p>Iterable PyTorch <code>DataLoader</code> for training set.</p>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.val_dataloader","title":"val_dataloader  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>val_dataloader: Optional[DisjointTimeBasedDataloader] = field(default=None, init=False)\n</code></pre> <p>Iterable PyTorch <code>DataLoader</code> for validation set.</p>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.test_dataloader","title":"test_dataloader  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>test_dataloader: Optional[DisjointTimeBasedDataloader] = field(default=None, init=False)\n</code></pre> <p>Iterable PyTorch <code>DataLoader</code> for test set.</p>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.dataloader_factory","title":"dataloader_factory  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dataloader_factory: DisjointTimeBasedDataloaderFactory = field(default=DisjointTimeBasedDataloaderFactory(), init=False)\n</code></pre> <p>Factory used to create DisjointTimeBasedDataloader.</p>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.dataset_type","title":"dataset_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dataset_type: DatasetType = field(default=DISJOINT_TIME_BASED, init=False)\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset._export_config_copy","title":"_export_config_copy  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>_export_config_copy: Optional[DisjointTimeBasedConfig] = field(default=None, init=False)\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata: DatasetMetadata\n</code></pre> <p>Holds various metadata used in dataset for its creation, loading data and similar.</p>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.all_dataset","title":"all_dataset  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>all_dataset: Optional[Dataset] = field(default=None, init=False)\n</code></pre> <p>All set as a <code>BaseDataset</code> instance wrapping the PyTables database.</p>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.all_dataloader","title":"all_dataloader  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>all_dataloader: Optional[DataLoader] = field(default=None, init=False)\n</code></pre> <p>Iterable PyTorch <code>DataLoader</code> for all set.</p>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.related_to","title":"related_to  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>related_to: Optional[str] = field(default=None, init=False)\n</code></pre> <p>Name of file with relevant results to used benchmark.</p>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset._collate_fn","title":"_collate_fn  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>_collate_fn: Optional[Callable] = field(default=None, init=False)\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.__init__","title":"__init__","text":"<pre><code>__init__(metadata: DatasetMetadata) -&gt; None\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> Source code in <code>cesnet_tszoo\\datasets\\disjoint_time_based_cesnet_dataset.py</code> <pre><code>def __post_init__(self):\n    super().__post_init__()\n\n    self.logger.info(\"Dataset is disjoint_time_based. Use cesnet_tszoo.configs.DisjointTimeBasedConfig\")\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.set_dataset_config_and_initialize","title":"set_dataset_config_and_initialize","text":"<pre><code>set_dataset_config_and_initialize(dataset_config: DisjointTimeBasedConfig, display_config_details: Optional[Literal['text', 'diagram']] = 'text', workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Initialize training set, validation set, test set etc.. This method must be called before any data can be accessed. It is required for the final initialization of <code>dataset_config</code>.</p> <p>The following configuration attributes are used during initialization:</p> Dataset config Description <code>init_workers</code> Specifies the number of workers to use for initialization. Applied when <code>workers</code> = \"config\". <code>partial_fit_initialized_transformers</code> Determines whether initialized transformers should be partially fitted on the training data. <code>nan_threshold</code> Filters out time series with missing values exceeding the specified threshold. <p>Parameters:</p> Name Type Description Default <code>dataset_config</code> <code>DisjointTimeBasedConfig</code> <p>Desired configuration of the dataset.</p> required <code>display_config_details</code> <code>Optional[Literal['text', 'diagram']]</code> <p>Flag indicating whether and how to display the configuration values after initialization. <code>Default: text</code> </p> <code>'text'</code> <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use during initialization. <code>Default: \"config\"</code></p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\disjoint_time_based_cesnet_dataset.py</code> <pre><code>def set_dataset_config_and_initialize(self, dataset_config: DisjointTimeBasedConfig, display_config_details: Optional[Literal[\"text\", \"diagram\"]] = \"text\", workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"\n    Initialize training set, validation set, test set etc.. This method must be called before any data can be accessed. It is required for the final initialization of [`dataset_config`](reference_disjoint_time_based_config.md#references.DisjointTimeBasedConfig).\n\n    The following configuration attributes are used during initialization:\n\n    Dataset config | Description\n    -------------- | -----------\n    `init_workers` | Specifies the number of workers to use for initialization. Applied when `workers` = \"config\".\n    `partial_fit_initialized_transformers` | Determines whether initialized transformers should be partially fitted on the training data.\n    `nan_threshold` | Filters out time series with missing values exceeding the specified threshold.\n\n    Parameters:\n        dataset_config: Desired configuration of the dataset.\n        display_config_details: Flag indicating whether and how to display the configuration values after initialization. `Default: text`  \n        workers: The number of workers to use during initialization. `Default: \"config\"`  \n    \"\"\"\n\n    assert dataset_config is not None, \"Used dataset_config cannot be None.\"\n    assert isinstance(dataset_config, DisjointTimeBasedConfig), f\"This config is used for dataset of type '{dataset_config.dataset_type}'. Meanwhile this dataset is of type '{self.metadata.dataset_type}'.\"\n\n    super(DisjointTimeBasedCesnetDataset, self).set_dataset_config_and_initialize(dataset_config, display_config_details, workers)\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.apply_transformer","title":"apply_transformer","text":"<pre><code>apply_transformer(transform_with: type | list[Transformer] | ndarray[Transformer] | TransformerType | Transformer | Literal['min_max_scaler', 'standard_scaler', 'max_abs_scaler', 'log_transformer', 'l2_normalizer'] | None | Literal['config'] = 'config', partial_fit_initialized_transformers: bool | Literal['config'] = 'config', workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Used for updating transformer and relevant configurations set in config. Set parameter to <code>config</code> to keep it as it is config. If exception is thrown during set, no changes are made.</p> <p>Affects following configuration:</p> Dataset config Description <code>transform_with</code> Defines the transformer to transform the dataset. <code>partial_fit_initialized_transformers</code> If <code>True</code>, partial fitting on train set is performed when using initialized transformers. <p>Parameters:</p> Name Type Description Default <code>transform_with</code> <code>type | list[Transformer] | ndarray[Transformer] | TransformerType | Transformer | Literal['min_max_scaler', 'standard_scaler', 'max_abs_scaler', 'log_transformer', 'l2_normalizer'] | None | Literal['config']</code> <p>Defines the transformer to transform the dataset. <code>Defaults: config</code>.  </p> <code>'config'</code> <code>partial_fit_initialized_transformers</code> <code>bool | Literal['config']</code> <p>If <code>True</code>, partial fitting on train set is performed when using initiliazed transformers. <code>Defaults: config</code>.  </p> <code>'config'</code> <code>workers</code> <code>int | Literal['config']</code> <p>How many workers to use when setting new transformer. <code>Defaults: config</code>.</p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\disjoint_time_based_cesnet_dataset.py</code> <pre><code>def apply_transformer(self, transform_with: type | list[Transformer] | np.ndarray[Transformer] | TransformerType | Transformer | Literal[\"min_max_scaler\", \"standard_scaler\", \"max_abs_scaler\", \"log_transformer\", \"l2_normalizer\"] | None | Literal[\"config\"] = \"config\",\n                      partial_fit_initialized_transformers: bool | Literal[\"config\"] = \"config\", workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"Used for updating transformer and relevant configurations set in config.\n    Set parameter to `config` to keep it as it is config.\n    If exception is thrown during set, no changes are made.\n\n    Affects following configuration:\n\n    Dataset config | Description\n    -------------- | -----------\n    `transform_with` | Defines the transformer to transform the dataset.\n    `partial_fit_initialized_transformers` | If `True`, partial fitting on train set is performed when using initialized transformers.\n\n    Parameters:\n        transform_with: Defines the transformer to transform the dataset. `Defaults: config`.  \n        partial_fit_initialized_transformers: If `True`, partial fitting on train set is performed when using initiliazed transformers. `Defaults: config`.  \n        workers: How many workers to use when setting new transformer. `Defaults: config`.      \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating transformer values.\")\n\n    self.update_dataset_config_and_initialize(transform_with=transform_with, partial_fit_initialized_transformers=partial_fit_initialized_transformers, workers=workers)\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.update_dataset_config_and_initialize","title":"update_dataset_config_and_initialize","text":"<pre><code>update_dataset_config_and_initialize(default_values: list[Number] | NDArray[number] | dict[str, Number] | Number | Literal['default'] | None | Literal['config'] = 'config', sliding_window_size: int | None | Literal['config'] = 'config', sliding_window_prediction_size: int | None | Literal['config'] = 'config', sliding_window_step: int | Literal['config'] = 'config', set_shared_size: float | int | Literal['config'] = 'config', train_batch_size: int | Literal['config'] = 'config', val_batch_size: int | Literal['config'] = 'config', test_batch_size: int | Literal['config'] = 'config', preprocess_order: list[str, type] | Literal['config'] = 'config', fill_missing_with: type | FillerType | Literal['mean_filler', 'forward_filler', 'linear_interpolation_filler'] | None | Literal['config'] = 'config', transform_with: type | list[Transformer] | ndarray[Transformer] | TransformerType | Transformer | Literal['min_max_scaler', 'standard_scaler', 'max_abs_scaler', 'log_transformer', 'l2_normalizer'] | None | Literal['config'] = 'config', handle_anomalies_with: type | AnomalyHandlerType | Literal['z-score', 'interquartile_range'] | None | Literal['config'] = 'config', partial_fit_initialized_transformers: bool | Literal['config'] = 'config', train_workers: int | Literal['config'] = 'config', val_workers: int | Literal['config'] = 'config', test_workers: int | Literal['config'] = 'config', init_workers: int | Literal['config'] = 'config', workers: int | Literal['config'] = 'config', display_config_details: Optional[Literal['text', 'diagram']] = None)\n</code></pre> <p>Used for updating selected configurations set in config. Set parameter to <code>config</code> to keep it as it is config. If exception is thrown during set, no changes are made.</p> <p>Can affect following configuration:</p> Dataset config Description <code>default_values</code> Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature. <code>sliding_window_size</code> Number of times in one window. Impacts dataloader behavior. Refer to relevant config for details. <code>sliding_window_prediction_size</code> Number of times to predict from sliding_window_size. Refer to relevant config for details. <code>sliding_window_step</code> Number of times to move by after each window. Refer to relevant config for details. <code>set_shared_size</code> How much times should time periods share. Order of sharing is training set &lt; validation set &lt; test set. Refer to relevant config for details. <code>train_batch_size</code> Number of samples per batch for train set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>val_batch_size</code> Number of samples per batch for val set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>test_batch_size</code> Number of samples per batch for test set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>preprocess_order</code> Used order of when preprocesses are applied. Can be also used to add/remove custom handlers. <code>fill_missing_with</code> Defines how to fill missing values in the dataset. <code>transform_with</code> Defines the transformer to transform the dataset. <code>handle_anomalies_with</code> Defines the anomaly handler to handle anomalies in the train set. <code>partial_fit_initialized_transformers</code> If <code>True</code>, partial fitting on train set is performed when using initialized transformers. <code>train_workers</code> Number of workers for loading training data. <code>val_workers</code> Number of workers for loading validation data. <code>test_workers</code> Number of workers for loading test data. <code>init_workers</code> Number of workers for dataset configuration. <p>Parameters:</p> Name Type Description Default <code>default_values</code> <code>list[Number] | NDArray[number] | dict[str, Number] | Number | Literal['default'] | None | Literal['config']</code> <p>Default values for missing data, applied before fillers. <code>Defaults: config</code>.  </p> <code>'config'</code> <code>sliding_window_size</code> <code>int | None | Literal['config']</code> <p>Number of times in one window. <code>Defaults: config</code>.</p> <code>'config'</code> <code>sliding_window_prediction_size</code> <code>int | None | Literal['config']</code> <p>Number of times to predict from sliding_window_size. <code>Defaults: config</code>.</p> <code>'config'</code> <code>sliding_window_step</code> <code>int | Literal['config']</code> <p>Number of times to move by after each window. <code>Defaults: config</code>.</p> <code>'config'</code> <code>set_shared_size</code> <code>float | int | Literal['config']</code> <p>How much times should time periods share. <code>Defaults: config</code>.            </p> <code>'config'</code> <code>train_batch_size</code> <code>int | Literal['config']</code> <p>Number of samples per batch for train set. <code>Defaults: config</code>.</p> <code>'config'</code> <code>val_batch_size</code> <code>int | Literal['config']</code> <p>Number of samples per batch for val set. <code>Defaults: config</code>.</p> <code>'config'</code> <code>test_batch_size</code> <code>int | Literal['config']</code> <p>Number of samples per batch for test set. <code>Defaults: config</code>. </p> <code>'config'</code> <code>preprocess_order</code> <code>list[str, type] | Literal['config']</code> <p>Used order of when preprocesses are applied. Can be also used to add/remove custom handlers. <code>Defaults: config</code>.                  </p> <code>'config'</code> <code>fill_missing_with</code> <code>type | FillerType | Literal['mean_filler', 'forward_filler', 'linear_interpolation_filler'] | None | Literal['config']</code> <p>Defines how to fill missing values in the dataset. <code>Defaults: config</code>. </p> <code>'config'</code> <code>transform_with</code> <code>type | list[Transformer] | ndarray[Transformer] | TransformerType | Transformer | Literal['min_max_scaler', 'standard_scaler', 'max_abs_scaler', 'log_transformer', 'l2_normalizer'] | None | Literal['config']</code> <p>Defines the transformer to transform the dataset. <code>Defaults: config</code>. </p> <code>'config'</code> <code>handle_anomalies_with</code> <code>type | AnomalyHandlerType | Literal['z-score', 'interquartile_range'] | None | Literal['config']</code> <p>Defines the anomaly handler to handle anomalies in the train set. <code>Defaults: config</code>. </p> <code>'config'</code> <code>partial_fit_initialized_transformers</code> <code>bool | Literal['config']</code> <p>If <code>True</code>, partial fitting on train set is performed when using initiliazed transformers. <code>Defaults: config</code>.    </p> <code>'config'</code> <code>train_workers</code> <code>int | Literal['config']</code> <p>Number of workers for loading training data. <code>Defaults: config</code>.</p> <code>'config'</code> <code>val_workers</code> <code>int | Literal['config']</code> <p>Number of workers for loading validation data. <code>Defaults: config</code>.</p> <code>'config'</code> <code>test_workers</code> <code>int | Literal['config']</code> <p>Number of workers for loading test data. <code>Defaults: config</code>.</p> <code>'config'</code> <code>init_workers</code> <code>int | Literal['config']</code> <p>Number of workers for dataset configuration. <code>Defaults: config</code>.                          </p> <code>'config'</code> <code>workers</code> <code>int | Literal['config']</code> <p>How many workers to use when updating configuration. <code>Defaults: config</code>.  </p> <code>'config'</code> <code>display_config_details</code> <code>Optional[Literal['text', 'diagram']]</code> <p>Whether config details should be displayed after configuration. <code>Defaults: False</code>.</p> <code>None</code> Source code in <code>cesnet_tszoo\\datasets\\disjoint_time_based_cesnet_dataset.py</code> <pre><code>def update_dataset_config_and_initialize(self,\n                                         default_values: list[Number] | npt.NDArray[np.number] | dict[str, Number] | Number | Literal[\"default\"] | None | Literal[\"config\"] = \"config\",\n                                         sliding_window_size: int | None | Literal[\"config\"] = \"config\",\n                                         sliding_window_prediction_size: int | None | Literal[\"config\"] = \"config\",\n                                         sliding_window_step: int | Literal[\"config\"] = \"config\",\n                                         set_shared_size: float | int | Literal[\"config\"] = \"config\",\n                                         train_batch_size: int | Literal[\"config\"] = \"config\",\n                                         val_batch_size: int | Literal[\"config\"] = \"config\",\n                                         test_batch_size: int | Literal[\"config\"] = \"config\",\n                                         preprocess_order: list[str, type] | Literal[\"config\"] = \"config\",\n                                         fill_missing_with: type | FillerType | Literal[\"mean_filler\", \"forward_filler\", \"linear_interpolation_filler\"] | None | Literal[\"config\"] = \"config\",\n                                         transform_with: type | list[Transformer] | np.ndarray[Transformer] | TransformerType | Transformer | Literal[\"min_max_scaler\", \"standard_scaler\", \"max_abs_scaler\", \"log_transformer\", \"l2_normalizer\"] | None | Literal[\"config\"] = \"config\",\n                                         handle_anomalies_with: type | AnomalyHandlerType | Literal[\"z-score\", \"interquartile_range\"] | None | Literal[\"config\"] = \"config\",\n                                         partial_fit_initialized_transformers: bool | Literal[\"config\"] = \"config\",\n                                         train_workers: int | Literal[\"config\"] = \"config\",\n                                         val_workers: int | Literal[\"config\"] = \"config\",\n                                         test_workers: int | Literal[\"config\"] = \"config\",\n                                         init_workers: int | Literal[\"config\"] = \"config\",\n                                         workers: int | Literal[\"config\"] = \"config\",\n                                         display_config_details: Optional[Literal[\"text\", \"diagram\"]] = None):\n    \"\"\"Used for updating selected configurations set in config.\n    Set parameter to `config` to keep it as it is config.\n    If exception is thrown during set, no changes are made.\n\n    Can affect following configuration:\n\n    Dataset config | Description\n    -------------- | -----------\n    `default_values` | Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature.\n    `sliding_window_size` | Number of times in one window. Impacts dataloader behavior. Refer to relevant config for details.\n    `sliding_window_prediction_size` | Number of times to predict from sliding_window_size. Refer to relevant config for details.\n    `sliding_window_step` | Number of times to move by after each window. Refer to relevant config for details.\n    `set_shared_size` | How much times should time periods share. Order of sharing is training set &lt; validation set &lt; test set. Refer to relevant config for details.\n    `train_batch_size` | Number of samples per batch for train set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `val_batch_size` | Number of samples per batch for val set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `test_batch_size` | Number of samples per batch for test set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `preprocess_order` | Used order of when preprocesses are applied. Can be also used to add/remove custom handlers.\n    `fill_missing_with` | Defines how to fill missing values in the dataset.\n    `transform_with` | Defines the transformer to transform the dataset.\n    `handle_anomalies_with` | Defines the anomaly handler to handle anomalies in the train set.\n    `partial_fit_initialized_transformers` | If `True`, partial fitting on train set is performed when using initialized transformers.\n    `train_workers` | Number of workers for loading training data.\n    `val_workers` | Number of workers for loading validation data.\n    `test_workers` | Number of workers for loading test data.\n    `init_workers` | Number of workers for dataset configuration.\n\n\n    Parameters:\n        default_values: Default values for missing data, applied before fillers. `Defaults: config`.  \n        sliding_window_size: Number of times in one window. `Defaults: config`.\n        sliding_window_prediction_size: Number of times to predict from sliding_window_size. `Defaults: config`.\n        sliding_window_step: Number of times to move by after each window. `Defaults: config`.\n        set_shared_size: How much times should time periods share. `Defaults: config`.            \n        train_batch_size: Number of samples per batch for train set. `Defaults: config`.\n        val_batch_size: Number of samples per batch for val set. `Defaults: config`.\n        test_batch_size: Number of samples per batch for test set. `Defaults: config`. \n        preprocess_order: Used order of when preprocesses are applied. Can be also used to add/remove custom handlers. `Defaults: config`.                  \n        fill_missing_with: Defines how to fill missing values in the dataset. `Defaults: config`. \n        transform_with: Defines the transformer to transform the dataset. `Defaults: config`. \n        handle_anomalies_with: Defines the anomaly handler to handle anomalies in the train set. `Defaults: config`. \n        partial_fit_initialized_transformers: If `True`, partial fitting on train set is performed when using initiliazed transformers. `Defaults: config`.    \n        train_workers: Number of workers for loading training data. `Defaults: config`.\n        val_workers: Number of workers for loading validation data. `Defaults: config`.\n        test_workers: Number of workers for loading test data. `Defaults: config`.\n        init_workers: Number of workers for dataset configuration. `Defaults: config`.                          \n        workers: How many workers to use when updating configuration. `Defaults: config`.  \n        display_config_details: Whether config details should be displayed after configuration. `Defaults: False`. \n    \"\"\"\n\n    config_editor = DisjointTimeBasedConfigEditor(self._export_config_copy,\n                                                  default_values,\n                                                  train_batch_size,\n                                                  val_batch_size,\n                                                  test_batch_size,\n                                                  preprocess_order,\n                                                  fill_missing_with,\n                                                  transform_with,\n                                                  handle_anomalies_with,\n                                                  \"config\",\n                                                  partial_fit_initialized_transformers,\n                                                  train_workers,\n                                                  val_workers,\n                                                  test_workers,\n                                                  init_workers,\n                                                  sliding_window_size,\n                                                  sliding_window_prediction_size,\n                                                  sliding_window_step,\n                                                  set_shared_size\n                                                  )\n\n    self._update_dataset_config_and_initialize(config_editor, workers, display_config_details)\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_data_about_set","title":"get_data_about_set","text":"<pre><code>get_data_about_set(about: SplitType | Literal['train', 'val', 'test']) -&gt; dict\n</code></pre> <p>Retrieve data related to the specified set.</p> <p>Parameters:</p> Name Type Description Default <code>about</code> <code>SplitType | Literal['train', 'val', 'test']</code> <p>Specifies the set to retrieve data about.</p> required <p>Returned dictionary contains:</p> <ul> <li>ts_ids: Ids of time series in <code>about</code> set.</li> <li>TimeFormat.ID_TIME: Times in <code>about</code> set, where time format is <code>TimeFormat.ID_TIME</code>.</li> <li>TimeFormat.DATETIME: Times in <code>about</code> set, where time format is <code>TimeFormat.DATETIME</code>.</li> <li>TimeFormat.UNIX_TIME: Times in <code>about</code> set, where time format is <code>TimeFormat.UNIX_TIME</code>.</li> <li>TimeFormat.SHIFTED_UNIX_TIME: Times in <code>about</code> set, where time format is <code>TimeFormat.SHIFTED_UNIX_TIME</code>.</li> </ul> <p>Returns:</p> Type Description <code>dict</code> <p>Returns dictionary with details about set.</p> Source code in <code>cesnet_tszoo\\datasets\\disjoint_time_based_cesnet_dataset.py</code> <pre><code>def get_data_about_set(self, about: SplitType | Literal[\"train\", \"val\", \"test\"]) -&gt; dict:\n    \"\"\"\n    Retrieve data related to the specified set.\n\n    Parameters:\n        about: Specifies the set to retrieve data about.\n\n    Returned dictionary contains:\n\n    - **ts_ids:** Ids of time series in `about` set.\n    - **TimeFormat.ID_TIME:** Times in `about` set, where time format is `TimeFormat.ID_TIME`.\n    - **TimeFormat.DATETIME:** Times in `about` set, where time format is `TimeFormat.DATETIME`.\n    - **TimeFormat.UNIX_TIME:** Times in `about` set, where time format is `TimeFormat.UNIX_TIME`.\n    - **TimeFormat.SHIFTED_UNIX_TIME:** Times in `about` set, where time format is `TimeFormat.SHIFTED_UNIX_TIME`.\n\n    Returns:\n        Returns dictionary with details about set.\n    \"\"\"\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before getting data about set.\")\n\n    about = SplitType(about)\n\n    time_period = None\n    time_series = None\n\n    result = {}\n\n    if about == SplitType.TRAIN:\n        if not self.dataset_config.has_train():\n            raise ValueError(\"Train split is not used.\")\n        time_period = self.dataset_config.train_time_period\n        time_series = self.dataset_config.train_ts\n    elif about == SplitType.VAL:\n        if not self.dataset_config.has_val():\n            raise ValueError(\"Val split is not used.\")\n        time_period = self.dataset_config.val_time_period\n        time_series = self.dataset_config.val_ts\n    elif about == SplitType.TEST:\n        if not self.dataset_config.has_test():\n            raise ValueError(\"Test split is not used.\")\n        time_period = self.dataset_config.test_time_period\n        time_series = self.dataset_config.test_ts\n    else:\n        raise ValueError(\"Specified about parameter is not supported.\")\n\n    datetime_temp = np.array([datetime.fromtimestamp(time, timezone.utc) for time in self.metadata.time_indices[TIME_COLUMN_NAME][time_period[ID_TIME_COLUMN_NAME]]])\n\n    result[\"ts_ids\"] = time_series.copy()\n    result[TimeFormat.ID_TIME] = time_period[ID_TIME_COLUMN_NAME].copy()\n    result[TimeFormat.DATETIME] = datetime_temp.copy()\n    result[TimeFormat.UNIX_TIME] = self.metadata.time_indices[TIME_COLUMN_NAME][time_period[ID_TIME_COLUMN_NAME]].copy()\n    result[TimeFormat.SHIFTED_UNIX_TIME] = self.metadata.time_indices[TIME_COLUMN_NAME][time_period[ID_TIME_COLUMN_NAME]] - self.metadata.time_indices[TIME_COLUMN_NAME][0]\n\n    return result\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.set_sliding_window","title":"set_sliding_window","text":"<pre><code>set_sliding_window(sliding_window_size: int | None | Literal['config'] = 'config', sliding_window_prediction_size: int | None | Literal['config'] = 'config', sliding_window_step: int | None | Literal['config'] = 'config', set_shared_size: float | int | Literal['config'] = 'config', workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Used for updating sliding window related values set in config. Set parameter to <code>config</code> to keep it as it is config. If exception is thrown during set, no changes are made.</p> <p>Affects following configuration: </p> Dataset config Description <code>sliding_window_size</code> Number of times in one window. Impacts dataloader behavior. Refer to relevant config for details. <code>sliding_window_prediction_size</code> Number of times to predict from sliding_window_size. Refer to relevant config for details. <code>sliding_window_step</code> Number of times to move by after each window. Refer to relevant config for details. <code>set_shared_size</code> How much times should time periods share. Order of sharing is training set &lt; validation set &lt; test set. Refer to relevant config for details. <p>Parameters:</p> Name Type Description Default <code>sliding_window_size</code> <code>int | None | Literal['config']</code> <p>Number of times in one window. <code>Defaults: config</code>.</p> <code>'config'</code> <code>sliding_window_prediction_size</code> <code>int | None | Literal['config']</code> <p>Number of times to predict from sliding_window_size. <code>Defaults: config</code>.</p> <code>'config'</code> <code>sliding_window_step</code> <code>int | None | Literal['config']</code> <p>Number of times to move by after each window. <code>Defaults: config</code>.</p> <code>'config'</code> <code>set_shared_size</code> <code>float | int | Literal['config']</code> <p>How much times should time periods share. <code>Defaults: config</code>.</p> <code>'config'</code> <code>workers</code> <code>int | Literal['config']</code> <p>How many workers to use when setting new sliding window values. <code>Defaults: config</code>.</p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\disjoint_time_based_cesnet_dataset.py</code> <pre><code>def set_sliding_window(self, sliding_window_size: int | None | Literal[\"config\"] = \"config\", sliding_window_prediction_size: int | None | Literal[\"config\"] = \"config\",\n                       sliding_window_step: int | None | Literal[\"config\"] = \"config\", set_shared_size: float | int | Literal[\"config\"] = \"config\", workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"Used for updating sliding window related values set in config.\n    Set parameter to `config` to keep it as it is config.\n    If exception is thrown during set, no changes are made.\n\n    Affects following configuration: \n\n    Dataset config | Description\n    -------------- | -----------\n    `sliding_window_size` | Number of times in one window. Impacts dataloader behavior. Refer to relevant config for details.\n    `sliding_window_prediction_size` | Number of times to predict from sliding_window_size. Refer to relevant config for details.\n    `sliding_window_step` | Number of times to move by after each window. Refer to relevant config for details.\n    `set_shared_size` | How much times should time periods share. Order of sharing is training set &lt; validation set &lt; test set. Refer to relevant config for details.\n\n    Parameters:\n        sliding_window_size: Number of times in one window. `Defaults: config`.\n        sliding_window_prediction_size: Number of times to predict from sliding_window_size. `Defaults: config`.\n        sliding_window_step: Number of times to move by after each window. `Defaults: config`.\n        set_shared_size: How much times should time periods share. `Defaults: config`.\n        workers: How many workers to use when setting new sliding window values. `Defaults: config`.  \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating sliding window values.\")\n\n    self.update_dataset_config_and_initialize(sliding_window_size=sliding_window_size, sliding_window_prediction_size=sliding_window_prediction_size, sliding_window_step=sliding_window_step, set_shared_size=set_shared_size, workers=workers)\n    self.logger.info(\"Sliding window values has been changed successfuly.\")\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.set_batch_sizes","title":"set_batch_sizes","text":"<pre><code>set_batch_sizes(train_batch_size: int | Literal['config'] = 'config', val_batch_size: int | Literal['config'] = 'config', test_batch_size: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Used for updating batch sizes set in config. Set parameter to <code>config</code> to keep it as it is config. If exception is thrown during set, no changes are made.</p> <p>Affects following configuration: </p> Dataset config Description <code>train_batch_size</code> Number of samples per batch for train set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>val_batch_size</code> Number of samples per batch for val set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>test_batch_size</code> Number of samples per batch for test set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <p>Parameters:</p> Name Type Description Default <code>train_batch_size</code> <code>int | Literal['config']</code> <p>Number of samples per batch for train set. <code>Defaults: config</code>.</p> <code>'config'</code> <code>val_batch_size</code> <code>int | Literal['config']</code> <p>Number of samples per batch for val set. <code>Defaults: config</code>.</p> <code>'config'</code> <code>test_batch_size</code> <code>int | Literal['config']</code> <p>Number of samples per batch for test set. <code>Defaults: config</code>.</p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\disjoint_time_based_cesnet_dataset.py</code> <pre><code>def set_batch_sizes(self, train_batch_size: int | Literal[\"config\"] = \"config\", val_batch_size: int | Literal[\"config\"] = \"config\", test_batch_size: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"Used for updating batch sizes set in config.\n    Set parameter to `config` to keep it as it is config.\n    If exception is thrown during set, no changes are made.\n\n    Affects following configuration: \n\n    Dataset config | Description\n    -------------- | -----------\n    `train_batch_size` | Number of samples per batch for train set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `val_batch_size` | Number of samples per batch for val set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `test_batch_size` | Number of samples per batch for test set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n\n    Parameters:\n        train_batch_size: Number of samples per batch for train set. `Defaults: config`.\n        val_batch_size: Number of samples per batch for val set. `Defaults: config`.\n        test_batch_size: Number of samples per batch for test set. `Defaults: config`.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating batch sizes.\")\n\n    self.update_dataset_config_and_initialize(train_batch_size=train_batch_size, val_batch_size=val_batch_size, test_batch_size=test_batch_size, workers=\"config\")\n    self.logger.info(\"Batch sizes has been changed successfuly.\")\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.set_workers","title":"set_workers","text":"<pre><code>set_workers(train_workers: int | Literal['config'] = 'config', val_workers: int | Literal['config'] = 'config', test_workers: int | Literal['config'] = 'config', init_workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Used for updating workers set in config. Set parameter to <code>config</code> to keep it as it is config. If exception is thrown during set, no changes are made.</p> <p>Affects following configuration: </p> Dataset config Description <code>train_workers</code> Number of workers for loading training data. <code>val_workers</code> Number of workers for loading validation data. <code>test_workers</code> Number of workers for loading test data. <code>init_workers</code> Number of workers for dataset configuration. <p>Parameters:</p> Name Type Description Default <code>train_workers</code> <code>int | Literal['config']</code> <p>Number of workers for loading training data. <code>Defaults: config</code>.</p> <code>'config'</code> <code>val_workers</code> <code>int | Literal['config']</code> <p>Number of workers for loading validation data. <code>Defaults: config</code>.</p> <code>'config'</code> <code>test_workers</code> <code>int | Literal['config']</code> <p>Number of workers for loading test data. <code>Defaults: config</code>.</p> <code>'config'</code> <code>init_workers</code> <code>int | Literal['config']</code> <p>Number of workers for dataset configuration. <code>Defaults: config</code>.</p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\disjoint_time_based_cesnet_dataset.py</code> <pre><code>def set_workers(self, train_workers: int | Literal[\"config\"] = \"config\", val_workers: int | Literal[\"config\"] = \"config\",\n                test_workers: int | Literal[\"config\"] = \"config\", init_workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"Used for updating workers set in config.\n    Set parameter to `config` to keep it as it is config.\n    If exception is thrown during set, no changes are made.\n\n    Affects following configuration: \n\n    Dataset config | Description\n    -------------- | -----------\n    `train_workers` | Number of workers for loading training data.\n    `val_workers` | Number of workers for loading validation data.\n    `test_workers` | Number of workers for loading test data.\n    `init_workers` | Number of workers for dataset configuration.\n\n    Parameters:\n        train_workers: Number of workers for loading training data. `Defaults: config`.\n        val_workers: Number of workers for loading validation data. `Defaults: config`.\n        test_workers: Number of workers for loading test data. `Defaults: config`.\n        init_workers: Number of workers for dataset configuration. `Defaults: config`.            \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating workers.\")\n\n    self.update_dataset_config_and_initialize(train_workers=train_workers, val_workers=val_workers, test_workers=test_workers, init_workers=init_workers, workers=\"config\")\n    self.logger.info(\"Workers has been changed successfuly.\")\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset._initialize_datasets","title":"_initialize_datasets","text":"<pre><code>_initialize_datasets() -&gt; None\n</code></pre> <p>Called in <code>set_dataset_config_and_initialize</code>, this method initializes the set datasets (train, validation, test and all).</p> Source code in <code>cesnet_tszoo\\datasets\\disjoint_time_based_cesnet_dataset.py</code> <pre><code>def _initialize_datasets(self) -&gt; None:\n    \"\"\"Called in [`set_dataset_config_and_initialize`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.set_dataset_config_and_initialize), this method initializes the set datasets (train, validation, test and all). \"\"\"\n\n    if self.dataset_config.has_train():\n        load_config = DisjointTimeLoadConfig(self.dataset_config, SplitType.TRAIN)\n        self.train_dataset = DisjointTimeBasedSplittedDataset(self.metadata.dataset_path, self.metadata.data_table_path, load_config, self.dataset_config.train_workers)\n\n        self.logger.debug(\"train_dataset initiliazed.\")\n\n    if self.dataset_config.has_val():\n        load_config = DisjointTimeLoadConfig(self.dataset_config, SplitType.VAL)\n        self.val_dataset = DisjointTimeBasedSplittedDataset(self.metadata.dataset_path, self.metadata.data_table_path, load_config, self.dataset_config.val_workers)\n\n        self.logger.debug(\"val_dataset initiliazed.\")\n\n    if self.dataset_config.has_test():\n        load_config = DisjointTimeLoadConfig(self.dataset_config, SplitType.TEST)\n        self.test_dataset = DisjointTimeBasedSplittedDataset(self.metadata.dataset_path, self.metadata.data_table_path, load_config, self.dataset_config.test_workers)\n        self.logger.debug(\"test_dataset initiliazed.\")\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset._initialize_transformers_and_details","title":"_initialize_transformers_and_details","text":"<pre><code>_initialize_transformers_and_details(workers: int) -&gt; None\n</code></pre> <p>Called in <code>set_dataset_config_and_initialize</code>. </p> <p>Goes through data to validate time series against <code>nan_threshold</code>, fit/partial fit <code>transformers</code>, fit <code>anomaly handlers</code> and prepare <code>fillers</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\disjoint_time_based_cesnet_dataset.py</code> <pre><code>def _initialize_transformers_and_details(self, workers: int) -&gt; None:\n    \"\"\"\n    Called in [`set_dataset_config_and_initialize`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.set_dataset_config_and_initialize). \n\n    Goes through data to validate time series against `nan_threshold`, fit/partial fit `transformers`, fit `anomaly handlers` and prepare `fillers`.\n    \"\"\"\n\n    if self.dataset_config.has_train():\n\n        self.__initialize_config_for_train_set(workers)\n\n        self.logger.debug(\"Train set updated: %s time series left.\", len(self.dataset_config.train_ts))\n\n    if self.dataset_config.has_val():\n        init_config = DisjointTimeDatasetInitConfig(self.dataset_config, SplitType.VAL, PreprocessOrderGroup([]))\n\n        ts_ids_to_take = self.__initialize_config_for_non_fit_sets(init_config, workers, \"val\")\n        self.dataset_config.val_ts = self.dataset_config.val_ts[ts_ids_to_take]\n        self.dataset_config.val_ts_row_ranges = self.dataset_config.val_ts_row_ranges[ts_ids_to_take]\n        self.dataset_config._update_preprocess_order_supported_ids(self.dataset_config.val_preprocess_order, ts_ids_to_take)\n\n        self.logger.debug(\"Val set updated: %s time series left.\", len(self.dataset_config.val_ts))\n\n    if self.dataset_config.has_test():\n        init_config = DisjointTimeDatasetInitConfig(self.dataset_config, SplitType.TEST, PreprocessOrderGroup([]))\n\n        ts_ids_to_take = self.__initialize_config_for_non_fit_sets(init_config, workers, \"test\")\n        self.dataset_config.test_ts = self.dataset_config.test_ts[ts_ids_to_take]\n        self.dataset_config.test_ts_row_ranges = self.dataset_config.test_ts_row_ranges[ts_ids_to_take]\n        self.dataset_config._update_preprocess_order_supported_ids(self.dataset_config.test_preprocess_order, ts_ids_to_take)\n\n        self.logger.debug(\"Test set updated: %s time series left.\", len(self.dataset_config.test_ts))\n\n    self.logger.info(\"Dataset initialization complete. Configuration updated.\")\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.__initialize_config_for_train_set","title":"__initialize_config_for_train_set","text":"<pre><code>__initialize_config_for_train_set(workers: int) -&gt; None\n</code></pre> <p>Initializes config for provided time series.</p> Source code in <code>cesnet_tszoo\\datasets\\disjoint_time_based_cesnet_dataset.py</code> <pre><code>def __initialize_config_for_train_set(self, workers: int) -&gt; None:\n    \"\"\"Initializes config for provided time series. \"\"\"\n\n    self.logger.info(\"Updating config for train set and fitting values.\")\n\n    is_first_cycle = True\n\n    ts_ids_to_take = []\n\n    groups = self.dataset_config._get_train_preprocess_init_order_groups()\n    for i, group in enumerate(groups):\n        ts_ids_to_take = []\n        self.logger.info(\"Starting fitting cycle %s/%s.\", i + 1, len(groups))\n\n        init_config = DisjointTimeDatasetInitConfig(self.dataset_config, SplitType.TRAIN, group)\n        init_dataset = DisjointTimeBasedInitializerDataset(self.metadata.dataset_path, self.metadata.data_table_path, init_config)\n\n        sampler = SequentialSampler(init_dataset)\n        dataloader = DataLoader(init_dataset, num_workers=workers, collate_fn=self._collate_fn, worker_init_fn=DisjointTimeBasedInitializerDataset.worker_init_fn, persistent_workers=False, sampler=sampler)\n\n        if workers == 0:\n            init_dataset.pytables_worker_init()\n\n        for ts_id, data in enumerate(tqdm(dataloader, total=len(init_config.ts_row_ranges))):\n\n            init_dataset_return: InitDatasetReturn = data[0]\n\n            if init_dataset_return.is_under_nan_threshold:\n                ts_ids_to_take.append(ts_id)\n\n                # updates inner preprocessors passed from InitDataset\n                fitted_inner_index = 0\n                for inner_preprocess_order in group.preprocess_inner_orders:\n                    if inner_preprocess_order.should_be_fitted:\n                        inner_preprocess_order.holder.update_instance(init_dataset_return.preprocess_fitted_instances[fitted_inner_index].instance, ts_id)\n                        fitted_inner_index += 1\n\n                # updates outer preprocessors based on passed train data from InitDataset\n                for outer_preprocess_order in group.preprocess_outer_orders:\n                    if outer_preprocess_order.should_be_fitted:\n                        outer_preprocess_order.holder.fit(init_dataset_return.train_data, ts_id)\n\n                    if outer_preprocess_order.can_be_applied:\n                        init_dataset_return.train_data = outer_preprocess_order.holder.apply(init_dataset_return.train_data, ts_id)\n\n        if workers == 0:\n            init_dataset.cleanup()\n\n        # Update config based on filtered time series\n        if is_first_cycle:\n\n            if len(ts_ids_to_take) == 0:\n                raise ValueError(\"No valid time series left in train set after applying nan_threshold.\")\n\n            self.dataset_config.train_ts_row_ranges = self.dataset_config.train_ts_row_ranges[ts_ids_to_take]\n            self.dataset_config.train_ts = self.dataset_config.train_ts[ts_ids_to_take]\n            self.dataset_config._update_preprocess_order_supported_ids(self.dataset_config.train_preprocess_order, ts_ids_to_take)\n            self.logger.debug(\"invalid ts_ids removed: %s time series left.\", len(ts_ids_to_take))\n\n            is_first_cycle = False\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.__initialize_config_for_non_fit_sets","title":"__initialize_config_for_non_fit_sets","text":"<pre><code>__initialize_config_for_non_fit_sets(init_config: DisjointTimeDatasetInitConfig, workers: int, set_name: str) -&gt; np.ndarray\n</code></pre> <p>Initializes config for provided time series without fitting.</p> Source code in <code>cesnet_tszoo\\datasets\\disjoint_time_based_cesnet_dataset.py</code> <pre><code>def __initialize_config_for_non_fit_sets(self, init_config: DisjointTimeDatasetInitConfig, workers: int, set_name: str) -&gt; np.ndarray:\n    \"\"\"Initializes config for provided time series without fitting. \"\"\"\n    init_dataset = DisjointTimeBasedInitializerDataset(self.metadata.dataset_path, self.metadata.data_table_path, init_config)\n\n    sampler = SequentialSampler(init_dataset)\n    dataloader = DataLoader(init_dataset, num_workers=workers, collate_fn=self._collate_fn, worker_init_fn=DisjointTimeBasedInitializerDataset.worker_init_fn, persistent_workers=False, sampler=sampler)\n\n    if workers == 0:\n        init_dataset.pytables_worker_init()\n\n    ts_ids_to_take = []\n\n    self.logger.info(\"Updating config for %s set.\", set_name)\n    for i, data in enumerate(tqdm(dataloader, total=len(init_config.ts_row_ranges))):\n        init_dataset_return: InitDatasetReturn = data[0]\n\n        if init_dataset_return.is_under_nan_threshold:\n            ts_ids_to_take.append(i)\n\n    if workers == 0:\n        init_dataset.cleanup()\n\n    if len(ts_ids_to_take) == 0:\n        raise ValueError(f\"No valid time series left in {set_name} set after applying nan_threshold.\")\n\n    return ts_ids_to_take\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset._update_export_config_copy","title":"_update_export_config_copy","text":"<pre><code>_update_export_config_copy() -&gt; None\n</code></pre> <p>Called at the end of <code>set_dataset_config_and_initialize</code> or when changing config values. </p> <p>Updates values of config used for saving config.</p> Source code in <code>cesnet_tszoo\\datasets\\disjoint_time_based_cesnet_dataset.py</code> <pre><code>def _update_export_config_copy(self) -&gt; None:\n    \"\"\"\n    Called at the end of [`set_dataset_config_and_initialize`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.set_dataset_config_and_initialize) or when changing config values. \n\n    Updates values of config used for saving config.\n    \"\"\"\n    self._export_config_copy.database_name = self.metadata.database_name\n\n    self._export_config_copy.train_ts = self.dataset_config.train_ts.copy() if self.dataset_config.has_train() else None\n    self._export_config_copy.val_ts = self.dataset_config.val_ts.copy() if self.dataset_config.has_val() else None\n    self._export_config_copy.test_ts = self.dataset_config.test_ts.copy() if self.dataset_config.has_test() else None\n\n    self._export_config_copy.sliding_window_size = self.dataset_config.sliding_window_size\n    self._export_config_copy.sliding_window_prediction_size = self.dataset_config.sliding_window_prediction_size\n    self._export_config_copy.sliding_window_step = self.dataset_config.sliding_window_step\n    self._export_config_copy.set_shared_size = self.dataset_config.set_shared_size\n\n    super(DisjointTimeBasedCesnetDataset, self)._update_export_config_copy()\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset._get_singular_time_series_dataset","title":"_get_singular_time_series_dataset","text":"<pre><code>_get_singular_time_series_dataset(parent_dataset: DisjointTimeBasedSplittedDataset, ts_id: int) -&gt; DisjointTimeBasedSplittedDataset\n</code></pre> <p>Returns dataset for single time series</p> Source code in <code>cesnet_tszoo\\datasets\\disjoint_time_based_cesnet_dataset.py</code> <pre><code>def _get_singular_time_series_dataset(self, parent_dataset: DisjointTimeBasedSplittedDataset, ts_id: int) -&gt; DisjointTimeBasedSplittedDataset:\n    \"\"\"Returns dataset for single time series \"\"\"\n\n    temp = np.where(np.isin(parent_dataset.load_config.ts_row_ranges[self.metadata.ts_id_name], [ts_id]))[0]\n\n    if len(temp) == 0:\n        raise ValueError(f\"ts_id {ts_id} was not found in valid time series for this set. Available time series are: {parent_dataset.load_config.ts_row_ranges[self.metadata.ts_id_name]}\")\n\n    time_series_position = temp[0]\n\n    split_load_config = parent_dataset.load_config.create_split_copy(slice(time_series_position, time_series_position + 1))\n\n    dataset = DisjointTimeBasedSplittedDataset(self.metadata.dataset_path, self.metadata.data_table_path, split_load_config, 0)\n    self.logger.debug(\"Singular time series dataset initiliazed.\")\n\n    return dataset\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset._get_data_for_plot","title":"_get_data_for_plot","text":"<pre><code>_get_data_for_plot(ts_id: int, feature_indices: ndarray[int], time_format: TimeFormat) -&gt; tuple[np.ndarray, np.ndarray]\n</code></pre> <p>Dataset type specific retrieval of data.</p> Source code in <code>cesnet_tszoo\\datasets\\disjoint_time_based_cesnet_dataset.py</code> <pre><code>def _get_data_for_plot(self, ts_id: int, feature_indices: np.ndarray[int], time_format: TimeFormat) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Dataset type specific retrieval of data. \"\"\"\n\n    train_id_result, val_id_result, test_id_result = None, None, None\n\n    if (self.dataset_config.has_train()):\n        train_id_result = np.argwhere(np.isin(self.dataset_config.train_ts, ts_id)).ravel()\n    if (self.dataset_config.has_val()):\n        val_id_result = np.argwhere(np.isin(self.dataset_config.val_ts, ts_id)).ravel()\n    if (self.dataset_config.has_test()):\n        test_id_result = np.argwhere(np.isin(self.dataset_config.test_ts, ts_id)).ravel()\n\n    data = None\n    time_period = None\n\n    if self.dataset_config.has_train() and len(train_id_result) &gt; 0:\n        data = self.__get_ts_data_for_plot(self.train_dataset, ts_id, feature_indices)\n        time_period = self.get_data_about_set(SplitType.TRAIN)[time_format]\n        self.logger.debug(\"Valid ts_id found: %d\", train_id_result[0])\n\n    elif self.dataset_config.has_val() and len(val_id_result) &gt; 0:\n        data = self.__get_ts_data_for_plot(self.val_dataset, ts_id, feature_indices)\n        time_period = self.get_data_about_set(SplitType.VAL)[time_format]\n        self.logger.debug(\"Valid ts_id found: %d\", val_id_result[0])\n\n    elif self.dataset_config.has_test() and len(test_id_result) &gt; 0:\n        data = self.__get_ts_data_for_plot(self.test_dataset, ts_id, feature_indices)\n        time_period = self.get_data_about_set(SplitType.TEST)[time_format]\n        self.logger.debug(\"Valid ts_id found: %d\", test_id_result[0])\n    else:\n        raise ValueError(f\"Invalid ts_id '{ts_id}'. The provided ts_id is not found in the available time series IDs.\", self.dataset_config.train_ts, self.dataset_config.val_ts, self.dataset_config.test_ts)\n\n    return data, time_period\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.__get_ts_data_for_plot","title":"__get_ts_data_for_plot","text":"<pre><code>__get_ts_data_for_plot(dataset: DisjointTimeBasedSplittedDataset, ts_id: int, feature_indices: list[int])\n</code></pre> Source code in <code>cesnet_tszoo\\datasets\\disjoint_time_based_cesnet_dataset.py</code> <pre><code>def __get_ts_data_for_plot(self, dataset: DisjointTimeBasedSplittedDataset, ts_id: int, feature_indices: list[int]):\n    dataset = self._get_singular_time_series_dataset(dataset, ts_id)\n\n    dataloader = self.dataloader_factory.create_dataloader(dataset, self.dataset_config, 0, True, None)\n\n    temp_data = dataset_loaders.create_numpy_from_dataloader(dataloader, np.array([ts_id]), dataset.load_config.time_format, dataset.load_config.include_time, DatasetType.TIME_BASED, True)\n\n    if (dataset.load_config.time_format == TimeFormat.DATETIME and dataset.load_config.include_time):\n        temp_data = temp_data[0]\n\n    temp_data = temp_data[0][:, feature_indices]\n\n    return temp_data\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_train_dataloader","title":"get_train_dataloader","text":"<pre><code>get_train_dataloader(ts_id: int | None = None, workers: int | Literal['config'] = 'config', **kwargs) -&gt; DataLoader\n</code></pre> <p>Returns a PyTorch <code>DataLoader</code> for training set.</p> <p>The <code>DataLoader</code> is created on the first call and cached for subsequent use.  The cached dataloader is cleared when either <code>get_train_df</code> or <code>get_train_numpy</code> is called.</p> <p>The structure of the returned batch depends on the <code>time_format</code> and whether <code>sliding_window_size</code> is used:</p> <ul> <li>When <code>sliding_window_size</code> is used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> <li><code>np.ndarray</code> of times with shape <code>(times - 1)</code></li> <li><code>np.ndarray</code> of time with shape <code>(1)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> </ul> </li> </ul> </li> <li>When <code>sliding_window_size</code> is not used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> <li><code>np.ndarray</code> of time with shape <code>(times)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> </ul> </li> </ul> </li> </ul> <p>The <code>DataLoader</code> is configured with the following config attributes:</p> Dataset config Description <code>train_batch_size</code> Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>sliding_window_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_prediction_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_step</code> Available only for time-based datasets. Number of times to move by after each window. <code>train_workers</code> Specifies the number of workers to use for loading train data. Applied when <code>workers</code> = \"config\". <code>train_dataloader_order</code> Available only for series-based datasets. Whether to load train data in sequential or random order. <code>random_state</code> Seed for loading train data in random order. <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading train data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>ts_id</code> <code>int | None</code> <p>Specifies time series to take. If None returns all time series as normal. <code>Default: \"None\"</code></p> <code>None</code> <p>Returns:</p> Type Description <code>DataLoader</code> <p>An iterable <code>DataLoader</code> containing data from training set.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_train_dataloader(self, ts_id: int | None = None, workers: int | Literal[\"config\"] = \"config\", **kwargs) -&gt; DataLoader:\n    \"\"\"\n    Returns a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for training set.\n\n    The `DataLoader` is created on the first call and cached for subsequent use. &lt;br/&gt;\n    The cached dataloader is cleared when either [`get_train_df`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_train_df) or [`get_train_numpy`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_train_numpy) is called.\n\n    The structure of the returned batch depends on the `time_format` and whether `sliding_window_size` is used:\n\n    - When `sliding_window_size` is used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n            - `np.ndarray` of times with shape `(times - 1)`\n            - `np.ndarray` of time with shape `(1)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n    - When `sliding_window_size` is not used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n            - `np.ndarray` of time with shape `(times)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n\n    The `DataLoader` is configured with the following config attributes:\n\n    Dataset config | Description\n    -------------- | -----------\n    `train_batch_size` | Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `sliding_window_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n    `sliding_window_prediction_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n    `sliding_window_step` | Available only for time-based datasets. Number of times to move by after each window.\n    `train_workers` | Specifies the number of workers to use for loading train data. Applied when `workers` = \"config\".\n    `train_dataloader_order` | Available only for series-based datasets. Whether to load train data in sequential or random order.\n    `random_state` | Seed for loading train data in random order.\n\n    Parameters:\n        workers: The number of workers to use for loading train data. `Default: \"config\"` \n        ts_id: Specifies time series to take. If None returns all time series as normal. `Default: \"None\"`\n\n    Returns:\n        An iterable `DataLoader` containing data from training set.          \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access train_dataloader.\")\n\n    if not self.dataset_config.has_train():\n        raise ValueError(\"Dataloader for training set is not available in the dataset configuration.\")\n\n    assert self.train_dataset is not None, \"The train_dataset must be initialized before accessing data from training set.\"\n\n    default_kwargs = {'take_all': False, \"cache_loader\": True}\n    kwargs = {**default_kwargs, **kwargs}\n\n    if ts_id is not None:\n\n        if ts_id == self.dataset_config.used_singular_train_time_series and self.train_dataloader is not None:\n            self.logger.debug(\"Returning cached train_dataloader.\")\n            return self.train_dataloader\n\n        dataset = self._get_singular_time_series_dataset(self.train_dataset, ts_id)\n        self.dataset_config.used_singular_train_time_series = ts_id\n        if self.train_dataloader:\n            del self.train_dataloader\n            self.train_dataloader = None\n            self.logger.info(\"Destroyed previous cached train_dataloader.\")\n\n        self.dataset_config.used_train_workers = 0\n        self.train_dataloader = self.dataloader_factory.create_dataloader(dataset, self.dataset_config, 0, False, self.dataset_config.train_batch_size)\n        self.logger.info(\"Created new cached train_dataloader.\")\n        return self.train_dataloader\n    elif self.dataset_config.used_singular_train_time_series is not None and self.train_dataloader is not None:\n        del self.train_dataloader\n        self.train_dataloader = None\n        self.dataset_config.used_singular_train_time_series = None\n        self.logger.info(\"Destroyed previous cached train_dataloader.\")\n\n    if workers == \"config\":\n        workers = self.dataset_config.train_workers\n\n    # If the dataloader is cached and number of used workers did not change, return the cached dataloader\n    if self.train_dataloader and kwargs[\"cache_loader\"] and workers == self.dataset_config.used_train_workers:\n        self.logger.debug(\"Returning cached train_dataloader.\")\n        return self.train_dataloader\n\n    # Update the used workers count\n    self.dataset_config.used_train_workers = workers\n\n    # If there's a previously cached dataloader, destroy it\n    if self.train_dataloader:\n        del self.train_dataloader\n        self.train_dataloader = None\n        self.logger.info(\"Destroyed previous cached train_dataloader.\")\n\n    # If caching is enabled, create a new cached dataloader\n    if kwargs[\"cache_loader\"]:\n        self.train_dataloader = self.dataloader_factory.create_dataloader(self.train_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.train_batch_size, order=self.dataset_config.train_dataloader_order)\n        self.logger.info(\"Created new cached train_dataloader.\")\n        return self.train_dataloader\n\n    # If caching is disabled, create a new uncached dataloader\n    self.logger.debug(\"Created new uncached train_dataloader.\")\n    return self.dataloader_factory.create_dataloader(self.train_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.train_batch_size, order=self.dataset_config.train_dataloader_order)\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_val_dataloader","title":"get_val_dataloader","text":"<pre><code>get_val_dataloader(ts_id: int | None = None, workers: int | Literal['config'] = 'config', **kwargs) -&gt; DataLoader\n</code></pre> <p>Returns a PyTorch <code>DataLoader</code> for validation set.</p> <p>The <code>DataLoader</code> is created on the first call and cached for subsequent use.  The cached dataloader is cleared when either <code>get_val_df</code> or <code>get_val_numpy</code> is called.</p> <p>The structure of the returned batch depends on the <code>time_format</code> and whether <code>sliding_window_size</code> is used:</p> <ul> <li>When <code>sliding_window_size</code> is used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> <li><code>np.ndarray</code> of times with shape <code>(times - 1)</code></li> <li><code>np.ndarray</code> of time with shape <code>(1)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> </ul> </li> </ul> </li> <li>When <code>sliding_window_size</code> is not used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> <li><code>np.ndarray</code> of time with shape <code>(times)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> </ul> </li> </ul> </li> </ul> <p>The <code>DataLoader</code> is configured with the following config attributes:</p> Dataset config Description <code>val_batch_size</code> Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>sliding_window_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_prediction_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_step</code> Available only for time-based datasets. Number of times to move by after each window. <code>val_workers</code> Specifies the number of workers to use for loading validation data. Applied when <code>workers</code> = \"config\". <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading validation data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>ts_id</code> <code>int | None</code> <p>Specifies time series to take. If None returns all time series as normal. <code>Default: \"None\"</code></p> <code>None</code> <p>Returns:</p> Type Description <code>DataLoader</code> <p>An iterable <code>DataLoader</code> containing data from validation set.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_val_dataloader(self, ts_id: int | None = None, workers: int | Literal[\"config\"] = \"config\", **kwargs) -&gt; DataLoader:\n    \"\"\"\n    Returns a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for validation set.\n\n    The `DataLoader` is created on the first call and cached for subsequent use. &lt;br/&gt;\n    The cached dataloader is cleared when either [`get_val_df`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_val_df) or [`get_val_numpy`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_val_numpy) is called.\n\n    The structure of the returned batch depends on the `time_format` and whether `sliding_window_size` is used:\n\n    - When `sliding_window_size` is used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n            - `np.ndarray` of times with shape `(times - 1)`\n            - `np.ndarray` of time with shape `(1)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n    - When `sliding_window_size` is not used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n            - `np.ndarray` of time with shape `(times)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n\n    The `DataLoader` is configured with the following config attributes:\n\n    Dataset config | Description\n    -------------- | -----------\n    `val_batch_size` | Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `sliding_window_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n    `sliding_window_prediction_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n    `sliding_window_step` | Available only for time-based datasets. Number of times to move by after each window.\n    `val_workers` | Specifies the number of workers to use for loading validation data. Applied when `workers` = \"config\".\n\n\n    Parameters:\n        workers: The number of workers to use for loading validation data. `Default: \"config\"`  \n        ts_id: Specifies time series to take. If None returns all time series as normal. `Default: \"None\"`\n\n    Returns:\n        An iterable `DataLoader` containing data from validation set.        \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access val_dataloader.\")\n\n    if not self.dataset_config.has_val():\n        raise ValueError(\"Dataloader for validation set is not available in the dataset configuration.\")\n\n    assert self.val_dataset is not None, \"The val_dataset must be initialized before accessing data from validation set.\"\n\n    default_kwargs = {'take_all': False, \"cache_loader\": True}\n    kwargs = {**default_kwargs, **kwargs}\n\n    if ts_id is not None:\n\n        if ts_id == self.dataset_config.used_singular_val_time_series and self.val_dataloader is not None:\n            self.logger.debug(\"Returning cached val_dataloader.\")\n            return self.val_dataloader\n\n        dataset = self._get_singular_time_series_dataset(self.val_dataset, ts_id)\n        self.dataset_config.used_singular_val_time_series = ts_id\n        if self.val_dataloader:\n            del self.val_dataloader\n            self.val_dataloader = None\n            self.logger.info(\"Destroyed previous cached val_dataloader.\")\n\n        self.dataset_config.used_val_workers = 0\n        self.val_dataloader = self.dataloader_factory.create_dataloader(dataset, self.dataset_config, 0, False, self.dataset_config.val_batch_size)\n        self.logger.info(\"Created new cached val_dataloader.\")\n        return self.val_dataloader\n    elif self.dataset_config.used_singular_val_time_series is not None and self.val_dataloader is not None:\n        del self.val_dataloader\n        self.val_dataloader = None\n        self.dataset_config.used_singular_val_time_series = None\n        self.logger.info(\"Destroyed previous cached val_dataloader.\")\n\n    if workers == \"config\":\n        workers = self.dataset_config.val_workers\n\n    # If the dataloader is cached and number of used workers did not change, return the cached dataloader\n    if self.val_dataloader and kwargs[\"cache_loader\"] and workers == self.dataset_config.used_val_workers:\n        self.logger.debug(\"Returning cached val_dataloader.\")\n        return self.val_dataloader\n\n    # Update the used workers count\n    self.dataset_config.used_val_workers = workers\n\n    # If there's a previously cached dataloader, destroy it\n    if self.val_dataloader:\n        del self.val_dataloader\n        self.val_dataloader = None\n        self.logger.info(\"Destroyed previous cached val_dataloader.\")\n\n    # If caching is enabled, create a new cached dataloader\n    if kwargs[\"cache_loader\"]:\n        self.val_dataloader = self.dataloader_factory.create_dataloader(self.val_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.val_batch_size)\n        self.logger.info(\"Created new cached val_dataloader.\")\n        return self.val_dataloader\n\n    # If caching is disabled, create a new uncached dataloader\n    self.logger.debug(\"Created new uncached val_dataloader.\")\n    return self.dataloader_factory.create_dataloader(self.val_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.val_batch_size)\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_test_dataloader","title":"get_test_dataloader","text":"<pre><code>get_test_dataloader(ts_id: int | None = None, workers: int | Literal['config'] = 'config', **kwargs) -&gt; DataLoader\n</code></pre> <p>Returns a PyTorch <code>DataLoader</code> for test set.</p> <p>The <code>DataLoader</code> is created on the first call and cached for subsequent use.  The cached dataloader is cleared when either <code>get_test_df</code> or <code>get_test_numpy</code> is called.</p> <p>The structure of the returned batch depends on the <code>time_format</code> and whether <code>sliding_window_size</code> is used:</p> <ul> <li>When <code>sliding_window_size</code> is used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> <li><code>np.ndarray</code> of times with shape <code>(times - 1)</code></li> <li><code>np.ndarray</code> of time with shape <code>(1)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> </ul> </li> </ul> </li> <li>When <code>sliding_window_size</code> is not used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> <li><code>np.ndarray</code> of time with shape <code>(times)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> </ul> </li> </ul> </li> </ul> <p>The <code>DataLoader</code> is configured with the following config attributes:</p> Dataset config Description <code>test_batch_size</code> Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>sliding_window_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_prediction_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_step</code> Available only for time-based datasets. Number of times to move by after each window. <code>test_workers</code> Specifies the number of workers to use for loading test data. Applied when <code>workers</code> = \"config\". <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading test data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>ts_id</code> <code>int | None</code> <p>Specifies time series to take. If None returns all time series as normal. <code>Default: \"None\"</code></p> <code>None</code> <p>Returns:</p> Type Description <code>DataLoader</code> <p>An iterable <code>DataLoader</code> containing data from test set.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_test_dataloader(self, ts_id: int | None = None, workers: int | Literal[\"config\"] = \"config\", **kwargs) -&gt; DataLoader:\n    \"\"\"\n    Returns a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for test set.\n\n    The `DataLoader` is created on the first call and cached for subsequent use. &lt;br/&gt;\n    The cached dataloader is cleared when either [`get_test_df`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_test_df) or [`get_test_numpy`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_test_numpy) is called.\n\n    The structure of the returned batch depends on the `time_format` and whether `sliding_window_size` is used:\n\n    - When `sliding_window_size` is used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n            - `np.ndarray` of times with shape `(times - 1)`\n            - `np.ndarray` of time with shape `(1)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n    - When `sliding_window_size` is not used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n            - `np.ndarray` of time with shape `(times)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n\n    The `DataLoader` is configured with the following config attributes:\n\n    Dataset config | Description\n    -------------- | -----------\n    `test_batch_size` | Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `sliding_window_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n    `sliding_window_prediction_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n    `sliding_window_step` | Available only for time-based datasets. Number of times to move by after each window.\n    `test_workers` | Specifies the number of workers to use for loading test data. Applied when `workers` = \"config\".\n\n\n    Parameters:\n        workers: The number of workers to use for loading test data. `Default: \"config\"`  \n        ts_id: Specifies time series to take. If None returns all time series as normal. `Default: \"None\"`\n\n    Returns:\n        An iterable `DataLoader` containing data from test set.        \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access test_dataloader.\")\n\n    if not self.dataset_config.has_test():\n        raise ValueError(\"Dataloader for test set is not available in the dataset configuration.\")\n\n    assert self.test_dataset is not None, \"The test_dataset must be initialized before accessing data from test set.\"\n\n    default_kwargs = {'take_all': False, \"cache_loader\": True}\n    kwargs = {**default_kwargs, **kwargs}\n\n    if ts_id is not None:\n\n        if ts_id == self.dataset_config.used_singular_test_time_series and self.test_dataloader is not None:\n            self.logger.debug(\"Returning cached test_dataloader.\")\n            return self.test_dataloader\n\n        dataset = self._get_singular_time_series_dataset(self.test_dataset, ts_id)\n        self.dataset_config.used_singular_test_time_series = ts_id\n        if self.test_dataloader:\n            del self.test_dataloader\n            self.test_dataloader = None\n            self.logger.info(\"Destroyed previous cached test_dataloader.\")\n\n        self.dataset_config.used_test_workers = 0\n        self.test_dataloader = self.dataloader_factory.create_dataloader(dataset, self.dataset_config, 0, False, self.dataset_config.test_batch_size)\n        self.logger.info(\"Created new cached test_dataloader.\")\n        return self.test_dataloader\n    elif self.dataset_config.used_singular_test_time_series is not None and self.test_dataloader is not None:\n        del self.test_dataloader\n        self.test_dataloader = None\n        self.dataset_config.used_singular_test_time_series = None\n        self.logger.info(\"Destroyed previous cached test_dataloader.\")\n\n    if workers == \"config\":\n        workers = self.dataset_config.test_workers\n\n    # If the dataloader is cached and number of used workers did not change, return the cached dataloader\n    if self.test_dataloader and kwargs[\"cache_loader\"] and workers == self.dataset_config.used_test_workers:\n        self.logger.debug(\"Returning cached test_dataloader.\")\n        return self.test_dataloader\n\n    # Update the used workers count\n    self.dataset_config.used_test_workers = workers\n\n    # If there's a previously cached dataloader, destroy it\n    if self.test_dataloader:\n        del self.test_dataloader\n        self.test_dataloader = None\n        self.logger.info(\"Destroyed previous cached test_dataloader.\")\n\n    # If caching is enabled, create a new cached dataloader\n    if kwargs[\"cache_loader\"]:\n        self.test_dataloader = self.dataloader_factory.create_dataloader(self.test_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.test_batch_size)\n        self.logger.info(\"Created new cached test_dataloader.\")\n        return self.test_dataloader\n\n    # If caching is disabled, create a new uncached dataloader\n    self.logger.debug(\"Created new uncached test_dataloader.\")\n    return self.dataloader_factory.create_dataloader(self.test_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.test_batch_size)\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_all_dataloader","title":"get_all_dataloader","text":"<pre><code>get_all_dataloader(ts_id: int | None = None, workers: int | Literal['config'] = 'config', **kwargs) -&gt; DataLoader\n</code></pre> <p>Returns a PyTorch <code>DataLoader</code> for all set.</p> <p>The <code>DataLoader</code> is created on the first call and cached for subsequent use.  The cached dataloader is cleared when either <code>get_all_df</code> or <code>get_all_numpy</code> is called.</p> <p>The structure of the returned batch depends on the <code>time_format</code> and whether <code>sliding_window_size</code> is used:</p> <ul> <li>When <code>sliding_window_size</code> is used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> <li><code>np.ndarray</code> of times with shape <code>(times - 1)</code></li> <li><code>np.ndarray</code> of time with shape <code>(1)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> </ul> </li> </ul> </li> <li>When <code>sliding_window_size</code> is not used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> <li><code>np.ndarray</code> of time with shape <code>(times)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> </ul> </li> </ul> </li> </ul> <p>The <code>DataLoader</code> is configured with the following config attributes:</p> Dataset config Description <code>all_batch_size</code> Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>sliding_window_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_prediction_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_step</code> Available only for time-based datasets. Number of times to move by after each window. <code>all_workers</code> Specifies the number of workers to use for loading all data. Applied when <code>workers</code> = \"config\". <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading all data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>ts_id</code> <code>int | None</code> <p>Specifies time series to take. If None returns all time series as normal. <code>Default: \"None\"</code></p> <code>None</code> <p>Returns:</p> Type Description <code>DataLoader</code> <p>An iterable <code>DataLoader</code> containing data from all set.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_all_dataloader(self, ts_id: int | None = None, workers: int | Literal[\"config\"] = \"config\", **kwargs) -&gt; DataLoader:\n    \"\"\"\n    Returns a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for all set.\n\n    The `DataLoader` is created on the first call and cached for subsequent use. &lt;br/&gt;\n    The cached dataloader is cleared when either [`get_all_df`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_all_df) or [`get_all_numpy`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_all_numpy) is called.\n\n    The structure of the returned batch depends on the `time_format` and whether `sliding_window_size` is used:\n\n    - When `sliding_window_size` is used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n            - `np.ndarray` of times with shape `(times - 1)`\n            - `np.ndarray` of time with shape `(1)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n    - When `sliding_window_size` is not used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n            - `np.ndarray` of time with shape `(times)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n\n    The `DataLoader` is configured with the following config attributes:\n\n    Dataset config | Description\n    -------------- | -----------\n    `all_batch_size` | Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `sliding_window_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n    `sliding_window_prediction_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n    `sliding_window_step` | Available only for time-based datasets. Number of times to move by after each window.\n    `all_workers` | Specifies the number of workers to use for loading all data. Applied when `workers` = \"config\".\n\n\n    Parameters:\n        workers: The number of workers to use for loading all data. `Default: \"config\"`  \n        ts_id: Specifies time series to take. If None returns all time series as normal. `Default: \"None\"`\n\n    Returns:\n        An iterable `DataLoader` containing data from all set.       \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access all_dataloader.\")\n\n    if not self.dataset_config.has_all():\n        raise ValueError(\"Dataloader for all set is not available in the dataset configuration.\")\n\n    assert self.all_dataset is not None, \"The all_dataset must be initialized before accessing data from all set.\"\n\n    default_kwargs = {'take_all': False, \"cache_loader\": True}\n    kwargs = {**default_kwargs, **kwargs}\n\n    if ts_id is not None:\n\n        if ts_id == self.dataset_config.used_singular_all_time_series and self.all_dataloader is not None:\n            self.logger.debug(\"Returning cached all_dataloader.\")\n            return self.all_dataloader\n\n        dataset = self._get_singular_time_series_dataset(self.all_dataset, ts_id)\n        self.dataset_config.used_singular_all_time_series = ts_id\n        if self.all_dataloader:\n            del self.all_dataloader\n            self.all_dataloader = None\n            self.logger.info(\"Destroyed previous cached all_dataloader.\")\n\n        self.dataset_config.used_all_workers = 0\n        self.all_dataloader = self.dataloader_factory.create_dataloader(dataset, self.dataset_config, 0, False, self.dataset_config.all_batch_size)\n        self.logger.info(\"Created new cached all_dataloader.\")\n        return self.all_dataloader\n    elif self.dataset_config.used_singular_all_time_series is not None and self.all_dataloader is not None:\n        del self.all_dataloader\n        self.all_dataloader = None\n        self.dataset_config.used_singular_all_time_series = None\n        self.logger.info(\"Destroyed previous cached all_dataloader.\")\n\n    if workers == \"config\":\n        workers = self.dataset_config.all_workers\n\n    # If the dataloader is cached and number of used workers did not change, return the cached dataloader\n    if self.all_dataloader and kwargs[\"cache_loader\"] and workers == self.dataset_config.used_all_workers:\n        self.logger.debug(\"Returning cached all_dataloader.\")\n        return self.all_dataloader\n\n    # Update the used workers count\n    self.dataset_config.used_all_workers = workers\n\n    # If there's a previously cached dataloader, destroy it\n    if self.all_dataloader:\n        del self.all_dataloader\n        self.all_dataloader = None\n        self.logger.info(\"Destroyed previous cached all_dataloader.\")\n\n    # If caching is enabled, create a new cached dataloader\n    if kwargs[\"cache_loader\"]:\n        self.all_dataloader = self.dataloader_factory.create_dataloader(self.all_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.all_batch_size)\n        self.logger.info(\"Created new cached all_dataloader.\")\n        return self.all_dataloader\n\n    # If caching is disabled, create a new uncached dataloader\n    self.logger.debug(\"Creating new uncached all_dataloader.\")\n    return self.dataloader_factory.create_dataloader(self.all_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.all_batch_size)\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_train_df","title":"get_train_df","text":"<pre><code>get_train_df(workers: int | Literal['config'] = 'config', as_single_dataframe: bool = True) -&gt; pd.DataFrame\n</code></pre> <p>Creates a Pandas <code>DataFrame</code> containing all the data from training set grouped by time series.</p> <p>This method uses the <code>train_dataloader</code> with a batch size set to the total number of data in the training set. The cached <code>train_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire training set is loaded into memory, which may lead to high memory usage. If working with large training set, consider using <code>get_train_dataloader</code> instead to handle data in batches.</p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading train data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>as_single_dataframe</code> <code>bool</code> <p>Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. <code>Default: True</code> </p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A single Pandas DataFrame containing all data from training set, or a list of DataFrames (one per time series).</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_train_df(self, workers: int | Literal[\"config\"] = \"config\", as_single_dataframe: bool = True) -&gt; pd.DataFrame:\n    \"\"\"\n    Creates a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) containing all the data from training set grouped by time series.\n\n    This method uses the `train_dataloader` with a batch size set to the total number of data in the training set. The cached `train_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire training set is loaded into memory, which may lead to high memory usage. If working with large training set, consider using `get_train_dataloader` instead to handle data in batches.\n\n    Parameters:\n        workers: The number of workers to use for loading train data. `Default: \"config\"`  \n        as_single_dataframe: Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. `Default: True` \n\n    Returns:\n        A single Pandas DataFrame containing all data from training set, or a list of DataFrames (one per time series).\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access train_dataloader.\")\n\n    if not self.dataset_config.has_train():\n        raise ValueError(\"Dataloader for training set is not available in the dataset configuration.\")\n\n    assert self.train_dataset is not None, \"The train_dataset must be initialized before accessing data from training set.\"\n\n    ts_ids, time_period = self.dataset_config._get_train()\n\n    should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n    dataloader = self.get_train_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n    return self._get_df(dataloader, as_single_dataframe, ts_ids, time_period)\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_val_df","title":"get_val_df","text":"<pre><code>get_val_df(workers: int | Literal['config'] = 'config', as_single_dataframe: bool = True) -&gt; pd.DataFrame\n</code></pre> <p>Create a Pandas <code>DataFrame</code> containing all the data from validation set grouped by time series.</p> <p>This method uses the <code>val_dataloader</code> with a batch size set to the total number of data in the validation set. The cached <code>val_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire validation set is loaded into memory, which may lead to high memory usage. If working with large validation set, consider using <code>get_val_dataloader</code> instead to handle data in batches.</p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading validation data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>as_single_dataframe</code> <code>bool</code> <p>Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. <code>Default: True</code> </p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A single Pandas DataFrame containing all data from validation set, or a list of DataFrames (one per time series).</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_val_df(self, workers: int | Literal[\"config\"] = \"config\", as_single_dataframe: bool = True) -&gt; pd.DataFrame:\n    \"\"\"\n    Create a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) containing all the data from validation set grouped by time series.\n\n    This method uses the `val_dataloader` with a batch size set to the total number of data in the validation set. The cached `val_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire validation set is loaded into memory, which may lead to high memory usage. If working with large validation set, consider using `get_val_dataloader` instead to handle data in batches.\n\n    Parameters:\n        workers: The number of workers to use for loading validation data. `Default: \"config\"`  \n        as_single_dataframe: Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. `Default: True` \n\n    Returns:\n        A single Pandas DataFrame containing all data from validation set, or a list of DataFrames (one per time series).\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access val_dataloader.\")\n\n    if not self.dataset_config.has_val():\n        raise ValueError(\"Dataloader for validation set is not available in the dataset configuration.\")\n\n    assert self.val_dataset is not None, \"The val_dataset must be initialized before accessing data from validation set.\"\n\n    ts_ids, time_period = self.dataset_config._get_val()\n\n    should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n    dataloader = self.get_val_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n    return self._get_df(dataloader, as_single_dataframe, ts_ids, time_period)\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_test_df","title":"get_test_df","text":"<pre><code>get_test_df(workers: int | Literal['config'] = 'config', as_single_dataframe: bool = True) -&gt; pd.DataFrame\n</code></pre> <p>Creates a Pandas <code>DataFrame</code> containing all the data from test set grouped by time series.</p> <p>This method uses the <code>test_dataloader</code> with a batch size set to the total number of data in the test set. The cached <code>test_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire test set is loaded into memory, which may lead to high memory usage. If working with large test set, consider using <code>get_test_dataloader</code> instead to handle data in batches.</p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading test data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>as_single_dataframe</code> <code>bool</code> <p>Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. <code>Default: True</code> </p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A single Pandas DataFrame containing all data from test set, or a list of DataFrames (one per time series).</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_test_df(self, workers: int | Literal[\"config\"] = \"config\", as_single_dataframe: bool = True) -&gt; pd.DataFrame:\n    \"\"\"\n    Creates a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) containing all the data from test set grouped by time series.\n\n    This method uses the `test_dataloader` with a batch size set to the total number of data in the test set. The cached `test_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire test set is loaded into memory, which may lead to high memory usage. If working with large test set, consider using `get_test_dataloader` instead to handle data in batches.\n\n    Parameters:\n        workers: The number of workers to use for loading test data. `Default: \"config\"`  \n        as_single_dataframe: Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. `Default: True` \n\n    Returns:\n        A single Pandas DataFrame containing all data from test set, or a list of DataFrames (one per time series).\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access test_dataloader.\")\n\n    if not self.dataset_config.has_test():\n        raise ValueError(\"Dataloader for test set is not available in the dataset configuration.\")\n\n    assert self.test_dataset is not None, \"The test_dataset must be initialized before accessing data from test set.\"\n\n    ts_ids, time_period = self.dataset_config._get_test()\n\n    should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n    dataloader = self.get_test_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n    return self._get_df(dataloader, as_single_dataframe, ts_ids, time_period)\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_all_df","title":"get_all_df","text":"<pre><code>get_all_df(workers: int | Literal['config'] = 'config', as_single_dataframe: bool = True) -&gt; pd.DataFrame\n</code></pre> <p>Creates a Pandas <code>DataFrame</code> containing all the data from all set grouped by time series.</p> <p>This method uses the <code>all_dataloader</code> with a batch size set to the total number of data in the all set. The cached <code>all_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire all set is loaded into memory, which may lead to high memory usage. If working with large all set, consider using <code>get_all_dataloader</code> instead to handle data in batches.</p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading all data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>as_single_dataframe</code> <code>bool</code> <p>Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. <code>Default: True</code> </p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A single Pandas DataFrame containing all data from all set, or a list of DataFrames (one per time series).</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_all_df(self, workers: int | Literal[\"config\"] = \"config\", as_single_dataframe: bool = True) -&gt; pd.DataFrame:\n    \"\"\"\n    Creates a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) containing all the data from all set grouped by time series.\n\n    This method uses the `all_dataloader` with a batch size set to the total number of data in the all set. The cached `all_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire all set is loaded into memory, which may lead to high memory usage. If working with large all set, consider using `get_all_dataloader` instead to handle data in batches.\n\n    Parameters:\n        workers: The number of workers to use for loading all data. `Default: \"config\"`  \n        as_single_dataframe: Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. `Default: True` \n\n    Returns:\n        A single Pandas DataFrame containing all data from all set, or a list of DataFrames (one per time series).\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access all_dataloader.\")\n\n    if not self.dataset_config.has_all():\n        raise ValueError(\"Dataloader for all set is not available in the dataset configuration.\")\n\n    assert self.all_dataset is not None, \"The all_dataset must be initialized before accessing data from all set.\"\n\n    ts_ids, time_period = self.dataset_config._get_all()\n\n    should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n    dataloader = self.get_all_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n    return self._get_df(dataloader, as_single_dataframe, ts_ids, time_period)\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_train_numpy","title":"get_train_numpy","text":"<pre><code>get_train_numpy(workers: int | Literal['config'] = 'config') -&gt; np.ndarray\n</code></pre> <p>Creates a NumPy array containing all the data from training set grouped by time series, with the shape <code>(num_time_series, num_times, num_features)</code>.</p> <p>This method uses the <code>train_dataloader</code> with a batch size set to the total number of data in the training set. The cached <code>train_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire training set is loaded into memory, which may lead to high memory usage. If working with large training set, consider using <code>get_train_dataloader</code> instead to handle data in batches.        </p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading train data. <code>Default: \"config\"</code> </p> <code>'config'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A NumPy array containing all the data in training set with the shape <code>(num_time_series, num_times, num_features)</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_train_numpy(self, workers: int | Literal[\"config\"] = \"config\") -&gt; np.ndarray:\n    \"\"\"\n    Creates a NumPy array containing all the data from training set grouped by time series, with the shape `(num_time_series, num_times, num_features)`.\n\n    This method uses the `train_dataloader` with a batch size set to the total number of data in the training set. The cached `train_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire training set is loaded into memory, which may lead to high memory usage. If working with large training set, consider using `get_train_dataloader` instead to handle data in batches.        \n\n    Parameters:\n        workers: The number of workers to use for loading train data. `Default: \"config\"`  \n\n    Returns:\n        A NumPy array containing all the data in training set with the shape `(num_time_series, num_times, num_features)`.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access train_dataloader.\")\n\n    if not self.dataset_config.has_train():\n        raise ValueError(\"Dataloader for training set is not available in the dataset configuration.\")\n\n    assert self.train_dataset is not None, \"The train_dataset must be initialized before accessing data from training set.\"\n\n    ts_ids, time_period = self.dataset_config._get_train()\n\n    should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n    dataloader = self.get_train_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n    return self._get_numpy(dataloader, ts_ids, time_period)\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_val_numpy","title":"get_val_numpy","text":"<pre><code>get_val_numpy(workers: int | Literal['config'] = 'config') -&gt; np.ndarray\n</code></pre> <p>Creates a NumPy array containing all the data from validation set grouped by time series, with the shape <code>(num_time_series, num_times, num_features)</code>.</p> <p>This method uses the <code>val_dataloader</code> with a batch size set to the total number of data in the validation set. The cached <code>val_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire validation set is loaded into memory, which may lead to high memory usage. If working with large validation set, consider using <code>get_val_dataloader</code> instead to handle data in batches.        </p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading validation data. <code>Default: \"config\"</code> </p> <code>'config'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A NumPy array containing all the data in validation set with the shape <code>(num_time_series, num_times, num_features)</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_val_numpy(self, workers: int | Literal[\"config\"] = \"config\") -&gt; np.ndarray:\n    \"\"\"\n    Creates a NumPy array containing all the data from validation set grouped by time series, with the shape `(num_time_series, num_times, num_features)`.\n\n    This method uses the `val_dataloader` with a batch size set to the total number of data in the validation set. The cached `val_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire validation set is loaded into memory, which may lead to high memory usage. If working with large validation set, consider using `get_val_dataloader` instead to handle data in batches.        \n\n    Parameters:\n        workers: The number of workers to use for loading validation data. `Default: \"config\"`  \n\n    Returns:\n        A NumPy array containing all the data in validation set with the shape `(num_time_series, num_times, num_features)`.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access val_dataloader.\")\n\n    if not self.dataset_config.has_val():\n        raise ValueError(\"Dataloader for validation set is not available in the dataset configuration.\")\n\n    assert self.val_dataset is not None, \"The val_dataset must be initialized before accessing data from validation set.\"\n\n    ts_ids, time_period = self.dataset_config._get_val()\n\n    should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n    dataloader = self.get_val_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n    return self._get_numpy(dataloader, ts_ids, time_period)\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_test_numpy","title":"get_test_numpy","text":"<pre><code>get_test_numpy(workers: int | Literal['config'] = 'config') -&gt; np.ndarray\n</code></pre> <p>Creates a NumPy array containing all the data from test set grouped by time series, with the shape <code>(num_time_series, num_times, num_features)</code>.</p> <p>This method uses the <code>test_dataloader</code> with a batch size set to the total number of data in the test set. The cached <code>test_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire test set is loaded into memory, which may lead to high memory usage. If working with large test set, consider using <code>get_test_dataloader</code> instead to handle data in batches.        </p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading test data. <code>Default: \"config\"</code> </p> <code>'config'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A NumPy array containing all the data in test set with the shape <code>(num_time_series, num_times, num_features)</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_test_numpy(self, workers: int | Literal[\"config\"] = \"config\") -&gt; np.ndarray:\n    \"\"\"\n    Creates a NumPy array containing all the data from test set grouped by time series, with the shape `(num_time_series, num_times, num_features)`.\n\n    This method uses the `test_dataloader` with a batch size set to the total number of data in the test set. The cached `test_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire test set is loaded into memory, which may lead to high memory usage. If working with large test set, consider using `get_test_dataloader` instead to handle data in batches.        \n\n    Parameters:\n        workers: The number of workers to use for loading test data. `Default: \"config\"`  \n\n    Returns:\n        A NumPy array containing all the data in test set with the shape `(num_time_series, num_times, num_features)`.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access test_dataloader.\")\n\n    if not self.dataset_config.has_test():\n        raise ValueError(\"Dataloader for test set is not available in the dataset configuration.\")\n\n    assert self.test_dataset is not None, \"The test_dataset must be initialized before accessing data from test set.\"\n\n    ts_ids, time_period = self.dataset_config._get_test()\n\n    should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n    dataloader = self.get_test_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n    return self._get_numpy(dataloader, ts_ids, time_period)\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_all_numpy","title":"get_all_numpy","text":"<pre><code>get_all_numpy(workers: int | Literal['config'] = 'config') -&gt; np.ndarray\n</code></pre> <p>Creates a NumPy array containing all the data from all set grouped by time series, with the shape <code>(num_time_series, num_times, num_features)</code>.</p> <p>This method uses the <code>all_dataloader</code> with a batch size set to the total number of data in the all set. The cached <code>all_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire all set is loaded into memory, which may lead to high memory usage. If working with large all set, consider using <code>get_all_dataloader</code> instead to handle data in batches.        </p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading all data. <code>Default: \"config\"</code> </p> <code>'config'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A NumPy array containing all the data in all set with the shape <code>(num_time_series, num_times, num_features)</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_all_numpy(self, workers: int | Literal[\"config\"] = \"config\") -&gt; np.ndarray:\n    \"\"\"\n    Creates a NumPy array containing all the data from all set grouped by time series, with the shape `(num_time_series, num_times, num_features)`.\n\n    This method uses the `all_dataloader` with a batch size set to the total number of data in the all set. The cached `all_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire all set is loaded into memory, which may lead to high memory usage. If working with large all set, consider using `get_all_dataloader` instead to handle data in batches.        \n\n    Parameters:\n        workers: The number of workers to use for loading all data. `Default: \"config\"`  \n\n    Returns:\n        A NumPy array containing all the data in all set with the shape `(num_time_series, num_times, num_features)`.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access all_dataloader.\")\n\n    if not self.dataset_config.has_all():\n        raise ValueError(\"Dataloader for all set is not available in the dataset configuration.\")\n\n    assert self.all_dataset is not None, \"The all_dataset must be initialized before accessing data from all set.\"\n\n    ts_ids, time_period = self.dataset_config._get_all()\n\n    should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n    dataloader = self.get_all_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n    return self._get_numpy(dataloader, ts_ids, time_period)\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset._update_dataset_config_and_initialize","title":"_update_dataset_config_and_initialize","text":"<pre><code>_update_dataset_config_and_initialize(config_editor: ConfigEditor, workers: int | Literal['config'] = 'config', display_config_details: Optional[Literal['test', 'diagram']] = None)\n</code></pre> <p>Updates config via passed config editor.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def _update_dataset_config_and_initialize(self, config_editor: ConfigEditor, workers: int | Literal[\"config\"] = \"config\", display_config_details: Optional[Literal[\"test\", \"diagram\"]] = None):\n    \"\"\"Updates config via passed config editor. \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating dataset configuration.\")\n\n    if display_config_details is not None:\n        display_config_details = DisplayType(display_config_details)\n\n    original_config = deepcopy(self.dataset_config)\n    original_export_config = deepcopy(self._export_config_copy)\n\n    try:\n        if config_editor.requires_init:\n            self.logger.info(\"Re-initialization is required.\")\n            config_editor.modify_dataset_config(self._export_config_copy, self.metadata)\n            self.set_dataset_config_and_initialize(self._export_config_copy, None, workers)\n\n        else:\n            config_editor.modify_dataset_config(self.dataset_config, self.metadata)\n\n    except Exception:\n        self.dataset_config = original_config\n        self._export_config_copy = original_export_config\n        self.logger.error(\"Error occured, reverting changes.\")\n        raise\n\n    if self.train_dataloader is not None:\n        del self.train_dataloader\n        self.train_dataloader = None\n        self.logger.info(\"Destroyed cached train_dataloader.\")\n\n    if self.val_dataloader is not None:\n        del self.val_dataloader\n        self.val_dataloader = None\n        self.logger.info(\"Destroyed cached val_dataloader.\")\n\n    if self.test_dataloader is not None:\n        del self.test_dataloader\n        self.test_dataloader = None\n        self.logger.info(\"Destroyed cached test_dataloader.\")\n\n    if self.all_dataloader is not None:\n        del self.all_dataloader\n        self.all_dataloader = None\n        self.logger.info(\"Destroyed cached all_dataloader.\")\n\n    self._update_config_imported_status(None)\n    self._update_export_config_copy()\n\n    self.logger.info(\"Configuration has been changed successfuly.\")\n\n    if display_config_details is not None:\n        self.summary(display_config_details)\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.apply_filler","title":"apply_filler","text":"<pre><code>apply_filler(fill_missing_with: type | FillerType | Literal['mean_filler', 'forward_filler', 'linear_interpolation_filler'] | None, workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Used for updating filler set in config. Set parameter to <code>config</code> to keep it as it is config. If exception is thrown during set, no changes are made.</p> <p>Affects following configuration: </p> Dataset config Description <code>fill_missing_with</code> Defines how to fill missing values in the dataset. <p>Parameters:</p> Name Type Description Default <code>fill_missing_with</code> <code>type | FillerType | Literal['mean_filler', 'forward_filler', 'linear_interpolation_filler'] | None</code> <p>Defines how to fill missing values in the dataset. <code>Defaults: config</code>.  </p> required <code>workers</code> <code>int | Literal['config']</code> <p>How many workers to use when setting new filler. <code>Defaults: config</code>.</p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def apply_filler(self, fill_missing_with: type | FillerType | Literal[\"mean_filler\", \"forward_filler\", \"linear_interpolation_filler\"] | None, workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"Used for updating filler set in config.\n    Set parameter to `config` to keep it as it is config.\n    If exception is thrown during set, no changes are made.\n\n    Affects following configuration: \n\n    Dataset config | Description\n    -------------- | -----------\n    `fill_missing_with` | Defines how to fill missing values in the dataset.\n\n    Parameters:\n        fill_missing_with: Defines how to fill missing values in the dataset. `Defaults: config`.  \n        workers: How many workers to use when setting new filler. `Defaults: config`.      \n    \"\"\"\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating filler.\")\n\n    self.update_dataset_config_and_initialize(fill_missing_with=fill_missing_with, workers=workers)\n    self.logger.info(\"Filler has been changed successfuly.\")\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.apply_anomaly_handler","title":"apply_anomaly_handler","text":"<pre><code>apply_anomaly_handler(handle_anomalies_with: type | AnomalyHandlerType | Literal['z-score', 'interquartile_range'] | None | Literal['config'], workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Used for updating anomaly handler set in config.</p> <p>Set parameter to <code>config</code> to keep it as it is config.</p> <p>If exception is thrown during set, no changes are made.</p> <p>Affects following configuration:</p> Dataset config Description <code>handle_anomalies_with</code> Defines the anomaly handler to handle anomalies in the dataset. <p>Parameters:</p> Name Type Description Default <code>handle_anomalies_with</code> <code>type | AnomalyHandlerType | Literal['z-score', 'interquartile_range'] | None | Literal['config']</code> <p>Defines the anomaly handler to handle anomalies in the dataset. <code>Defaults: config</code>.  </p> required <code>workers</code> <code>int | Literal['config']</code> <p>How many workers to use when setting new filler. <code>Defaults: config</code>.</p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def apply_anomaly_handler(self, handle_anomalies_with: type | AnomalyHandlerType | Literal[\"z-score\", \"interquartile_range\"] | None | Literal[\"config\"], workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"Used for updating anomaly handler set in config.\n\n    Set parameter to `config` to keep it as it is config.\n\n    If exception is thrown during set, no changes are made.\n\n    Affects following configuration:\n\n    Dataset config | Description\n    -------------- | -----------\n    `handle_anomalies_with` | Defines the anomaly handler to handle anomalies in the dataset.\n\n    Parameters:\n        handle_anomalies_with: Defines the anomaly handler to handle anomalies in the dataset. `Defaults: config`.  \n        workers: How many workers to use when setting new filler. `Defaults: config`.      \n    \"\"\"\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating anomaly handler.\")\n\n    self.update_dataset_config_and_initialize(handle_anomalies_with=handle_anomalies_with, workers=workers)\n    self.logger.info(\"Anomaly handler has been changed successfuly.\")\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.set_default_values","title":"set_default_values","text":"<pre><code>set_default_values(default_values: list[Number] | NDArray[number] | dict[str, Number] | Number | Literal['default'] | None, workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Used for updating default values set in config. Set parameter to <code>config</code> to keep it as it is config. If exception is thrown during set, no changes are made.</p> <p>Affects following configuration:</p> Dataset config Description <code>default_values</code> Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature. <p>Parameters:</p> Name Type Description Default <code>default_values</code> <code>list[Number] | NDArray[number] | dict[str, Number] | Number | Literal['default'] | None</code> <p>Default values for missing data, applied before fillers. <code>Defaults: config</code>.  </p> required <code>workers</code> <code>int | Literal['config']</code> <p>How many workers to use when setting new default values. <code>Defaults: config</code>.</p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def set_default_values(self, default_values: list[Number] | npt.NDArray[np.number] | dict[str, Number] | Number | Literal[\"default\"] | None, workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"Used for updating default values set in config.\n    Set parameter to `config` to keep it as it is config.\n    If exception is thrown during set, no changes are made.\n\n    Affects following configuration:\n\n    Dataset config | Description\n    -------------- | -----------\n    `default_values` | Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature.\n\n    Parameters:\n        default_values: Default values for missing data, applied before fillers. `Defaults: config`.  \n        workers: How many workers to use when setting new default values. `Defaults: config`.      \n    \"\"\"\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating default values.\")\n\n    self.update_dataset_config_and_initialize(default_values=default_values, workers=workers)\n    self.logger.info(\"Default values has been changed successfuly.\")\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.set_preprocess_order","title":"set_preprocess_order","text":"<pre><code>set_preprocess_order(preprocess_order: list[str, type] | Literal['config'] = 'config', workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Used for updating preprocess_order set in config. Set parameter to <code>config</code> to keep it as it is config. If exception is thrown during set, no changes are made.</p> <p>Affects following configuration: </p> Dataset config Description <code>preprocess_order</code> Used order of when preprocesses are applied. Can be also used to add/remove custom handlers. <p>Parameters:</p> Name Type Description Default <code>preprocess_order</code> <code>list[str, type] | Literal['config']</code> <p>Used order of when preprocesses are applied. Can be also used to add/remove custom handlers. <code>Defaults: config</code>.  </p> <code>'config'</code> <code>workers</code> <code>int | Literal['config']</code> <p>How many workers to use when setting new default values. <code>Defaults: config</code>.</p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def set_preprocess_order(self, preprocess_order: list[str, type] | Literal[\"config\"] = \"config\", workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"Used for updating preprocess_order set in config.\n    Set parameter to `config` to keep it as it is config.\n    If exception is thrown during set, no changes are made.\n\n    Affects following configuration: \n\n    Dataset config | Description\n    -------------- | -----------\n    `preprocess_order` | Used order of when preprocesses are applied. Can be also used to add/remove custom handlers.\n\n    Parameters:\n        preprocess_order: Used order of when preprocesses are applied. Can be also used to add/remove custom handlers. `Defaults: config`.  \n        workers: How many workers to use when setting new default values. `Defaults: config`.      \n    \"\"\"\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating preprocess order.\")\n\n    self.update_dataset_config_and_initialize(preprocess_order=preprocess_order, workers=workers)\n    self.logger.info(\"Preprocess order has been changed successfuly.\")\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.display_dataset_details","title":"display_dataset_details","text":"<pre><code>display_dataset_details() -&gt; None\n</code></pre> <p>Display information about the contents of the dataset.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>    def display_dataset_details(self) -&gt; None:\n        \"\"\"Display information about the contents of the dataset.  \"\"\"\n\n        to_display = f'''\nDataset details:\n\n    {self.metadata.aggregation}\n        Time indices: {range(self.metadata.time_indices[ID_TIME_COLUMN_NAME][0], self.metadata.time_indices[ID_TIME_COLUMN_NAME][-1])}\n        Datetime: {(datetime.fromtimestamp(self.metadata.time_indices['time'][0], tz=timezone.utc), datetime.fromtimestamp(self.metadata.time_indices['time'][-1], timezone.utc))}\n\n    {self.metadata.source_type}\n        Time series indices: {get_abbreviated_list_string(self.metadata.ts_indices[self.metadata.ts_id_name])}; use 'get_available_ts_indices' for full list\n        Features with default values: {self.metadata.default_values}\n\n        Additional data: {list(self.metadata.additional_data.keys())}\n        '''\n\n        print(to_display)\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.summary","title":"summary","text":"<pre><code>summary(display_type: Literal['text', 'diagram']) -&gt; None\n</code></pre> <p>Used to display used configurations. Can be displayed as interactive html diagram or text summary.</p> <p>Parameters:</p> Name Type Description Default <code>display_type</code> <code>Literal['text', 'diagram']</code> <p>Whether configuration should be display as diagram or text summary.</p> required Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def summary(self, display_type: Literal[\"text\", \"diagram\"]) -&gt; None:\n    \"\"\"Used to display used configurations. Can be displayed as interactive html diagram or text summary.\n\n    Parameters:\n        display_type: Whether configuration should be display as diagram or text summary.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to display summary.\")\n\n    display_type = DisplayType(display_type)\n\n    if display_type == DisplayType.TEXT:\n        print(self.dataset_config)\n    elif display_type == DisplayType.DIAGRAM:\n        steps = self.dataset_config._get_summary_steps()\n        return css_utils.display_summary_diagram(steps)\n    else:\n        raise NotImplementedError()\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.save_summary_diagram_as_html","title":"save_summary_diagram_as_html","text":"<pre><code>save_summary_diagram_as_html(path: str)\n</code></pre> <p>Saves diagram produces from <code>summary</code> method as html file to specified path.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def save_summary_diagram_as_html(self, path: str):\n    \"\"\"Saves diagram produces from `summary` method as html file to specified path. \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to save summary diagram.\")\n\n    steps = self.dataset_config._get_summary_steps()\n    html = css_utils.get_summary_diagram(steps)\n\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        f.write(html)\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_feature_names","title":"get_feature_names","text":"<pre><code>get_feature_names() -&gt; list[str]\n</code></pre> <p>Returns a list of all available feature names in the dataset.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_feature_names(self) -&gt; list[str]:\n    \"\"\"Returns a list of all available feature names in the dataset. \"\"\"\n\n    return list(self.metadata.features.keys())\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_available_ts_indices","title":"get_available_ts_indices","text":"<pre><code>get_available_ts_indices() -&gt; np.ndarray\n</code></pre> <p>Returns the available time series indices in this dataset.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_available_ts_indices(self) -&gt; np.ndarray:\n    \"\"\"Returns the available time series indices in this dataset. \"\"\"\n    return self.metadata.ts_indices\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_additional_data","title":"get_additional_data","text":"<pre><code>get_additional_data(data_name: str) -&gt; pd.DataFrame\n</code></pre> <p>Create a Pandas <code>DataFrame</code> of additional data of <code>data_name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data_name</code> <code>str</code> <p>Name of additional data to return.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Dataframe of additional data of <code>data_name</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_additional_data(self, data_name: str) -&gt; pd.DataFrame:\n    \"\"\"Create a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) of additional data of `data_name`.\n\n    Parameters:\n        data_name: Name of additional data to return.\n\n    Returns:\n        Dataframe of additional data of `data_name`.\n    \"\"\"\n\n    if data_name not in self.metadata.additional_data:\n        self.logger.error(\"%s is not available for this dataset.\", data_name)\n        raise ValueError(f\"{data_name} is not available for this dataset.\", f\"Possible options are: {self.metadata.additional_data}\")\n\n    data = get_additional_data(self.metadata.dataset_path, data_name)\n    data_df = pd.DataFrame(data)\n\n    for column, column_type in self.metadata.additional_data[data_name]:\n        if column_type == datetime:\n            data_df[column] = data_df[column].apply(lambda x: datetime.fromtimestamp(x, tz=timezone.utc))\n        else:\n            data_df[column] = data_df[column].astype(column_type)\n\n    return data_df\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.plot","title":"plot","text":"<pre><code>plot(ts_id: int, plot_type: Literal['scatter', 'line'], features: list[str] | str | Literal['config'] = 'config', feature_per_plot: bool = True, time_format: TimeFormat | Literal['config', 'id_time', 'datetime', 'unix_time', 'shifted_unix_time'] = 'config', is_interactive: bool = True) -&gt; None\n</code></pre> <p>Displays a graph for the selected <code>ts_id</code> and its <code>features</code>.</p> <p>The plotting is done using the <code>Plotly</code> library, which provides interactive graphs.</p> <p>Parameters:</p> Name Type Description Default <code>ts_id</code> <code>int</code> <p>The ID of the time series to display.</p> required <code>plot_type</code> <code>Literal['scatter', 'line']</code> <p>The type of graph to plot.</p> required <code>features</code> <code>list[str] | str | Literal['config']</code> <p>The features to display in the plot. <code>Defaults: \"config\"</code>.</p> <code>'config'</code> <code>feature_per_plot</code> <code>bool</code> <p>Whether each feature should be displayed in a separate plot or combined into one. <code>Defaults: True</code>.</p> <code>True</code> <code>time_format</code> <code>TimeFormat | Literal['config', 'id_time', 'datetime', 'unix_time', 'shifted_unix_time']</code> <p>The time format to use for the x-axis. <code>Defaults: \"config\"</code>.</p> <code>'config'</code> <code>is_interactive</code> <code>bool</code> <p>Whether the plot should be interactive (e.g., zoom, hover). <code>Defaults: True</code>.</p> <code>True</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def plot(self, ts_id: int, plot_type: Literal[\"scatter\", \"line\"], features: list[str] | str | Literal[\"config\"] = \"config\", feature_per_plot: bool = True,\n         time_format: TimeFormat | Literal[\"config\", \"id_time\", \"datetime\", \"unix_time\", \"shifted_unix_time\"] = \"config\", is_interactive: bool = True) -&gt; None:\n    \"\"\"\n    Displays a graph for the selected `ts_id` and its `features`.\n\n    The plotting is done using the [`Plotly`](https://plotly.com/python/) library, which provides interactive graphs.\n\n    Parameters:\n        ts_id: The ID of the time series to display.\n        plot_type: The type of graph to plot.\n        features: The features to display in the plot. `Defaults: \"config\"`.\n        feature_per_plot: Whether each feature should be displayed in a separate plot or combined into one. `Defaults: True`.\n        time_format: The time format to use for the x-axis. `Defaults: \"config\"`.\n        is_interactive: Whether the plot should be interactive (e.g., zoom, hover). `Defaults: True`.\n    \"\"\"\n\n    if time_format == \"config\":\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to plot.\")\n\n        time_format = self.dataset_config.time_format\n        self.logger.debug(\"Using time format from dataset configuration: %s\", time_format)\n    else:\n        time_format = TimeFormat(time_format)\n        self.logger.debug(\"Using specified time format: %s\", time_format)\n\n    time_series, times, features = self.__get_data_for_plot(ts_id, features, time_format)\n    self.logger.debug(\"Received data for plotting. Time series, times, and features are ready.\")\n\n    plots = []\n\n    if feature_per_plot:\n        self.logger.debug(\"Creating individual plots for each feature.\")\n        fig = make_subplots(rows=len(features), cols=1, shared_xaxes=False, x_title=time_format.value)\n\n        for i, feature in enumerate(features):\n            if plot_type == \"scatter\":\n                plot = go.Scatter(x=times, y=time_series[:, i], mode=\"markers\", name=feature, legendgroup=feature)\n                self.logger.debug(\"Creating scatter plot for feature: %s\", feature)\n            elif plot_type == \"line\":\n                plot = go.Scatter(x=times, y=time_series[:, i], mode=\"lines\", name=feature)\n                self.logger.debug(\"Creating line plot for feature: %s\", feature)\n            else:\n                raise ValueError(\"Invalid plot type.\")\n\n            fig.add_traces(plot, rows=i + 1, cols=1)\n\n        fig.update_layout(height=200 + 120 * len(features), width=2000, autosize=len(features) == 1, showlegend=True)\n        self.logger.debug(\"Created subplots for features: %s.\", features)\n    else:\n        self.logger.debug(\"Creating a combined plot for all features.\")\n        for i, feature in enumerate(features):\n            if plot_type == \"scatter\":\n                plot = go.Scatter(x=times, y=time_series[:, i], mode=\"markers\", name=feature)\n                self.logger.debug(\"Creating scatter plot for feature: %s\", feature)\n            elif plot_type == \"line\":\n                plot = go.Scatter(x=times, y=time_series[:, i], mode=\"lines\", name=feature)\n                self.logger.debug(\"Creating line plot for feature: %s\", feature)\n            else:\n                raise ValueError(\"Invalid plot type.\")\n            plots.append(plot)\n\n        fig = go.Figure(data=plots)\n        fig.update_layout(xaxis_title=time_format.value, showlegend=True, height=200 + 120 * 2)\n        self.logger.debug(\"Created combined plot for features: %s.\", features)\n\n    if not is_interactive:\n        self.logger.debug(\"Disabling interactivity for the plot.\")\n        fig.update_layout(updatemenus=[], dragmode=False, hovermode=False)\n\n    self.logger.debug(\"Displaying the plot.\")\n    fig.show()\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.add_annotation","title":"add_annotation","text":"<pre><code>add_annotation(annotation: str, annotation_group: str, ts_id: int | None, id_time: int | None, enforce_ids: bool = True) -&gt; None\n</code></pre> <p>Adds an annotation to the specified <code>annotation_group</code>.</p> <ul> <li>If the provided <code>annotation_group</code> does not exist, it will be created.</li> <li>At least one of <code>ts_id</code> or <code>id_time</code> must be provided to associate the annotation with time series or/and time point.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>annotation</code> <code>str</code> <p>The annotation to be added.</p> required <code>annotation_group</code> <code>str</code> <p>The group to which the annotation should be added.</p> required <code>ts_id</code> <code>int | None</code> <p>The time series ID to which the annotation should be added.</p> required <code>id_time</code> <code>int | None</code> <p>The time ID to which the annotation should be added.</p> required <code>enforce_ids</code> <code>bool</code> <p>Flag indicating whether the <code>ts_id</code> and <code>id_time</code> must belong to this dataset. <code>Default: True</code></p> <code>True</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def add_annotation(self, annotation: str, annotation_group: str, ts_id: int | None, id_time: int | None, enforce_ids: bool = True) -&gt; None:\n    \"\"\" \n    Adds an annotation to the specified `annotation_group`.\n\n    - If the provided `annotation_group` does not exist, it will be created.\n    - At least one of `ts_id` or `id_time` must be provided to associate the annotation with time series or/and time point.\n\n    Parameters:\n        annotation: The annotation to be added.\n        annotation_group: The group to which the annotation should be added.\n        ts_id: The time series ID to which the annotation should be added.\n        id_time: The time ID to which the annotation should be added.\n        enforce_ids: Flag indicating whether the `ts_id` and `id_time` must belong to this dataset. `Default: True`  \n    \"\"\"\n\n    if enforce_ids:\n        self._validate_annotation_ids(ts_id, id_time)\n    self.annotations.add_annotation(annotation, annotation_group, ts_id, id_time)\n\n    if ts_id is not None and id_time is not None:\n        self._update_annotations_imported_status(AnnotationType.BOTH, None)\n    elif ts_id is not None and id_time is None:\n        self._update_annotations_imported_status(AnnotationType.TS_ID, None)\n    elif ts_id is None and id_time is not None:\n        self._update_annotations_imported_status(AnnotationType.ID_TIME, None)\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.remove_annotation","title":"remove_annotation","text":"<pre><code>remove_annotation(annotation_group: str, ts_id: int | None, id_time: int | None) -&gt; None\n</code></pre> <p>Removes an annotation from the specified <code>annotation_group</code>.</p> <ul> <li>At least one of <code>ts_id</code> or <code>id_time</code> must be provided to associate the annotation with time series or/and time point.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>annotation_group</code> <code>str</code> <p>The annotation group from which the annotation should be removed.</p> required <code>ts_id</code> <code>int | None</code> <p>The time series ID from which the annotation should be removed.</p> required <code>id_time</code> <code>int | None</code> <p>The time ID from which the annotation should be removed.</p> required Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def remove_annotation(self, annotation_group: str, ts_id: int | None, id_time: int | None) -&gt; None:\n    \"\"\"  \n    Removes an annotation from the specified `annotation_group`.\n\n    - At least one of `ts_id` or `id_time` must be provided to associate the annotation with time series or/and time point.\n\n    Parameters:\n        annotation_group: The annotation group from which the annotation should be removed.\n        ts_id: The time series ID from which the annotation should be removed.\n        id_time: The time ID from which the annotation should be removed. \n    \"\"\"\n\n    self.annotations.remove_annotation(annotation_group, ts_id, id_time, False)\n\n    if ts_id is not None and id_time is not None:\n        self._update_annotations_imported_status(AnnotationType.BOTH, None)\n    elif ts_id is not None and id_time is None:\n        self._update_annotations_imported_status(AnnotationType.TS_ID, None)\n    elif ts_id is None and id_time is not None:\n        self._update_annotations_imported_status(AnnotationType.ID_TIME, None)\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.add_annotation_group","title":"add_annotation_group","text":"<pre><code>add_annotation_group(annotation_group: str, on: AnnotationType | Literal['id_time', 'ts_id', 'both'])\n</code></pre> <p>Adds a new <code>annotation_group</code>.</p> <p>Parameters:</p> Name Type Description Default <code>annotation_group</code> <code>str</code> <p>The name of the annotation group to be added.</p> required <code>on</code> <code>AnnotationType | Literal['id_time', 'ts_id', 'both']</code> <p>Specifies which part of the data should be annotated. If set to <code>\"both\"</code>, annotations will be applied as if <code>id_time</code> and <code>ts_id</code> were both set.</p> required Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def add_annotation_group(self, annotation_group: str, on: AnnotationType | Literal[\"id_time\", \"ts_id\", \"both\"]):\n    \"\"\" \n    Adds a new `annotation_group`.\n\n    Parameters:\n        annotation_group: The name of the annotation group to be added.\n        on: Specifies which part of the data should be annotated. If set to `\"both\"`, annotations will be applied as if `id_time` and `ts_id` were both set.\n    \"\"\"\n    on = AnnotationType(on)\n\n    self.annotations.add_annotation_group(annotation_group, on, False)\n\n    self._update_annotations_imported_status(on, None)\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.remove_annotation_group","title":"remove_annotation_group","text":"<pre><code>remove_annotation_group(annotation_group: str, on: AnnotationType | Literal['id_time', 'ts_id', 'both'])\n</code></pre> <p>Removes the specified <code>annotation_group</code>.</p> <p>Parameters:</p> Name Type Description Default <code>annotation_group</code> <code>str</code> <p>The name of the annotation group to be removed.</p> required <code>on</code> <code>AnnotationType | Literal['id_time', 'ts_id', 'both']</code> <p>Specifies which part of the data the <code>annotation_group</code> should be removed from. If set to <code>\"both\"</code>, annotations will be applied as if <code>id_time</code> and <code>ts_id</code> were both set.</p> required Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def remove_annotation_group(self, annotation_group: str, on: AnnotationType | Literal[\"id_time\", \"ts_id\", \"both\"]):\n    \"\"\" \n    Removes the specified `annotation_group`.\n\n    Parameters:\n        annotation_group: The name of the annotation group to be removed.\n        on: Specifies which part of the data the `annotation_group` should be removed from. If set to `\"both\"`, annotations will be applied as if `id_time` and `ts_id` were both set.        \n    \"\"\"\n    on = AnnotationType(on)\n\n    self.annotations.remove_annotation_group(annotation_group, on, False)\n\n    self._update_annotations_imported_status(on, None)\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_annotations","title":"get_annotations","text":"<pre><code>get_annotations(on: AnnotationType | Literal['id_time', 'ts_id', 'both']) -&gt; pd.DataFrame\n</code></pre> <p>Returns the annotations as a Pandas <code>DataFrame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>on</code> <code>AnnotationType | Literal['id_time', 'ts_id', 'both']</code> <p>Specifies which annotations to return. If set to <code>\"both\"</code>, annotations will be applied as if <code>id_time</code> and <code>ts_id</code> were both set.         </p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A Pandas DataFrame containing the selected annotations.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_annotations(self, on: AnnotationType | Literal[\"id_time\", \"ts_id\", \"both\"]) -&gt; pd.DataFrame:\n    \"\"\" \n    Returns the annotations as a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html).\n\n    Parameters:\n        on: Specifies which annotations to return. If set to `\"both\"`, annotations will be applied as if `id_time` and `ts_id` were both set.         \n\n    Returns:\n        A Pandas DataFrame containing the selected annotations.      \n    \"\"\"\n    on = AnnotationType(on)\n\n    return self.annotations.get_annotations(on, self.metadata.ts_id_name)\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.import_annotations","title":"import_annotations","text":"<pre><code>import_annotations(identifier: str, enforce_ids: bool = True) -&gt; None\n</code></pre> <p>Imports annotations from a CSV file.</p> <p>First, it attempts to load the built-in annotations, if no built-in annotations with such an identifier exists, it attempts to load a custom annotations from the <code>\"data_root\"/tszoo/annotations/</code> directory.</p> <p><code>data_root</code> is specified when the dataset is created.     </p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The name of the CSV file.     </p> required <code>enforce_ids</code> <code>bool</code> <p>Flag indicating whether the <code>ts_id</code> and <code>id_time</code> must belong to this dataset. <code>Default: True</code></p> <code>True</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def import_annotations(self, identifier: str, enforce_ids: bool = True) -&gt; None:\n    \"\"\" \n    Imports annotations from a CSV file.\n\n    First, it attempts to load the built-in annotations, if no built-in annotations with such an identifier exists, it attempts to load a custom annotations from the `\"data_root\"/tszoo/annotations/` directory.\n\n    `data_root` is specified when the dataset is created.     \n\n    Parameters:\n        identifier: The name of the CSV file.     \n        enforce_ids: Flag indicating whether the `ts_id` and `id_time` must belong to this dataset. `Default: True`                \n    \"\"\"\n\n    annotations_file_path, is_built_in = get_annotations_path_and_whether_it_is_built_in(identifier, self.metadata.annotations_root, self.logger)\n\n    if is_built_in:\n        self.logger.info(\"Built-in annotations found: %s.\", identifier)\n        if not os.path.exists(annotations_file_path):\n            self.logger.info(\"Downloading annotations with identifier: %s\", identifier)\n            annotations_url = f\"{ANNOTATIONS_DOWNLOAD_BUCKET}&amp;file={identifier}\"  # probably will change annotations bucket... placeholder\n            resumable_download(url=annotations_url, file_path=annotations_file_path, silent=False)\n\n        self.logger.debug(\"Loading annotations from %s\", annotations_file_path)\n        temp_df = pd.read_csv(annotations_file_path)\n        self.logger.debug(\"Created DataFrame from file: %s\", annotations_file_path)\n    else:\n        self.logger.info(\"Custom annotations found: %s.\", identifier)\n        self.logger.debug(\"Loading annotations from %s\", annotations_file_path)\n        temp_df = pd.read_csv(annotations_file_path)\n        self.logger.debug(\"Created DataFrame from file: %s\", annotations_file_path)\n\n    ts_id_index = None\n    time_id_index = None\n    on = None\n\n    # Check the columns of the DataFrame to identify the type of annotation\n    if self.metadata.ts_id_name in temp_df.columns and ID_TIME_COLUMN_NAME in temp_df.columns:\n        self.annotations.clear_time_in_time_series()\n        time_id_index = temp_df.columns.tolist().index(ID_TIME_COLUMN_NAME)\n        ts_id_index = temp_df.columns.tolist().index(self.metadata.ts_id_name)\n        on = AnnotationType.BOTH\n        self.logger.info(\"Annotations detected as %s (both %s and id_time)\", AnnotationType.BOTH, self.metadata.ts_id_name)\n\n    elif self.metadata.ts_id_name in temp_df.columns:\n        self.annotations.clear_time_series()\n        ts_id_index = temp_df.columns.tolist().index(self.metadata.ts_id_name)\n        on = AnnotationType.TS_ID\n        self.logger.info(\"Annotations detected as %s (%s only)\", AnnotationType.TS_ID, self.metadata.ts_id_name)\n\n    elif ID_TIME_COLUMN_NAME in temp_df.columns:\n        self.annotations.clear_time()\n        time_id_index = temp_df.columns.tolist().index(ID_TIME_COLUMN_NAME)\n        on = AnnotationType.ID_TIME\n        self.logger.info(\"Annotations detected as %s (%s only)\", AnnotationType.ID_TIME, ID_TIME_COLUMN_NAME)\n\n    else:\n        raise ValueError(f\"Could not find {self.metadata.ts_id_name} and {ID_TIME_COLUMN_NAME} in the imported CSV.\")\n\n    # Process each row in the DataFrame and add annotations\n    for row in temp_df.itertuples(False):\n        for i, _ in enumerate(temp_df.columns):\n            if i == time_id_index or i == ts_id_index:\n                continue\n\n            ts_id = None\n            if ts_id_index is not None:\n                ts_id = row[ts_id_index]\n\n            id_time = None\n            if time_id_index is not None:\n                id_time = row[time_id_index]\n\n            self.add_annotation(row[i], temp_df.columns[i], ts_id, id_time, enforce_ids)\n\n    self._update_annotations_imported_status(on, identifier)\n    self.logger.info(\"Successfully imported annotations from %s\", annotations_file_path)\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.import_config","title":"import_config","text":"<pre><code>import_config(identifier: str, display_config_details: Optional[Literal['text', 'diagram']] = 'text', workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Import the dataset_config from a pickle file and initializes the dataset. Config type must correspond to dataset type.</p> <p>First, it attempts to load the built-in config, if no built-in config with such an identifier exists, it attempts to load a custom config from the <code>\"data_root\"/tszoo/configs/</code> directory.</p> <p><code>data_root</code> is specified when the dataset is created.       </p> <p>The following configuration attributes are used during initialization:</p> Dataset config Description <code>init_workers</code> Specifies the number of workers to use for initialization. Applied when <code>workers</code> = \"config\". <code>partial_fit_initialized_transformers</code> Determines whether initialized transformers should be partially fitted on the training data. <code>nan_threshold</code> Filters out time series with missing values exceeding the specified threshold. <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>Name of the pickle file.</p> required <code>display_config_details</code> <code>Optional[Literal['text', 'diagram']]</code> <p>Flag indicating whether to display the configuration values after initialization. <code>Default: True</code> </p> <code>'text'</code> <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use during initialization. <code>Default: \"config\"</code></p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def import_config(self, identifier: str, display_config_details: Optional[Literal[\"text\", \"diagram\"]] = \"text\", workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\" \n    Import the dataset_config from a pickle file and initializes the dataset. Config type must correspond to dataset type.\n\n    First, it attempts to load the built-in config, if no built-in config with such an identifier exists, it attempts to load a custom config from the `\"data_root\"/tszoo/configs/` directory.\n\n    `data_root` is specified when the dataset is created.       \n\n    The following configuration attributes are used during initialization:\n\n    Dataset config | Description\n    -------------- | -----------\n    `init_workers` | Specifies the number of workers to use for initialization. Applied when `workers` = \"config\".\n    `partial_fit_initialized_transformers` | Determines whether initialized transformers should be partially fitted on the training data.\n    `nan_threshold` | Filters out time series with missing values exceeding the specified threshold.\n\n    Parameters:\n        identifier: Name of the pickle file.\n        display_config_details: Flag indicating whether to display the configuration values after initialization. `Default: True` \n        workers: The number of workers to use during initialization. `Default: \"config\"`  \n    \"\"\"\n\n    if display_config_details is not None:\n        display_config_details = DisplayType(display_config_details)\n\n    # Load config\n    config = load_config(identifier, self.metadata.configs_root, self.metadata.database_name, self.metadata.source_type, self.metadata.aggregation, self.logger)\n\n    self.logger.info(\"Initializing dataset configuration with the imported config.\")\n    self.set_dataset_config_and_initialize(config, display_config_details, workers)\n\n    self._update_config_imported_status(identifier)\n    self.logger.info(\"Successfully used config with identifier %s\", identifier)\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.save_annotations","title":"save_annotations","text":"<pre><code>save_annotations(identifier: str, on: AnnotationType | Literal['id_time', 'ts_id', 'both'], force_write: bool = False) -&gt; None\n</code></pre> <p>Saves the annotations as a CSV file.</p> <p>The file will be saved to a path determined by the <code>data_root</code> specified when the dataset was created.</p> <p>The annotations will be saved under the directory <code>data_root/tszoo/annotations/</code>.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The name of the CSV file.</p> required <code>on</code> <code>AnnotationType | Literal['id_time', 'ts_id', 'both']</code> <p>What annotation type should be saved. If set to <code>\"both\"</code>, annotations will be applied as if <code>id_time</code> and <code>ts_id</code> were both set.   </p> required <code>force_write</code> <code>bool</code> <p>If set to <code>True</code>, will overwrite any existing files with the same name. <code>Default: False</code></p> <code>False</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def save_annotations(self, identifier: str, on: AnnotationType | Literal[\"id_time\", \"ts_id\", \"both\"], force_write: bool = False) -&gt; None:\n    \"\"\" \n    Saves the annotations as a CSV file.\n\n    The file will be saved to a path determined by the `data_root` specified when the dataset was created.\n\n    The annotations will be saved under the directory `data_root/tszoo/annotations/`.\n\n    Parameters:\n        identifier: The name of the CSV file.\n        on: What annotation type should be saved. If set to `\"both\"`, annotations will be applied as if `id_time` and `ts_id` were both set.   \n        force_write: If set to `True`, will overwrite any existing files with the same name. `Default: False`               \n    \"\"\"\n\n    if exists_built_in_annotations(identifier):\n        raise ValueError(\"Built-in annotations with this identifier already exists. Choose another identifier.\")\n\n    on = AnnotationType(on)\n\n    temp_df = self.get_annotations(on)\n\n    # Ensure the annotations root directory exists, creating it if necessary\n    if not os.path.exists(self.metadata.annotations_root):\n        os.makedirs(self.metadata.annotations_root)\n        self.logger.info(\"Created annotations directory at %s\", self.metadata.annotations_root)\n\n    path = os.path.join(self.metadata.annotations_root, f\"{identifier}.csv\")\n\n    if os.path.exists(path) and not force_write:\n        raise ValueError(f\"Annotations already exist at {path}. Set force_write=True to overwrite.\")\n    self.logger.debug(\"Annotations CSV file path: %s\", path)\n\n    temp_df.to_csv(path, index=False)\n\n    self._update_annotations_imported_status(on, identifier)\n    self.logger.info(\"Annotations successfully saved to %s\", path)\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.save_config","title":"save_config","text":"<pre><code>save_config(identifier: str, create_with_details_file: bool = True, force_write: bool = False, **kwargs) -&gt; None\n</code></pre> <p>Saves the config as a pickle file.</p> <p>The file will be saved to a path determined by the <code>data_root</code> specified when the dataset was created.  The config will be saved under the directory <code>data_root/tszoo/configs/</code>.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The name of the pickle file.</p> required <code>create_with_details_file</code> <code>bool</code> <p>Whether to export the config along with a readable text file that provides details. <code>Defaults: True</code>. </p> <code>True</code> <code>force_write</code> <code>bool</code> <p>If set to <code>True</code>, will overwrite any existing files with the same name. <code>Default: False</code></p> <code>False</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def save_config(self, identifier: str, create_with_details_file: bool = True, force_write: bool = False, **kwargs) -&gt; None:\n    \"\"\" \n    Saves the config as a pickle file.\n\n    The file will be saved to a path determined by the `data_root` specified when the dataset was created. \n    The config will be saved under the directory `data_root/tszoo/configs/`.\n\n    Parameters:\n        identifier: The name of the pickle file.\n        create_with_details_file: Whether to export the config along with a readable text file that provides details. `Defaults: True`. \n        force_write: If set to `True`, will overwrite any existing files with the same name. `Default: False`            \n    \"\"\"\n\n    default_kwargs = {'hard_force': False}\n    kwargs = {**default_kwargs, **kwargs}\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to save config.\")\n\n    if not kwargs[\"hard_force\"] and exists_built_in_config(identifier):\n        raise ValueError(\"Built-in config with this identifier already exists. Choose another identifier.\")\n\n    # Ensure the config directory exists\n    if not os.path.exists(self.metadata.configs_root):\n        os.makedirs(self.metadata.configs_root)\n        self.logger.info(\"Created config directory at %s\", self.metadata.configs_root)\n\n    path_pickle = os.path.join(self.metadata.configs_root, f\"{identifier}.pickle\")\n    path_details = os.path.join(self.metadata.configs_root, f\"{identifier}.txt\")\n\n    if os.path.exists(path_pickle) and not force_write:\n        raise ValueError(f\"Config at path {path_pickle} already exists. Set force_write=True to overwrite.\")\n    self.logger.debug(\"Config pickle path: %s\", path_pickle)\n\n    if create_with_details_file:\n        if os.path.exists(path_details) and not force_write:\n            raise ValueError(f\"Config details at path {path_details} already exists. Set force_write=True to overwrite.\")\n        self.logger.debug(\"Config details path: %s\", path_details)\n\n    if not self.dataset_config.filler_factory.creates_built_in:\n        self.logger.warning(\"You are using a custom filler. Ensure the config is distributed with the source code of the filler.\")\n\n    if not self.dataset_config.anomaly_handler_factory.creates_built_in:\n        self.logger.warning(\"You are using a custom anomaly handler. Ensure the config is distributed with the source code of the anomaly handler.\")\n\n    if not self.dataset_config.transformer_factory.creates_built_in:\n        self.logger.warning(\"You are using a custom transformer. Ensure the config is distributed with the source code of the transformer.\")\n\n    if len(self.dataset_config.preprocess_order) != len(MANDATORY_PREPROCESSES_ORDER):\n        self.logger.warning(\"You are using at least one custom handler. Ensure the config is distributed with the source code of every custom handler.\")\n\n    pickle_dump(self._export_config_copy, path_pickle)\n    self.logger.info(\"Config pickle saved to %s\", path_pickle)\n\n    if create_with_details_file:\n        with open(path_details, \"w\", encoding=\"utf-8\") as file:\n            file.write(str(self.dataset_config))\n        self.logger.info(\"Config details saved to %s\", path_details)\n\n    self._update_config_imported_status(identifier)\n    self.dataset_config.export_update_needed = False\n    self.logger.info(\"Config successfully saved\")\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.save_benchmark","title":"save_benchmark","text":"<pre><code>save_benchmark(identifier: str, force_write: bool = False, **kwargs) -&gt; None\n</code></pre> <p>Saves the benchmark as a YAML file.</p> <p>The benchmark, along with any associated annotations and config files, will be saved in a path determined by the <code>data_root</code> specified when creating the dataset.  The default save path for benchmark is <code>\"data_root/tszoo/benchmarks/\"</code>.</p> <p>If you are using imported <code>annotations</code> or <code>config</code> (whether custom or built-in), their file names will be set in the <code>benchmark</code> file.  If new <code>annotations</code> or <code>config</code> are created during the process, their filenames will be derived from the provided <code>identifier</code> and set in the <code>benchmark</code> file.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The name of the YAML file.</p> required <code>force_write</code> <code>bool</code> <p>If set to <code>True</code>, will overwrite any existing files with the same name. <code>Default: False</code></p> <code>False</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def save_benchmark(self, identifier: str, force_write: bool = False, **kwargs) -&gt; None:\n    \"\"\" \n    Saves the benchmark as a YAML file.\n\n    The benchmark, along with any associated annotations and config files, will be saved in a path determined by the `data_root` specified when creating the dataset. \n    The default save path for benchmark is `\"data_root/tszoo/benchmarks/\"`.\n\n    If you are using imported `annotations` or `config` (whether custom or built-in), their file names will be set in the `benchmark` file. \n    If new `annotations` or `config` are created during the process, their filenames will be derived from the provided `identifier` and set in the `benchmark` file.\n\n    Parameters:\n        identifier: The name of the YAML file.\n        force_write: If set to `True`, will overwrite any existing files with the same name. `Default: False`            \n    \"\"\"\n\n    default_kwargs = {'hard_force': False}\n    kwargs = {**default_kwargs, **kwargs}\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to save benchmark.\")\n\n    if not kwargs[\"hard_force\"] and exists_built_in_benchmark(identifier):\n        raise ValueError(\"Built-in benchmark with this identifier already exists. Choose another identifier.\")\n\n    # Determine annotation names based on the available annotations and whether the annotations were imported\n    if len(self.annotations.time_series_annotations) &gt; 0:\n        annotations_ts_name = self.imported_annotations_ts_identifier if self.imported_annotations_ts_identifier is not None else f\"{identifier}_{AnnotationType.TS_ID.value}\"\n    else:\n        annotations_ts_name = None\n\n    if len(self.annotations.time_annotations) &gt; 0:\n        annotations_time_name = self.imported_annotations_time_identifier if self.imported_annotations_time_identifier is not None else f\"{identifier}_{AnnotationType.ID_TIME.value}\"\n    else:\n        annotations_time_name = None\n\n    if len(self.annotations.time_in_series_annotations) &gt; 0:\n        annotations_both_name = self.imported_annotations_both_identifier if self.imported_annotations_both_identifier is not None else f\"{identifier}_{AnnotationType.BOTH.value}\"\n    else:\n        annotations_both_name = None\n\n    # Use the imported identifier if available and update is not necessary, otherwise default to the current identifier\n    config_name = self.dataset_config.import_identifier if (self.dataset_config.import_identifier is not None and not self.dataset_config.export_update_needed) else identifier\n\n    export_benchmark = ExportBenchmark(self.metadata.database_name,\n                                       self.metadata.source_type.value,\n                                       self.metadata.aggregation.value,\n                                       self.metadata.dataset_type.value,\n                                       config_name,\n                                       annotations_ts_name,\n                                       annotations_time_name,\n                                       annotations_both_name,\n                                       related_results_identifier=self.related_to,\n                                       version=version.config_and_benchmarks_current_version)\n\n    # If the config was not imported, save it\n    if self.dataset_config.import_identifier is None or self.dataset_config.export_update_needed:\n        self.save_config(export_benchmark.config_identifier, force_write=force_write, hard_force=kwargs[\"hard_force\"])\n    else:\n        self.logger.info(\"Using already existing config with identifier: %s\", self.dataset_config.import_identifier)\n\n    # Save ts_id annotations if available and not previously imported\n    if self.imported_annotations_ts_identifier is None and len(self.annotations.time_series_annotations) &gt; 0:\n        self.save_annotations(export_benchmark.annotations_ts_identifier, AnnotationType.TS_ID, force_write=force_write)\n    elif self.imported_annotations_ts_identifier is not None:\n        self.logger.info(\"Using already existing annotations with identifier: %s; type: %s\", self.imported_annotations_ts_identifier, AnnotationType.TS_ID)\n\n    # Save id_time annotations if available and not previously imported\n    if self.imported_annotations_time_identifier is None and len(self.annotations.time_annotations) &gt; 0:\n        self.save_annotations(export_benchmark.annotations_time_identifier, AnnotationType.ID_TIME, force_write=force_write)\n    elif self.imported_annotations_time_identifier is not None:\n        self.logger.info(\"Using already existing annotations with identifier: %s; type: %s\", self.imported_annotations_time_identifier, AnnotationType.ID_TIME)\n\n    # Save both annotations if available and not previously imported\n    if self.imported_annotations_both_identifier is None and len(self.annotations.time_in_series_annotations) &gt; 0:\n        self.save_annotations(export_benchmark.annotations_both_identifier, AnnotationType.BOTH, force_write=force_write)\n    elif self.imported_annotations_both_identifier is not None:\n        self.logger.info(\"Using already existing annotations with identifier: %s; type: %s\", self.imported_annotations_both_identifier, AnnotationType.BOTH)\n\n    # Ensure the benchmark directory exists\n    if not os.path.exists(self.metadata.benchmarks_root):\n        os.makedirs(self.metadata.benchmarks_root)\n        self.logger.info(\"Created benchmarks directory at %s\", self.metadata.benchmarks_root)\n\n    benchmark_path = os.path.join(self.metadata.benchmarks_root, f\"{identifier}.yaml\")\n\n    if os.path.exists(benchmark_path) and not force_write:\n        self.logger.error(\"Benchmark file already exists at %s\", benchmark_path)\n        raise ValueError(f\"Benchmark at path {benchmark_path} already exists. Set force_write=True to overwrite.\")\n    self.logger.debug(\"Benchmark YAML file path: %s\", benchmark_path)\n\n    yaml_dump(export_benchmark.to_dict(), benchmark_path)\n    self.logger.info(\"Benchmark successfully saved to %s\", benchmark_path)\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.get_transformers","title":"get_transformers","text":"<pre><code>get_transformers() -&gt; np.ndarray[Transformer] | Transformer | None\n</code></pre> <p>Returns used transformers from config.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_transformers(self) -&gt; np.ndarray[Transformer] | Transformer | None:\n    \"\"\"Returns used transformers from config. \"\"\"\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting get transformers.\")\n\n    for i, preprocess_type in enumerate(self.dataset_config.preprocess_order):\n        if preprocess_type == PreprocessType.TRANSFORMING:\n            holder: TransformerHolder = self.dataset_config.train_preprocess_order[i].holder\n            return holder.transformers\n\n    return None\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.check_errors","title":"check_errors","text":"<pre><code>check_errors() -&gt; None\n</code></pre> <p>Validates whether the dataset is corrupted. </p> <p>Raises an exception if corrupted.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def check_errors(self) -&gt; None:\n    \"\"\"\n    Validates whether the dataset is corrupted. \n\n    Raises an exception if corrupted.\n    \"\"\"\n\n    dataset, _ = load_database(self.metadata.dataset_path)\n\n    try:\n        node_iter = dataset.walk_nodes()\n\n        # Process each node in the dataset\n        for node in node_iter:\n            if isinstance(node, tb.Table):\n\n                iter_by = min(LOADING_WARNING_THRESHOLD, len(node))\n                iters_done = 0\n\n                # Process the node in chunks to avoid memory issues\n                while iters_done &lt; len(node):\n                    iter_by = min(LOADING_WARNING_THRESHOLD, len(node) - iters_done)\n                    _ = node[iters_done: iters_done + iter_by]  # Fetch the data in chunks\n                    iters_done += iter_by\n\n                self.logger.info(\"Table '%s' checked successfully. (%d rows processed)\", node._v_pathname, len(node))\n\n        self.logger.info(\"Dataset check completed with no errors found.\")\n\n    except Exception as e:\n        self.logger.error(\"Error encountered during dataset check: %s\", str(e))\n\n    finally:\n        dataset.close()\n        self.logger.debug(\"Dataset connection closed.\")\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset.__get_data_for_plot","title":"__get_data_for_plot","text":"<pre><code>__get_data_for_plot(ts_id: int, features: list[str] | str, time_format: TimeFormat) -&gt; tuple[np.ndarray, np.ndarray, list[str]]\n</code></pre> <p>Returns prepared data for plotting.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def __get_data_for_plot(self, ts_id: int, features: list[str] | str, time_format: TimeFormat) -&gt; tuple[np.ndarray, np.ndarray, list[str]]:\n    \"\"\"Returns prepared data for plotting. \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before getting data for plotting.\")\n\n    features_indices = []\n\n    if features == \"config\":\n        features = deepcopy(self.dataset_config.features_to_take_without_ids)\n        features_indices = np.arange(len(features))\n        self.logger.debug(\"Features set from dataset config: %s\", features)\n    else:\n        if isinstance(features, str):\n            features = [features]\n\n        if len(features) == 0:\n            raise ValueError(\"No features specified to plot. Please provide valid features.\")\n        if len(set(features)) != len(features):\n            raise ValueError(\"Duplicate features detected. All features must be unique.\")\n\n        for feature in features:\n            if feature not in self.dataset_config.features_to_take_without_ids:\n                raise ValueError(f\"Feature '{feature}' is not valid. It is not present in the dataset configuration.\", self.dataset_config.features_to_take_without_ids)\n\n            index_in_config_features = self.dataset_config.features_to_take_without_ids.index(feature)\n            features_indices.append(index_in_config_features)\n\n    real_feature_indices = np.array(self.dataset_config.indices_of_features_to_take_no_ids)[features_indices]\n    real_feature_indices = real_feature_indices.astype(int)\n\n    time_series, time_period = self._get_data_for_plot(ts_id, real_feature_indices, time_format)\n    self.logger.debug(\"Time series data and corresponding time values retrieved.\")\n\n    return time_series, time_period, features\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset._validate_annotation_ids","title":"_validate_annotation_ids","text":"<pre><code>_validate_annotation_ids(ts_id: int | None, id_time: int | None) -&gt; None\n</code></pre> <p>Validates whether the <code>ts_id</code> and <code>id_time</code> belong to this dataset.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def _validate_annotation_ids(self, ts_id: int | None, id_time: int | None) -&gt; None:\n    \"\"\"Validates whether the `ts_id` and `id_time` belong to this dataset. \"\"\"\n\n    assert ts_id is not None or id_time is not None, \"Either ts_id or id_time must be provided.\"\n\n    # Handle when id_time is provided\n    if id_time is not None:\n        time_indices = self.metadata.time_indices\n        if id_time &lt; time_indices[ID_TIME_COLUMN_NAME][0] or id_time &gt; time_indices[ID_TIME_COLUMN_NAME][-1]:\n            valid_range = range(time_indices[ID_TIME_COLUMN_NAME][0], time_indices[ID_TIME_COLUMN_NAME][-1])\n            raise ValueError(f\"id_time {id_time} does not fall within the valid range for {self.metadata.aggregation}. \"\n                             f\"Valid id_time range: {valid_range}.\")\n\n    # Handle when ts_id is provided\n    if ts_id is not None:\n        ts_indices = self.metadata.ts_indices[self.metadata.ts_id_name]\n\n        if ts_id not in ts_indices:\n            valid_ts_range = self.metadata.ts_indices[self.metadata.ts_id_name]\n            raise ValueError(f\"ts_id {ts_id} does not exist in the available range for {self.metadata.source_type}. \"\n                             f\"Valid ts_id values: {valid_ts_range}.\")\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset._get_df","title":"_get_df","text":"<pre><code>_get_df(dataloader: DataLoader, as_single_dataframe: bool, ts_ids: ndarray, time_period: ndarray) -&gt; pd.DataFrame\n</code></pre> <p>Returns all data from the DataLoader as a Pandas <code>DataFrame</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def _get_df(self, dataloader: DataLoader, as_single_dataframe: bool, ts_ids: np.ndarray, time_period: np.ndarray) -&gt; pd.DataFrame:\n    \"\"\"Returns all data from the DataLoader as a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html). \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before getting DataFrame.\")\n\n    total_samples = len(ts_ids) * len(time_period)\n    if total_samples &gt;= LOADING_WARNING_THRESHOLD:\n        self.logger.warning(\"The dataset contains %d samples (%d time series \u00d7 %d times). Consider using get_*_dataloader() for batch loading.\", total_samples, len(ts_ids), len(time_period))\n\n    if as_single_dataframe:\n        self.logger.debug(\"Returning a single DataFrame with all features for all time series.\")\n        return dataset_loaders.create_single_df_from_dataloader(\n            dataloader,\n            ts_ids,\n            self.dataset_config.features_to_take,\n            self.dataset_config.time_format,\n            self.dataset_config.include_ts_id,\n            self.dataset_config.include_time,\n            self.dataset_config.dataset_type,\n            True\n        )\n    else:\n        self.logger.debug(\"Returning multiple DataFrames, one per time series.\")\n        return dataset_loaders.create_multiple_df_from_dataloader(\n            dataloader,\n            ts_ids,\n            self.dataset_config.features_to_take,\n            self.dataset_config.time_format,\n            self.dataset_config.include_ts_id,\n            self.dataset_config.include_time,\n            self.dataset_config.dataset_type,\n            True\n        )\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset._get_numpy","title":"_get_numpy","text":"<pre><code>_get_numpy(dataloader: DataLoader, ts_ids: ndarray, time_period: ndarray) -&gt; np.ndarray\n</code></pre> <p>Returns all data from the DataLoader as a NumPy <code>ndarray</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def _get_numpy(self, dataloader: DataLoader, ts_ids: np.ndarray, time_period: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Returns all data from the DataLoader as a NumPy `ndarray`. \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before getting Numpy array.\")\n\n    total_samples = len(ts_ids) * len(time_period)\n    if total_samples &gt;= LOADING_WARNING_THRESHOLD:\n        self.logger.warning(\"The dataset contains %d samples (%d time series \u00d7 %d times). Consider using get_*_dataloader() for batch loading.\", total_samples, len(ts_ids), len(time_period))\n\n    self.logger.debug(\"Creating numpy array from dataloader.\")\n    return dataset_loaders.create_numpy_from_dataloader(\n        dataloader,\n        ts_ids,\n        self.dataset_config.time_format,\n        self.dataset_config.include_time,\n        self.dataset_config.dataset_type,\n        True\n    )\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset._clear","title":"_clear","text":"<pre><code>_clear() -&gt; None\n</code></pre> <p>Clears set data. Mainly called when initializing new config.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def _clear(self) -&gt; None:\n    \"\"\"Clears set data. Mainly called when initializing new config. \"\"\"\n    self.train_dataset = None\n    self.train_dataloader = None\n    self.val_dataset = None\n    self.val_dataloader = None\n    self.test_dataset = None\n    self.test_dataloader = None\n    self.all_dataset = None\n    self.all_dataloader = None\n    self.dataset_config = None\n    self.logger.debug(\"Dataset attributes had been cleared. \")\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset._update_annotations_imported_status","title":"_update_annotations_imported_status","text":"<pre><code>_update_annotations_imported_status(on: AnnotationType, identifier: str)\n</code></pre> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def _update_annotations_imported_status(self, on: AnnotationType, identifier: str):\n    if on == AnnotationType.TS_ID:\n        self.imported_annotations_ts_identifier = identifier\n    elif on == AnnotationType.ID_TIME:\n        self.imported_annotations_time_identifier = identifier\n    elif on == AnnotationType.BOTH:\n        self.imported_annotations_both_identifier = identifier\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset._update_config_imported_status","title":"_update_config_imported_status","text":"<pre><code>_update_config_imported_status(identifier: str) -&gt; None\n</code></pre> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def _update_config_imported_status(self, identifier: str) -&gt; None:\n    self.dataset_config.import_identifier = identifier\n    self._export_config_copy.import_identifier = identifier\n</code></pre>"},{"location":"reference_disjoint_time_based_cesnet_dataset/#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset._validate_config_for_dataset","title":"_validate_config_for_dataset","text":"<pre><code>_validate_config_for_dataset(config: DatasetConfig) -&gt; bool\n</code></pre> <p>Validates whether config is supposed to be used for this dataset.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def _validate_config_for_dataset(self, config: DatasetConfig) -&gt; bool:\n    \"\"\"Validates whether config is supposed to be used for this dataset. \"\"\"\n\n    if config.database_name != self.metadata.database_name:\n        self.logger.error(\"This config is not compatible with the current dataset. Difference in database name between config and this dataset.\")\n        raise ValueError(\"This config is not compatible with the current dataset.\", f\"config.database_name == {config.database_name} and dataset.database_name == {self.metadata.database_name}\")\n\n    if config.dataset_type != self.metadata.dataset_type:\n        self.logger.error(\"This config is not compatible with the current dataset. Difference in is_series_based between config and this dataset.\")\n        raise ValueError(\"This config is not compatible with the current dataset.\", f\"config.dataset_type == {config.dataset_type} and dataset.dataset_type == {self.metadata.dataset_type}\")\n\n    if config.aggregation != self.metadata.aggregation:\n        self.logger.error(\"This config is not compatible with the current dataset. Difference in aggregation type between config and this dataset.\")\n        raise ValueError(\"This config is not compatible with the current dataset.\", f\"config.aggregation == {config.aggregation} and dataset.aggregation == {self.metadata.aggregation}\")\n\n    if config.source_type != self.metadata.source_type:\n        self.logger.error(\"This config is not compatible with the current dataset. Difference in source type between config and this dataset.\")\n        raise ValueError(\"This config is not compatible with the current dataset.\", f\"config.source_type == {config.source_type} and dataset.source_type == {self.metadata.source_type}\")\n</code></pre>"},{"location":"reference_disjoint_time_based_config/","title":"Disjoint-time-based config class","text":""},{"location":"reference_disjoint_time_based_config/#cesnet_tszoo.configs.disjoint_time_based_config.DisjointTimeBasedConfig","title":"<code>cesnet_tszoo.configs.disjoint_time_based_config.DisjointTimeBasedConfig</code>","text":"<p>               Bases: <code>SeriesBasedHandler</code>, <code>TimeBasedHandler</code>, <code>DatasetConfig</code></p> <p>This class is used for configuring the <code>DisjointTimeBasedCesnetDataset</code>.</p> <p>Used to configure the following:</p> <ul> <li>Train, validation, test, all sets (time period, sizes, features, window size)</li> <li>Handling missing values (default values, <code>fillers</code>)</li> <li>Handling anomalies (<code>anomaly handlers</code>)</li> <li>Data transformation using <code>transformers</code></li> <li>Applying custom handlers (<code>custom handlers</code>)</li> <li>Changing order of preprocesses</li> <li>Dataloader options (train/val/test/all/init workers, batch sizes)</li> <li>Plotting</li> </ul> <p>Important Notes:</p> <ul> <li>Custom fillers must inherit from the <code>fillers</code> base class.</li> <li>Custom anomaly handlers must inherit from the <code>anomaly handlers</code> base class.</li> <li>Selected anomaly handler is only used for train set.</li> <li>It is recommended to use the <code>transformers</code> base class, though this is not mandatory as long as it meets the required methods.<ul> <li>If a transformer is already initialized and <code>partial_fit_initialized_transformers</code> is <code>False</code>, the transformer does not require <code>partial_fit</code>.</li> <li>Otherwise, the transformer must support <code>partial_fit</code>.</li> <li>Transformers must implement <code>transform</code> method.</li> <li>Both <code>partial_fit</code> and <code>transform</code> methods must accept an input of type <code>np.ndarray</code> with shape <code>(times, features)</code>.</li> </ul> </li> <li>Custom handlers must be derived from one of the built-in <code>custom handler</code> classes </li> <li><code>train_time_period</code>, <code>val_time_period</code>, <code>test_time_period</code> can overlap, but they should keep order of <code>train_time_period</code> &lt; <code>val_time_period</code> &lt; <code>test_time_period</code></li> </ul> Source code in <code>cesnet_tszoo\\configs\\disjoint_time_based_config.py</code> <pre><code>class DisjointTimeBasedConfig(SeriesBasedHandler, TimeBasedHandler, DatasetConfig):\n    \"\"\"\n    This class is used for configuring the [`DisjointTimeBasedCesnetDataset`](reference_disjoint_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.disjoint_time_based_cesnet_dataset.DisjointTimeBasedCesnetDataset).\n\n    Used to configure the following:\n\n    - Train, validation, test, all sets (time period, sizes, features, window size)\n    - Handling missing values (default values, [`fillers`](reference_fillers.md#cesnet_tszoo.utils.filler.filler))\n    - Handling anomalies ([`anomaly handlers`](reference_anomaly_handlers.md#cesnet_tszoo.utils.anomaly_handler.anomaly_handler))\n    - Data transformation using [`transformers`](reference_transformers.md#cesnet_tszoo.utils.transformer.transformer)\n    - Applying custom handlers ([`custom handlers`](reference_custom_handlers.md#cesnet_tszoo.utils.custom_handler.custom_handler))\n    - Changing order of preprocesses\n    - Dataloader options (train/val/test/all/init workers, batch sizes)\n    - Plotting\n\n    **Important Notes:**\n\n    - Custom fillers must inherit from the [`fillers`](reference_fillers.md#cesnet_tszoo.utils.filler.filler.Filler) base class.\n    - Custom anomaly handlers must inherit from the [`anomaly handlers`](reference_anomaly_handlers.md#cesnet_tszoo.utils.anomaly_handler.anomaly_handler.AnomalyHandler) base class.\n    - Selected anomaly handler is only used for train set.\n    - It is recommended to use the [`transformers`](reference_transformers.md#cesnet_tszoo.utils.transformer.transformer.Transformer) base class, though this is not mandatory as long as it meets the required methods.\n        - If a transformer is already initialized and `partial_fit_initialized_transformers` is `False`, the transformer does not require `partial_fit`.\n        - Otherwise, the transformer must support `partial_fit`.\n        - Transformers must implement `transform` method.\n        - Both `partial_fit` and `transform` methods must accept an input of type `np.ndarray` with shape `(times, features)`.\n    - Custom handlers must be derived from one of the built-in [`custom handler`](reference_custom_handlers.md#cesnet_tszoo.utils.custom_handler.custom_handler) classes \n    - `train_time_period`, `val_time_period`, `test_time_period` can overlap, but they should keep order of `train_time_period` &lt; `val_time_period` &lt; `test_time_period`\n\n    Attributes:\n        used_train_workers: Tracks the number of train workers in use. Helps determine if the train dataloader should be recreated based on worker changes.\n        used_val_workers: Tracks the number of validation workers in use. Helps determine if the validation dataloader should be recreated based on worker changes.\n        used_test_workers: Tracks the number of test workers in use. Helps determine if the test dataloader should be recreated based on worker changes.\n        uses_all_time_period: Whether all time period set should be used.\n        uses_all_ts: Whether all time series set should be used.\n        import_identifier: Tracks the name of the config upon import. None if not imported.\n        filler_factory: Represents factory used to create passed Filler type.\n        anomaly_handler_factory: Represents factory used to create passed Anomaly Handler type.\n        transformer_factory: Represents factory used to create passed Transformer type.\n        can_fit_fillers: Whether fillers in this config, can be fitted.        \n        logger: Logger for displaying information.     \n        display_train_time_period: Used to display the configured value of `train_time_period`.\n        display_val_time_period: Used to display the configured value of `val_time_period`.\n        display_test_time_period: Used to display the configured value of `test_time_period`.\n        train_ts_row_ranges: Initialized when `train_ts` is set. Contains time series IDs in train set with their respective time ID ranges.\n        val_ts_row_ranges: Initialized when `val_ts` is set. Contains time series IDs in validation set with their respective time ID ranges.\n        test_ts_row_ranges: Initialized when `test_ts` is set. Contains time series IDs in test set with their respective time ID ranges.        \n        all_time_period: Contains total used time period.\n        all_ts: Contains all used time series.\n        all_ts_row_ranges: Contains time series IDs in all set with their respective time ID ranges.\n        aggregation: The aggregation period used for the data.\n        source_type: The source type of the data.\n        database_name: Specifies which database this config applies to.\n        features_to_take_without_ids: Features to be returned, excluding time or time series IDs.\n        indices_of_features_to_take_no_ids: Indices of non-ID features in `features_to_take`.\n        ts_id_name: Name of the time series ID, dependent on `source_type`.\n        used_singular_train_time_series: Currently used singular train set time series for dataloader.\n        used_singular_val_time_series: Currently used singular validation set time series for dataloader.\n        used_singular_test_time_series: Currently used singular test set time series for dataloader.     \n        train_preprocess_order: All preprocesses used for train set. \n        val_preprocess_order: All preprocesses used for val set. \n        test_preprocess_order: All preprocesses used for test set.      \n        is_initialized: Flag indicating if the configuration has already been initialized. If true, config initialization will be skipped.  \n        version: Version of cesnet-tszoo this config was made in.\n        export_update_needed: Whether config was updated to newer version and should be exported.     \n        train_ts: Defines which time series IDs are used in the training set. Can be a list of IDs, or an integer/float to specify a random selection. An `int` specifies the number of random time series, and a `float` specifies the proportion of available time series. \n                  `int` and `float` must be greater than 0, and a float should be smaller or equal to 1.0. Using `int` or `float` guarantees that no time series from other sets will be used. Must be used with `train_time_period`.\n        val_ts: Defines which time series IDs are used in the validation set. Same as `train_ts` but for the validation set. Must be used with `val_time_period`.\n        test_ts: Defines which time series IDs are used in the test set. Same as `train_ts` but for the test set. Must be used with `test_time_period`.\n        train_time_period: Defines the time period for training set. Can be a range of time IDs or a tuple of datetime objects. Float value is equivalent to percentage of available times with offseted position from previous used set. Must be used with `train_ts`.\n        val_time_period: Defines the time period for validation set. Can be a range of time IDs or a tuple of datetime objects. Float value is equivalent to percentage of available times with offseted position from previous used set. Must be used with `val_ts`.\n        test_time_period: Defines the time period for test set. Can be a range of time IDs or a tuple of datetime objects. Must be used with `test_ts`.\n        features_to_take: Defines which features are used.           \n        default_values: Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature.\n        sliding_window_size: Number of times in one window. Impacts dataloader behavior. Batch sizes affects how much data will be cached for creating windows.\n        sliding_window_prediction_size: Number of times to predict from sliding_window_size. Impacts dataloader behavior. Batch sizes affects how much data will be cached for creating windows.\n        sliding_window_step: Number of times to move by after each window.\n        set_shared_size: How much times should time periods share. Order of sharing is training set &lt; validation set &lt; test set. Only in effect if sets share less values than set_shared_size. Use float value for percentage of total times or int for count.\n        train_batch_size: Batch size for the train dataloader. Affects number of returned times in one batch.\n        val_batch_size: Batch size for the validation dataloader. Affects number of returned times in one batch.\n        test_batch_size: Batch size for the test dataloader. Affects number of returned times in one batch.\n        preprocess_order: Defines in which order preprocesses are used. Also can add to order a type of `AllSeriesCustomHandler` or `NoFitCustomHandler`.\n        partial_fit_initialized_transformers: If `True`, partial fitting on train set is performed when using initiliazed transformers.\n        include_time: If `True`, time data is included in the returned values.\n        include_ts_id: If `True`, time series IDs are included in the returned values.\n        time_format: Format for the returned time data. When using TimeFormat.DATETIME, time will be returned as separate list along rest of the values.\n        train_workers: Number of workers for loading training data. `0` means that the data will be loaded in the main process.\n        val_workers: Number of workers for loading validation data. `0` means that the data will be loaded in the main process.\n        test_workers: Number of workers for loading test. `0` means that the data will be loaded in the main process.\n        init_workers: Number of workers for initial dataset processing during configuration. `0` means that the data will be loaded in the main process.\n        nan_threshold: Maximum allowable percentage of missing data. Time series exceeding this threshold are excluded. Time series over the threshold will not be used. Used for `train/val/test/all` separately.\n        random_state: Fixes randomness for reproducibility during configuration and dataset initialization.              \n    \"\"\"\n\n    def __init__(self,\n                 train_ts: list[int] | npt.NDArray[np.int_] | float | int | None,\n                 val_ts: list[int] | npt.NDArray[np.int_] | float | int | None,\n                 test_ts: list[int] | npt.NDArray[np.int_] | float | int | None,\n                 train_time_period: tuple[datetime, datetime] | range | float | None = None,\n                 val_time_period: tuple[datetime, datetime] | range | float | None = None,\n                 test_time_period: tuple[datetime, datetime] | range | float | None = None,\n                 features_to_take: list[str] | Literal[\"all\"] = \"all\",\n                 default_values: list[Number] | npt.NDArray[np.number] | dict[str, Number] | Number | Literal[\"default\"] | None = \"default\",\n                 sliding_window_size: int | None = None,\n                 sliding_window_prediction_size: int | None = None,\n                 sliding_window_step: int = 1,\n                 set_shared_size: float | int = 0,\n                 train_batch_size: int = 32,\n                 val_batch_size: int = 64,\n                 test_batch_size: int = 128,\n                 preprocess_order: list[str, type] = [\"handling_anomalies\", \"filling_gaps\", \"transforming\"],\n                 fill_missing_with: type | FillerType | Literal[\"mean_filler\", \"forward_filler\", \"linear_interpolation_filler\"] | None = None,\n                 transform_with: type | list[Transformer] | np.ndarray[Transformer] | TransformerType | Transformer | Literal[\"min_max_scaler\", \"standard_scaler\", \"max_abs_scaler\", \"log_transformer\", \"l2_normalizer\"] | None = None,\n                 handle_anomalies_with: type | AnomalyHandlerType | Literal[\"z-score\", \"interquartile_range\"] | None = None,\n                 partial_fit_initialized_transformer: bool = False,\n                 include_time: bool = True,\n                 include_ts_id: bool = True,\n                 time_format: TimeFormat | Literal[\"id_time\", \"datetime\", \"unix_time\", \"shifted_unix_time\"] = TimeFormat.ID_TIME,\n                 train_workers: int = 4,\n                 val_workers: int = 3,\n                 test_workers: int = 2,\n                 init_workers: int = 4,\n                 nan_threshold: float = 1.0,\n                 random_state: int | None = None):\n        \"\"\"\n        Parameters:\n            train_ts: Defines which time series IDs are used in the training set. Can be a list of IDs, or an integer/float to specify a random selection. An `int` specifies the number of random time series, and a `float` specifies the proportion of available time series. \n                    `int` and `float` must be greater than 0, and a float should be smaller or equal to 1.0. Using `int` or `float` guarantees that no time series from other sets will be used. Must be used with `train_time_period`.\n            val_ts: Defines which time series IDs are used in the validation set. Same as `train_ts` but for the validation set. Must be used with `val_time_period`.\n            test_ts: Defines which time series IDs are used in the test set. Same as `train_ts` but for the test set. Must be used with `test_time_period`.\n            train_time_period: Defines the time period for training set. Can be a range of time IDs or a tuple of datetime objects. Float value is equivalent to percentage of available times with offseted position from previous used set. Must be used with `train_ts`. `Default: None`\n            val_time_period: Defines the time period for validation set. Can be a range of time IDs or a tuple of datetime objects. Float value is equivalent to percentage of available times with offseted position from previous used set. Must be used with `val_ts`. `Default: None`\n            test_time_period: Defines the time period for test set. Can be a range of time IDs or a tuple of datetime objects. Must be used with `test_ts`. `Default: None`\n            features_to_take: Defines which features are used. `Default: \"all\"`                  \n            default_values: Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature. `Default: \"default\"`\n            sliding_window_size: Number of times in one window. Impacts dataloader behavior. Batch sizes affects how much data will be cached for creating windows. `Default: None`\n            sliding_window_prediction_size: Number of times to predict from sliding_window_size. Impacts dataloader behavior. Batch sizes affects how much data will be cached for creating windows. `Default: None`\n            sliding_window_step: Number of times to move by after each window. `Default: 1`\n            set_shared_size: How much times should time periods share. Order of sharing is training set &lt; validation set &lt; test set. Only in effect if sets share less values than set_shared_size. Use float value for percentage of total times or int for count. `Default: 0`\n            train_batch_size: Batch size for the train dataloader. Affects number of returned times in one batch. `Default: 32`\n            val_batch_size: Batch size for the validation dataloader. Affects number of returned times in one batch. `Default: 64`\n            test_batch_size: Batch size for the test dataloader. Affects number of returned times in one batch. `Default: 128`\n            preprocess_order: Defines in which order preprocesses are used. Also can add to order a type of `AllSeriesCustomHandler` or `NoFitCustomHandler`. `Default: [\"handling_anomalies\", \"filling_gaps\", \"transforming\"]`\n            fill_missing_with: Defines how to fill missing values in the dataset. Can pass enum `FillerType` for built-in filler or pass a type of custom filler that must derive from `Filler` base class. `Default: None`        \n            transform_with: Defines the transformer used to transform the dataset. Can pass enum `TransformerType`, pass a type of custom transformer or instance of already fitted transformer(s). `Default: None`\n            handle_anomalies_with: Defines the anomaly handler for handling anomalies in the train set. Can pass enum `AnomalyHandlerType` for built-in anomaly handler or a type of custom anomaly handler. `Default: None`\n            partial_fit_initialized_transformer: If `True`, partial fitting on train set is performed when using initiliazed transformers. `Default: False`\n            include_time: If `True`, time data is included in the returned values. `Default: True`\n            include_ts_id: If `True`, time series IDs are included in the returned values. `Default: True`\n            time_format: Format for the returned time data. When using TimeFormat.DATETIME, time will be returned as separate list along rest of the values. `Default: TimeFormat.ID_TIME`\n            train_workers: Number of workers for loading training data. `0` means that the data will be loaded in the main process. `Default: 4`\n            val_workers: Number of workers for loading validation data. `0` means that the data will be loaded in the main process. `Default: 3`\n            test_workers: Number of workers for loading test. `0` means that the data will be loaded in the main process. `Default: 2`\n            init_workers: Number of workers for initial dataset processing during configuration. `0` means that the data will be loaded in the main process. `Default: 4`\n            nan_threshold: Maximum allowable percentage of missing data. Time series exceeding this threshold are excluded. Time series over the threshold will not be used. Used for `train/val/test/all` separately. `Default: 1.0`\n            random_state: Fixes randomness for reproducibility during configuration and dataset initialization. `Default: None`   \n        \"\"\"\n\n        self.logger = logging.getLogger(\"disjoint_time_based_config\")\n\n        TimeBasedHandler.__init__(self, self.logger, train_batch_size, val_batch_size, test_batch_size, 1, False, sliding_window_size, sliding_window_prediction_size, sliding_window_step, set_shared_size, train_time_period, val_time_period, test_time_period)\n        SeriesBasedHandler.__init__(self, self.logger, True, train_ts, val_ts, test_ts)\n        DatasetConfig.__init__(self, features_to_take, default_values, train_batch_size, val_batch_size, test_batch_size, 1, preprocess_order, fill_missing_with, transform_with, handle_anomalies_with, partial_fit_initialized_transformer, include_time, include_ts_id, time_format,\n                               train_workers, val_workers, test_workers, 1, init_workers, nan_threshold, False, DatasetType.DISJOINT_TIME_BASED, DataloaderOrder.SEQUENTIAL, random_state, False, self.logger)\n\n    def _validate_construction(self) -&gt; None:\n        \"\"\"Performs basic parameter validation to ensure correct configuration. More comprehensive validation, which requires dataset-specific data, is handled in [`_dataset_init`][cesnet_tszoo.configs.disjoint_time_based_config.DisjointTimeBasedConfig._dataset_init]. \"\"\"\n\n        DatasetConfig._validate_construction(self)\n\n        if self.train_ts is None or self.train_time_period is None:\n            if self.train_ts is not None:\n                self.logger.error(\"When train_ts is not None you must set train_time_period or set train_ts as None.\")\n                raise ValueError(\"When train_ts is not None you must set train_time_period or set train_ts as None.\")\n            if self.train_time_period is not None:\n                self.logger.error(\"When train_time_period is not None you must set train_ts or set train_time_period as None.\")\n                raise ValueError(\"When train_time_period is not None you must set train_ts or set train_time_period as None.\")\n\n        if self.val_ts is None or self.val_time_period is None:\n            if self.val_ts is not None:\n                self.logger.error(\"When val_ts is not None you must set val_time_period or set val_ts as None.\")\n                raise ValueError(\"When val_ts is not None you must set val_time_period or set val_ts as None.\")\n            if self.val_time_period is not None:\n                self.logger.error(\"When val_time_period is not None you must set val_ts or set val_time_period as None.\")\n                raise ValueError(\"When val_time_period is not None you must set val_ts or set val_time_period as None.\")\n\n        if self.test_ts is None or self.test_time_period is None:\n            if self.test_ts is not None:\n                self.logger.error(\"When test_ts is not None you must set test_time_period or set test_ts as None.\")\n                raise ValueError(\"When test_ts is not None you must set test_time_period or set test_ts as None.\")\n            if self.test_time_period is not None:\n                self.logger.error(\"When test_time_period is not None you must set test_ts or set test_time_period as None.\")\n                raise ValueError(\"When test_time_period is not None you must set test_ts or set test_time_period as None.\")\n\n        if self.train_ts is None and self.val_ts is None and self.test_ts is None:\n            self.logger.error(\"No set for time series has been set. You must set at least one time series set and its respective time period.\")\n            raise ValueError(\"No set for time series has been set. You must set at least one time series set and its respective time period.\")\n\n        self._validate_time_periods_init()\n        self._validate_ts_init()\n        self._validate_set_shared_size_init()\n        self._validate_sliding_window_init()\n        self._update_batch_sizes(self.train_batch_size, self.val_batch_size, self.test_batch_size, self.all_batch_size)\n\n        self.logger.debug(\"Disjoint-time-based configuration validated successfully.\")\n\n    def _update_batch_sizes(self, train_batch_size: int, val_batch_size: int, test_batch_size: int, all_batch_size: int) -&gt; None:\n\n        # Adjust batch sizes based on sliding_window_size\n        if self.sliding_window_size is not None:\n\n            if self.sliding_window_step &lt;= 0:\n                raise ValueError(\"sliding_window_step must be greater or equal to 1.\")\n\n            total_window_size = self.sliding_window_size + self.sliding_window_prediction_size\n\n            if isinstance(self.train_batch_size, int) and total_window_size &gt; self.train_batch_size:\n                train_batch_size = self.sliding_window_size + self.sliding_window_prediction_size\n                self.logger.info(\"train_batch_size adjusted to %s as it should be greater than or equal to sliding_window_size + sliding_window_prediction_size.\", total_window_size)\n            if isinstance(self.val_batch_size, int) and total_window_size &gt; self.val_batch_size:\n                val_batch_size = self.sliding_window_size + self.sliding_window_prediction_size\n                self.logger.info(\"val_batch_size adjusted to %s as it should be greater than or equal to sliding_window_size + sliding_window_prediction_size.\", total_window_size)\n            if isinstance(self.test_batch_size, int) and total_window_size &gt; self.test_batch_size:\n                test_batch_size = self.sliding_window_size + self.sliding_window_prediction_size\n                self.logger.info(\"test_batch_size adjusted to %s as it should be greater than or equal to sliding_window_size + sliding_window_prediction_size.\", total_window_size)\n\n        DatasetConfig._update_batch_sizes(self, train_batch_size, val_batch_size, test_batch_size, all_batch_size)\n\n    def _update_sliding_window(self, sliding_window_size: int | None, sliding_window_prediction_size: int | None, sliding_window_step: int | None, set_shared_size: float | int, all_time_ids: np.ndarray):\n        \"\"\"Updates values related to sliding window. \"\"\"\n        TimeBasedHandler._update_sliding_window(self, sliding_window_size, sliding_window_prediction_size, sliding_window_step, set_shared_size, all_time_ids, self.has_train(), self.has_val(), self.has_test(), self.has_all())\n\n    def _get_train(self) -&gt; tuple[np.ndarray, np.ndarray] | tuple[None, None]:\n        \"\"\"Returns the indices corresponding to the training set. \"\"\"\n        return self.train_ts, self.train_time_period\n\n    def _get_val(self) -&gt; tuple[np.ndarray, np.ndarray] | tuple[None, None]:\n        \"\"\"Returns the indices corresponding to the validation set. \"\"\"\n        return self.val_ts, self.val_time_period\n\n    def _get_test(self) -&gt; tuple[np.ndarray, np.ndarray] | tuple[None, None]:\n        \"\"\"Returns the indices corresponding to the test set. \"\"\"\n        return self.test_ts, self.test_time_period\n\n    def _get_all(self) -&gt; tuple[np.ndarray, np.ndarray] | tuple[None, None]:\n        \"\"\"Returns the indices corresponding to the all set. \"\"\"\n        return None, None\n\n    def has_train(self) -&gt; bool:\n        \"\"\"Returns whether training set is used. \"\"\"\n        return self.train_ts is not None and self.train_time_period is not None\n\n    def has_val(self) -&gt; bool:\n        \"\"\"Returns whether validation set is used. \"\"\"\n        return self.val_ts is not None and self.val_time_period is not None\n\n    def has_test(self) -&gt; bool:\n        \"\"\"Returns whether test set is used. \"\"\"\n        return self.test_ts is not None and self.test_time_period is not None\n\n    def has_all(self) -&gt; bool:\n        \"\"\"Returns whether all set is used. \"\"\"\n        return False\n\n    def _set_time_period(self, all_time_ids: np.ndarray) -&gt; None:\n        \"\"\"Validates and filters `train_time_period`, `val_time_period`, `test_time_period` and `all_time_period` based on `dataset` and `aggregation`. \"\"\"\n\n        self._prepare_and_set_time_period_sets(all_time_ids, self.time_format)\n\n    def _set_ts(self, all_ts_ids: np.ndarray, all_ts_row_ranges: np.ndarray, rd: np.random.RandomState) -&gt; None:\n        \"\"\" Validates and filters inputted time series id from `train_ts`, `val_ts` and `test_ts` based on `dataset` and `source_type`. Handles random set.\"\"\"\n\n        self._prepare_and_set_ts_sets(all_ts_ids, all_ts_row_ranges, self.ts_id_name, self.random_state, rd)\n\n    def _get_feature_transformers(self) -&gt; Transformer:\n        \"\"\"Creates transformer with `transformer_factory`. \"\"\"\n\n        if self.transformer_factory.has_already_initialized:\n            if not self.has_train() and self.partial_fit_initialized_transformers:\n                self.partial_fit_initialized_transformers = False\n                self.logger.warning(\"partial_fit_initialized_transformers will be ignored because train set is not used.\")\n\n            transformers = self.transformer_factory.get_already_initialized_transformers()\n            self.logger.debug(\"Using already initialized transformer %s.\", self.transformer_factory.name)\n\n        else:\n            if not self.has_train() and not self.transformer_factory.is_empty_factory:\n                self.transformer_factory = transformer_factories.get_transformer_factory(None, self.create_transformer_per_time_series, self.partial_fit_initialized_transformers)\n                self.logger.warning(\"No transformer will be used because train set is not used.\")\n\n            transformers = self.transformer_factory.create_transformer()\n            self.logger.debug(\"Using transformer %s.\", self.transformer_factory.name)\n\n        return transformers\n\n    def _get_fillers(self) -&gt; tuple:\n        \"\"\"Creates fillers with `filler_factory`. \"\"\"\n\n        train_fillers = None\n        # Set the fillers for the training set\n        if self.has_train():\n            train_fillers = np.array([self.filler_factory.create_filler(self.features_to_take_without_ids) for _ in self.train_ts])\n            self.logger.debug(\"Fillers for training set are set.\")\n\n        val_fillers = None\n        # Set the fillers for the validation set\n        if self.has_val():\n            val_fillers = np.array([self.filler_factory.create_filler(self.features_to_take_without_ids) for _ in self.val_ts])\n            self.logger.debug(\"Fillers for validation set are set.\")\n\n        test_fillers = None\n        # Set the fillers for the test set\n        if self.has_test():\n            test_fillers = np.array([self.filler_factory.create_filler(self.features_to_take_without_ids) for _ in self.test_ts])\n            self.logger.debug(\"Fillers for test set are set.\")\n\n        self.logger.debug(\"Using filler %s\", self.filler_factory.name)\n\n        return train_fillers, val_fillers, test_fillers, None\n\n    def _get_anomaly_handlers(self) -&gt; np.ndarray:\n        \"\"\"Creates anomaly handlers with `anomaly_handler_factory`. \"\"\"\n\n        if not self.has_train() and not self.anomaly_handler_factory.is_empty_factory:\n            self.anomaly_handler_factory = anomaly_handler_factories.get_anomaly_handler_factory(None)\n            self.logger.warning(\"No anomaly handler will be used because train set is not used.\")\n\n        anomaly_handlers = None\n        if self.has_train():\n            anomaly_handlers = np.array([self.anomaly_handler_factory.create_anomaly_handler() for _ in self.train_ts])\n\n        self.logger.debug(\"Using anomaly handler %s\", self.anomaly_handler_factory.name)\n\n        return anomaly_handlers\n\n    def _set_per_series_custom_handler(self, factory: PerSeriesCustomHandlerFactory):\n        raise ValueError(f\"Cannot use {factory.name} CustomHandler, because PerSeriesCustomHandler is not supported for {self.dataset_type}. Use AllSeriesCustomHandler or NoFitCustomHandler instead. \")\n\n    def _set_no_fit_custom_handler(self, factory: NoFitCustomHandlerFactory):\n\n        train_handlers = np.array([factory.create_handler() for _ in self.train_ts]) if self.has_train() else None\n        self.train_preprocess_order.append(PreprocessNote(factory.preprocess_enum_type, False, False, factory.can_apply_to_train and self.has_train(), True, NoFitCustomHandlerHolder(train_handlers)))\n\n        val_handlers = np.array([factory.create_handler() for _ in self.val_ts]) if self.has_val() else None\n        self.val_preprocess_order.append(PreprocessNote(factory.preprocess_enum_type, False, False, factory.can_apply_to_val and self.has_val(), True, NoFitCustomHandlerHolder(val_handlers)))\n\n        test_handlers = np.array([factory.create_handler() for _ in self.test_ts]) if self.has_test() else None\n        self.test_preprocess_order.append(PreprocessNote(factory.preprocess_enum_type, False, False, factory.can_apply_to_test and self.has_test(), True, NoFitCustomHandlerHolder(test_handlers)))\n\n        all_handlers = np.array([factory.create_handler() for _ in self.all_ts]) if self.has_all() else None\n        self.all_preprocess_order.append(PreprocessNote(factory.preprocess_enum_type, False, False, factory.can_apply_to_all and self.has_all(), True, NoFitCustomHandlerHolder(all_handlers)))\n\n    def _validate_finalization(self) -&gt; None:\n        \"\"\" Performs final validation of the configuration. Validates whether `train/val/test` are continuos.\"\"\"\n\n        self._validate_time_periods_overlap()\n        self._validate_ts_overlap()\n\n    def _get_summary_filter_time_series(self) -&gt; css_utils.SummaryDiagramStep:\n        attributes = [css_utils.StepAttribute(\"Train time series IDs\", get_abbreviated_list_string(self.train_ts)),\n                      css_utils.StepAttribute(\"Val time series IDs\", get_abbreviated_list_string(self.val_ts)),\n                      css_utils.StepAttribute(\"Test time series IDs\", get_abbreviated_list_string(self.test_ts)),\n                      css_utils.StepAttribute(\"Train time periods\", self.display_train_time_period),\n                      css_utils.StepAttribute(\"Val time periods\", self.display_val_time_period),\n                      css_utils.StepAttribute(\"Test time periods\", self.display_test_time_period),\n                      css_utils.StepAttribute(\"Nan threshold\", self.nan_threshold)]\n\n        return css_utils.SummaryDiagramStep(\"Filter time series\", attributes)\n\n    def _get_summary_loader(self) -&gt; list[css_utils.SummaryDiagramStep]:\n\n        steps = []\n\n        if self.sliding_window_size is not None:\n            attributes = [\n                css_utils.StepAttribute(\"Window size\", self.sliding_window_size),\n                css_utils.StepAttribute(\"Prediction size\", self.sliding_window_prediction_size),\n                css_utils.StepAttribute(\"Step\", self.sliding_window_step)\n            ]\n\n            steps.append(css_utils.SummaryDiagramStep(\"Apply sliding window\", attributes))\n\n        attributes = [css_utils.StepAttribute(\"Train batch size\", self.train_batch_size),\n                      css_utils.StepAttribute(\"Val batch size\", self.val_batch_size),\n                      css_utils.StepAttribute(\"Test batch size\", self.test_batch_size)]\n\n        steps.append(css_utils.SummaryDiagramStep(\"Transform into specific format\", attributes))\n\n        return steps\n\n    def __str__(self) -&gt; str:\n\n        if self.transformer_factory.is_empty_factory:\n            transformer_part = f\"Transformer type: {self.transformer_factory.name}\"\n        else:\n            transformer_part = f'''Transformer type: {self.transformer_factory.name}\n        Are transformers premade: {self.transformer_factory.has_already_initialized}\n        Are premade transformers partial_fitted: {self.partial_fit_initialized_transformers}'''\n\n        if self.include_time:\n            time_part = f'''Time included: {str(self.include_time)}    \n        Time format: {str(self.time_format)}'''\n        else:\n            time_part = f\"Time included: {str(self.include_time)}\"\n\n        return f'''\nConfig Details\n    Used for database: {self.database_name}\n    Aggregation: {str(self.aggregation)}\n    Source: {str(self.source_type)}\n\n    Time series\n        Train time series IDs: {get_abbreviated_list_string(self.train_ts)}\n        Val time series IDs: {get_abbreviated_list_string(self.val_ts)}\n        Test time series IDs: {get_abbreviated_list_string(self.test_ts)}\n    Time periods\n        Train time periods: {str(self.display_train_time_period)}\n        Val time periods: {str(self.display_val_time_period)}\n        Test time periods: {str(self.display_test_time_period)}\n    Features\n        Taken features: {str(self.features_to_take_without_ids)}\n        Default values: {self.default_values}\n        Time series ID included: {str(self.include_ts_id)}\n        {time_part}\n    Sliding window\n        Sliding window size: {self.sliding_window_size}\n        Sliding window prediction size: {self.sliding_window_prediction_size}\n        Sliding window step size: {self.sliding_window_step}\n    Fillers\n        Filler type: {self.filler_factory.name}\n    Transformers\n        {transformer_part}\n    Anomaly handler\n        Anomaly handler type (train set): {self.anomaly_handler_factory.name}\n    Batch sizes\n        Train batch size: {self.train_batch_size}\n        Val batch size: {self.val_batch_size}\n        Test batch size: {self.test_batch_size}\n    Default workers\n        Init worker count: {str(self.init_workers)}\n        Train worker count: {str(self.train_workers)}\n        Val worker count: {str(self.val_workers)}\n        Test worker count: {str(self.test_workers)}\n    Other\n        Preprocess order: {normalize_display_list(self.preprocess_order)}\n        Nan threshold: {str(self.nan_threshold)}\n        Random state: {self.random_state}\n        Version: {self.version}\n                '''\n</code></pre>"},{"location":"reference_disjoint_time_based_config/#configuration-options","title":"Configuration options","text":"<p>Parameters:</p> Name Type Description Default <code>train_ts</code> <code>list[int] | NDArray[int_] | float | int | None</code> <p>Defines which time series IDs are used in the training set. Can be a list of IDs, or an integer/float to specify a random selection. An <code>int</code> specifies the number of random time series, and a <code>float</code> specifies the proportion of available time series.      <code>int</code> and <code>float</code> must be greater than 0, and a float should be smaller or equal to 1.0. Using <code>int</code> or <code>float</code> guarantees that no time series from other sets will be used. Must be used with <code>train_time_period</code>.</p> required <code>val_ts</code> <code>list[int] | NDArray[int_] | float | int | None</code> <p>Defines which time series IDs are used in the validation set. Same as <code>train_ts</code> but for the validation set. Must be used with <code>val_time_period</code>.</p> required <code>test_ts</code> <code>list[int] | NDArray[int_] | float | int | None</code> <p>Defines which time series IDs are used in the test set. Same as <code>train_ts</code> but for the test set. Must be used with <code>test_time_period</code>.</p> required <code>train_time_period</code> <code>tuple[datetime, datetime] | range | float | None</code> <p>Defines the time period for training set. Can be a range of time IDs or a tuple of datetime objects. Float value is equivalent to percentage of available times with offseted position from previous used set. Must be used with <code>train_ts</code>. <code>Default: None</code></p> <code>None</code> <code>val_time_period</code> <code>tuple[datetime, datetime] | range | float | None</code> <p>Defines the time period for validation set. Can be a range of time IDs or a tuple of datetime objects. Float value is equivalent to percentage of available times with offseted position from previous used set. Must be used with <code>val_ts</code>. <code>Default: None</code></p> <code>None</code> <code>test_time_period</code> <code>tuple[datetime, datetime] | range | float | None</code> <p>Defines the time period for test set. Can be a range of time IDs or a tuple of datetime objects. Must be used with <code>test_ts</code>. <code>Default: None</code></p> <code>None</code> <code>features_to_take</code> <code>list[str] | Literal['all']</code> <p>Defines which features are used. <code>Default: \"all\"</code> </p> <code>'all'</code> <code>default_values</code> <code>list[Number] | NDArray[number] | dict[str, Number] | Number | Literal['default'] | None</code> <p>Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature. <code>Default: \"default\"</code></p> <code>'default'</code> <code>sliding_window_size</code> <code>int | None</code> <p>Number of times in one window. Impacts dataloader behavior. Batch sizes affects how much data will be cached for creating windows. <code>Default: None</code></p> <code>None</code> <code>sliding_window_prediction_size</code> <code>int | None</code> <p>Number of times to predict from sliding_window_size. Impacts dataloader behavior. Batch sizes affects how much data will be cached for creating windows. <code>Default: None</code></p> <code>None</code> <code>sliding_window_step</code> <code>int</code> <p>Number of times to move by after each window. <code>Default: 1</code></p> <code>1</code> <code>set_shared_size</code> <code>float | int</code> <p>How much times should time periods share. Order of sharing is training set &lt; validation set &lt; test set. Only in effect if sets share less values than set_shared_size. Use float value for percentage of total times or int for count. <code>Default: 0</code></p> <code>0</code> <code>train_batch_size</code> <code>int</code> <p>Batch size for the train dataloader. Affects number of returned times in one batch. <code>Default: 32</code></p> <code>32</code> <code>val_batch_size</code> <code>int</code> <p>Batch size for the validation dataloader. Affects number of returned times in one batch. <code>Default: 64</code></p> <code>64</code> <code>test_batch_size</code> <code>int</code> <p>Batch size for the test dataloader. Affects number of returned times in one batch. <code>Default: 128</code></p> <code>128</code> <code>preprocess_order</code> <code>list[str, type]</code> <p>Defines in which order preprocesses are used. Also can add to order a type of <code>AllSeriesCustomHandler</code> or <code>NoFitCustomHandler</code>. <code>Default: [\"handling_anomalies\", \"filling_gaps\", \"transforming\"]</code></p> <code>['handling_anomalies', 'filling_gaps', 'transforming']</code> <code>fill_missing_with</code> <code>type | FillerType | Literal['mean_filler', 'forward_filler', 'linear_interpolation_filler'] | None</code> <p>Defines how to fill missing values in the dataset. Can pass enum <code>FillerType</code> for built-in filler or pass a type of custom filler that must derive from <code>Filler</code> base class. <code>Default: None</code> </p> <code>None</code> <code>transform_with</code> <code>type | list[Transformer] | ndarray[Transformer] | TransformerType | Transformer | Literal['min_max_scaler', 'standard_scaler', 'max_abs_scaler', 'log_transformer', 'l2_normalizer'] | None</code> <p>Defines the transformer used to transform the dataset. Can pass enum <code>TransformerType</code>, pass a type of custom transformer or instance of already fitted transformer(s). <code>Default: None</code></p> <code>None</code> <code>handle_anomalies_with</code> <code>type | AnomalyHandlerType | Literal['z-score', 'interquartile_range'] | None</code> <p>Defines the anomaly handler for handling anomalies in the train set. Can pass enum <code>AnomalyHandlerType</code> for built-in anomaly handler or a type of custom anomaly handler. <code>Default: None</code></p> <code>None</code> <code>partial_fit_initialized_transformer</code> <code>bool</code> <p>If <code>True</code>, partial fitting on train set is performed when using initiliazed transformers. <code>Default: False</code></p> <code>False</code> <code>include_time</code> <code>bool</code> <p>If <code>True</code>, time data is included in the returned values. <code>Default: True</code></p> <code>True</code> <code>include_ts_id</code> <code>bool</code> <p>If <code>True</code>, time series IDs are included in the returned values. <code>Default: True</code></p> <code>True</code> <code>time_format</code> <code>TimeFormat | Literal['id_time', 'datetime', 'unix_time', 'shifted_unix_time']</code> <p>Format for the returned time data. When using TimeFormat.DATETIME, time will be returned as separate list along rest of the values. <code>Default: TimeFormat.ID_TIME</code></p> <code>ID_TIME</code> <code>train_workers</code> <code>int</code> <p>Number of workers for loading training data. <code>0</code> means that the data will be loaded in the main process. <code>Default: 4</code></p> <code>4</code> <code>val_workers</code> <code>int</code> <p>Number of workers for loading validation data. <code>0</code> means that the data will be loaded in the main process. <code>Default: 3</code></p> <code>3</code> <code>test_workers</code> <code>int</code> <p>Number of workers for loading test. <code>0</code> means that the data will be loaded in the main process. <code>Default: 2</code></p> <code>2</code> <code>init_workers</code> <code>int</code> <p>Number of workers for initial dataset processing during configuration. <code>0</code> means that the data will be loaded in the main process. <code>Default: 4</code></p> <code>4</code> <code>nan_threshold</code> <code>float</code> <p>Maximum allowable percentage of missing data. Time series exceeding this threshold are excluded. Time series over the threshold will not be used. Used for <code>train/val/test/all</code> separately. <code>Default: 1.0</code></p> <code>1.0</code> <code>random_state</code> <code>int | None</code> <p>Fixes randomness for reproducibility during configuration and dataset initialization. <code>Default: None</code></p> <code>None</code>"},{"location":"reference_disjoint_time_based_config/#config-attributes","title":"Config attributes","text":"<p>Attributes:</p> Name Type Description <code>used_train_workers</code> <code>Optional[int]</code> <p>Tracks the number of train workers in use. Helps determine if the train dataloader should be recreated based on worker changes.</p> <code>used_val_workers</code> <code>Optional[int]</code> <p>Tracks the number of validation workers in use. Helps determine if the validation dataloader should be recreated based on worker changes.</p> <code>used_test_workers</code> <code>Optional[int]</code> <p>Tracks the number of test workers in use. Helps determine if the test dataloader should be recreated based on worker changes.</p> <code>uses_all_time_period</code> <code>bool</code> <p>Whether all time period set should be used.</p> <code>uses_all_ts</code> <code>bool</code> <p>Whether all time series set should be used.</p> <code>import_identifier</code> <code>Optional[str]</code> <p>Tracks the name of the config upon import. None if not imported.</p> <code>filler_factory</code> <code>FillerFactory</code> <p>Represents factory used to create passed Filler type.</p> <code>anomaly_handler_factory</code> <code>AnomalyHandlerFactory</code> <p>Represents factory used to create passed Anomaly Handler type.</p> <code>transformer_factory</code> <code>TransformerFactory</code> <p>Represents factory used to create passed Transformer type.</p> <code>can_fit_fillers</code> <code>bool</code> <p>Whether fillers in this config, can be fitted.        </p> <code>logger</code> <p>Logger for displaying information.     </p> <code>display_train_time_period</code> <code>Optional[range]</code> <p>Used to display the configured value of <code>train_time_period</code>.</p> <code>display_val_time_period</code> <code>Optional[range]</code> <p>Used to display the configured value of <code>val_time_period</code>.</p> <code>display_test_time_period</code> <code>Optional[range]</code> <p>Used to display the configured value of <code>test_time_period</code>.</p> <code>train_ts_row_ranges</code> <code>Optional[ndarray]</code> <p>Initialized when <code>train_ts</code> is set. Contains time series IDs in train set with their respective time ID ranges.</p> <code>val_ts_row_ranges</code> <code>Optional[ndarray]</code> <p>Initialized when <code>val_ts</code> is set. Contains time series IDs in validation set with their respective time ID ranges.</p> <code>test_ts_row_ranges</code> <code>Optional[ndarray]</code> <p>Initialized when <code>test_ts</code> is set. Contains time series IDs in test set with their respective time ID ranges.        </p> <code>all_time_period</code> <code>Optional[ndarray]</code> <p>Contains total used time period.</p> <code>all_ts</code> <code>Optional[ndarray]</code> <p>Contains all used time series.</p> <code>all_ts_row_ranges</code> <code>Optional[ndarray]</code> <p>Contains time series IDs in all set with their respective time ID ranges.</p> <code>aggregation</code> <code>Optional[AgreggationType]</code> <p>The aggregation period used for the data.</p> <code>source_type</code> <code>Optional[SourceType]</code> <p>The source type of the data.</p> <code>database_name</code> <code>Optional[str]</code> <p>Specifies which database this config applies to.</p> <code>features_to_take_without_ids</code> <code>Optional[ndarray]</code> <p>Features to be returned, excluding time or time series IDs.</p> <code>indices_of_features_to_take_no_ids</code> <code>Optional[ndarray]</code> <p>Indices of non-ID features in <code>features_to_take</code>.</p> <code>ts_id_name</code> <code>Optional[str]</code> <p>Name of the time series ID, dependent on <code>source_type</code>.</p> <code>used_singular_train_time_series</code> <code>Optional[int]</code> <p>Currently used singular train set time series for dataloader.</p> <code>used_singular_val_time_series</code> <code>Optional[int]</code> <p>Currently used singular validation set time series for dataloader.</p> <code>used_singular_test_time_series</code> <code>Optional[int]</code> <p>Currently used singular test set time series for dataloader.     </p> <code>train_preprocess_order</code> <code>list[PreprocessNote]</code> <p>All preprocesses used for train set. </p> <code>val_preprocess_order</code> <code>list[PreprocessNote]</code> <p>All preprocesses used for val set. </p> <code>test_preprocess_order</code> <code>list[PreprocessNote]</code> <p>All preprocesses used for test set.      </p> <code>is_initialized</code> <code>bool</code> <p>Flag indicating if the configuration has already been initialized. If true, config initialization will be skipped.  </p> <code>version</code> <code>str</code> <p>Version of cesnet-tszoo this config was made in.</p> <code>export_update_needed</code> <code>bool</code> <p>Whether config was updated to newer version and should be exported.     </p> <code>train_ts</code> <code>Optional[ndarray]</code> <p>Defines which time series IDs are used in the training set. Can be a list of IDs, or an integer/float to specify a random selection. An <code>int</code> specifies the number of random time series, and a <code>float</code> specifies the proportion of available time series.        <code>int</code> and <code>float</code> must be greater than 0, and a float should be smaller or equal to 1.0. Using <code>int</code> or <code>float</code> guarantees that no time series from other sets will be used. Must be used with <code>train_time_period</code>.</p> <code>val_ts</code> <code>Optional[ndarray]</code> <p>Defines which time series IDs are used in the validation set. Same as <code>train_ts</code> but for the validation set. Must be used with <code>val_time_period</code>.</p> <code>test_ts</code> <code>Optional[ndarray]</code> <p>Defines which time series IDs are used in the test set. Same as <code>train_ts</code> but for the test set. Must be used with <code>test_time_period</code>.</p> <code>train_time_period</code> <code>Optional[ndarray]</code> <p>Defines the time period for training set. Can be a range of time IDs or a tuple of datetime objects. Float value is equivalent to percentage of available times with offseted position from previous used set. Must be used with <code>train_ts</code>.</p> <code>val_time_period</code> <code>Optional[ndarray]</code> <p>Defines the time period for validation set. Can be a range of time IDs or a tuple of datetime objects. Float value is equivalent to percentage of available times with offseted position from previous used set. Must be used with <code>val_ts</code>.</p> <code>test_time_period</code> <code>Optional[ndarray]</code> <p>Defines the time period for test set. Can be a range of time IDs or a tuple of datetime objects. Must be used with <code>test_ts</code>.</p> <code>features_to_take</code> <code>list[str]</code> <p>Defines which features are used.           </p> <code>default_values</code> <code>ndarray</code> <p>Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature.</p> <code>sliding_window_size</code> <code>Optional[int]</code> <p>Number of times in one window. Impacts dataloader behavior. Batch sizes affects how much data will be cached for creating windows.</p> <code>sliding_window_prediction_size</code> <code>Optional[int]</code> <p>Number of times to predict from sliding_window_size. Impacts dataloader behavior. Batch sizes affects how much data will be cached for creating windows.</p> <code>sliding_window_step</code> <code>int</code> <p>Number of times to move by after each window.</p> <code>set_shared_size</code> <code>int | float</code> <p>How much times should time periods share. Order of sharing is training set &lt; validation set &lt; test set. Only in effect if sets share less values than set_shared_size. Use float value for percentage of total times or int for count.</p> <code>train_batch_size</code> <code>int</code> <p>Batch size for the train dataloader. Affects number of returned times in one batch.</p> <code>val_batch_size</code> <code>int</code> <p>Batch size for the validation dataloader. Affects number of returned times in one batch.</p> <code>test_batch_size</code> <code>int</code> <p>Batch size for the test dataloader. Affects number of returned times in one batch.</p> <code>preprocess_order</code> <code>list[PreprocessType]</code> <p>Defines in which order preprocesses are used. Also can add to order a type of <code>AllSeriesCustomHandler</code> or <code>NoFitCustomHandler</code>.</p> <code>partial_fit_initialized_transformers</code> <code>bool</code> <p>If <code>True</code>, partial fitting on train set is performed when using initiliazed transformers.</p> <code>include_time</code> <code>bool</code> <p>If <code>True</code>, time data is included in the returned values.</p> <code>include_ts_id</code> <code>bool</code> <p>If <code>True</code>, time series IDs are included in the returned values.</p> <code>time_format</code> <code>TimeFormat</code> <p>Format for the returned time data. When using TimeFormat.DATETIME, time will be returned as separate list along rest of the values.</p> <code>train_workers</code> <code>int</code> <p>Number of workers for loading training data. <code>0</code> means that the data will be loaded in the main process.</p> <code>val_workers</code> <code>int</code> <p>Number of workers for loading validation data. <code>0</code> means that the data will be loaded in the main process.</p> <code>test_workers</code> <code>int</code> <p>Number of workers for loading test. <code>0</code> means that the data will be loaded in the main process.</p> <code>init_workers</code> <code>int</code> <p>Number of workers for initial dataset processing during configuration. <code>0</code> means that the data will be loaded in the main process.</p> <code>nan_threshold</code> <code>float</code> <p>Maximum allowable percentage of missing data. Time series exceeding this threshold are excluded. Time series over the threshold will not be used. Used for <code>train/val/test/all</code> separately.</p> <code>random_state</code> <code>Optional[int]</code> <p>Fixes randomness for reproducibility during configuration and dataset initialization.</p>"},{"location":"reference_enums/","title":"Enums","text":""},{"location":"reference_enums/#cesnet_tszoo.utils.enums","title":"cesnet_tszoo.utils.enums","text":""},{"location":"reference_enums/#cesnet_tszoo.utils.enums.AgreggationType","title":"AgreggationType","text":"<p>               Bases: <code>Enum</code></p> <p>Possible aggregations for database.</p> Source code in <code>cesnet_tszoo\\utils\\enums.py</code> <pre><code>class AgreggationType(Enum):\n    \"\"\"Possible aggregations for database. \"\"\"\n\n    AGG_1_DAY = \"1_day\"\n    \"\"\"1 day aggregation for source type. \"\"\"\n\n    AGG_1_HOUR = \"1_hour\"\n    \"\"\"1 hour aggregation for source type. \"\"\"\n\n    AGG_10_MINUTES = \"10_minutes\"\n    \"\"\"10 minutes aggregation for source type. \"\"\"\n\n    AGG_1_MINUTE = \"1_minute\"\n    \"\"\"1 minute aggregation for source type. \"\"\"\n\n    @staticmethod\n    def _to_str_with_agg(aggregation_type):\n        \"\"\"For paths. \"\"\"\n\n        return f\"agg_{aggregation_type.value}\"\n\n    @staticmethod\n    def _to_str_without_number(aggregation_type) -&gt; str:\n        \"\"\"For paths. \"\"\"\n\n        if aggregation_type == AgreggationType.AGG_10_MINUTES:\n            return \"minutes\"\n        elif aggregation_type == AgreggationType.AGG_1_HOUR:\n            return \"hour\"\n        elif aggregation_type == AgreggationType.AGG_1_DAY:\n            return \"day\"\n        elif aggregation_type == AgreggationType.AGG_1_MINUTE:\n            return \"minute\"\n        else:\n            raise NotImplementedError()\n</code></pre>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.AgreggationType.AGG_1_DAY","title":"AGG_1_DAY  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>AGG_1_DAY = '1_day'\n</code></pre> <p>1 day aggregation for source type.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.AgreggationType.AGG_1_HOUR","title":"AGG_1_HOUR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>AGG_1_HOUR = '1_hour'\n</code></pre> <p>1 hour aggregation for source type.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.AgreggationType.AGG_10_MINUTES","title":"AGG_10_MINUTES  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>AGG_10_MINUTES = '10_minutes'\n</code></pre> <p>10 minutes aggregation for source type.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.AgreggationType.AGG_1_MINUTE","title":"AGG_1_MINUTE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>AGG_1_MINUTE = '1_minute'\n</code></pre> <p>1 minute aggregation for source type.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.SourceType","title":"SourceType","text":"<p>               Bases: <code>Enum</code></p> <p>Possible source types for database.</p> Source code in <code>cesnet_tszoo\\utils\\enums.py</code> <pre><code>class SourceType(Enum):\n    \"\"\"Possible source types for database. \"\"\"\n\n    IP_ADDRESSES_FULL = \"ip_addresses_full\"\n    \"\"\"Traffic of ip addresses of specific devices. \"\"\"\n\n    IP_ADDRESSES_SAMPLE = \"ip_addresses_sample\"\n    \"\"\"Traffic of subset from `ip_addresses_full`. \"\"\"\n\n    INSTITUTION_SUBNETS = \"institution_subnets\"\n    \"\"\"Traffic of subnets in institutions`. \"\"\"\n\n    INSTITUTIONS = \"institutions\"\n    \"\"\"Traffic Institutions of CESNET3 network. \"\"\"\n\n    CESNET2 = \"CESNET2\"\n    \"\"\"Traffic of CESNET2 network. \"\"\"\n</code></pre>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.SourceType.IP_ADDRESSES_FULL","title":"IP_ADDRESSES_FULL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IP_ADDRESSES_FULL = 'ip_addresses_full'\n</code></pre> <p>Traffic of ip addresses of specific devices.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.SourceType.IP_ADDRESSES_SAMPLE","title":"IP_ADDRESSES_SAMPLE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IP_ADDRESSES_SAMPLE = 'ip_addresses_sample'\n</code></pre> <p>Traffic of subset from <code>ip_addresses_full</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.SourceType.INSTITUTION_SUBNETS","title":"INSTITUTION_SUBNETS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>INSTITUTION_SUBNETS = 'institution_subnets'\n</code></pre> <p>Traffic of subnets in institutions`.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.SourceType.INSTITUTIONS","title":"INSTITUTIONS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>INSTITUTIONS = 'institutions'\n</code></pre> <p>Traffic Institutions of CESNET3 network.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.SourceType.CESNET2","title":"CESNET2  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CESNET2 = 'CESNET2'\n</code></pre> <p>Traffic of CESNET2 network.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.FillerType","title":"FillerType","text":"<p>               Bases: <code>Enum</code></p> <p>Built-in filler types.</p> Source code in <code>cesnet_tszoo\\utils\\enums.py</code> <pre><code>class FillerType(Enum):\n    \"\"\"Built-in filler types. \"\"\"\n\n    MEAN_FILLER = \"mean_filler\"\n    \"\"\"Represents filler [`MeanFiller`](reference_fillers.md#cesnet_tszoo.utils.filler.filler.MeanFiller). Equivalent to literal `mean_filler`. \"\"\"\n\n    FORWARD_FILLER = \"forward_filler\"\n    \"\"\"Represents filler [`ForwardFiller`](reference_fillers.md#cesnet_tszoo.utils.filler.filler.ForwardFiller). Equivalent to literal `forward_filler`. \"\"\"\n\n    LINEAR_INTERPOLATION_FILLER = \"linear_interpolation_filler\"\n    \"\"\"Represents filler [`LinearInterpolationFiller`](reference_fillers.md#cesnet_tszoo.utils.filler.filler.LinearInterpolationFiller). Equivalent to literal `linear_interpolation_filler`. \"\"\"\n\n    NO_FILLER = \"no_filler\"\n    \"\"\"Represents filler [`NoFiller`](reference_fillers.md#cesnet_tszoo.utils.filler.filler.LinearInterpolationFiller). Equivalent to literal `no_filler`. \"\"\"\n</code></pre>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.FillerType.MEAN_FILLER","title":"MEAN_FILLER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MEAN_FILLER = 'mean_filler'\n</code></pre> <p>Represents filler <code>MeanFiller</code>. Equivalent to literal <code>mean_filler</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.FillerType.FORWARD_FILLER","title":"FORWARD_FILLER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FORWARD_FILLER = 'forward_filler'\n</code></pre> <p>Represents filler <code>ForwardFiller</code>. Equivalent to literal <code>forward_filler</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.FillerType.LINEAR_INTERPOLATION_FILLER","title":"LINEAR_INTERPOLATION_FILLER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>LINEAR_INTERPOLATION_FILLER = 'linear_interpolation_filler'\n</code></pre> <p>Represents filler <code>LinearInterpolationFiller</code>. Equivalent to literal <code>linear_interpolation_filler</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.FillerType.NO_FILLER","title":"NO_FILLER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NO_FILLER = 'no_filler'\n</code></pre> <p>Represents filler <code>NoFiller</code>. Equivalent to literal <code>no_filler</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.TransformerType","title":"TransformerType","text":"<p>               Bases: <code>Enum</code></p> <p>Built-in transformer types.</p> Source code in <code>cesnet_tszoo\\utils\\enums.py</code> <pre><code>class TransformerType(Enum):\n    \"\"\"Built-in transformer types. \"\"\"\n\n    MIN_MAX_SCALER = \"min_max_scaler\"\n    \"\"\"Represents transformer [`MinMaxScaler`](reference_transformers.md#cesnet_tszoo.utils.transformer.transformer.MinMaxScaler). Equivalent to literal `min_max_scaler`. \"\"\"\n\n    STANDARD_SCALER = \"standard_scaler\"\n    \"\"\"Represents transformer [`StandardScaler`](reference_transformers.md#cesnet_tszoo.utils.transformer.transformer.StandardScaler). Equivalent to literal `standard_scaler`. \"\"\"\n\n    MAX_ABS_SCALER = \"max_abs_scaler\"\n    \"\"\"Represents transformer [`MaxAbsScaler`](reference_transformers.md#cesnet_tszoo.utils.transformer.transformer.MaxAbsScaler). Equivalent to literal `max_abs_scaler`. \"\"\"\n\n    LOG_TRANSFORMER = \"log_transformer\"\n    \"\"\"Represents transformer [`LogTransformer`](reference_transformers.md#cesnet_tszoo.utils.transformer.transformer.LogTransformer). Equivalent to literal `log_transformer`. \"\"\"\n\n    L2_NORMALIZER = \"l2_normalizer\"\n    \"\"\"Represents transformer [`L2Normalizer`](reference_transformers.md#cesnet_tszoo.utils.transformer.transformer.L2Normalizer). Equivalent to literal `l2_normalizer`. \"\"\"\n\n    ROBUST_SCALER = \"robust_scaler\"\n    \"\"\"Represents transformer [`RobustScaler`](reference_transformers.md#cesnet_tszoo.utils.transformer.transformer.LogTransformer). Equivalent to literal `robust_scaler`. \"\"\"\n\n    POWER_TRANSFORMER = \"power_transformer\"\n    \"\"\"Represents transformer [`PowerTransformer`](reference_transformers.md#cesnet_tszoo.utils.transformer.transformer.PowerTransformer). Equivalent to literal `power_transformer`. \"\"\"\n\n    QUANTILE_TRANSFORMER = \"quantile_transformer\"\n    \"\"\"Represents transformer [`QuantileTransformer`](reference_transformers.md#cesnet_tszoo.utils.transformer.transformer.QuantileTransformer). Equivalent to literal `quantile_transformer`. \"\"\"\n\n    NO_TRANSFORMER = \"no_transformer\"\n    \"\"\"Represents transformer [`NoTransformer`](reference_transformers.md#cesnet_tszoo.utils.transformer.transformer.NoTransformer). Equivalent to literal `no_transformer`. \"\"\"\n</code></pre>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.TransformerType.MIN_MAX_SCALER","title":"MIN_MAX_SCALER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MIN_MAX_SCALER = 'min_max_scaler'\n</code></pre> <p>Represents transformer <code>MinMaxScaler</code>. Equivalent to literal <code>min_max_scaler</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.TransformerType.STANDARD_SCALER","title":"STANDARD_SCALER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>STANDARD_SCALER = 'standard_scaler'\n</code></pre> <p>Represents transformer <code>StandardScaler</code>. Equivalent to literal <code>standard_scaler</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.TransformerType.MAX_ABS_SCALER","title":"MAX_ABS_SCALER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MAX_ABS_SCALER = 'max_abs_scaler'\n</code></pre> <p>Represents transformer <code>MaxAbsScaler</code>. Equivalent to literal <code>max_abs_scaler</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.TransformerType.LOG_TRANSFORMER","title":"LOG_TRANSFORMER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>LOG_TRANSFORMER = 'log_transformer'\n</code></pre> <p>Represents transformer <code>LogTransformer</code>. Equivalent to literal <code>log_transformer</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.TransformerType.L2_NORMALIZER","title":"L2_NORMALIZER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>L2_NORMALIZER = 'l2_normalizer'\n</code></pre> <p>Represents transformer <code>L2Normalizer</code>. Equivalent to literal <code>l2_normalizer</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.TransformerType.ROBUST_SCALER","title":"ROBUST_SCALER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ROBUST_SCALER = 'robust_scaler'\n</code></pre> <p>Represents transformer <code>RobustScaler</code>. Equivalent to literal <code>robust_scaler</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.TransformerType.POWER_TRANSFORMER","title":"POWER_TRANSFORMER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>POWER_TRANSFORMER = 'power_transformer'\n</code></pre> <p>Represents transformer <code>PowerTransformer</code>. Equivalent to literal <code>power_transformer</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.TransformerType.QUANTILE_TRANSFORMER","title":"QUANTILE_TRANSFORMER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>QUANTILE_TRANSFORMER = 'quantile_transformer'\n</code></pre> <p>Represents transformer <code>QuantileTransformer</code>. Equivalent to literal <code>quantile_transformer</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.TransformerType.NO_TRANSFORMER","title":"NO_TRANSFORMER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NO_TRANSFORMER = 'no_transformer'\n</code></pre> <p>Represents transformer <code>NoTransformer</code>. Equivalent to literal <code>no_transformer</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.AnomalyHandlerType","title":"AnomalyHandlerType","text":"<p>               Bases: <code>Enum</code></p> <p>Built-in anomaly handler types.</p> Source code in <code>cesnet_tszoo\\utils\\enums.py</code> <pre><code>class AnomalyHandlerType(Enum):\n    \"\"\"Built-in anomaly handler types. \"\"\"\n\n    Z_SCORE = \"z-score\"\n    \"\"\"Represents anomaly handler [`ZScore`](reference_anomaly_handlers.md#cesnet_tszoo.utils.anomaly_handler.anomaly_handler.ZScore). Equivalent to literal `z-score`. \"\"\"\n\n    INTERQUARTILE_RANGE = \"interquartile_range\"\n    \"\"\"Represents anomaly handler [`InterquartileRange`](reference_anomaly_handlers.md#cesnet_tszoo.utils.anomaly_handler.anomaly_handler.InterquartileRange). Equivalent to literal `interquartile_range`. \"\"\"\n\n    NO_ANOMALY_HANDLER = \"no_anomaly_handler\"\n    \"\"\"Represents anomaly handler [`NoAnomalyHandler`](reference_anomaly_handlers.md#cesnet_tszoo.utils.anomaly_handler.anomaly_handler.NoAnomalyHandler). Equivalent to literal `no_anomaly_handler`. \"\"\"\n</code></pre>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.AnomalyHandlerType.Z_SCORE","title":"Z_SCORE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>Z_SCORE = 'z-score'\n</code></pre> <p>Represents anomaly handler <code>ZScore</code>. Equivalent to literal <code>z-score</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.AnomalyHandlerType.INTERQUARTILE_RANGE","title":"INTERQUARTILE_RANGE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>INTERQUARTILE_RANGE = 'interquartile_range'\n</code></pre> <p>Represents anomaly handler <code>InterquartileRange</code>. Equivalent to literal <code>interquartile_range</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.AnomalyHandlerType.NO_ANOMALY_HANDLER","title":"NO_ANOMALY_HANDLER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NO_ANOMALY_HANDLER = 'no_anomaly_handler'\n</code></pre> <p>Represents anomaly handler <code>NoAnomalyHandler</code>. Equivalent to literal <code>no_anomaly_handler</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.TimeFormat","title":"TimeFormat","text":"<p>               Bases: <code>Enum</code></p> <p>Different supported time formats.</p> Source code in <code>cesnet_tszoo\\utils\\enums.py</code> <pre><code>class TimeFormat(Enum):\n    \"\"\"Different supported time formats. \"\"\"\n\n    ID_TIME = \"id_time\"\n    \"\"\"Time as indices, starting from 0. \"\"\"\n\n    DATETIME = \"datetime\"\n    \"\"\"Time as [`datetime`](https://docs.python.org/3/library/datetime.html) object. \"\"\"\n\n    UNIX_TIME = \"unix_time\"\n    \"\"\"Time in unix time format. \"\"\"\n\n    SHIFTED_UNIX_TIME = \"shifted_unix_time\"\n    \"\"\"Unix time format but offsetted so it starts from 0. \"\"\"\n</code></pre>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.TimeFormat.ID_TIME","title":"ID_TIME  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ID_TIME = 'id_time'\n</code></pre> <p>Time as indices, starting from 0.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.TimeFormat.DATETIME","title":"DATETIME  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DATETIME = 'datetime'\n</code></pre> <p>Time as <code>datetime</code> object.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.TimeFormat.UNIX_TIME","title":"UNIX_TIME  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>UNIX_TIME = 'unix_time'\n</code></pre> <p>Time in unix time format.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.TimeFormat.SHIFTED_UNIX_TIME","title":"SHIFTED_UNIX_TIME  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SHIFTED_UNIX_TIME = 'shifted_unix_time'\n</code></pre> <p>Unix time format but offsetted so it starts from 0.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.SplitType","title":"SplitType","text":"<p>               Bases: <code>Enum</code></p> <p>Different split variants.</p> Source code in <code>cesnet_tszoo\\utils\\enums.py</code> <pre><code>class SplitType(Enum):\n    \"\"\"Different split variants. \"\"\"\n\n    TRAIN = \"train\"\n    \"\"\"Represents training set of dataset. \"\"\"\n\n    VAL = \"val\"\n    \"\"\"Represents validation set of dataset. \"\"\"\n\n    TEST = \"test\"\n    \"\"\"Represents test set of dataset. \"\"\"\n\n    ALL = \"all\"\n    \"\"\"Represents train/val/test sets as one or just everything. \"\"\"\n</code></pre>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.SplitType.TRAIN","title":"TRAIN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TRAIN = 'train'\n</code></pre> <p>Represents training set of dataset.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.SplitType.VAL","title":"VAL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>VAL = 'val'\n</code></pre> <p>Represents validation set of dataset.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.SplitType.TEST","title":"TEST  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TEST = 'test'\n</code></pre> <p>Represents test set of dataset.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.SplitType.ALL","title":"ALL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ALL = 'all'\n</code></pre> <p>Represents train/val/test sets as one or just everything.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.AnnotationType","title":"AnnotationType","text":"<p>               Bases: <code>Enum</code></p> <p>Categories of Annotations.</p> Source code in <code>cesnet_tszoo\\utils\\enums.py</code> <pre><code>class AnnotationType(Enum):\n    \"\"\"Categories of Annotations. \"\"\"\n\n    ID_TIME = \"id_time\"\n    \"\"\"Represents annotations for time. \"\"\"\n\n    TS_ID = \"ts_id\"\n    \"\"\"Represents annotations for time series. \"\"\"\n\n    BOTH = \"both\"\n    \"\"\"Represents annotations for time in time series. \"\"\"\n</code></pre>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.AnnotationType.ID_TIME","title":"ID_TIME  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ID_TIME = 'id_time'\n</code></pre> <p>Represents annotations for time.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.AnnotationType.TS_ID","title":"TS_ID  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TS_ID = 'ts_id'\n</code></pre> <p>Represents annotations for time series.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.AnnotationType.BOTH","title":"BOTH  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>BOTH = 'both'\n</code></pre> <p>Represents annotations for time in time series.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.DataloaderOrder","title":"DataloaderOrder","text":"<p>               Bases: <code>Enum</code></p> <p>Order for loading data with PyTorch <code>DataLoader</code>.</p> Source code in <code>cesnet_tszoo\\utils\\enums.py</code> <pre><code>class DataloaderOrder(Enum):\n    \"\"\"Order for loading data with PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). \"\"\"\n\n    RANDOM = \"random\"\n    \"\"\"Loaded data will be randomly selected. \"\"\"\n\n    SEQUENTIAL = \"sequential\"\n    \"\"\"Loaded data will be in the selected order. \"\"\"\n</code></pre>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.DataloaderOrder.RANDOM","title":"RANDOM  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>RANDOM = 'random'\n</code></pre> <p>Loaded data will be randomly selected.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.DataloaderOrder.SEQUENTIAL","title":"SEQUENTIAL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SEQUENTIAL = 'sequential'\n</code></pre> <p>Loaded data will be in the selected order.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.DatasetType","title":"DatasetType","text":"<p>               Bases: <code>Enum</code></p> <p>Types of datasets.</p> Source code in <code>cesnet_tszoo\\utils\\enums.py</code> <pre><code>class DatasetType(Enum):\n    \"\"\"Types of datasets. \"\"\"\n\n    TIME_BASED = \"time_based\"\n    \"\"\"This type of dataset is defined by train/val/test time periods and one time series set. \"\"\"\n\n    SERIES_BASED = \"series_based\"\n    \"\"\"This type of dataset is defined by train/val/test time series sets and one time period set. \"\"\"\n\n    DISJOINT_TIME_BASED = \"disjoint_time_based\"\n    \"\"\"This type of dataset is defined by train/val/test time series sets and their respective time period sets. \"\"\"\n</code></pre>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.DatasetType.TIME_BASED","title":"TIME_BASED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TIME_BASED = 'time_based'\n</code></pre> <p>This type of dataset is defined by train/val/test time periods and one time series set.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.DatasetType.SERIES_BASED","title":"SERIES_BASED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SERIES_BASED = 'series_based'\n</code></pre> <p>This type of dataset is defined by train/val/test time series sets and one time period set.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.DatasetType.DISJOINT_TIME_BASED","title":"DISJOINT_TIME_BASED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DISJOINT_TIME_BASED = 'disjoint_time_based'\n</code></pre> <p>This type of dataset is defined by train/val/test time series sets and their respective time period sets.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.PreprocessType","title":"PreprocessType","text":"<p>               Bases: <code>Enum</code></p> <p>Helper enum to decide which preprocess it is.</p> Source code in <code>cesnet_tszoo\\utils\\enums.py</code> <pre><code>class PreprocessType(Enum):\n    \"\"\"Helper enum to decide which preprocess it is. \"\"\"\n\n    FILLING_GAPS = \"filling_gaps\"\n    \"\"\"Represents default_values and Filler preprocess. \"\"\"\n\n    TRANSFORMING = \"transforming\"\n    \"\"\"Represents Transformer preprocess. \"\"\"\n\n    HANDLING_ANOMALIES = \"handling_anomalies\"\n    \"\"\"Represents AnomalyHandler preprocess. \"\"\"\n\n    PER_SERIES_CUSTOM = \"per_series_custom\"\n    \"\"\"Represents PerSeriesCustomHandler preprocess \"\"\"\n\n    ALL_SERIES_CUSTOM = \"all_series_custom\"\n    \"\"\"Represents AllSeriesCustomHandler preprocess \"\"\"\n\n    NO_FIT_CUSTOM = \"no_fit_custom\"\n    \"\"\"Represents NoFitCustomHandler preprocess \"\"\"\n</code></pre>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.PreprocessType.FILLING_GAPS","title":"FILLING_GAPS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FILLING_GAPS = 'filling_gaps'\n</code></pre> <p>Represents default_values and Filler preprocess.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.PreprocessType.TRANSFORMING","title":"TRANSFORMING  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TRANSFORMING = 'transforming'\n</code></pre> <p>Represents Transformer preprocess.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.PreprocessType.HANDLING_ANOMALIES","title":"HANDLING_ANOMALIES  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>HANDLING_ANOMALIES = 'handling_anomalies'\n</code></pre> <p>Represents AnomalyHandler preprocess.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.PreprocessType.PER_SERIES_CUSTOM","title":"PER_SERIES_CUSTOM  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PER_SERIES_CUSTOM = 'per_series_custom'\n</code></pre> <p>Represents PerSeriesCustomHandler preprocess</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.PreprocessType.ALL_SERIES_CUSTOM","title":"ALL_SERIES_CUSTOM  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ALL_SERIES_CUSTOM = 'all_series_custom'\n</code></pre> <p>Represents AllSeriesCustomHandler preprocess</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.PreprocessType.NO_FIT_CUSTOM","title":"NO_FIT_CUSTOM  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NO_FIT_CUSTOM = 'no_fit_custom'\n</code></pre> <p>Represents NoFitCustomHandler preprocess</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.ScalerType","title":"ScalerType","text":"<p>               Bases: <code>Enum</code></p> <p>Obsolete, dont use. Only for backward compatibility.</p> Source code in <code>cesnet_tszoo\\utils\\enums.py</code> <pre><code>class ScalerType(Enum):\n    \"\"\"Obsolete, dont use. Only for backward compatibility. \"\"\"\n\n    MIN_MAX_SCALER = \"min_max_scaler\"\n    \"\"\"Represents transformer [`MinMaxScaler`](reference_transformers.md#cesnet_tszoo.utils.transformer.transformer.MinMaxScaler). Equivalent to literal `min_max_scaler`. \"\"\"\n\n    STANDARD_SCALER = \"standard_scaler\"\n    \"\"\"Represents transformer [`StandardScaler`](reference_transformers.md#cesnet_tszoo.utils.transformer.transformer.StandardScaler). Equivalent to literal `standard_scaler`. \"\"\"\n\n    MAX_ABS_SCALER = \"max_abs_scaler\"\n    \"\"\"Represents transformer [`MaxAbsScaler`](reference_transformers.md#cesnet_tszoo.utils.transformer.transformer.MaxAbsScaler). Equivalent to literal `max_abs_scaler`. \"\"\"\n\n    LOG_TRANSFORMER = \"log_transformer\"\n    \"\"\"Represents transformer [`LogTransformer`](reference_transformers.md#cesnet_tszoo.utils.transformer.transformer.LogTransformer). Equivalent to literal `log_transformer`. \"\"\"\n\n    L2_NORMALIZER = \"l2_normalizer\"\n    \"\"\"Represents transformer [`L2Normalizer`](reference_transformers.md#cesnet_tszoo.utils.transformer.transformer.L2Normalizer). Equivalent to literal `l2_normalizer`. \"\"\"\n\n    ROBUST_SCALER = \"robust_scaler\"\n    \"\"\"Represents transformer [`RobustScaler`](reference_transformers.md#cesnet_tszoo.utils.transformer.transformer.LogTransformer). Equivalent to literal `robust_scaler`. \"\"\"\n\n    POWER_TRANSFORMER = \"power_transformer\"\n    \"\"\"Represents transformer [`PowerTransformer`](reference_transformers.md#cesnet_tszoo.utils.transformer.transformer.PowerTransformer). Equivalent to literal `power_transformer`. \"\"\"\n\n    QUANTILE_TRANSFORMER = \"quantile_transformer\"\n    \"\"\"Represents transformer [`QuantileTransformer`](reference_transformers.md#cesnet_tszoo.utils.transformer.transformer.QuantileTransformer). Equivalent to literal `quantile_transformer`. \"\"\"\n</code></pre>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.ScalerType.MIN_MAX_SCALER","title":"MIN_MAX_SCALER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MIN_MAX_SCALER = 'min_max_scaler'\n</code></pre> <p>Represents transformer <code>MinMaxScaler</code>. Equivalent to literal <code>min_max_scaler</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.ScalerType.STANDARD_SCALER","title":"STANDARD_SCALER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>STANDARD_SCALER = 'standard_scaler'\n</code></pre> <p>Represents transformer <code>StandardScaler</code>. Equivalent to literal <code>standard_scaler</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.ScalerType.MAX_ABS_SCALER","title":"MAX_ABS_SCALER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MAX_ABS_SCALER = 'max_abs_scaler'\n</code></pre> <p>Represents transformer <code>MaxAbsScaler</code>. Equivalent to literal <code>max_abs_scaler</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.ScalerType.LOG_TRANSFORMER","title":"LOG_TRANSFORMER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>LOG_TRANSFORMER = 'log_transformer'\n</code></pre> <p>Represents transformer <code>LogTransformer</code>. Equivalent to literal <code>log_transformer</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.ScalerType.L2_NORMALIZER","title":"L2_NORMALIZER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>L2_NORMALIZER = 'l2_normalizer'\n</code></pre> <p>Represents transformer <code>L2Normalizer</code>. Equivalent to literal <code>l2_normalizer</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.ScalerType.ROBUST_SCALER","title":"ROBUST_SCALER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ROBUST_SCALER = 'robust_scaler'\n</code></pre> <p>Represents transformer <code>RobustScaler</code>. Equivalent to literal <code>robust_scaler</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.ScalerType.POWER_TRANSFORMER","title":"POWER_TRANSFORMER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>POWER_TRANSFORMER = 'power_transformer'\n</code></pre> <p>Represents transformer <code>PowerTransformer</code>. Equivalent to literal <code>power_transformer</code>.</p>"},{"location":"reference_enums/#cesnet_tszoo.utils.enums.ScalerType.QUANTILE_TRANSFORMER","title":"QUANTILE_TRANSFORMER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>QUANTILE_TRANSFORMER = 'quantile_transformer'\n</code></pre> <p>Represents transformer <code>QuantileTransformer</code>. Equivalent to literal <code>quantile_transformer</code>.</p>"},{"location":"reference_fillers/","title":"Fillers","text":""},{"location":"reference_fillers/#cesnet_tszoo.utils.filler.filler","title":"cesnet_tszoo.utils.filler.filler","text":""},{"location":"reference_fillers/#cesnet_tszoo.utils.filler.filler.Filler","title":"Filler","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for data fillers.</p> <p>This class serves as the foundation for creating custom fillers. To implement a custom filler, this class must be subclassed and extended. Fillers are used to handle missing data in a dataset.</p> <p>Example:</p> <pre><code>import numpy as np\n\nclass ForwardFiller(Filler):\n\n    def __init__(self, features):\n        super().__init__(features)\n\n        self.last_values = None\n\n    def fill(self, batch_values: np.ndarray, missing_mask: np.ndarray, **kwargs) -&gt; None:\n        if self.last_values is not None and np.any(missing_mask[0]):\n            batch_values[0, missing_mask[0]] = self.last_values[missing_mask[0]]\n\n        mask = missing_mask.T\n\n        idx = np.where(~mask, np.arange(mask.shape[1]), 0)\n        np.maximum.accumulate(idx, axis=1, out=idx)\n\n        batch_values = batch_values.T\n        batch_values[mask] = batch_values[np.nonzero(mask)[0], idx[mask]]\n        batch_values = batch_values.T\n\n        self.last_values = np.copy(batch_values[-1])\n</code></pre> Source code in <code>cesnet_tszoo\\utils\\filler\\filler.py</code> <pre><code>class Filler(ABC):\n    \"\"\"\n    Base class for data fillers.\n\n    This class serves as the foundation for creating custom fillers. To implement a custom filler, this class must be subclassed and extended.\n    Fillers are used to handle missing data in a dataset.\n\n    Example:\n\n        import numpy as np\n\n        class ForwardFiller(Filler):\n\n            def __init__(self, features):\n                super().__init__(features)\n\n                self.last_values = None\n\n            def fill(self, batch_values: np.ndarray, missing_mask: np.ndarray, **kwargs) -&gt; None:\n                if self.last_values is not None and np.any(missing_mask[0]):\n                    batch_values[0, missing_mask[0]] = self.last_values[missing_mask[0]]\n\n                mask = missing_mask.T\n\n                idx = np.where(~mask, np.arange(mask.shape[1]), 0)\n                np.maximum.accumulate(idx, axis=1, out=idx)\n\n                batch_values = batch_values.T\n                batch_values[mask] = batch_values[np.nonzero(mask)[0], idx[mask]]\n                batch_values = batch_values.T\n\n                self.last_values = np.copy(batch_values[-1])\n            \"\"\"\n\n    def __init__(self, features):\n        super().__init__()\n\n        self.features = features\n\n    @abstractmethod\n    def fill(self, batch_values: np.ndarray, missing_mask: np.ndarray, **kwargs) -&gt; None:\n        \"\"\"Fills missing data in the `batch_values`.\n\n        This method is responsible for filling missing data within a single time series.\n\n        Parameters:\n            batch_values: Data of a single time series with shape `(times, features)` excluding IDs.\n            missing_mask: Mask of missing values in batch_values.\n            kwargs: first_next_existing_values, first_next_existing_values_distance, default_values \n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference_fillers/#cesnet_tszoo.utils.filler.filler.Filler.fill","title":"fill  <code>abstractmethod</code>","text":"<pre><code>fill(batch_values: ndarray, missing_mask: ndarray, **kwargs) -&gt; None\n</code></pre> <p>Fills missing data in the <code>batch_values</code>.</p> <p>This method is responsible for filling missing data within a single time series.</p> <p>Parameters:</p> Name Type Description Default <code>batch_values</code> <code>ndarray</code> <p>Data of a single time series with shape <code>(times, features)</code> excluding IDs.</p> required <code>missing_mask</code> <code>ndarray</code> <p>Mask of missing values in batch_values.</p> required <code>kwargs</code> <p>first_next_existing_values, first_next_existing_values_distance, default_values</p> <code>{}</code> Source code in <code>cesnet_tszoo\\utils\\filler\\filler.py</code> <pre><code>@abstractmethod\ndef fill(self, batch_values: np.ndarray, missing_mask: np.ndarray, **kwargs) -&gt; None:\n    \"\"\"Fills missing data in the `batch_values`.\n\n    This method is responsible for filling missing data within a single time series.\n\n    Parameters:\n        batch_values: Data of a single time series with shape `(times, features)` excluding IDs.\n        missing_mask: Mask of missing values in batch_values.\n        kwargs: first_next_existing_values, first_next_existing_values_distance, default_values \n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference_fillers/#cesnet_tszoo.utils.filler.filler.MeanFiller","title":"MeanFiller","text":"<p>               Bases: <code>Filler</code></p> <p>Fills values from total mean of all previous values.</p> <p>Corresponds to enum <code>FillerType.MEAN_FILLER</code> or literal <code>mean_filler</code>.</p> Source code in <code>cesnet_tszoo\\utils\\filler\\filler.py</code> <pre><code>class MeanFiller(Filler):\n    \"\"\"\n    Fills values from total mean of all previous values.\n\n    Corresponds to enum [`FillerType.MEAN_FILLER`][cesnet_tszoo.utils.enums.FillerType] or literal `mean_filler`.\n    \"\"\"\n\n    def __init__(self, features):\n        super().__init__(features)\n\n        self.averages = np.zeros(len(features), dtype=np.float64)\n        self.total_existing_values = np.zeros(len(features), dtype=np.float64)\n\n    def fill(self, batch_values: np.ndarray, missing_mask: np.ndarray, **kwargs) -&gt; None:\n\n        existing_mask = ~missing_mask\n        batch_counts = np.cumsum(existing_mask, axis=0)\n        batch_sums = np.cumsum(np.where(existing_mask, batch_values, 0.0), axis=0)\n\n        prev_counts = self.total_existing_values\n        total_counts = batch_counts + prev_counts\n\n        with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n            running_avg = ((self.averages / total_counts) * prev_counts) + (batch_sums / total_counts)\n\n        running_avg = np.vstack([self.averages, running_avg[:-1, :]])\n\n        fill_mask = missing_mask &amp; (total_counts &gt; 0)\n        batch_values[fill_mask] = running_avg[fill_mask]\n\n        self.total_existing_values = total_counts[-1]\n        valid_cols = self.total_existing_values &gt; 0\n        self.averages[valid_cols] = running_avg[-1, valid_cols]\n</code></pre>"},{"location":"reference_fillers/#cesnet_tszoo.utils.filler.filler.ForwardFiller","title":"ForwardFiller","text":"<p>               Bases: <code>Filler</code></p> <p>Fills missing values based on last existing value. </p> <p>Corresponds to enum <code>FillerType.FORWARD_FILLER</code> or literal <code>forward_filler</code>.</p> Source code in <code>cesnet_tszoo\\utils\\filler\\filler.py</code> <pre><code>class ForwardFiller(Filler):\n    \"\"\"\n    Fills missing values based on last existing value. \n\n    Corresponds to enum [`FillerType.FORWARD_FILLER`][cesnet_tszoo.utils.enums.FillerType] or literal `forward_filler`.\n    \"\"\"\n\n    def __init__(self, features):\n        super().__init__(features)\n\n        self.last_values = None\n\n    def fill(self, batch_values: np.ndarray, missing_mask: np.ndarray, **kwargs) -&gt; None:\n        if self.last_values is not None and np.any(missing_mask[0]):\n            batch_values[0, missing_mask[0]] = self.last_values[missing_mask[0]]\n\n        mask = missing_mask.T\n\n        idx = np.where(~mask, np.arange(mask.shape[1]), 0)\n        np.maximum.accumulate(idx, axis=1, out=idx)\n\n        batch_values = batch_values.T\n        batch_values[mask] = batch_values[np.nonzero(mask)[0], idx[mask]]\n        batch_values = batch_values.T\n\n        self.last_values = np.copy(batch_values[-1])\n</code></pre>"},{"location":"reference_fillers/#cesnet_tszoo.utils.filler.filler.LinearInterpolationFiller","title":"LinearInterpolationFiller","text":"<p>               Bases: <code>Filler</code></p> <p>Fills values with linear interpolation. </p> <p>Corresponds to enum <code>FillerType.LINEAR_INTERPOLATION_FILLER</code> or literal <code>linear_interpolation_filler</code>.</p> Source code in <code>cesnet_tszoo\\utils\\filler\\filler.py</code> <pre><code>class LinearInterpolationFiller(Filler):\n    \"\"\"\n    Fills values with linear interpolation. \n\n    Corresponds to enum [`FillerType.LINEAR_INTERPOLATION_FILLER`][cesnet_tszoo.utils.enums.FillerType] or literal `linear_interpolation_filler`.\n    \"\"\"\n\n    def __init__(self, features):\n        super().__init__(features)\n\n        self.last_values = None\n        self.last_values_x_pos = None\n\n    def fill(self, batch_values: np.ndarray, missing_mask: np.ndarray, **kwargs) -&gt; None:\n\n        default_values = kwargs[\"default_values\"]\n\n        existing_mask = ~missing_mask\n        any_existing = np.any(existing_mask, axis=0)\n        any_missing = np.any(missing_mask, axis=0)\n\n        no_missing = not np.any(any_missing)\n        no_existing = not np.any(any_existing)\n\n        if no_missing:\n            self.last_values = np.copy(batch_values[-1, :])\n            return\n\n        if no_existing and self.last_values is None:\n            return\n\n        x_positions = np.arange(batch_values.shape[0])\n\n        for i in range(batch_values.shape[1]):\n            existing_x = x_positions[existing_mask[:, i]]\n            existing_y = batch_values[existing_mask[:, i], i]\n\n            if self.last_values is not None:\n                existing_x = np.insert(existing_x, 0, self.last_values_x_pos)\n                existing_y = np.insert(existing_y, 0, self.last_values[i])\n\n            missing_x = x_positions[missing_mask[:, i]]\n\n            if len(missing_x) &gt; 0 and len(existing_x) &gt; 0:\n                batch_values[missing_mask[:, i], i] = np.interp(\n                    missing_x,\n                    existing_x,\n                    existing_y,\n                    left=default_values[i],\n                    right=default_values[i],\n                )\n\n        self.last_values = np.copy(batch_values[-1, :])\n        self.last_values_x_pos = -1\n</code></pre>"},{"location":"reference_fillers/#cesnet_tszoo.utils.filler.filler.NoFiller","title":"NoFiller","text":"<p>               Bases: <code>Filler</code></p> <p>Does nothing. </p> <p>Corresponds to enum <code>FillerType.NO_FILLER</code> or literal <code>no_filler</code>.</p> Source code in <code>cesnet_tszoo\\utils\\filler\\filler.py</code> <pre><code>class NoFiller(Filler):\n    \"\"\"\n    Does nothing. \n\n    Corresponds to enum [`FillerType.NO_FILLER`][cesnet_tszoo.utils.enums.FillerType] or literal `no_filler`.\n    \"\"\"\n\n    def fill(self, batch_values: np.ndarray, missing_mask: np.ndarray, **kwargs) -&gt; None:\n        ...\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/","title":"Series-based dataset class","text":""},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset","title":"cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset  <code>dataclass</code>","text":"<p>               Bases: <code>CesnetDataset</code></p> <p>This class is used for series-based returning of data. Can be created by using <code>get_dataset</code> with parameter <code>dataset_type</code> = <code>DatasetType.SERIES_BASED</code>.</p> <p>Series-based means batch size affects number of returned time series in one batch. Which times for each time series are returned does not change.</p> <p>The dataset provides multiple ways to access the data:</p> <ul> <li>Iterable PyTorch DataLoader: For batch processing.</li> <li>Pandas DataFrame: For loading the entire training, validation, test or all set at once.</li> <li>Numpy array: For loading the entire training, validation, test or all set at once.      </li> <li>See loading data for more details.</li> </ul> <p>The dataset is stored in a PyTables database. The internal <code>SeriesBasedDataset</code> and <code>SeriesBasedInitializerDataset</code> classes (used only when calling <code>set_dataset_config_and_initialize</code>) act as wrappers that implement the PyTorch <code>Dataset</code>  interface. These wrappers are compatible with PyTorch\u2019s <code>DataLoader</code>, providing efficient parallel data loading. </p> <p>The dataset configuration is done through the <code>SeriesBasedConfig</code> class.     </p> <p>Intended usage:</p> <ol> <li>Create an instance of the dataset with the desired data root by calling <code>get_dataset</code>. This will download the dataset if it has not been previously downloaded and return instance of dataset.</li> <li>Create an instance of <code>SeriesBasedConfig</code> and set it using <code>set_dataset_config_and_initialize</code>.     This initializes the dataset, including data splitting (train/validation/test), fitting transformers (if needed), selecting features, and more. This is cached for later use.</li> <li>Use <code>get_train_dataloader</code>/<code>get_train_df</code>/<code>get_train_numpy</code> to get training data for chosen model.</li> <li>Validate the model and perform the hyperparameter optimalization on <code>get_val_dataloader</code>/<code>get_val_df</code>/<code>get_val_numpy</code>.</li> <li>Evaluate the model on <code>get_test_dataloader</code>/<code>get_test_df</code>/<code>get_test_numpy</code>.     </li> </ol> <p>Alternatively you can use <code>load_benchmark</code></p> <ol> <li>Call <code>load_benchmark</code> with the desired benchmark. You can use your own saved benchmark or you can use already built-in one. This will download the dataset and annotations (if available) if they have not been previously downloaded.</li> <li>Retrieve the initialized dataset using <code>get_initialized_dataset</code>. This will provide a dataset that is ready to use.</li> <li>Use <code>get_train_dataloader</code>/<code>get_train_df</code>/<code>get_train_numpy</code> to get training data for chosen model.</li> <li>Validate the model and perform the hyperparameter optimalization on <code>get_val_dataloader</code>/<code>get_val_df</code>/<code>get_val_numpy</code>.</li> <li>Evaluate the model on <code>get_test_dataloader</code>/<code>get_test_df</code>/<code>get_test_numpy</code>.</li> </ol> Source code in <code>cesnet_tszoo\\datasets\\series_based_cesnet_dataset.py</code> <pre><code>@dataclass\nclass SeriesBasedCesnetDataset(CesnetDataset):\n    \"\"\"\n    This class is used for series-based returning of data. Can be created by using [`get_dataset`](reference_cesnet_database.md#cesnet_tszoo.datasets.databases.cesnet_database.CesnetDatabase.get_dataset) with parameter `dataset_type` = `DatasetType.SERIES_BASED`.\n\n    Series-based means batch size affects number of returned time series in one batch. Which times for each time series are returned does not change.\n\n    The dataset provides multiple ways to access the data:\n\n    - **Iterable PyTorch DataLoader**: For batch processing.\n    - **Pandas DataFrame**: For loading the entire training, validation, test or all set at once.\n    - **Numpy array**: For loading the entire training, validation, test or all set at once.      \n    - See [loading data][loading-data] for more details.\n\n    The dataset is stored in a [PyTables](https://www.pytables.org/) database. The internal `SeriesBasedDataset` and `SeriesBasedInitializerDataset` classes (used only when calling [`set_dataset_config_and_initialize`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.set_dataset_config_and_initialize)) act as wrappers that implement the PyTorch [`Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) \n    interface. These wrappers are compatible with PyTorch\u2019s [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader), providing efficient parallel data loading. \n\n    The dataset configuration is done through the [`SeriesBasedConfig`](reference_series_based_config.md#references.SeriesBasedConfig) class.     \n\n    **Intended usage:**\n\n    1. Create an instance of the dataset with the desired data root by calling [`get_dataset`](reference_cesnet_database.md#cesnet_tszoo.datasets.databases.cesnet_database.CesnetDatabase.get_dataset). This will download the dataset if it has not been previously downloaded and return instance of dataset.\n    2. Create an instance of [`SeriesBasedConfig`](reference_series_based_config.md#references.SeriesBasedConfig) and set it using [`set_dataset_config_and_initialize`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.set_dataset_config_and_initialize). \n       This initializes the dataset, including data splitting (train/validation/test), fitting transformers (if needed), selecting features, and more. This is cached for later use.\n    3. Use [`get_train_dataloader`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_train_dataloader)/[`get_train_df`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_train_df)/[`get_train_numpy`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_train_numpy) to get training data for chosen model.\n    4. Validate the model and perform the hyperparameter optimalization on [`get_val_dataloader`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_val_dataloader)/[`get_val_df`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_val_df)/[`get_val_numpy`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_val_numpy).\n    5. Evaluate the model on [`get_test_dataloader`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_test_dataloader)/[`get_test_df`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_test_df)/[`get_test_numpy`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_test_numpy).     \n\n    Alternatively you can use [`load_benchmark`][cesnet_tszoo.benchmarks.load_benchmark]\n\n    1. Call [`load_benchmark`][cesnet_tszoo.benchmarks.load_benchmark] with the desired benchmark. You can use your own saved benchmark or you can use already built-in one. This will download the dataset and annotations (if available) if they have not been previously downloaded.\n    2. Retrieve the initialized dataset using [`get_initialized_dataset`](reference_benchmarks.md#cesnet_tszoo.benchmarks.Benchmark.get_initialized_dataset). This will provide a dataset that is ready to use.\n    3. Use [`get_train_dataloader`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_train_dataloader)/[`get_train_df`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_train_df)/[`get_train_numpy`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_train_numpy) to get training data for chosen model.\n    4. Validate the model and perform the hyperparameter optimalization on [`get_val_dataloader`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_val_dataloader)/[`get_val_df`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_val_df)/[`get_val_numpy`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_val_numpy).\n    5. Evaluate the model on [`get_test_dataloader`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_test_dataloader)/[`get_test_df`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_test_df)/[`get_test_numpy`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_test_numpy).                     \n    \"\"\"\n\n    dataset_config: Optional[SeriesBasedConfig] = field(default=None, init=False)\n    \"\"\"Configuration of the dataset.\"\"\"\n\n    train_dataset: Optional[SeriesBasedDataset] = field(default=None, init=False)\n    \"\"\"Training set as a `SeriesBasedDataset` instance wrapping the PyTables database.\"\"\"\n    val_dataset: Optional[SeriesBasedDataset] = field(default=None, init=False)\n    \"\"\"Validation set as a `SeriesBasedDataset` instance wrapping the PyTables database.\"\"\"\n    test_dataset: Optional[SeriesBasedDataset] = field(default=None, init=False)\n    \"\"\"Test set as a `SeriesBasedDataset` instance wrapping the PyTables database.\"\"\"\n    all_dataset: Optional[SeriesBasedDataset] = field(default=None, init=False)\n    \"\"\"All set as a `SeriesBasedDataset` instance wrapping the PyTables database.  \"\"\"\n\n    train_dataloader: Optional[SeriesBasedDataloader] = field(default=None, init=False)\n    \"\"\"Iterable PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for training set.\"\"\"\n    val_dataloader: Optional[SeriesBasedDataloader] = field(default=None, init=False)\n    \"\"\"Iterable PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for validation set.\"\"\"\n    test_dataloader: Optional[SeriesBasedDataloader] = field(default=None, init=False)\n    \"\"\"Iterable PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for test set.\"\"\"\n    all_dataloader: Optional[SeriesBasedDataloader] = field(default=None, init=False)\n    \"\"\"Iterable PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for all set.\"\"\"\n\n    dataloader_factory: SeriesBasedDataloaderFactory = field(default=SeriesBasedDataloaderFactory(), init=False)\n    \"\"\"Factory used to create SeriesBasedDataloader.  \"\"\"\n\n    dataset_type: DatasetType = field(default=DatasetType.SERIES_BASED, init=False)\n\n    _export_config_copy: Optional[SeriesBasedConfig] = field(default=None, init=False)\n\n    def __post_init__(self):\n        super().__post_init__()\n\n        self.logger.info(\"Dataset is series-based. Use cesnet_tszoo.configs.SeriesBasedConfig\")\n\n    def set_dataset_config_and_initialize(self, dataset_config: SeriesBasedConfig, display_config_details: Optional[Literal[\"text\", \"diagram\"]] = \"text\", workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n        \"\"\"\n        Initialize training set, validation set, test set etc.. This method must be called before any data can be accessed. It is required for the final initialization of [`dataset_config`](reference_series_based_config.md#references.SeriesBasedConfig).\n\n        The following configuration attributes are used during initialization:\n\n        Dataset config | Description\n        -------------- | -----------\n        `init_workers` | Specifies the number of workers to use for initialization. Applied when `workers` = \"config\".\n        `partial_fit_initialized_transformers` | Determines whether initialized transformers should be partially fitted on the training data.\n        `nan_threshold` | Filters out time series with missing values exceeding the specified threshold.\n\n\n        Parameters:\n            dataset_config: Desired configuration of the dataset.\n            display_config_details: Flag indicating whether and how to display the configuration values after initialization. `Default: text`  \n            workers: The number of workers to use during initialization. `Default: \"config\"`  \n        \"\"\"\n\n        assert dataset_config is not None, \"Used dataset_config cannot be None.\"\n        assert isinstance(dataset_config, SeriesBasedConfig), f\"This config is used for dataset of type '{dataset_config.dataset_type}'. Meanwhile this dataset is of type '{self.metadata.dataset_type}'.\"\n\n        super(SeriesBasedCesnetDataset, self).set_dataset_config_and_initialize(dataset_config, display_config_details, workers)\n\n    def update_dataset_config_and_initialize(self,\n                                             default_values: list[Number] | npt.NDArray[np.number] | dict[str, Number] | Number | Literal[\"default\"] | None | Literal[\"config\"] = \"config\",\n                                             train_batch_size: int | Literal[\"config\"] = \"config\",\n                                             val_batch_size: int | Literal[\"config\"] = \"config\",\n                                             test_batch_size: int | Literal[\"config\"] = \"config\",\n                                             all_batch_size: int | Literal[\"config\"] = \"config\",\n                                             preprocess_order: list[str, type] | Literal[\"config\"] = \"config\",\n                                             fill_missing_with: type | FillerType | Literal[\"mean_filler\", \"forward_filler\", \"linear_interpolation_filler\"] | None | Literal[\"config\"] = \"config\",\n                                             transform_with: type | list[Transformer] | np.ndarray[Transformer] | TransformerType | Transformer | Literal[\"min_max_scaler\", \"standard_scaler\", \"max_abs_scaler\", \"log_transformer\", \"l2_normalizer\"] | None | Literal[\"config\"] = \"config\",\n                                             handle_anomalies_with: type | AnomalyHandlerType | Literal[\"z-score\", \"interquartile_range\"] | None | Literal[\"config\"] = \"config\",\n                                             partial_fit_initialized_transformers: bool | Literal[\"config\"] = \"config\",\n                                             train_workers: int | Literal[\"config\"] = \"config\",\n                                             val_workers: int | Literal[\"config\"] = \"config\",\n                                             test_workers: int | Literal[\"config\"] = \"config\",\n                                             all_workers: int | Literal[\"config\"] = \"config\",\n                                             init_workers: int | Literal[\"config\"] = \"config\",\n                                             workers: int | Literal[\"config\"] = \"config\",\n                                             display_config_details: Optional[Literal[\"text\", \"diagram\"]] = None):\n        \"\"\"Used for updating selected configurations set in config.\n        Set parameter to `config` to keep it as it is config.\n        If exception is thrown during set, no changes are made.\n\n        Can affect following configuration: \n\n        Dataset config | Description\n        -------------- | -----------\n        `default_values` | Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature.\n        `train_batch_size` | Number of samples per batch for train set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n        `val_batch_size` | Number of samples per batch for val set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n        `test_batch_size` | Number of samples per batch for test set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n        `all_batch_size` | Number of samples per batch for all set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n        `preprocess_order` | Used order of when preprocesses are applied. Can be also used to add/remove custom handlers.\n        `fill_missing_with` | Defines how to fill missing values in the dataset.\n        `transform_with` | Defines the transformer to transform the dataset.\n        `handle_anomalies_with` | Defines the anomaly handler to handle anomalies in the train set.\n        `partial_fit_initialized_transformers` | If `True`, partial fitting on train set is performed when using initialized transformers.\n        `train_workers` | Number of workers for loading training data.\n        `val_workers` | Number of workers for loading validation data.\n        `test_workers` | Number of workers for loading test data.\n        `all_workers` | Number of workers for loading all data.\n        `init_workers` | Number of workers for dataset configuration.\n\n        Parameters:\n            default_values: Default values for missing data, applied before fillers. `Defaults: config`.            \n            train_batch_size: Number of samples per batch for train set. `Defaults: config`.\n            val_batch_size: Number of samples per batch for val set. `Defaults: config`.\n            test_batch_size: Number of samples per batch for test set. `Defaults: config`.\n            all_batch_size: Number of samples per batch for all set. `Defaults: config`.      \n            preprocess_order: Used order of when preprocesses are applied. Can be also used to add/remove custom handlers. `Defaults: config`.               \n            fill_missing_with: Defines how to fill missing values in the dataset. `Defaults: config`. \n            transform_with: Defines the transformer to transform the dataset. `Defaults: config`.  \n            handle_anomalies_with: Defines the anomaly handler to handle anomalies in the train set. `Defaults: config`.\n            partial_fit_initialized_transformers: If `True`, partial fitting on train set is performed when using initiliazed transformers. `Defaults: config`.    \n            train_workers: Number of workers for loading training data. `Defaults: config`.\n            val_workers: Number of workers for loading validation data. `Defaults: config`.\n            test_workers: Number of workers for loading test data. `Defaults: config`.\n            all_workers: Number of workers for loading all data.  `Defaults: config`.\n            init_workers: Number of workers for dataset configuration. `Defaults: config`.                          \n            workers: How many workers to use when updating configuration. `Defaults: config`.  \n            display_config_details: Whether config details should be displayed after configuration. `Defaults: False`. \n        \"\"\"\n\n        config_editor = SeriesBasedConfigEditor(self._export_config_copy,\n                                                default_values,\n                                                train_batch_size,\n                                                val_batch_size,\n                                                test_batch_size,\n                                                all_batch_size,\n                                                preprocess_order,\n                                                fill_missing_with,\n                                                transform_with,\n                                                handle_anomalies_with,\n                                                \"config\",\n                                                partial_fit_initialized_transformers,\n                                                train_workers,\n                                                val_workers,\n                                                test_workers,\n                                                all_workers,\n                                                init_workers\n                                                )\n\n        self._update_dataset_config_and_initialize(config_editor, workers, display_config_details)\n\n    def get_data_about_set(self, about: SplitType | Literal[\"train\", \"val\", \"test\", \"all\"]) -&gt; dict:\n        \"\"\"\n        Retrieves data related to the specified set.\n\n        Parameters:\n            about: Specifies the set to retrieve data about.\n\n        Returned dictionary contains:\n\n        - **ts_ids:** Ids of time series in `about` set.\n        - **TimeFormat.ID_TIME:** Times in `about` set, where time format is `TimeFormat.ID_TIME`.\n        - **TimeFormat.DATETIME:** Times in `about` set, where time format is `TimeFormat.DATETIME`.\n        - **TimeFormat.UNIX_TIME:** Times in `about` set, where time format is `TimeFormat.UNIX_TIME`.\n        - **TimeFormat.SHIFTED_UNIX_TIME:** Times in `about` set, where time format is `TimeFormat.SHIFTED_UNIX_TIME`.        \n\n        Returns:\n            Returns dictionary with details about set.\n        \"\"\"\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before getting data about set.\")\n\n        about = SplitType(about)\n\n        time_period = self.dataset_config.time_period\n\n        result = {}\n\n        if about == SplitType.TRAIN:\n            if not self.dataset_config.has_train():\n                raise ValueError(\"Train set is not used.\")\n            ts_ids = self.dataset_config.train_ts\n        elif about == SplitType.VAL:\n            if not self.dataset_config.has_val():\n                raise ValueError(\"Val set is not used.\")\n            ts_ids = self.dataset_config.val_ts\n        elif about == SplitType.TEST:\n            if not self.dataset_config.has_test():\n                raise ValueError(\"Test set is not used.\")\n            ts_ids = self.dataset_config.test_ts\n        elif about == SplitType.ALL:\n            if not self.dataset_config.has_all():\n                raise ValueError(\"All set is not used.\")\n            ts_ids = self.dataset_config.all_ts\n        else:\n            raise NotImplementedError(\"Should not happen\")\n\n        datetime_temp = np.array([datetime.fromtimestamp(time, tz=timezone.utc) for time in self.metadata.time_indices[TIME_COLUMN_NAME][time_period[ID_TIME_COLUMN_NAME]]])\n\n        result[\"ts_ids\"] = ts_ids.copy()\n        result[TimeFormat.ID_TIME] = time_period[ID_TIME_COLUMN_NAME].copy()\n        result[TimeFormat.DATETIME] = datetime_temp.copy()\n        result[TimeFormat.UNIX_TIME] = self.metadata.time_indices[TIME_COLUMN_NAME][time_period[ID_TIME_COLUMN_NAME]].copy()\n        result[TimeFormat.SHIFTED_UNIX_TIME] = self.metadata.time_indices[TIME_COLUMN_NAME][time_period[ID_TIME_COLUMN_NAME]] - self.metadata.time_indices[TIME_COLUMN_NAME][0]\n\n        return result\n\n    def _initialize_datasets(self) -&gt; None:\n        \"\"\"Called in [`set_dataset_config_and_initialize`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.set_dataset_config_and_initialize), this method initializes the set datasets (train, validation, test and all). \"\"\"\n\n        if self.dataset_config.has_train():\n            load_config = SeriesLoadConfig(self.dataset_config, SplitType.TRAIN)\n            self.train_dataset = SeriesBasedDataset(self.metadata.dataset_path, self.metadata.data_table_path, load_config)\n\n            self.logger.debug(\"train_dataset initiliazed.\")\n\n        if self.dataset_config.has_val():\n            load_config = SeriesLoadConfig(self.dataset_config, SplitType.VAL)\n            self.val_dataset = SeriesBasedDataset(self.metadata.dataset_path, self.metadata.data_table_path, load_config)\n\n            self.logger.debug(\"val_dataset initiliazed.\")\n\n        if self.dataset_config.has_test():\n            load_config = SeriesLoadConfig(self.dataset_config, SplitType.TEST)\n            self.test_dataset = SeriesBasedDataset(self.metadata.dataset_path, self.metadata.data_table_path, load_config)\n\n            self.logger.debug(\"test_dataset initiliazed.\")\n\n        if self.dataset_config.has_all():\n            load_config = SeriesLoadConfig(self.dataset_config, SplitType.ALL)\n            self.all_dataset = SeriesBasedDataset(self.metadata.dataset_path, self.metadata.data_table_path, load_config)\n\n            self.logger.debug(\"all_dataset initiliazed.\")\n\n    def _initialize_transformers_and_details(self, workers: int) -&gt; None:\n        \"\"\"\n        Called in [`set_dataset_config_and_initialize`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.set_dataset_config_and_initialize). \n\n        Goes through data to validate time series against `nan_threshold`, partial fit `transformers`, fit `anomaly handlers` and prepare `fillers`.\n        \"\"\"\n\n        ts_ids_to_take_for_all = np.array([])\n\n        if self.dataset_config.has_train():\n\n            ts_ids_to_take = self.__initialize_config_for_train_set(workers)\n            ts_ids_to_take_for_all = try_concatenate(ts_ids_to_take_for_all, np.array(ts_ids_to_take))\n\n            self.logger.debug(\"Train set updated: %s time series left.\", len(self.dataset_config.train_ts))\n\n        if self.dataset_config.has_val():\n            init_config = SeriesDatasetInitConfig(self.dataset_config, SplitType.VAL, PreprocessOrderGroup([]))\n\n            ts_ids_to_take = self.__initialize_config_for_non_fit_sets(init_config, workers, \"val\")\n            self.dataset_config.val_ts = self.dataset_config.val_ts[ts_ids_to_take]\n            self.dataset_config.val_ts_row_ranges = self.dataset_config.val_ts_row_ranges[ts_ids_to_take]\n            self.dataset_config._update_preprocess_order_supported_ids(self.dataset_config.val_preprocess_order, ts_ids_to_take)\n            ts_ids_to_take_for_all = try_concatenate(ts_ids_to_take_for_all, np.array(ts_ids_to_take))\n\n            self.logger.debug(\"Val set updated: %s time series left.\", len(self.dataset_config.val_ts))\n\n        if self.dataset_config.has_test():\n            init_config = SeriesDatasetInitConfig(self.dataset_config, SplitType.TEST, PreprocessOrderGroup([]))\n\n            ts_ids_to_take = self.__initialize_config_for_non_fit_sets(init_config, workers, \"test\")\n            self.dataset_config.test_ts = self.dataset_config.test_ts[ts_ids_to_take]\n            self.dataset_config.test_ts_row_ranges = self.dataset_config.test_ts_row_ranges[ts_ids_to_take]\n            self.dataset_config._update_preprocess_order_supported_ids(self.dataset_config.test_preprocess_order, ts_ids_to_take)\n            ts_ids_to_take_for_all = try_concatenate(ts_ids_to_take_for_all, np.array(ts_ids_to_take))\n\n            self.logger.debug(\"Test set updated: %s time series left.\", len(self.dataset_config.test_ts))\n\n        if self.dataset_config.has_all():\n\n            if not self.dataset_config.has_train() and not self.dataset_config.has_val() and not self.dataset_config.has_test():\n                init_config = SeriesDatasetInitConfig(self.dataset_config, SplitType.ALL, PreprocessOrderGroup([]))\n\n                ts_ids_to_take = self.__initialize_config_for_non_fit_sets(init_config, workers, \"all\")\n                self.dataset_config.all_ts = self.dataset_config.all_ts[ts_ids_to_take]\n                self.dataset_config.all_ts_row_ranges = self.dataset_config.all_ts_row_ranges[ts_ids_to_take]\n                self.dataset_config._update_preprocess_order_supported_ids(self.dataset_config.all_preprocess_order, ts_ids_to_take)\n            else:\n                self.dataset_config.all_ts = try_concatenate(self.dataset_config.train_ts, self.dataset_config.val_ts, self.dataset_config.test_ts)\n                self.dataset_config.all_ts_row_ranges = try_concatenate(self.dataset_config.train_ts_row_ranges, self.dataset_config.val_ts_row_ranges, self.dataset_config.test_ts_row_ranges)\n                self.dataset_config._update_preprocess_order_supported_ids(self.dataset_config.all_preprocess_order, ts_ids_to_take_for_all)\n\n            self.logger.debug(\"All set updated: %s time series left.\", len(self.dataset_config.all_ts))\n\n        self.logger.info(\"Dataset initialization complete. Configuration updated.\")\n\n    def __initialize_config_for_train_set(self, workers: int) -&gt; list[int]:\n        \"\"\"Initializes config for provided time series. \"\"\"\n\n        self.logger.info(\"Updating config for train set and fitting values.\")\n\n        is_first_cycle = True\n\n        ts_ids_to_take = []\n\n        groups = self.dataset_config._get_train_preprocess_init_order_groups()\n        for i, group in enumerate(groups):\n            ts_ids_to_take = []\n            self.logger.info(\"Starting fitting cycle %s/%s.\", i + 1, len(groups))\n\n            init_config = SeriesDatasetInitConfig(self.dataset_config, SplitType.TRAIN, group)\n            init_dataset = SeriesBasedInitializerDataset(self.metadata.dataset_path, self.metadata.data_table_path, init_config)\n\n            sampler = SequentialSampler(init_dataset)\n            dataloader = DataLoader(init_dataset, num_workers=workers, collate_fn=self._collate_fn, worker_init_fn=SeriesBasedInitializerDataset.worker_init_fn, persistent_workers=False, sampler=sampler)\n\n            if workers == 0:\n                init_dataset.pytables_worker_init()\n\n            for ts_id, data in enumerate(tqdm(dataloader, total=len(init_config.ts_row_ranges))):\n\n                init_dataset_return: InitDatasetReturn = data[0]\n\n                if init_dataset_return.is_under_nan_threshold:\n                    ts_ids_to_take.append(ts_id)\n\n                    # updates inner preprocessors passed from InitDataset\n                    fitted_inner_index = 0\n                    for inner_preprocess_order in group.preprocess_inner_orders:\n                        if inner_preprocess_order.should_be_fitted:\n                            inner_preprocess_order.holder.update_instance(init_dataset_return.preprocess_fitted_instances[fitted_inner_index].instance, ts_id)\n                            fitted_inner_index += 1\n\n                    # updates outer preprocessors based on passed train data from InitDataset\n                    for outer_preprocess_order in group.preprocess_outer_orders:\n                        if outer_preprocess_order.should_be_fitted:\n                            outer_preprocess_order.holder.fit(init_dataset_return.train_data, ts_id)\n\n                        if outer_preprocess_order.can_be_applied:\n                            init_dataset_return.train_data = outer_preprocess_order.holder.apply(init_dataset_return.train_data, ts_id)\n\n            if workers == 0:\n                init_dataset.cleanup()\n\n            # Update config based on filtered time series\n            if is_first_cycle:\n\n                if len(ts_ids_to_take) == 0:\n                    raise ValueError(\"No valid time series left in train set after applying nan_threshold.\")\n\n                self.dataset_config.train_ts_row_ranges = self.dataset_config.train_ts_row_ranges[ts_ids_to_take]\n                self.dataset_config.train_ts = self.dataset_config.train_ts[ts_ids_to_take]\n                self.dataset_config._update_preprocess_order_supported_ids(self.dataset_config.train_preprocess_order, ts_ids_to_take)\n                self.logger.debug(\"invalid ts_ids removed: %s time series left.\", len(ts_ids_to_take))\n\n                is_first_cycle = False\n\n        return ts_ids_to_take\n\n    def __initialize_config_for_non_fit_sets(self, init_config: SeriesDatasetInitConfig, workers: int, set_name: str) -&gt; np.ndarray:\n        \"\"\"Initializes config for provided time series without fitting. \"\"\"\n        init_dataset = SeriesBasedInitializerDataset(self.metadata.dataset_path, self.metadata.data_table_path, init_config)\n\n        sampler = SequentialSampler(init_dataset)\n        dataloader = DataLoader(init_dataset, num_workers=workers, collate_fn=self._collate_fn, worker_init_fn=SeriesBasedInitializerDataset.worker_init_fn, persistent_workers=False, sampler=sampler)\n\n        if workers == 0:\n            init_dataset.pytables_worker_init()\n\n        ts_ids_to_take = []\n\n        self.logger.info(\"Updating config for %s set.\", set_name)\n        for i, data in enumerate(tqdm(dataloader, total=len(init_config.ts_row_ranges))):\n            init_dataset_return: InitDatasetReturn = data[0]\n\n            if init_dataset_return.is_under_nan_threshold:\n                ts_ids_to_take.append(i)\n\n        if workers == 0:\n            init_dataset.cleanup()\n\n        if len(ts_ids_to_take) == 0:\n            raise ValueError(f\"No valid time series left in {set_name} set after applying nan_threshold.\")\n\n        return ts_ids_to_take\n\n    def _update_export_config_copy(self) -&gt; None:\n        \"\"\"\n        Called at the end of [`set_dataset_config_and_initialize`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.set_dataset_config_and_initialize). \n\n        Updates values of config used for saving config.\n        \"\"\"\n\n        self._export_config_copy.database_name = self.metadata.database_name\n\n        if self.dataset_config.has_train():\n            self._export_config_copy.train_ts = self.dataset_config.train_ts.copy()\n            self.logger.debug(\"Updated train_ts of _export_config_copy.\")\n\n        if self.dataset_config.has_val():\n            self._export_config_copy.val_ts = self.dataset_config.val_ts.copy()\n            self.logger.debug(\"Updated val_ts of _export_config_copy.\")\n\n        if self.dataset_config.has_test():\n            self._export_config_copy.test_ts = self.dataset_config.test_ts.copy()\n            self.logger.debug(\"Updated test_ts of _export_config_copy.\")\n\n        super(SeriesBasedCesnetDataset, self)._update_export_config_copy()\n\n    def apply_transformer(self, transform_with: type | list[Transformer] | np.ndarray[Transformer] | TransformerType | Transformer | Literal[\"min_max_scaler\", \"standard_scaler\", \"max_abs_scaler\", \"log_transformer\", \"l2_normalizer\"] | None | Literal[\"config\"] = \"config\",\n                          partial_fit_initialized_transformers: bool | Literal[\"config\"] = \"config\", workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n        \"\"\"Used for updating transformer and relevenat configurations set in config.\n        Set parameter to `config` to keep it as it is config.\n        If exception is thrown during set, no changes are made.\n\n        Affects following configuration: \n\n        Dataset config | Description\n        -------------- | -----------\n        `transform_with` | Defines the transformer to transform the dataset.\n        `partial_fit_initialized_transformers` | If `True`, partial fitting on train set is performed when using initialized transformers.\n\n        Parameters:\n            transform_with: Defines the transformer to transform the dataset. `Defaults: config`.  \n            partial_fit_initialized_transformers: If `True`, partial fitting on train set is performed when using initiliazed transformers. `Defaults: config`.  \n            workers: How many workers to use when setting new transformer. `Defaults: config`.      \n        \"\"\"\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating transformer values.\")\n\n        self.update_dataset_config_and_initialize(transform_with=transform_with, partial_fit_initialized_transformers=partial_fit_initialized_transformers, workers=workers)\n\n    def _get_singular_time_series_dataset(self, parent_dataset: SeriesBasedDataset, ts_id: int) -&gt; SeriesBasedDataset:\n        \"\"\"Returns dataset for single time series \"\"\"\n\n        temp = np.where(np.isin(parent_dataset.load_config.ts_row_ranges[self.metadata.ts_id_name], [ts_id]))[0]\n\n        if len(temp) == 0:\n            raise ValueError(f\"ts_id {ts_id} was not found in valid time series for this set. Available time series are: {parent_dataset.load_config.ts_row_ranges[self.metadata.ts_id_name]}\")\n\n        time_series_position = temp[0]\n\n        split_load_config = parent_dataset.load_config.create_split_copy(slice(time_series_position, time_series_position + 1))\n\n        dataset = SeriesBasedDataset(self.metadata.dataset_path, self.metadata.data_table_path, split_load_config)\n        self.logger.debug(\"Singular time series dataset initiliazed.\")\n\n        return dataset\n\n    def _get_data_for_plot(self, ts_id: int, feature_indices: np.ndarray[int], time_format: TimeFormat) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"Dataset type specific retrieval of data. \"\"\"\n\n        train_id_result, val_id_result, test_id_result = None, None, None\n\n        if (self.dataset_config.has_train()):\n            train_id_result = np.argwhere(np.isin(self.dataset_config.train_ts, ts_id)).ravel()\n        if (self.dataset_config.has_val()):\n            val_id_result = np.argwhere(np.isin(self.dataset_config.val_ts, ts_id)).ravel()\n        if (self.dataset_config.has_test()):\n            test_id_result = np.argwhere(np.isin(self.dataset_config.test_ts, ts_id)).ravel()\n\n        data = None\n        time_period = None\n\n        if self.dataset_config.has_train() and len(train_id_result) &gt; 0:\n            data = self.__get_ts_data_for_plot(self.train_dataset, ts_id, feature_indices)\n            time_period = self.get_data_about_set(SplitType.TRAIN)[time_format]\n            self.logger.debug(\"Valid ts_id found: %d\", train_id_result[0])\n\n        elif self.dataset_config.has_val() and len(val_id_result) &gt; 0:\n            data = self.__get_ts_data_for_plot(self.val_dataset, ts_id, feature_indices)\n            time_period = self.get_data_about_set(SplitType.VAL)[time_format]\n            self.logger.debug(\"Valid ts_id found: %d\", val_id_result[0])\n\n        elif self.dataset_config.has_test() and len(test_id_result) &gt; 0:\n            data = self.__get_ts_data_for_plot(self.test_dataset, ts_id, feature_indices)\n            time_period = self.get_data_about_set(SplitType.TEST)[time_format]\n            self.logger.debug(\"Valid ts_id found: %d\", test_id_result[0])\n        else:\n            raise ValueError(f\"Invalid ts_id '{ts_id}'. The provided ts_id is not found in the available time series IDs.\", self.dataset_config.train_ts, self.dataset_config.val_ts, self.dataset_config.test_ts)\n\n        return data, time_period\n\n    def __get_ts_data_for_plot(self, dataset: SeriesBasedDataset, ts_id: int, feature_indices: list[int]):\n        dataset = self._get_singular_time_series_dataset(dataset, ts_id)\n        dataloader = self.dataloader_factory.create_dataloader(dataset, self.dataset_config, 0, True, None)\n\n        temp_data = dataset_loaders.create_numpy_from_dataloader(dataloader, np.array([ts_id]), dataset.load_config.time_format, dataset.load_config.include_time, DatasetType.SERIES_BASED, True)\n\n        if (dataset.load_config.time_format == TimeFormat.DATETIME and dataset.load_config.include_time):\n            temp_data = temp_data[0]\n\n        temp_data = temp_data[0][:, feature_indices]\n\n        return temp_data\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.dataset_config","title":"dataset_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dataset_config: Optional[SeriesBasedConfig] = field(default=None, init=False)\n</code></pre> <p>Configuration of the dataset.</p>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.train_dataset","title":"train_dataset  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>train_dataset: Optional[SeriesBasedDataset] = field(default=None, init=False)\n</code></pre> <p>Training set as a <code>SeriesBasedDataset</code> instance wrapping the PyTables database.</p>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.val_dataset","title":"val_dataset  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>val_dataset: Optional[SeriesBasedDataset] = field(default=None, init=False)\n</code></pre> <p>Validation set as a <code>SeriesBasedDataset</code> instance wrapping the PyTables database.</p>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.test_dataset","title":"test_dataset  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>test_dataset: Optional[SeriesBasedDataset] = field(default=None, init=False)\n</code></pre> <p>Test set as a <code>SeriesBasedDataset</code> instance wrapping the PyTables database.</p>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.all_dataset","title":"all_dataset  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>all_dataset: Optional[SeriesBasedDataset] = field(default=None, init=False)\n</code></pre> <p>All set as a <code>SeriesBasedDataset</code> instance wrapping the PyTables database.</p>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.train_dataloader","title":"train_dataloader  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>train_dataloader: Optional[SeriesBasedDataloader] = field(default=None, init=False)\n</code></pre> <p>Iterable PyTorch <code>DataLoader</code> for training set.</p>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.val_dataloader","title":"val_dataloader  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>val_dataloader: Optional[SeriesBasedDataloader] = field(default=None, init=False)\n</code></pre> <p>Iterable PyTorch <code>DataLoader</code> for validation set.</p>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.test_dataloader","title":"test_dataloader  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>test_dataloader: Optional[SeriesBasedDataloader] = field(default=None, init=False)\n</code></pre> <p>Iterable PyTorch <code>DataLoader</code> for test set.</p>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.all_dataloader","title":"all_dataloader  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>all_dataloader: Optional[SeriesBasedDataloader] = field(default=None, init=False)\n</code></pre> <p>Iterable PyTorch <code>DataLoader</code> for all set.</p>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.dataloader_factory","title":"dataloader_factory  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dataloader_factory: SeriesBasedDataloaderFactory = field(default=SeriesBasedDataloaderFactory(), init=False)\n</code></pre> <p>Factory used to create SeriesBasedDataloader.</p>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.dataset_type","title":"dataset_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dataset_type: DatasetType = field(default=SERIES_BASED, init=False)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset._export_config_copy","title":"_export_config_copy  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>_export_config_copy: Optional[SeriesBasedConfig] = field(default=None, init=False)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata: DatasetMetadata\n</code></pre> <p>Holds various metadata used in dataset for its creation, loading data and similar.</p>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.related_to","title":"related_to  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>related_to: Optional[str] = field(default=None, init=False)\n</code></pre> <p>Name of file with relevant results to used benchmark.</p>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset._collate_fn","title":"_collate_fn  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>_collate_fn: Optional[Callable] = field(default=None, init=False)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.__init__","title":"__init__","text":"<pre><code>__init__(metadata: DatasetMetadata) -&gt; None\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> Source code in <code>cesnet_tszoo\\datasets\\series_based_cesnet_dataset.py</code> <pre><code>def __post_init__(self):\n    super().__post_init__()\n\n    self.logger.info(\"Dataset is series-based. Use cesnet_tszoo.configs.SeriesBasedConfig\")\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.set_dataset_config_and_initialize","title":"set_dataset_config_and_initialize","text":"<pre><code>set_dataset_config_and_initialize(dataset_config: SeriesBasedConfig, display_config_details: Optional[Literal['text', 'diagram']] = 'text', workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Initialize training set, validation set, test set etc.. This method must be called before any data can be accessed. It is required for the final initialization of <code>dataset_config</code>.</p> <p>The following configuration attributes are used during initialization:</p> Dataset config Description <code>init_workers</code> Specifies the number of workers to use for initialization. Applied when <code>workers</code> = \"config\". <code>partial_fit_initialized_transformers</code> Determines whether initialized transformers should be partially fitted on the training data. <code>nan_threshold</code> Filters out time series with missing values exceeding the specified threshold. <p>Parameters:</p> Name Type Description Default <code>dataset_config</code> <code>SeriesBasedConfig</code> <p>Desired configuration of the dataset.</p> required <code>display_config_details</code> <code>Optional[Literal['text', 'diagram']]</code> <p>Flag indicating whether and how to display the configuration values after initialization. <code>Default: text</code> </p> <code>'text'</code> <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use during initialization. <code>Default: \"config\"</code></p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\series_based_cesnet_dataset.py</code> <pre><code>def set_dataset_config_and_initialize(self, dataset_config: SeriesBasedConfig, display_config_details: Optional[Literal[\"text\", \"diagram\"]] = \"text\", workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"\n    Initialize training set, validation set, test set etc.. This method must be called before any data can be accessed. It is required for the final initialization of [`dataset_config`](reference_series_based_config.md#references.SeriesBasedConfig).\n\n    The following configuration attributes are used during initialization:\n\n    Dataset config | Description\n    -------------- | -----------\n    `init_workers` | Specifies the number of workers to use for initialization. Applied when `workers` = \"config\".\n    `partial_fit_initialized_transformers` | Determines whether initialized transformers should be partially fitted on the training data.\n    `nan_threshold` | Filters out time series with missing values exceeding the specified threshold.\n\n\n    Parameters:\n        dataset_config: Desired configuration of the dataset.\n        display_config_details: Flag indicating whether and how to display the configuration values after initialization. `Default: text`  \n        workers: The number of workers to use during initialization. `Default: \"config\"`  \n    \"\"\"\n\n    assert dataset_config is not None, \"Used dataset_config cannot be None.\"\n    assert isinstance(dataset_config, SeriesBasedConfig), f\"This config is used for dataset of type '{dataset_config.dataset_type}'. Meanwhile this dataset is of type '{self.metadata.dataset_type}'.\"\n\n    super(SeriesBasedCesnetDataset, self).set_dataset_config_and_initialize(dataset_config, display_config_details, workers)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.update_dataset_config_and_initialize","title":"update_dataset_config_and_initialize","text":"<pre><code>update_dataset_config_and_initialize(default_values: list[Number] | NDArray[number] | dict[str, Number] | Number | Literal['default'] | None | Literal['config'] = 'config', train_batch_size: int | Literal['config'] = 'config', val_batch_size: int | Literal['config'] = 'config', test_batch_size: int | Literal['config'] = 'config', all_batch_size: int | Literal['config'] = 'config', preprocess_order: list[str, type] | Literal['config'] = 'config', fill_missing_with: type | FillerType | Literal['mean_filler', 'forward_filler', 'linear_interpolation_filler'] | None | Literal['config'] = 'config', transform_with: type | list[Transformer] | ndarray[Transformer] | TransformerType | Transformer | Literal['min_max_scaler', 'standard_scaler', 'max_abs_scaler', 'log_transformer', 'l2_normalizer'] | None | Literal['config'] = 'config', handle_anomalies_with: type | AnomalyHandlerType | Literal['z-score', 'interquartile_range'] | None | Literal['config'] = 'config', partial_fit_initialized_transformers: bool | Literal['config'] = 'config', train_workers: int | Literal['config'] = 'config', val_workers: int | Literal['config'] = 'config', test_workers: int | Literal['config'] = 'config', all_workers: int | Literal['config'] = 'config', init_workers: int | Literal['config'] = 'config', workers: int | Literal['config'] = 'config', display_config_details: Optional[Literal['text', 'diagram']] = None)\n</code></pre> <p>Used for updating selected configurations set in config. Set parameter to <code>config</code> to keep it as it is config. If exception is thrown during set, no changes are made.</p> <p>Can affect following configuration: </p> Dataset config Description <code>default_values</code> Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature. <code>train_batch_size</code> Number of samples per batch for train set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>val_batch_size</code> Number of samples per batch for val set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>test_batch_size</code> Number of samples per batch for test set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>all_batch_size</code> Number of samples per batch for all set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>preprocess_order</code> Used order of when preprocesses are applied. Can be also used to add/remove custom handlers. <code>fill_missing_with</code> Defines how to fill missing values in the dataset. <code>transform_with</code> Defines the transformer to transform the dataset. <code>handle_anomalies_with</code> Defines the anomaly handler to handle anomalies in the train set. <code>partial_fit_initialized_transformers</code> If <code>True</code>, partial fitting on train set is performed when using initialized transformers. <code>train_workers</code> Number of workers for loading training data. <code>val_workers</code> Number of workers for loading validation data. <code>test_workers</code> Number of workers for loading test data. <code>all_workers</code> Number of workers for loading all data. <code>init_workers</code> Number of workers for dataset configuration. <p>Parameters:</p> Name Type Description Default <code>default_values</code> <code>list[Number] | NDArray[number] | dict[str, Number] | Number | Literal['default'] | None | Literal['config']</code> <p>Default values for missing data, applied before fillers. <code>Defaults: config</code>.            </p> <code>'config'</code> <code>train_batch_size</code> <code>int | Literal['config']</code> <p>Number of samples per batch for train set. <code>Defaults: config</code>.</p> <code>'config'</code> <code>val_batch_size</code> <code>int | Literal['config']</code> <p>Number of samples per batch for val set. <code>Defaults: config</code>.</p> <code>'config'</code> <code>test_batch_size</code> <code>int | Literal['config']</code> <p>Number of samples per batch for test set. <code>Defaults: config</code>.</p> <code>'config'</code> <code>all_batch_size</code> <code>int | Literal['config']</code> <p>Number of samples per batch for all set. <code>Defaults: config</code>.      </p> <code>'config'</code> <code>preprocess_order</code> <code>list[str, type] | Literal['config']</code> <p>Used order of when preprocesses are applied. Can be also used to add/remove custom handlers. <code>Defaults: config</code>.               </p> <code>'config'</code> <code>fill_missing_with</code> <code>type | FillerType | Literal['mean_filler', 'forward_filler', 'linear_interpolation_filler'] | None | Literal['config']</code> <p>Defines how to fill missing values in the dataset. <code>Defaults: config</code>. </p> <code>'config'</code> <code>transform_with</code> <code>type | list[Transformer] | ndarray[Transformer] | TransformerType | Transformer | Literal['min_max_scaler', 'standard_scaler', 'max_abs_scaler', 'log_transformer', 'l2_normalizer'] | None | Literal['config']</code> <p>Defines the transformer to transform the dataset. <code>Defaults: config</code>.  </p> <code>'config'</code> <code>handle_anomalies_with</code> <code>type | AnomalyHandlerType | Literal['z-score', 'interquartile_range'] | None | Literal['config']</code> <p>Defines the anomaly handler to handle anomalies in the train set. <code>Defaults: config</code>.</p> <code>'config'</code> <code>partial_fit_initialized_transformers</code> <code>bool | Literal['config']</code> <p>If <code>True</code>, partial fitting on train set is performed when using initiliazed transformers. <code>Defaults: config</code>.    </p> <code>'config'</code> <code>train_workers</code> <code>int | Literal['config']</code> <p>Number of workers for loading training data. <code>Defaults: config</code>.</p> <code>'config'</code> <code>val_workers</code> <code>int | Literal['config']</code> <p>Number of workers for loading validation data. <code>Defaults: config</code>.</p> <code>'config'</code> <code>test_workers</code> <code>int | Literal['config']</code> <p>Number of workers for loading test data. <code>Defaults: config</code>.</p> <code>'config'</code> <code>all_workers</code> <code>int | Literal['config']</code> <p>Number of workers for loading all data.  <code>Defaults: config</code>.</p> <code>'config'</code> <code>init_workers</code> <code>int | Literal['config']</code> <p>Number of workers for dataset configuration. <code>Defaults: config</code>.                          </p> <code>'config'</code> <code>workers</code> <code>int | Literal['config']</code> <p>How many workers to use when updating configuration. <code>Defaults: config</code>.  </p> <code>'config'</code> <code>display_config_details</code> <code>Optional[Literal['text', 'diagram']]</code> <p>Whether config details should be displayed after configuration. <code>Defaults: False</code>.</p> <code>None</code> Source code in <code>cesnet_tszoo\\datasets\\series_based_cesnet_dataset.py</code> <pre><code>def update_dataset_config_and_initialize(self,\n                                         default_values: list[Number] | npt.NDArray[np.number] | dict[str, Number] | Number | Literal[\"default\"] | None | Literal[\"config\"] = \"config\",\n                                         train_batch_size: int | Literal[\"config\"] = \"config\",\n                                         val_batch_size: int | Literal[\"config\"] = \"config\",\n                                         test_batch_size: int | Literal[\"config\"] = \"config\",\n                                         all_batch_size: int | Literal[\"config\"] = \"config\",\n                                         preprocess_order: list[str, type] | Literal[\"config\"] = \"config\",\n                                         fill_missing_with: type | FillerType | Literal[\"mean_filler\", \"forward_filler\", \"linear_interpolation_filler\"] | None | Literal[\"config\"] = \"config\",\n                                         transform_with: type | list[Transformer] | np.ndarray[Transformer] | TransformerType | Transformer | Literal[\"min_max_scaler\", \"standard_scaler\", \"max_abs_scaler\", \"log_transformer\", \"l2_normalizer\"] | None | Literal[\"config\"] = \"config\",\n                                         handle_anomalies_with: type | AnomalyHandlerType | Literal[\"z-score\", \"interquartile_range\"] | None | Literal[\"config\"] = \"config\",\n                                         partial_fit_initialized_transformers: bool | Literal[\"config\"] = \"config\",\n                                         train_workers: int | Literal[\"config\"] = \"config\",\n                                         val_workers: int | Literal[\"config\"] = \"config\",\n                                         test_workers: int | Literal[\"config\"] = \"config\",\n                                         all_workers: int | Literal[\"config\"] = \"config\",\n                                         init_workers: int | Literal[\"config\"] = \"config\",\n                                         workers: int | Literal[\"config\"] = \"config\",\n                                         display_config_details: Optional[Literal[\"text\", \"diagram\"]] = None):\n    \"\"\"Used for updating selected configurations set in config.\n    Set parameter to `config` to keep it as it is config.\n    If exception is thrown during set, no changes are made.\n\n    Can affect following configuration: \n\n    Dataset config | Description\n    -------------- | -----------\n    `default_values` | Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature.\n    `train_batch_size` | Number of samples per batch for train set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `val_batch_size` | Number of samples per batch for val set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `test_batch_size` | Number of samples per batch for test set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `all_batch_size` | Number of samples per batch for all set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `preprocess_order` | Used order of when preprocesses are applied. Can be also used to add/remove custom handlers.\n    `fill_missing_with` | Defines how to fill missing values in the dataset.\n    `transform_with` | Defines the transformer to transform the dataset.\n    `handle_anomalies_with` | Defines the anomaly handler to handle anomalies in the train set.\n    `partial_fit_initialized_transformers` | If `True`, partial fitting on train set is performed when using initialized transformers.\n    `train_workers` | Number of workers for loading training data.\n    `val_workers` | Number of workers for loading validation data.\n    `test_workers` | Number of workers for loading test data.\n    `all_workers` | Number of workers for loading all data.\n    `init_workers` | Number of workers for dataset configuration.\n\n    Parameters:\n        default_values: Default values for missing data, applied before fillers. `Defaults: config`.            \n        train_batch_size: Number of samples per batch for train set. `Defaults: config`.\n        val_batch_size: Number of samples per batch for val set. `Defaults: config`.\n        test_batch_size: Number of samples per batch for test set. `Defaults: config`.\n        all_batch_size: Number of samples per batch for all set. `Defaults: config`.      \n        preprocess_order: Used order of when preprocesses are applied. Can be also used to add/remove custom handlers. `Defaults: config`.               \n        fill_missing_with: Defines how to fill missing values in the dataset. `Defaults: config`. \n        transform_with: Defines the transformer to transform the dataset. `Defaults: config`.  \n        handle_anomalies_with: Defines the anomaly handler to handle anomalies in the train set. `Defaults: config`.\n        partial_fit_initialized_transformers: If `True`, partial fitting on train set is performed when using initiliazed transformers. `Defaults: config`.    \n        train_workers: Number of workers for loading training data. `Defaults: config`.\n        val_workers: Number of workers for loading validation data. `Defaults: config`.\n        test_workers: Number of workers for loading test data. `Defaults: config`.\n        all_workers: Number of workers for loading all data.  `Defaults: config`.\n        init_workers: Number of workers for dataset configuration. `Defaults: config`.                          \n        workers: How many workers to use when updating configuration. `Defaults: config`.  \n        display_config_details: Whether config details should be displayed after configuration. `Defaults: False`. \n    \"\"\"\n\n    config_editor = SeriesBasedConfigEditor(self._export_config_copy,\n                                            default_values,\n                                            train_batch_size,\n                                            val_batch_size,\n                                            test_batch_size,\n                                            all_batch_size,\n                                            preprocess_order,\n                                            fill_missing_with,\n                                            transform_with,\n                                            handle_anomalies_with,\n                                            \"config\",\n                                            partial_fit_initialized_transformers,\n                                            train_workers,\n                                            val_workers,\n                                            test_workers,\n                                            all_workers,\n                                            init_workers\n                                            )\n\n    self._update_dataset_config_and_initialize(config_editor, workers, display_config_details)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_data_about_set","title":"get_data_about_set","text":"<pre><code>get_data_about_set(about: SplitType | Literal['train', 'val', 'test', 'all']) -&gt; dict\n</code></pre> <p>Retrieves data related to the specified set.</p> <p>Parameters:</p> Name Type Description Default <code>about</code> <code>SplitType | Literal['train', 'val', 'test', 'all']</code> <p>Specifies the set to retrieve data about.</p> required <p>Returned dictionary contains:</p> <ul> <li>ts_ids: Ids of time series in <code>about</code> set.</li> <li>TimeFormat.ID_TIME: Times in <code>about</code> set, where time format is <code>TimeFormat.ID_TIME</code>.</li> <li>TimeFormat.DATETIME: Times in <code>about</code> set, where time format is <code>TimeFormat.DATETIME</code>.</li> <li>TimeFormat.UNIX_TIME: Times in <code>about</code> set, where time format is <code>TimeFormat.UNIX_TIME</code>.</li> <li>TimeFormat.SHIFTED_UNIX_TIME: Times in <code>about</code> set, where time format is <code>TimeFormat.SHIFTED_UNIX_TIME</code>.        </li> </ul> <p>Returns:</p> Type Description <code>dict</code> <p>Returns dictionary with details about set.</p> Source code in <code>cesnet_tszoo\\datasets\\series_based_cesnet_dataset.py</code> <pre><code>def get_data_about_set(self, about: SplitType | Literal[\"train\", \"val\", \"test\", \"all\"]) -&gt; dict:\n    \"\"\"\n    Retrieves data related to the specified set.\n\n    Parameters:\n        about: Specifies the set to retrieve data about.\n\n    Returned dictionary contains:\n\n    - **ts_ids:** Ids of time series in `about` set.\n    - **TimeFormat.ID_TIME:** Times in `about` set, where time format is `TimeFormat.ID_TIME`.\n    - **TimeFormat.DATETIME:** Times in `about` set, where time format is `TimeFormat.DATETIME`.\n    - **TimeFormat.UNIX_TIME:** Times in `about` set, where time format is `TimeFormat.UNIX_TIME`.\n    - **TimeFormat.SHIFTED_UNIX_TIME:** Times in `about` set, where time format is `TimeFormat.SHIFTED_UNIX_TIME`.        \n\n    Returns:\n        Returns dictionary with details about set.\n    \"\"\"\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before getting data about set.\")\n\n    about = SplitType(about)\n\n    time_period = self.dataset_config.time_period\n\n    result = {}\n\n    if about == SplitType.TRAIN:\n        if not self.dataset_config.has_train():\n            raise ValueError(\"Train set is not used.\")\n        ts_ids = self.dataset_config.train_ts\n    elif about == SplitType.VAL:\n        if not self.dataset_config.has_val():\n            raise ValueError(\"Val set is not used.\")\n        ts_ids = self.dataset_config.val_ts\n    elif about == SplitType.TEST:\n        if not self.dataset_config.has_test():\n            raise ValueError(\"Test set is not used.\")\n        ts_ids = self.dataset_config.test_ts\n    elif about == SplitType.ALL:\n        if not self.dataset_config.has_all():\n            raise ValueError(\"All set is not used.\")\n        ts_ids = self.dataset_config.all_ts\n    else:\n        raise NotImplementedError(\"Should not happen\")\n\n    datetime_temp = np.array([datetime.fromtimestamp(time, tz=timezone.utc) for time in self.metadata.time_indices[TIME_COLUMN_NAME][time_period[ID_TIME_COLUMN_NAME]]])\n\n    result[\"ts_ids\"] = ts_ids.copy()\n    result[TimeFormat.ID_TIME] = time_period[ID_TIME_COLUMN_NAME].copy()\n    result[TimeFormat.DATETIME] = datetime_temp.copy()\n    result[TimeFormat.UNIX_TIME] = self.metadata.time_indices[TIME_COLUMN_NAME][time_period[ID_TIME_COLUMN_NAME]].copy()\n    result[TimeFormat.SHIFTED_UNIX_TIME] = self.metadata.time_indices[TIME_COLUMN_NAME][time_period[ID_TIME_COLUMN_NAME]] - self.metadata.time_indices[TIME_COLUMN_NAME][0]\n\n    return result\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset._initialize_datasets","title":"_initialize_datasets","text":"<pre><code>_initialize_datasets() -&gt; None\n</code></pre> <p>Called in <code>set_dataset_config_and_initialize</code>, this method initializes the set datasets (train, validation, test and all).</p> Source code in <code>cesnet_tszoo\\datasets\\series_based_cesnet_dataset.py</code> <pre><code>def _initialize_datasets(self) -&gt; None:\n    \"\"\"Called in [`set_dataset_config_and_initialize`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.set_dataset_config_and_initialize), this method initializes the set datasets (train, validation, test and all). \"\"\"\n\n    if self.dataset_config.has_train():\n        load_config = SeriesLoadConfig(self.dataset_config, SplitType.TRAIN)\n        self.train_dataset = SeriesBasedDataset(self.metadata.dataset_path, self.metadata.data_table_path, load_config)\n\n        self.logger.debug(\"train_dataset initiliazed.\")\n\n    if self.dataset_config.has_val():\n        load_config = SeriesLoadConfig(self.dataset_config, SplitType.VAL)\n        self.val_dataset = SeriesBasedDataset(self.metadata.dataset_path, self.metadata.data_table_path, load_config)\n\n        self.logger.debug(\"val_dataset initiliazed.\")\n\n    if self.dataset_config.has_test():\n        load_config = SeriesLoadConfig(self.dataset_config, SplitType.TEST)\n        self.test_dataset = SeriesBasedDataset(self.metadata.dataset_path, self.metadata.data_table_path, load_config)\n\n        self.logger.debug(\"test_dataset initiliazed.\")\n\n    if self.dataset_config.has_all():\n        load_config = SeriesLoadConfig(self.dataset_config, SplitType.ALL)\n        self.all_dataset = SeriesBasedDataset(self.metadata.dataset_path, self.metadata.data_table_path, load_config)\n\n        self.logger.debug(\"all_dataset initiliazed.\")\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset._initialize_transformers_and_details","title":"_initialize_transformers_and_details","text":"<pre><code>_initialize_transformers_and_details(workers: int) -&gt; None\n</code></pre> <p>Called in <code>set_dataset_config_and_initialize</code>. </p> <p>Goes through data to validate time series against <code>nan_threshold</code>, partial fit <code>transformers</code>, fit <code>anomaly handlers</code> and prepare <code>fillers</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\series_based_cesnet_dataset.py</code> <pre><code>def _initialize_transformers_and_details(self, workers: int) -&gt; None:\n    \"\"\"\n    Called in [`set_dataset_config_and_initialize`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.set_dataset_config_and_initialize). \n\n    Goes through data to validate time series against `nan_threshold`, partial fit `transformers`, fit `anomaly handlers` and prepare `fillers`.\n    \"\"\"\n\n    ts_ids_to_take_for_all = np.array([])\n\n    if self.dataset_config.has_train():\n\n        ts_ids_to_take = self.__initialize_config_for_train_set(workers)\n        ts_ids_to_take_for_all = try_concatenate(ts_ids_to_take_for_all, np.array(ts_ids_to_take))\n\n        self.logger.debug(\"Train set updated: %s time series left.\", len(self.dataset_config.train_ts))\n\n    if self.dataset_config.has_val():\n        init_config = SeriesDatasetInitConfig(self.dataset_config, SplitType.VAL, PreprocessOrderGroup([]))\n\n        ts_ids_to_take = self.__initialize_config_for_non_fit_sets(init_config, workers, \"val\")\n        self.dataset_config.val_ts = self.dataset_config.val_ts[ts_ids_to_take]\n        self.dataset_config.val_ts_row_ranges = self.dataset_config.val_ts_row_ranges[ts_ids_to_take]\n        self.dataset_config._update_preprocess_order_supported_ids(self.dataset_config.val_preprocess_order, ts_ids_to_take)\n        ts_ids_to_take_for_all = try_concatenate(ts_ids_to_take_for_all, np.array(ts_ids_to_take))\n\n        self.logger.debug(\"Val set updated: %s time series left.\", len(self.dataset_config.val_ts))\n\n    if self.dataset_config.has_test():\n        init_config = SeriesDatasetInitConfig(self.dataset_config, SplitType.TEST, PreprocessOrderGroup([]))\n\n        ts_ids_to_take = self.__initialize_config_for_non_fit_sets(init_config, workers, \"test\")\n        self.dataset_config.test_ts = self.dataset_config.test_ts[ts_ids_to_take]\n        self.dataset_config.test_ts_row_ranges = self.dataset_config.test_ts_row_ranges[ts_ids_to_take]\n        self.dataset_config._update_preprocess_order_supported_ids(self.dataset_config.test_preprocess_order, ts_ids_to_take)\n        ts_ids_to_take_for_all = try_concatenate(ts_ids_to_take_for_all, np.array(ts_ids_to_take))\n\n        self.logger.debug(\"Test set updated: %s time series left.\", len(self.dataset_config.test_ts))\n\n    if self.dataset_config.has_all():\n\n        if not self.dataset_config.has_train() and not self.dataset_config.has_val() and not self.dataset_config.has_test():\n            init_config = SeriesDatasetInitConfig(self.dataset_config, SplitType.ALL, PreprocessOrderGroup([]))\n\n            ts_ids_to_take = self.__initialize_config_for_non_fit_sets(init_config, workers, \"all\")\n            self.dataset_config.all_ts = self.dataset_config.all_ts[ts_ids_to_take]\n            self.dataset_config.all_ts_row_ranges = self.dataset_config.all_ts_row_ranges[ts_ids_to_take]\n            self.dataset_config._update_preprocess_order_supported_ids(self.dataset_config.all_preprocess_order, ts_ids_to_take)\n        else:\n            self.dataset_config.all_ts = try_concatenate(self.dataset_config.train_ts, self.dataset_config.val_ts, self.dataset_config.test_ts)\n            self.dataset_config.all_ts_row_ranges = try_concatenate(self.dataset_config.train_ts_row_ranges, self.dataset_config.val_ts_row_ranges, self.dataset_config.test_ts_row_ranges)\n            self.dataset_config._update_preprocess_order_supported_ids(self.dataset_config.all_preprocess_order, ts_ids_to_take_for_all)\n\n        self.logger.debug(\"All set updated: %s time series left.\", len(self.dataset_config.all_ts))\n\n    self.logger.info(\"Dataset initialization complete. Configuration updated.\")\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.__initialize_config_for_train_set","title":"__initialize_config_for_train_set","text":"<pre><code>__initialize_config_for_train_set(workers: int) -&gt; list[int]\n</code></pre> <p>Initializes config for provided time series.</p> Source code in <code>cesnet_tszoo\\datasets\\series_based_cesnet_dataset.py</code> <pre><code>def __initialize_config_for_train_set(self, workers: int) -&gt; list[int]:\n    \"\"\"Initializes config for provided time series. \"\"\"\n\n    self.logger.info(\"Updating config for train set and fitting values.\")\n\n    is_first_cycle = True\n\n    ts_ids_to_take = []\n\n    groups = self.dataset_config._get_train_preprocess_init_order_groups()\n    for i, group in enumerate(groups):\n        ts_ids_to_take = []\n        self.logger.info(\"Starting fitting cycle %s/%s.\", i + 1, len(groups))\n\n        init_config = SeriesDatasetInitConfig(self.dataset_config, SplitType.TRAIN, group)\n        init_dataset = SeriesBasedInitializerDataset(self.metadata.dataset_path, self.metadata.data_table_path, init_config)\n\n        sampler = SequentialSampler(init_dataset)\n        dataloader = DataLoader(init_dataset, num_workers=workers, collate_fn=self._collate_fn, worker_init_fn=SeriesBasedInitializerDataset.worker_init_fn, persistent_workers=False, sampler=sampler)\n\n        if workers == 0:\n            init_dataset.pytables_worker_init()\n\n        for ts_id, data in enumerate(tqdm(dataloader, total=len(init_config.ts_row_ranges))):\n\n            init_dataset_return: InitDatasetReturn = data[0]\n\n            if init_dataset_return.is_under_nan_threshold:\n                ts_ids_to_take.append(ts_id)\n\n                # updates inner preprocessors passed from InitDataset\n                fitted_inner_index = 0\n                for inner_preprocess_order in group.preprocess_inner_orders:\n                    if inner_preprocess_order.should_be_fitted:\n                        inner_preprocess_order.holder.update_instance(init_dataset_return.preprocess_fitted_instances[fitted_inner_index].instance, ts_id)\n                        fitted_inner_index += 1\n\n                # updates outer preprocessors based on passed train data from InitDataset\n                for outer_preprocess_order in group.preprocess_outer_orders:\n                    if outer_preprocess_order.should_be_fitted:\n                        outer_preprocess_order.holder.fit(init_dataset_return.train_data, ts_id)\n\n                    if outer_preprocess_order.can_be_applied:\n                        init_dataset_return.train_data = outer_preprocess_order.holder.apply(init_dataset_return.train_data, ts_id)\n\n        if workers == 0:\n            init_dataset.cleanup()\n\n        # Update config based on filtered time series\n        if is_first_cycle:\n\n            if len(ts_ids_to_take) == 0:\n                raise ValueError(\"No valid time series left in train set after applying nan_threshold.\")\n\n            self.dataset_config.train_ts_row_ranges = self.dataset_config.train_ts_row_ranges[ts_ids_to_take]\n            self.dataset_config.train_ts = self.dataset_config.train_ts[ts_ids_to_take]\n            self.dataset_config._update_preprocess_order_supported_ids(self.dataset_config.train_preprocess_order, ts_ids_to_take)\n            self.logger.debug(\"invalid ts_ids removed: %s time series left.\", len(ts_ids_to_take))\n\n            is_first_cycle = False\n\n    return ts_ids_to_take\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.__initialize_config_for_non_fit_sets","title":"__initialize_config_for_non_fit_sets","text":"<pre><code>__initialize_config_for_non_fit_sets(init_config: SeriesDatasetInitConfig, workers: int, set_name: str) -&gt; np.ndarray\n</code></pre> <p>Initializes config for provided time series without fitting.</p> Source code in <code>cesnet_tszoo\\datasets\\series_based_cesnet_dataset.py</code> <pre><code>def __initialize_config_for_non_fit_sets(self, init_config: SeriesDatasetInitConfig, workers: int, set_name: str) -&gt; np.ndarray:\n    \"\"\"Initializes config for provided time series without fitting. \"\"\"\n    init_dataset = SeriesBasedInitializerDataset(self.metadata.dataset_path, self.metadata.data_table_path, init_config)\n\n    sampler = SequentialSampler(init_dataset)\n    dataloader = DataLoader(init_dataset, num_workers=workers, collate_fn=self._collate_fn, worker_init_fn=SeriesBasedInitializerDataset.worker_init_fn, persistent_workers=False, sampler=sampler)\n\n    if workers == 0:\n        init_dataset.pytables_worker_init()\n\n    ts_ids_to_take = []\n\n    self.logger.info(\"Updating config for %s set.\", set_name)\n    for i, data in enumerate(tqdm(dataloader, total=len(init_config.ts_row_ranges))):\n        init_dataset_return: InitDatasetReturn = data[0]\n\n        if init_dataset_return.is_under_nan_threshold:\n            ts_ids_to_take.append(i)\n\n    if workers == 0:\n        init_dataset.cleanup()\n\n    if len(ts_ids_to_take) == 0:\n        raise ValueError(f\"No valid time series left in {set_name} set after applying nan_threshold.\")\n\n    return ts_ids_to_take\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset._update_export_config_copy","title":"_update_export_config_copy","text":"<pre><code>_update_export_config_copy() -&gt; None\n</code></pre> <p>Called at the end of <code>set_dataset_config_and_initialize</code>. </p> <p>Updates values of config used for saving config.</p> Source code in <code>cesnet_tszoo\\datasets\\series_based_cesnet_dataset.py</code> <pre><code>def _update_export_config_copy(self) -&gt; None:\n    \"\"\"\n    Called at the end of [`set_dataset_config_and_initialize`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.set_dataset_config_and_initialize). \n\n    Updates values of config used for saving config.\n    \"\"\"\n\n    self._export_config_copy.database_name = self.metadata.database_name\n\n    if self.dataset_config.has_train():\n        self._export_config_copy.train_ts = self.dataset_config.train_ts.copy()\n        self.logger.debug(\"Updated train_ts of _export_config_copy.\")\n\n    if self.dataset_config.has_val():\n        self._export_config_copy.val_ts = self.dataset_config.val_ts.copy()\n        self.logger.debug(\"Updated val_ts of _export_config_copy.\")\n\n    if self.dataset_config.has_test():\n        self._export_config_copy.test_ts = self.dataset_config.test_ts.copy()\n        self.logger.debug(\"Updated test_ts of _export_config_copy.\")\n\n    super(SeriesBasedCesnetDataset, self)._update_export_config_copy()\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.apply_transformer","title":"apply_transformer","text":"<pre><code>apply_transformer(transform_with: type | list[Transformer] | ndarray[Transformer] | TransformerType | Transformer | Literal['min_max_scaler', 'standard_scaler', 'max_abs_scaler', 'log_transformer', 'l2_normalizer'] | None | Literal['config'] = 'config', partial_fit_initialized_transformers: bool | Literal['config'] = 'config', workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Used for updating transformer and relevenat configurations set in config. Set parameter to <code>config</code> to keep it as it is config. If exception is thrown during set, no changes are made.</p> <p>Affects following configuration: </p> Dataset config Description <code>transform_with</code> Defines the transformer to transform the dataset. <code>partial_fit_initialized_transformers</code> If <code>True</code>, partial fitting on train set is performed when using initialized transformers. <p>Parameters:</p> Name Type Description Default <code>transform_with</code> <code>type | list[Transformer] | ndarray[Transformer] | TransformerType | Transformer | Literal['min_max_scaler', 'standard_scaler', 'max_abs_scaler', 'log_transformer', 'l2_normalizer'] | None | Literal['config']</code> <p>Defines the transformer to transform the dataset. <code>Defaults: config</code>.  </p> <code>'config'</code> <code>partial_fit_initialized_transformers</code> <code>bool | Literal['config']</code> <p>If <code>True</code>, partial fitting on train set is performed when using initiliazed transformers. <code>Defaults: config</code>.  </p> <code>'config'</code> <code>workers</code> <code>int | Literal['config']</code> <p>How many workers to use when setting new transformer. <code>Defaults: config</code>.</p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\series_based_cesnet_dataset.py</code> <pre><code>def apply_transformer(self, transform_with: type | list[Transformer] | np.ndarray[Transformer] | TransformerType | Transformer | Literal[\"min_max_scaler\", \"standard_scaler\", \"max_abs_scaler\", \"log_transformer\", \"l2_normalizer\"] | None | Literal[\"config\"] = \"config\",\n                      partial_fit_initialized_transformers: bool | Literal[\"config\"] = \"config\", workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"Used for updating transformer and relevenat configurations set in config.\n    Set parameter to `config` to keep it as it is config.\n    If exception is thrown during set, no changes are made.\n\n    Affects following configuration: \n\n    Dataset config | Description\n    -------------- | -----------\n    `transform_with` | Defines the transformer to transform the dataset.\n    `partial_fit_initialized_transformers` | If `True`, partial fitting on train set is performed when using initialized transformers.\n\n    Parameters:\n        transform_with: Defines the transformer to transform the dataset. `Defaults: config`.  \n        partial_fit_initialized_transformers: If `True`, partial fitting on train set is performed when using initiliazed transformers. `Defaults: config`.  \n        workers: How many workers to use when setting new transformer. `Defaults: config`.      \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating transformer values.\")\n\n    self.update_dataset_config_and_initialize(transform_with=transform_with, partial_fit_initialized_transformers=partial_fit_initialized_transformers, workers=workers)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset._get_singular_time_series_dataset","title":"_get_singular_time_series_dataset","text":"<pre><code>_get_singular_time_series_dataset(parent_dataset: SeriesBasedDataset, ts_id: int) -&gt; SeriesBasedDataset\n</code></pre> <p>Returns dataset for single time series</p> Source code in <code>cesnet_tszoo\\datasets\\series_based_cesnet_dataset.py</code> <pre><code>def _get_singular_time_series_dataset(self, parent_dataset: SeriesBasedDataset, ts_id: int) -&gt; SeriesBasedDataset:\n    \"\"\"Returns dataset for single time series \"\"\"\n\n    temp = np.where(np.isin(parent_dataset.load_config.ts_row_ranges[self.metadata.ts_id_name], [ts_id]))[0]\n\n    if len(temp) == 0:\n        raise ValueError(f\"ts_id {ts_id} was not found in valid time series for this set. Available time series are: {parent_dataset.load_config.ts_row_ranges[self.metadata.ts_id_name]}\")\n\n    time_series_position = temp[0]\n\n    split_load_config = parent_dataset.load_config.create_split_copy(slice(time_series_position, time_series_position + 1))\n\n    dataset = SeriesBasedDataset(self.metadata.dataset_path, self.metadata.data_table_path, split_load_config)\n    self.logger.debug(\"Singular time series dataset initiliazed.\")\n\n    return dataset\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset._get_data_for_plot","title":"_get_data_for_plot","text":"<pre><code>_get_data_for_plot(ts_id: int, feature_indices: ndarray[int], time_format: TimeFormat) -&gt; tuple[np.ndarray, np.ndarray]\n</code></pre> <p>Dataset type specific retrieval of data.</p> Source code in <code>cesnet_tszoo\\datasets\\series_based_cesnet_dataset.py</code> <pre><code>def _get_data_for_plot(self, ts_id: int, feature_indices: np.ndarray[int], time_format: TimeFormat) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Dataset type specific retrieval of data. \"\"\"\n\n    train_id_result, val_id_result, test_id_result = None, None, None\n\n    if (self.dataset_config.has_train()):\n        train_id_result = np.argwhere(np.isin(self.dataset_config.train_ts, ts_id)).ravel()\n    if (self.dataset_config.has_val()):\n        val_id_result = np.argwhere(np.isin(self.dataset_config.val_ts, ts_id)).ravel()\n    if (self.dataset_config.has_test()):\n        test_id_result = np.argwhere(np.isin(self.dataset_config.test_ts, ts_id)).ravel()\n\n    data = None\n    time_period = None\n\n    if self.dataset_config.has_train() and len(train_id_result) &gt; 0:\n        data = self.__get_ts_data_for_plot(self.train_dataset, ts_id, feature_indices)\n        time_period = self.get_data_about_set(SplitType.TRAIN)[time_format]\n        self.logger.debug(\"Valid ts_id found: %d\", train_id_result[0])\n\n    elif self.dataset_config.has_val() and len(val_id_result) &gt; 0:\n        data = self.__get_ts_data_for_plot(self.val_dataset, ts_id, feature_indices)\n        time_period = self.get_data_about_set(SplitType.VAL)[time_format]\n        self.logger.debug(\"Valid ts_id found: %d\", val_id_result[0])\n\n    elif self.dataset_config.has_test() and len(test_id_result) &gt; 0:\n        data = self.__get_ts_data_for_plot(self.test_dataset, ts_id, feature_indices)\n        time_period = self.get_data_about_set(SplitType.TEST)[time_format]\n        self.logger.debug(\"Valid ts_id found: %d\", test_id_result[0])\n    else:\n        raise ValueError(f\"Invalid ts_id '{ts_id}'. The provided ts_id is not found in the available time series IDs.\", self.dataset_config.train_ts, self.dataset_config.val_ts, self.dataset_config.test_ts)\n\n    return data, time_period\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.__get_ts_data_for_plot","title":"__get_ts_data_for_plot","text":"<pre><code>__get_ts_data_for_plot(dataset: SeriesBasedDataset, ts_id: int, feature_indices: list[int])\n</code></pre> Source code in <code>cesnet_tszoo\\datasets\\series_based_cesnet_dataset.py</code> <pre><code>def __get_ts_data_for_plot(self, dataset: SeriesBasedDataset, ts_id: int, feature_indices: list[int]):\n    dataset = self._get_singular_time_series_dataset(dataset, ts_id)\n    dataloader = self.dataloader_factory.create_dataloader(dataset, self.dataset_config, 0, True, None)\n\n    temp_data = dataset_loaders.create_numpy_from_dataloader(dataloader, np.array([ts_id]), dataset.load_config.time_format, dataset.load_config.include_time, DatasetType.SERIES_BASED, True)\n\n    if (dataset.load_config.time_format == TimeFormat.DATETIME and dataset.load_config.include_time):\n        temp_data = temp_data[0]\n\n    temp_data = temp_data[0][:, feature_indices]\n\n    return temp_data\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_train_dataloader","title":"get_train_dataloader","text":"<pre><code>get_train_dataloader(ts_id: int | None = None, workers: int | Literal['config'] = 'config', **kwargs) -&gt; DataLoader\n</code></pre> <p>Returns a PyTorch <code>DataLoader</code> for training set.</p> <p>The <code>DataLoader</code> is created on the first call and cached for subsequent use.  The cached dataloader is cleared when either <code>get_train_df</code> or <code>get_train_numpy</code> is called.</p> <p>The structure of the returned batch depends on the <code>time_format</code> and whether <code>sliding_window_size</code> is used:</p> <ul> <li>When <code>sliding_window_size</code> is used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> <li><code>np.ndarray</code> of times with shape <code>(times - 1)</code></li> <li><code>np.ndarray</code> of time with shape <code>(1)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> </ul> </li> </ul> </li> <li>When <code>sliding_window_size</code> is not used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> <li><code>np.ndarray</code> of time with shape <code>(times)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> </ul> </li> </ul> </li> </ul> <p>The <code>DataLoader</code> is configured with the following config attributes:</p> Dataset config Description <code>train_batch_size</code> Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>sliding_window_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_prediction_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_step</code> Available only for time-based datasets. Number of times to move by after each window. <code>train_workers</code> Specifies the number of workers to use for loading train data. Applied when <code>workers</code> = \"config\". <code>train_dataloader_order</code> Available only for series-based datasets. Whether to load train data in sequential or random order. <code>random_state</code> Seed for loading train data in random order. <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading train data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>ts_id</code> <code>int | None</code> <p>Specifies time series to take. If None returns all time series as normal. <code>Default: \"None\"</code></p> <code>None</code> <p>Returns:</p> Type Description <code>DataLoader</code> <p>An iterable <code>DataLoader</code> containing data from training set.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_train_dataloader(self, ts_id: int | None = None, workers: int | Literal[\"config\"] = \"config\", **kwargs) -&gt; DataLoader:\n    \"\"\"\n    Returns a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for training set.\n\n    The `DataLoader` is created on the first call and cached for subsequent use. &lt;br/&gt;\n    The cached dataloader is cleared when either [`get_train_df`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_train_df) or [`get_train_numpy`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_train_numpy) is called.\n\n    The structure of the returned batch depends on the `time_format` and whether `sliding_window_size` is used:\n\n    - When `sliding_window_size` is used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n            - `np.ndarray` of times with shape `(times - 1)`\n            - `np.ndarray` of time with shape `(1)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n    - When `sliding_window_size` is not used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n            - `np.ndarray` of time with shape `(times)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n\n    The `DataLoader` is configured with the following config attributes:\n\n    Dataset config | Description\n    -------------- | -----------\n    `train_batch_size` | Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `sliding_window_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n    `sliding_window_prediction_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n    `sliding_window_step` | Available only for time-based datasets. Number of times to move by after each window.\n    `train_workers` | Specifies the number of workers to use for loading train data. Applied when `workers` = \"config\".\n    `train_dataloader_order` | Available only for series-based datasets. Whether to load train data in sequential or random order.\n    `random_state` | Seed for loading train data in random order.\n\n    Parameters:\n        workers: The number of workers to use for loading train data. `Default: \"config\"` \n        ts_id: Specifies time series to take. If None returns all time series as normal. `Default: \"None\"`\n\n    Returns:\n        An iterable `DataLoader` containing data from training set.          \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access train_dataloader.\")\n\n    if not self.dataset_config.has_train():\n        raise ValueError(\"Dataloader for training set is not available in the dataset configuration.\")\n\n    assert self.train_dataset is not None, \"The train_dataset must be initialized before accessing data from training set.\"\n\n    default_kwargs = {'take_all': False, \"cache_loader\": True}\n    kwargs = {**default_kwargs, **kwargs}\n\n    if ts_id is not None:\n\n        if ts_id == self.dataset_config.used_singular_train_time_series and self.train_dataloader is not None:\n            self.logger.debug(\"Returning cached train_dataloader.\")\n            return self.train_dataloader\n\n        dataset = self._get_singular_time_series_dataset(self.train_dataset, ts_id)\n        self.dataset_config.used_singular_train_time_series = ts_id\n        if self.train_dataloader:\n            del self.train_dataloader\n            self.train_dataloader = None\n            self.logger.info(\"Destroyed previous cached train_dataloader.\")\n\n        self.dataset_config.used_train_workers = 0\n        self.train_dataloader = self.dataloader_factory.create_dataloader(dataset, self.dataset_config, 0, False, self.dataset_config.train_batch_size)\n        self.logger.info(\"Created new cached train_dataloader.\")\n        return self.train_dataloader\n    elif self.dataset_config.used_singular_train_time_series is not None and self.train_dataloader is not None:\n        del self.train_dataloader\n        self.train_dataloader = None\n        self.dataset_config.used_singular_train_time_series = None\n        self.logger.info(\"Destroyed previous cached train_dataloader.\")\n\n    if workers == \"config\":\n        workers = self.dataset_config.train_workers\n\n    # If the dataloader is cached and number of used workers did not change, return the cached dataloader\n    if self.train_dataloader and kwargs[\"cache_loader\"] and workers == self.dataset_config.used_train_workers:\n        self.logger.debug(\"Returning cached train_dataloader.\")\n        return self.train_dataloader\n\n    # Update the used workers count\n    self.dataset_config.used_train_workers = workers\n\n    # If there's a previously cached dataloader, destroy it\n    if self.train_dataloader:\n        del self.train_dataloader\n        self.train_dataloader = None\n        self.logger.info(\"Destroyed previous cached train_dataloader.\")\n\n    # If caching is enabled, create a new cached dataloader\n    if kwargs[\"cache_loader\"]:\n        self.train_dataloader = self.dataloader_factory.create_dataloader(self.train_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.train_batch_size, order=self.dataset_config.train_dataloader_order)\n        self.logger.info(\"Created new cached train_dataloader.\")\n        return self.train_dataloader\n\n    # If caching is disabled, create a new uncached dataloader\n    self.logger.debug(\"Created new uncached train_dataloader.\")\n    return self.dataloader_factory.create_dataloader(self.train_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.train_batch_size, order=self.dataset_config.train_dataloader_order)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_val_dataloader","title":"get_val_dataloader","text":"<pre><code>get_val_dataloader(ts_id: int | None = None, workers: int | Literal['config'] = 'config', **kwargs) -&gt; DataLoader\n</code></pre> <p>Returns a PyTorch <code>DataLoader</code> for validation set.</p> <p>The <code>DataLoader</code> is created on the first call and cached for subsequent use.  The cached dataloader is cleared when either <code>get_val_df</code> or <code>get_val_numpy</code> is called.</p> <p>The structure of the returned batch depends on the <code>time_format</code> and whether <code>sliding_window_size</code> is used:</p> <ul> <li>When <code>sliding_window_size</code> is used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> <li><code>np.ndarray</code> of times with shape <code>(times - 1)</code></li> <li><code>np.ndarray</code> of time with shape <code>(1)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> </ul> </li> </ul> </li> <li>When <code>sliding_window_size</code> is not used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> <li><code>np.ndarray</code> of time with shape <code>(times)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> </ul> </li> </ul> </li> </ul> <p>The <code>DataLoader</code> is configured with the following config attributes:</p> Dataset config Description <code>val_batch_size</code> Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>sliding_window_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_prediction_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_step</code> Available only for time-based datasets. Number of times to move by after each window. <code>val_workers</code> Specifies the number of workers to use for loading validation data. Applied when <code>workers</code> = \"config\". <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading validation data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>ts_id</code> <code>int | None</code> <p>Specifies time series to take. If None returns all time series as normal. <code>Default: \"None\"</code></p> <code>None</code> <p>Returns:</p> Type Description <code>DataLoader</code> <p>An iterable <code>DataLoader</code> containing data from validation set.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_val_dataloader(self, ts_id: int | None = None, workers: int | Literal[\"config\"] = \"config\", **kwargs) -&gt; DataLoader:\n    \"\"\"\n    Returns a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for validation set.\n\n    The `DataLoader` is created on the first call and cached for subsequent use. &lt;br/&gt;\n    The cached dataloader is cleared when either [`get_val_df`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_val_df) or [`get_val_numpy`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_val_numpy) is called.\n\n    The structure of the returned batch depends on the `time_format` and whether `sliding_window_size` is used:\n\n    - When `sliding_window_size` is used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n            - `np.ndarray` of times with shape `(times - 1)`\n            - `np.ndarray` of time with shape `(1)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n    - When `sliding_window_size` is not used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n            - `np.ndarray` of time with shape `(times)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n\n    The `DataLoader` is configured with the following config attributes:\n\n    Dataset config | Description\n    -------------- | -----------\n    `val_batch_size` | Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `sliding_window_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n    `sliding_window_prediction_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n    `sliding_window_step` | Available only for time-based datasets. Number of times to move by after each window.\n    `val_workers` | Specifies the number of workers to use for loading validation data. Applied when `workers` = \"config\".\n\n\n    Parameters:\n        workers: The number of workers to use for loading validation data. `Default: \"config\"`  \n        ts_id: Specifies time series to take. If None returns all time series as normal. `Default: \"None\"`\n\n    Returns:\n        An iterable `DataLoader` containing data from validation set.        \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access val_dataloader.\")\n\n    if not self.dataset_config.has_val():\n        raise ValueError(\"Dataloader for validation set is not available in the dataset configuration.\")\n\n    assert self.val_dataset is not None, \"The val_dataset must be initialized before accessing data from validation set.\"\n\n    default_kwargs = {'take_all': False, \"cache_loader\": True}\n    kwargs = {**default_kwargs, **kwargs}\n\n    if ts_id is not None:\n\n        if ts_id == self.dataset_config.used_singular_val_time_series and self.val_dataloader is not None:\n            self.logger.debug(\"Returning cached val_dataloader.\")\n            return self.val_dataloader\n\n        dataset = self._get_singular_time_series_dataset(self.val_dataset, ts_id)\n        self.dataset_config.used_singular_val_time_series = ts_id\n        if self.val_dataloader:\n            del self.val_dataloader\n            self.val_dataloader = None\n            self.logger.info(\"Destroyed previous cached val_dataloader.\")\n\n        self.dataset_config.used_val_workers = 0\n        self.val_dataloader = self.dataloader_factory.create_dataloader(dataset, self.dataset_config, 0, False, self.dataset_config.val_batch_size)\n        self.logger.info(\"Created new cached val_dataloader.\")\n        return self.val_dataloader\n    elif self.dataset_config.used_singular_val_time_series is not None and self.val_dataloader is not None:\n        del self.val_dataloader\n        self.val_dataloader = None\n        self.dataset_config.used_singular_val_time_series = None\n        self.logger.info(\"Destroyed previous cached val_dataloader.\")\n\n    if workers == \"config\":\n        workers = self.dataset_config.val_workers\n\n    # If the dataloader is cached and number of used workers did not change, return the cached dataloader\n    if self.val_dataloader and kwargs[\"cache_loader\"] and workers == self.dataset_config.used_val_workers:\n        self.logger.debug(\"Returning cached val_dataloader.\")\n        return self.val_dataloader\n\n    # Update the used workers count\n    self.dataset_config.used_val_workers = workers\n\n    # If there's a previously cached dataloader, destroy it\n    if self.val_dataloader:\n        del self.val_dataloader\n        self.val_dataloader = None\n        self.logger.info(\"Destroyed previous cached val_dataloader.\")\n\n    # If caching is enabled, create a new cached dataloader\n    if kwargs[\"cache_loader\"]:\n        self.val_dataloader = self.dataloader_factory.create_dataloader(self.val_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.val_batch_size)\n        self.logger.info(\"Created new cached val_dataloader.\")\n        return self.val_dataloader\n\n    # If caching is disabled, create a new uncached dataloader\n    self.logger.debug(\"Created new uncached val_dataloader.\")\n    return self.dataloader_factory.create_dataloader(self.val_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.val_batch_size)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_test_dataloader","title":"get_test_dataloader","text":"<pre><code>get_test_dataloader(ts_id: int | None = None, workers: int | Literal['config'] = 'config', **kwargs) -&gt; DataLoader\n</code></pre> <p>Returns a PyTorch <code>DataLoader</code> for test set.</p> <p>The <code>DataLoader</code> is created on the first call and cached for subsequent use.  The cached dataloader is cleared when either <code>get_test_df</code> or <code>get_test_numpy</code> is called.</p> <p>The structure of the returned batch depends on the <code>time_format</code> and whether <code>sliding_window_size</code> is used:</p> <ul> <li>When <code>sliding_window_size</code> is used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> <li><code>np.ndarray</code> of times with shape <code>(times - 1)</code></li> <li><code>np.ndarray</code> of time with shape <code>(1)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> </ul> </li> </ul> </li> <li>When <code>sliding_window_size</code> is not used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> <li><code>np.ndarray</code> of time with shape <code>(times)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> </ul> </li> </ul> </li> </ul> <p>The <code>DataLoader</code> is configured with the following config attributes:</p> Dataset config Description <code>test_batch_size</code> Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>sliding_window_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_prediction_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_step</code> Available only for time-based datasets. Number of times to move by after each window. <code>test_workers</code> Specifies the number of workers to use for loading test data. Applied when <code>workers</code> = \"config\". <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading test data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>ts_id</code> <code>int | None</code> <p>Specifies time series to take. If None returns all time series as normal. <code>Default: \"None\"</code></p> <code>None</code> <p>Returns:</p> Type Description <code>DataLoader</code> <p>An iterable <code>DataLoader</code> containing data from test set.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_test_dataloader(self, ts_id: int | None = None, workers: int | Literal[\"config\"] = \"config\", **kwargs) -&gt; DataLoader:\n    \"\"\"\n    Returns a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for test set.\n\n    The `DataLoader` is created on the first call and cached for subsequent use. &lt;br/&gt;\n    The cached dataloader is cleared when either [`get_test_df`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_test_df) or [`get_test_numpy`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_test_numpy) is called.\n\n    The structure of the returned batch depends on the `time_format` and whether `sliding_window_size` is used:\n\n    - When `sliding_window_size` is used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n            - `np.ndarray` of times with shape `(times - 1)`\n            - `np.ndarray` of time with shape `(1)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n    - When `sliding_window_size` is not used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n            - `np.ndarray` of time with shape `(times)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n\n    The `DataLoader` is configured with the following config attributes:\n\n    Dataset config | Description\n    -------------- | -----------\n    `test_batch_size` | Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `sliding_window_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n    `sliding_window_prediction_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n    `sliding_window_step` | Available only for time-based datasets. Number of times to move by after each window.\n    `test_workers` | Specifies the number of workers to use for loading test data. Applied when `workers` = \"config\".\n\n\n    Parameters:\n        workers: The number of workers to use for loading test data. `Default: \"config\"`  \n        ts_id: Specifies time series to take. If None returns all time series as normal. `Default: \"None\"`\n\n    Returns:\n        An iterable `DataLoader` containing data from test set.        \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access test_dataloader.\")\n\n    if not self.dataset_config.has_test():\n        raise ValueError(\"Dataloader for test set is not available in the dataset configuration.\")\n\n    assert self.test_dataset is not None, \"The test_dataset must be initialized before accessing data from test set.\"\n\n    default_kwargs = {'take_all': False, \"cache_loader\": True}\n    kwargs = {**default_kwargs, **kwargs}\n\n    if ts_id is not None:\n\n        if ts_id == self.dataset_config.used_singular_test_time_series and self.test_dataloader is not None:\n            self.logger.debug(\"Returning cached test_dataloader.\")\n            return self.test_dataloader\n\n        dataset = self._get_singular_time_series_dataset(self.test_dataset, ts_id)\n        self.dataset_config.used_singular_test_time_series = ts_id\n        if self.test_dataloader:\n            del self.test_dataloader\n            self.test_dataloader = None\n            self.logger.info(\"Destroyed previous cached test_dataloader.\")\n\n        self.dataset_config.used_test_workers = 0\n        self.test_dataloader = self.dataloader_factory.create_dataloader(dataset, self.dataset_config, 0, False, self.dataset_config.test_batch_size)\n        self.logger.info(\"Created new cached test_dataloader.\")\n        return self.test_dataloader\n    elif self.dataset_config.used_singular_test_time_series is not None and self.test_dataloader is not None:\n        del self.test_dataloader\n        self.test_dataloader = None\n        self.dataset_config.used_singular_test_time_series = None\n        self.logger.info(\"Destroyed previous cached test_dataloader.\")\n\n    if workers == \"config\":\n        workers = self.dataset_config.test_workers\n\n    # If the dataloader is cached and number of used workers did not change, return the cached dataloader\n    if self.test_dataloader and kwargs[\"cache_loader\"] and workers == self.dataset_config.used_test_workers:\n        self.logger.debug(\"Returning cached test_dataloader.\")\n        return self.test_dataloader\n\n    # Update the used workers count\n    self.dataset_config.used_test_workers = workers\n\n    # If there's a previously cached dataloader, destroy it\n    if self.test_dataloader:\n        del self.test_dataloader\n        self.test_dataloader = None\n        self.logger.info(\"Destroyed previous cached test_dataloader.\")\n\n    # If caching is enabled, create a new cached dataloader\n    if kwargs[\"cache_loader\"]:\n        self.test_dataloader = self.dataloader_factory.create_dataloader(self.test_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.test_batch_size)\n        self.logger.info(\"Created new cached test_dataloader.\")\n        return self.test_dataloader\n\n    # If caching is disabled, create a new uncached dataloader\n    self.logger.debug(\"Created new uncached test_dataloader.\")\n    return self.dataloader_factory.create_dataloader(self.test_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.test_batch_size)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_all_dataloader","title":"get_all_dataloader","text":"<pre><code>get_all_dataloader(ts_id: int | None = None, workers: int | Literal['config'] = 'config', **kwargs) -&gt; DataLoader\n</code></pre> <p>Returns a PyTorch <code>DataLoader</code> for all set.</p> <p>The <code>DataLoader</code> is created on the first call and cached for subsequent use.  The cached dataloader is cleared when either <code>get_all_df</code> or <code>get_all_numpy</code> is called.</p> <p>The structure of the returned batch depends on the <code>time_format</code> and whether <code>sliding_window_size</code> is used:</p> <ul> <li>When <code>sliding_window_size</code> is used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> <li><code>np.ndarray</code> of times with shape <code>(times - 1)</code></li> <li><code>np.ndarray</code> of time with shape <code>(1)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> </ul> </li> </ul> </li> <li>When <code>sliding_window_size</code> is not used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> <li><code>np.ndarray</code> of time with shape <code>(times)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> </ul> </li> </ul> </li> </ul> <p>The <code>DataLoader</code> is configured with the following config attributes:</p> Dataset config Description <code>all_batch_size</code> Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>sliding_window_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_prediction_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_step</code> Available only for time-based datasets. Number of times to move by after each window. <code>all_workers</code> Specifies the number of workers to use for loading all data. Applied when <code>workers</code> = \"config\". <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading all data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>ts_id</code> <code>int | None</code> <p>Specifies time series to take. If None returns all time series as normal. <code>Default: \"None\"</code></p> <code>None</code> <p>Returns:</p> Type Description <code>DataLoader</code> <p>An iterable <code>DataLoader</code> containing data from all set.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_all_dataloader(self, ts_id: int | None = None, workers: int | Literal[\"config\"] = \"config\", **kwargs) -&gt; DataLoader:\n    \"\"\"\n    Returns a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for all set.\n\n    The `DataLoader` is created on the first call and cached for subsequent use. &lt;br/&gt;\n    The cached dataloader is cleared when either [`get_all_df`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_all_df) or [`get_all_numpy`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_all_numpy) is called.\n\n    The structure of the returned batch depends on the `time_format` and whether `sliding_window_size` is used:\n\n    - When `sliding_window_size` is used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n            - `np.ndarray` of times with shape `(times - 1)`\n            - `np.ndarray` of time with shape `(1)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n    - When `sliding_window_size` is not used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n            - `np.ndarray` of time with shape `(times)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n\n    The `DataLoader` is configured with the following config attributes:\n\n    Dataset config | Description\n    -------------- | -----------\n    `all_batch_size` | Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `sliding_window_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n    `sliding_window_prediction_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n    `sliding_window_step` | Available only for time-based datasets. Number of times to move by after each window.\n    `all_workers` | Specifies the number of workers to use for loading all data. Applied when `workers` = \"config\".\n\n\n    Parameters:\n        workers: The number of workers to use for loading all data. `Default: \"config\"`  \n        ts_id: Specifies time series to take. If None returns all time series as normal. `Default: \"None\"`\n\n    Returns:\n        An iterable `DataLoader` containing data from all set.       \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access all_dataloader.\")\n\n    if not self.dataset_config.has_all():\n        raise ValueError(\"Dataloader for all set is not available in the dataset configuration.\")\n\n    assert self.all_dataset is not None, \"The all_dataset must be initialized before accessing data from all set.\"\n\n    default_kwargs = {'take_all': False, \"cache_loader\": True}\n    kwargs = {**default_kwargs, **kwargs}\n\n    if ts_id is not None:\n\n        if ts_id == self.dataset_config.used_singular_all_time_series and self.all_dataloader is not None:\n            self.logger.debug(\"Returning cached all_dataloader.\")\n            return self.all_dataloader\n\n        dataset = self._get_singular_time_series_dataset(self.all_dataset, ts_id)\n        self.dataset_config.used_singular_all_time_series = ts_id\n        if self.all_dataloader:\n            del self.all_dataloader\n            self.all_dataloader = None\n            self.logger.info(\"Destroyed previous cached all_dataloader.\")\n\n        self.dataset_config.used_all_workers = 0\n        self.all_dataloader = self.dataloader_factory.create_dataloader(dataset, self.dataset_config, 0, False, self.dataset_config.all_batch_size)\n        self.logger.info(\"Created new cached all_dataloader.\")\n        return self.all_dataloader\n    elif self.dataset_config.used_singular_all_time_series is not None and self.all_dataloader is not None:\n        del self.all_dataloader\n        self.all_dataloader = None\n        self.dataset_config.used_singular_all_time_series = None\n        self.logger.info(\"Destroyed previous cached all_dataloader.\")\n\n    if workers == \"config\":\n        workers = self.dataset_config.all_workers\n\n    # If the dataloader is cached and number of used workers did not change, return the cached dataloader\n    if self.all_dataloader and kwargs[\"cache_loader\"] and workers == self.dataset_config.used_all_workers:\n        self.logger.debug(\"Returning cached all_dataloader.\")\n        return self.all_dataloader\n\n    # Update the used workers count\n    self.dataset_config.used_all_workers = workers\n\n    # If there's a previously cached dataloader, destroy it\n    if self.all_dataloader:\n        del self.all_dataloader\n        self.all_dataloader = None\n        self.logger.info(\"Destroyed previous cached all_dataloader.\")\n\n    # If caching is enabled, create a new cached dataloader\n    if kwargs[\"cache_loader\"]:\n        self.all_dataloader = self.dataloader_factory.create_dataloader(self.all_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.all_batch_size)\n        self.logger.info(\"Created new cached all_dataloader.\")\n        return self.all_dataloader\n\n    # If caching is disabled, create a new uncached dataloader\n    self.logger.debug(\"Creating new uncached all_dataloader.\")\n    return self.dataloader_factory.create_dataloader(self.all_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.all_batch_size)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_train_df","title":"get_train_df","text":"<pre><code>get_train_df(workers: int | Literal['config'] = 'config', as_single_dataframe: bool = True) -&gt; pd.DataFrame\n</code></pre> <p>Creates a Pandas <code>DataFrame</code> containing all the data from training set grouped by time series.</p> <p>This method uses the <code>train_dataloader</code> with a batch size set to the total number of data in the training set. The cached <code>train_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire training set is loaded into memory, which may lead to high memory usage. If working with large training set, consider using <code>get_train_dataloader</code> instead to handle data in batches.</p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading train data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>as_single_dataframe</code> <code>bool</code> <p>Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. <code>Default: True</code> </p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A single Pandas DataFrame containing all data from training set, or a list of DataFrames (one per time series).</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_train_df(self, workers: int | Literal[\"config\"] = \"config\", as_single_dataframe: bool = True) -&gt; pd.DataFrame:\n    \"\"\"\n    Creates a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) containing all the data from training set grouped by time series.\n\n    This method uses the `train_dataloader` with a batch size set to the total number of data in the training set. The cached `train_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire training set is loaded into memory, which may lead to high memory usage. If working with large training set, consider using `get_train_dataloader` instead to handle data in batches.\n\n    Parameters:\n        workers: The number of workers to use for loading train data. `Default: \"config\"`  \n        as_single_dataframe: Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. `Default: True` \n\n    Returns:\n        A single Pandas DataFrame containing all data from training set, or a list of DataFrames (one per time series).\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access train_dataloader.\")\n\n    if not self.dataset_config.has_train():\n        raise ValueError(\"Dataloader for training set is not available in the dataset configuration.\")\n\n    assert self.train_dataset is not None, \"The train_dataset must be initialized before accessing data from training set.\"\n\n    ts_ids, time_period = self.dataset_config._get_train()\n\n    should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n    dataloader = self.get_train_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n    return self._get_df(dataloader, as_single_dataframe, ts_ids, time_period)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_val_df","title":"get_val_df","text":"<pre><code>get_val_df(workers: int | Literal['config'] = 'config', as_single_dataframe: bool = True) -&gt; pd.DataFrame\n</code></pre> <p>Create a Pandas <code>DataFrame</code> containing all the data from validation set grouped by time series.</p> <p>This method uses the <code>val_dataloader</code> with a batch size set to the total number of data in the validation set. The cached <code>val_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire validation set is loaded into memory, which may lead to high memory usage. If working with large validation set, consider using <code>get_val_dataloader</code> instead to handle data in batches.</p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading validation data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>as_single_dataframe</code> <code>bool</code> <p>Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. <code>Default: True</code> </p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A single Pandas DataFrame containing all data from validation set, or a list of DataFrames (one per time series).</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_val_df(self, workers: int | Literal[\"config\"] = \"config\", as_single_dataframe: bool = True) -&gt; pd.DataFrame:\n    \"\"\"\n    Create a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) containing all the data from validation set grouped by time series.\n\n    This method uses the `val_dataloader` with a batch size set to the total number of data in the validation set. The cached `val_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire validation set is loaded into memory, which may lead to high memory usage. If working with large validation set, consider using `get_val_dataloader` instead to handle data in batches.\n\n    Parameters:\n        workers: The number of workers to use for loading validation data. `Default: \"config\"`  \n        as_single_dataframe: Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. `Default: True` \n\n    Returns:\n        A single Pandas DataFrame containing all data from validation set, or a list of DataFrames (one per time series).\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access val_dataloader.\")\n\n    if not self.dataset_config.has_val():\n        raise ValueError(\"Dataloader for validation set is not available in the dataset configuration.\")\n\n    assert self.val_dataset is not None, \"The val_dataset must be initialized before accessing data from validation set.\"\n\n    ts_ids, time_period = self.dataset_config._get_val()\n\n    should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n    dataloader = self.get_val_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n    return self._get_df(dataloader, as_single_dataframe, ts_ids, time_period)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_test_df","title":"get_test_df","text":"<pre><code>get_test_df(workers: int | Literal['config'] = 'config', as_single_dataframe: bool = True) -&gt; pd.DataFrame\n</code></pre> <p>Creates a Pandas <code>DataFrame</code> containing all the data from test set grouped by time series.</p> <p>This method uses the <code>test_dataloader</code> with a batch size set to the total number of data in the test set. The cached <code>test_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire test set is loaded into memory, which may lead to high memory usage. If working with large test set, consider using <code>get_test_dataloader</code> instead to handle data in batches.</p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading test data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>as_single_dataframe</code> <code>bool</code> <p>Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. <code>Default: True</code> </p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A single Pandas DataFrame containing all data from test set, or a list of DataFrames (one per time series).</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_test_df(self, workers: int | Literal[\"config\"] = \"config\", as_single_dataframe: bool = True) -&gt; pd.DataFrame:\n    \"\"\"\n    Creates a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) containing all the data from test set grouped by time series.\n\n    This method uses the `test_dataloader` with a batch size set to the total number of data in the test set. The cached `test_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire test set is loaded into memory, which may lead to high memory usage. If working with large test set, consider using `get_test_dataloader` instead to handle data in batches.\n\n    Parameters:\n        workers: The number of workers to use for loading test data. `Default: \"config\"`  \n        as_single_dataframe: Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. `Default: True` \n\n    Returns:\n        A single Pandas DataFrame containing all data from test set, or a list of DataFrames (one per time series).\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access test_dataloader.\")\n\n    if not self.dataset_config.has_test():\n        raise ValueError(\"Dataloader for test set is not available in the dataset configuration.\")\n\n    assert self.test_dataset is not None, \"The test_dataset must be initialized before accessing data from test set.\"\n\n    ts_ids, time_period = self.dataset_config._get_test()\n\n    should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n    dataloader = self.get_test_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n    return self._get_df(dataloader, as_single_dataframe, ts_ids, time_period)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_all_df","title":"get_all_df","text":"<pre><code>get_all_df(workers: int | Literal['config'] = 'config', as_single_dataframe: bool = True) -&gt; pd.DataFrame\n</code></pre> <p>Creates a Pandas <code>DataFrame</code> containing all the data from all set grouped by time series.</p> <p>This method uses the <code>all_dataloader</code> with a batch size set to the total number of data in the all set. The cached <code>all_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire all set is loaded into memory, which may lead to high memory usage. If working with large all set, consider using <code>get_all_dataloader</code> instead to handle data in batches.</p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading all data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>as_single_dataframe</code> <code>bool</code> <p>Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. <code>Default: True</code> </p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A single Pandas DataFrame containing all data from all set, or a list of DataFrames (one per time series).</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_all_df(self, workers: int | Literal[\"config\"] = \"config\", as_single_dataframe: bool = True) -&gt; pd.DataFrame:\n    \"\"\"\n    Creates a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) containing all the data from all set grouped by time series.\n\n    This method uses the `all_dataloader` with a batch size set to the total number of data in the all set. The cached `all_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire all set is loaded into memory, which may lead to high memory usage. If working with large all set, consider using `get_all_dataloader` instead to handle data in batches.\n\n    Parameters:\n        workers: The number of workers to use for loading all data. `Default: \"config\"`  \n        as_single_dataframe: Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. `Default: True` \n\n    Returns:\n        A single Pandas DataFrame containing all data from all set, or a list of DataFrames (one per time series).\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access all_dataloader.\")\n\n    if not self.dataset_config.has_all():\n        raise ValueError(\"Dataloader for all set is not available in the dataset configuration.\")\n\n    assert self.all_dataset is not None, \"The all_dataset must be initialized before accessing data from all set.\"\n\n    ts_ids, time_period = self.dataset_config._get_all()\n\n    should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n    dataloader = self.get_all_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n    return self._get_df(dataloader, as_single_dataframe, ts_ids, time_period)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_train_numpy","title":"get_train_numpy","text":"<pre><code>get_train_numpy(workers: int | Literal['config'] = 'config') -&gt; np.ndarray\n</code></pre> <p>Creates a NumPy array containing all the data from training set grouped by time series, with the shape <code>(num_time_series, num_times, num_features)</code>.</p> <p>This method uses the <code>train_dataloader</code> with a batch size set to the total number of data in the training set. The cached <code>train_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire training set is loaded into memory, which may lead to high memory usage. If working with large training set, consider using <code>get_train_dataloader</code> instead to handle data in batches.        </p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading train data. <code>Default: \"config\"</code> </p> <code>'config'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A NumPy array containing all the data in training set with the shape <code>(num_time_series, num_times, num_features)</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_train_numpy(self, workers: int | Literal[\"config\"] = \"config\") -&gt; np.ndarray:\n    \"\"\"\n    Creates a NumPy array containing all the data from training set grouped by time series, with the shape `(num_time_series, num_times, num_features)`.\n\n    This method uses the `train_dataloader` with a batch size set to the total number of data in the training set. The cached `train_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire training set is loaded into memory, which may lead to high memory usage. If working with large training set, consider using `get_train_dataloader` instead to handle data in batches.        \n\n    Parameters:\n        workers: The number of workers to use for loading train data. `Default: \"config\"`  \n\n    Returns:\n        A NumPy array containing all the data in training set with the shape `(num_time_series, num_times, num_features)`.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access train_dataloader.\")\n\n    if not self.dataset_config.has_train():\n        raise ValueError(\"Dataloader for training set is not available in the dataset configuration.\")\n\n    assert self.train_dataset is not None, \"The train_dataset must be initialized before accessing data from training set.\"\n\n    ts_ids, time_period = self.dataset_config._get_train()\n\n    should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n    dataloader = self.get_train_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n    return self._get_numpy(dataloader, ts_ids, time_period)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_val_numpy","title":"get_val_numpy","text":"<pre><code>get_val_numpy(workers: int | Literal['config'] = 'config') -&gt; np.ndarray\n</code></pre> <p>Creates a NumPy array containing all the data from validation set grouped by time series, with the shape <code>(num_time_series, num_times, num_features)</code>.</p> <p>This method uses the <code>val_dataloader</code> with a batch size set to the total number of data in the validation set. The cached <code>val_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire validation set is loaded into memory, which may lead to high memory usage. If working with large validation set, consider using <code>get_val_dataloader</code> instead to handle data in batches.        </p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading validation data. <code>Default: \"config\"</code> </p> <code>'config'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A NumPy array containing all the data in validation set with the shape <code>(num_time_series, num_times, num_features)</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_val_numpy(self, workers: int | Literal[\"config\"] = \"config\") -&gt; np.ndarray:\n    \"\"\"\n    Creates a NumPy array containing all the data from validation set grouped by time series, with the shape `(num_time_series, num_times, num_features)`.\n\n    This method uses the `val_dataloader` with a batch size set to the total number of data in the validation set. The cached `val_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire validation set is loaded into memory, which may lead to high memory usage. If working with large validation set, consider using `get_val_dataloader` instead to handle data in batches.        \n\n    Parameters:\n        workers: The number of workers to use for loading validation data. `Default: \"config\"`  \n\n    Returns:\n        A NumPy array containing all the data in validation set with the shape `(num_time_series, num_times, num_features)`.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access val_dataloader.\")\n\n    if not self.dataset_config.has_val():\n        raise ValueError(\"Dataloader for validation set is not available in the dataset configuration.\")\n\n    assert self.val_dataset is not None, \"The val_dataset must be initialized before accessing data from validation set.\"\n\n    ts_ids, time_period = self.dataset_config._get_val()\n\n    should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n    dataloader = self.get_val_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n    return self._get_numpy(dataloader, ts_ids, time_period)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_test_numpy","title":"get_test_numpy","text":"<pre><code>get_test_numpy(workers: int | Literal['config'] = 'config') -&gt; np.ndarray\n</code></pre> <p>Creates a NumPy array containing all the data from test set grouped by time series, with the shape <code>(num_time_series, num_times, num_features)</code>.</p> <p>This method uses the <code>test_dataloader</code> with a batch size set to the total number of data in the test set. The cached <code>test_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire test set is loaded into memory, which may lead to high memory usage. If working with large test set, consider using <code>get_test_dataloader</code> instead to handle data in batches.        </p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading test data. <code>Default: \"config\"</code> </p> <code>'config'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A NumPy array containing all the data in test set with the shape <code>(num_time_series, num_times, num_features)</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_test_numpy(self, workers: int | Literal[\"config\"] = \"config\") -&gt; np.ndarray:\n    \"\"\"\n    Creates a NumPy array containing all the data from test set grouped by time series, with the shape `(num_time_series, num_times, num_features)`.\n\n    This method uses the `test_dataloader` with a batch size set to the total number of data in the test set. The cached `test_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire test set is loaded into memory, which may lead to high memory usage. If working with large test set, consider using `get_test_dataloader` instead to handle data in batches.        \n\n    Parameters:\n        workers: The number of workers to use for loading test data. `Default: \"config\"`  \n\n    Returns:\n        A NumPy array containing all the data in test set with the shape `(num_time_series, num_times, num_features)`.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access test_dataloader.\")\n\n    if not self.dataset_config.has_test():\n        raise ValueError(\"Dataloader for test set is not available in the dataset configuration.\")\n\n    assert self.test_dataset is not None, \"The test_dataset must be initialized before accessing data from test set.\"\n\n    ts_ids, time_period = self.dataset_config._get_test()\n\n    should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n    dataloader = self.get_test_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n    return self._get_numpy(dataloader, ts_ids, time_period)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_all_numpy","title":"get_all_numpy","text":"<pre><code>get_all_numpy(workers: int | Literal['config'] = 'config') -&gt; np.ndarray\n</code></pre> <p>Creates a NumPy array containing all the data from all set grouped by time series, with the shape <code>(num_time_series, num_times, num_features)</code>.</p> <p>This method uses the <code>all_dataloader</code> with a batch size set to the total number of data in the all set. The cached <code>all_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire all set is loaded into memory, which may lead to high memory usage. If working with large all set, consider using <code>get_all_dataloader</code> instead to handle data in batches.        </p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading all data. <code>Default: \"config\"</code> </p> <code>'config'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A NumPy array containing all the data in all set with the shape <code>(num_time_series, num_times, num_features)</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_all_numpy(self, workers: int | Literal[\"config\"] = \"config\") -&gt; np.ndarray:\n    \"\"\"\n    Creates a NumPy array containing all the data from all set grouped by time series, with the shape `(num_time_series, num_times, num_features)`.\n\n    This method uses the `all_dataloader` with a batch size set to the total number of data in the all set. The cached `all_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire all set is loaded into memory, which may lead to high memory usage. If working with large all set, consider using `get_all_dataloader` instead to handle data in batches.        \n\n    Parameters:\n        workers: The number of workers to use for loading all data. `Default: \"config\"`  \n\n    Returns:\n        A NumPy array containing all the data in all set with the shape `(num_time_series, num_times, num_features)`.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access all_dataloader.\")\n\n    if not self.dataset_config.has_all():\n        raise ValueError(\"Dataloader for all set is not available in the dataset configuration.\")\n\n    assert self.all_dataset is not None, \"The all_dataset must be initialized before accessing data from all set.\"\n\n    ts_ids, time_period = self.dataset_config._get_all()\n\n    should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n    dataloader = self.get_all_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n    return self._get_numpy(dataloader, ts_ids, time_period)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset._update_dataset_config_and_initialize","title":"_update_dataset_config_and_initialize","text":"<pre><code>_update_dataset_config_and_initialize(config_editor: ConfigEditor, workers: int | Literal['config'] = 'config', display_config_details: Optional[Literal['test', 'diagram']] = None)\n</code></pre> <p>Updates config via passed config editor.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def _update_dataset_config_and_initialize(self, config_editor: ConfigEditor, workers: int | Literal[\"config\"] = \"config\", display_config_details: Optional[Literal[\"test\", \"diagram\"]] = None):\n    \"\"\"Updates config via passed config editor. \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating dataset configuration.\")\n\n    if display_config_details is not None:\n        display_config_details = DisplayType(display_config_details)\n\n    original_config = deepcopy(self.dataset_config)\n    original_export_config = deepcopy(self._export_config_copy)\n\n    try:\n        if config_editor.requires_init:\n            self.logger.info(\"Re-initialization is required.\")\n            config_editor.modify_dataset_config(self._export_config_copy, self.metadata)\n            self.set_dataset_config_and_initialize(self._export_config_copy, None, workers)\n\n        else:\n            config_editor.modify_dataset_config(self.dataset_config, self.metadata)\n\n    except Exception:\n        self.dataset_config = original_config\n        self._export_config_copy = original_export_config\n        self.logger.error(\"Error occured, reverting changes.\")\n        raise\n\n    if self.train_dataloader is not None:\n        del self.train_dataloader\n        self.train_dataloader = None\n        self.logger.info(\"Destroyed cached train_dataloader.\")\n\n    if self.val_dataloader is not None:\n        del self.val_dataloader\n        self.val_dataloader = None\n        self.logger.info(\"Destroyed cached val_dataloader.\")\n\n    if self.test_dataloader is not None:\n        del self.test_dataloader\n        self.test_dataloader = None\n        self.logger.info(\"Destroyed cached test_dataloader.\")\n\n    if self.all_dataloader is not None:\n        del self.all_dataloader\n        self.all_dataloader = None\n        self.logger.info(\"Destroyed cached all_dataloader.\")\n\n    self._update_config_imported_status(None)\n    self._update_export_config_copy()\n\n    self.logger.info(\"Configuration has been changed successfuly.\")\n\n    if display_config_details is not None:\n        self.summary(display_config_details)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.apply_filler","title":"apply_filler","text":"<pre><code>apply_filler(fill_missing_with: type | FillerType | Literal['mean_filler', 'forward_filler', 'linear_interpolation_filler'] | None, workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Used for updating filler set in config. Set parameter to <code>config</code> to keep it as it is config. If exception is thrown during set, no changes are made.</p> <p>Affects following configuration: </p> Dataset config Description <code>fill_missing_with</code> Defines how to fill missing values in the dataset. <p>Parameters:</p> Name Type Description Default <code>fill_missing_with</code> <code>type | FillerType | Literal['mean_filler', 'forward_filler', 'linear_interpolation_filler'] | None</code> <p>Defines how to fill missing values in the dataset. <code>Defaults: config</code>.  </p> required <code>workers</code> <code>int | Literal['config']</code> <p>How many workers to use when setting new filler. <code>Defaults: config</code>.</p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def apply_filler(self, fill_missing_with: type | FillerType | Literal[\"mean_filler\", \"forward_filler\", \"linear_interpolation_filler\"] | None, workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"Used for updating filler set in config.\n    Set parameter to `config` to keep it as it is config.\n    If exception is thrown during set, no changes are made.\n\n    Affects following configuration: \n\n    Dataset config | Description\n    -------------- | -----------\n    `fill_missing_with` | Defines how to fill missing values in the dataset.\n\n    Parameters:\n        fill_missing_with: Defines how to fill missing values in the dataset. `Defaults: config`.  \n        workers: How many workers to use when setting new filler. `Defaults: config`.      \n    \"\"\"\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating filler.\")\n\n    self.update_dataset_config_and_initialize(fill_missing_with=fill_missing_with, workers=workers)\n    self.logger.info(\"Filler has been changed successfuly.\")\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.apply_anomaly_handler","title":"apply_anomaly_handler","text":"<pre><code>apply_anomaly_handler(handle_anomalies_with: type | AnomalyHandlerType | Literal['z-score', 'interquartile_range'] | None | Literal['config'], workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Used for updating anomaly handler set in config.</p> <p>Set parameter to <code>config</code> to keep it as it is config.</p> <p>If exception is thrown during set, no changes are made.</p> <p>Affects following configuration:</p> Dataset config Description <code>handle_anomalies_with</code> Defines the anomaly handler to handle anomalies in the dataset. <p>Parameters:</p> Name Type Description Default <code>handle_anomalies_with</code> <code>type | AnomalyHandlerType | Literal['z-score', 'interquartile_range'] | None | Literal['config']</code> <p>Defines the anomaly handler to handle anomalies in the dataset. <code>Defaults: config</code>.  </p> required <code>workers</code> <code>int | Literal['config']</code> <p>How many workers to use when setting new filler. <code>Defaults: config</code>.</p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def apply_anomaly_handler(self, handle_anomalies_with: type | AnomalyHandlerType | Literal[\"z-score\", \"interquartile_range\"] | None | Literal[\"config\"], workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"Used for updating anomaly handler set in config.\n\n    Set parameter to `config` to keep it as it is config.\n\n    If exception is thrown during set, no changes are made.\n\n    Affects following configuration:\n\n    Dataset config | Description\n    -------------- | -----------\n    `handle_anomalies_with` | Defines the anomaly handler to handle anomalies in the dataset.\n\n    Parameters:\n        handle_anomalies_with: Defines the anomaly handler to handle anomalies in the dataset. `Defaults: config`.  \n        workers: How many workers to use when setting new filler. `Defaults: config`.      \n    \"\"\"\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating anomaly handler.\")\n\n    self.update_dataset_config_and_initialize(handle_anomalies_with=handle_anomalies_with, workers=workers)\n    self.logger.info(\"Anomaly handler has been changed successfuly.\")\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.set_default_values","title":"set_default_values","text":"<pre><code>set_default_values(default_values: list[Number] | NDArray[number] | dict[str, Number] | Number | Literal['default'] | None, workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Used for updating default values set in config. Set parameter to <code>config</code> to keep it as it is config. If exception is thrown during set, no changes are made.</p> <p>Affects following configuration:</p> Dataset config Description <code>default_values</code> Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature. <p>Parameters:</p> Name Type Description Default <code>default_values</code> <code>list[Number] | NDArray[number] | dict[str, Number] | Number | Literal['default'] | None</code> <p>Default values for missing data, applied before fillers. <code>Defaults: config</code>.  </p> required <code>workers</code> <code>int | Literal['config']</code> <p>How many workers to use when setting new default values. <code>Defaults: config</code>.</p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def set_default_values(self, default_values: list[Number] | npt.NDArray[np.number] | dict[str, Number] | Number | Literal[\"default\"] | None, workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"Used for updating default values set in config.\n    Set parameter to `config` to keep it as it is config.\n    If exception is thrown during set, no changes are made.\n\n    Affects following configuration:\n\n    Dataset config | Description\n    -------------- | -----------\n    `default_values` | Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature.\n\n    Parameters:\n        default_values: Default values for missing data, applied before fillers. `Defaults: config`.  \n        workers: How many workers to use when setting new default values. `Defaults: config`.      \n    \"\"\"\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating default values.\")\n\n    self.update_dataset_config_and_initialize(default_values=default_values, workers=workers)\n    self.logger.info(\"Default values has been changed successfuly.\")\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.set_preprocess_order","title":"set_preprocess_order","text":"<pre><code>set_preprocess_order(preprocess_order: list[str, type] | Literal['config'] = 'config', workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Used for updating preprocess_order set in config. Set parameter to <code>config</code> to keep it as it is config. If exception is thrown during set, no changes are made.</p> <p>Affects following configuration: </p> Dataset config Description <code>preprocess_order</code> Used order of when preprocesses are applied. Can be also used to add/remove custom handlers. <p>Parameters:</p> Name Type Description Default <code>preprocess_order</code> <code>list[str, type] | Literal['config']</code> <p>Used order of when preprocesses are applied. Can be also used to add/remove custom handlers. <code>Defaults: config</code>.  </p> <code>'config'</code> <code>workers</code> <code>int | Literal['config']</code> <p>How many workers to use when setting new default values. <code>Defaults: config</code>.</p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def set_preprocess_order(self, preprocess_order: list[str, type] | Literal[\"config\"] = \"config\", workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"Used for updating preprocess_order set in config.\n    Set parameter to `config` to keep it as it is config.\n    If exception is thrown during set, no changes are made.\n\n    Affects following configuration: \n\n    Dataset config | Description\n    -------------- | -----------\n    `preprocess_order` | Used order of when preprocesses are applied. Can be also used to add/remove custom handlers.\n\n    Parameters:\n        preprocess_order: Used order of when preprocesses are applied. Can be also used to add/remove custom handlers. `Defaults: config`.  \n        workers: How many workers to use when setting new default values. `Defaults: config`.      \n    \"\"\"\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating preprocess order.\")\n\n    self.update_dataset_config_and_initialize(preprocess_order=preprocess_order, workers=workers)\n    self.logger.info(\"Preprocess order has been changed successfuly.\")\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.set_workers","title":"set_workers","text":"<pre><code>set_workers(train_workers: int | Literal['config'] = 'config', val_workers: int | Literal['config'] = 'config', test_workers: int | Literal['config'] = 'config', all_workers: int | Literal['config'] = 'config', init_workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Used for updating workers set in config. Set parameter to <code>config</code> to keep it as it is config. If exception is thrown during set, no changes are made.</p> <p>Affects following configuration:</p> Dataset config Description <code>train_workers</code> Number of workers for loading training data. <code>val_workers</code> Number of workers for loading validation data. <code>test_workers</code> Number of workers for loading test data. <code>all_workers</code> Number of workers for loading all data. <code>init_workers</code> Number of workers for dataset configuration. <p>Parameters:</p> Name Type Description Default <code>train_workers</code> <code>int | Literal['config']</code> <p>Number of workers for loading training data. <code>Defaults: config</code>.</p> <code>'config'</code> <code>val_workers</code> <code>int | Literal['config']</code> <p>Number of workers for loading validation data. <code>Defaults: config</code>.</p> <code>'config'</code> <code>test_workers</code> <code>int | Literal['config']</code> <p>Number of workers for loading test data. <code>Defaults: config</code>.</p> <code>'config'</code> <code>all_workers</code> <code>int | Literal['config']</code> <p>Number of workers for loading all data.  <code>Defaults: config</code>.</p> <code>'config'</code> <code>init_workers</code> <code>int | Literal['config']</code> <p>Number of workers for dataset configuration. <code>Defaults: config</code>.</p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def set_workers(self, train_workers: int | Literal[\"config\"] = \"config\", val_workers: int | Literal[\"config\"] = \"config\",\n                test_workers: int | Literal[\"config\"] = \"config\", all_workers: int | Literal[\"config\"] = \"config\", init_workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"Used for updating workers set in config.\n    Set parameter to `config` to keep it as it is config.\n    If exception is thrown during set, no changes are made.\n\n    Affects following configuration:\n\n    Dataset config | Description\n    -------------- | -----------\n    `train_workers` | Number of workers for loading training data.\n    `val_workers` | Number of workers for loading validation data.\n    `test_workers` | Number of workers for loading test data.\n    `all_workers` | Number of workers for loading all data.\n    `init_workers` | Number of workers for dataset configuration.\n\n    Parameters:\n        train_workers: Number of workers for loading training data. `Defaults: config`.\n        val_workers: Number of workers for loading validation data. `Defaults: config`.\n        test_workers: Number of workers for loading test data. `Defaults: config`.\n        all_workers: Number of workers for loading all data.  `Defaults: config`.\n        init_workers: Number of workers for dataset configuration. `Defaults: config`.            \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating workers.\")\n\n    self.update_dataset_config_and_initialize(train_workers=train_workers, val_workers=val_workers, test_workers=test_workers, all_workers=all_workers, init_workers=init_workers, workers=\"config\")\n    self.logger.info(\"Workers has been changed successfuly.\")\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.set_batch_sizes","title":"set_batch_sizes","text":"<pre><code>set_batch_sizes(train_batch_size: int | Literal['config'] = 'config', val_batch_size: int | Literal['config'] = 'config', test_batch_size: int | Literal['config'] = 'config', all_batch_size: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Used for updating batch sizes set in config. Set parameter to <code>config</code> to keep it as it is config. If exception is thrown during set, no changes are made.</p> <p>Affects following configuration:</p> Dataset config Description <code>train_batch_size</code> Number of samples per batch for train set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>val_batch_size</code> Number of samples per batch for val set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>test_batch_size</code> Number of samples per batch for test set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>all_batch_size</code> Number of samples per batch for all set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <p>Parameters:</p> Name Type Description Default <code>train_batch_size</code> <code>int | Literal['config']</code> <p>Number of samples per batch for train set. <code>Defaults: config</code>.</p> <code>'config'</code> <code>val_batch_size</code> <code>int | Literal['config']</code> <p>Number of samples per batch for val set. <code>Defaults: config</code>.</p> <code>'config'</code> <code>test_batch_size</code> <code>int | Literal['config']</code> <p>Number of samples per batch for test set. <code>Defaults: config</code>.</p> <code>'config'</code> <code>all_batch_size</code> <code>int | Literal['config']</code> <p>Number of samples per batch for all set. <code>Defaults: config</code>.</p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def set_batch_sizes(self, train_batch_size: int | Literal[\"config\"] = \"config\", val_batch_size: int | Literal[\"config\"] = \"config\",\n                    test_batch_size: int | Literal[\"config\"] = \"config\", all_batch_size: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"Used for updating batch sizes set in config.\n    Set parameter to `config` to keep it as it is config.\n    If exception is thrown during set, no changes are made.\n\n    Affects following configuration:\n\n    Dataset config | Description\n    -------------- | -----------\n    `train_batch_size` | Number of samples per batch for train set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `val_batch_size` | Number of samples per batch for val set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `test_batch_size` | Number of samples per batch for test set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `all_batch_size` | Number of samples per batch for all set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n\n    Parameters:\n        train_batch_size: Number of samples per batch for train set. `Defaults: config`.\n        val_batch_size: Number of samples per batch for val set. `Defaults: config`.\n        test_batch_size: Number of samples per batch for test set. `Defaults: config`.\n        all_batch_size: Number of samples per batch for all set. `Defaults: config`.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating batch sizes.\")\n\n    self.update_dataset_config_and_initialize(train_batch_size=train_batch_size, val_batch_size=val_batch_size, test_batch_size=test_batch_size, all_batch_size=all_batch_size, workers=\"config\")\n    self.logger.info(\"Batch sizes has been changed successfuly.\")\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.display_dataset_details","title":"display_dataset_details","text":"<pre><code>display_dataset_details() -&gt; None\n</code></pre> <p>Display information about the contents of the dataset.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>    def display_dataset_details(self) -&gt; None:\n        \"\"\"Display information about the contents of the dataset.  \"\"\"\n\n        to_display = f'''\nDataset details:\n\n    {self.metadata.aggregation}\n        Time indices: {range(self.metadata.time_indices[ID_TIME_COLUMN_NAME][0], self.metadata.time_indices[ID_TIME_COLUMN_NAME][-1])}\n        Datetime: {(datetime.fromtimestamp(self.metadata.time_indices['time'][0], tz=timezone.utc), datetime.fromtimestamp(self.metadata.time_indices['time'][-1], timezone.utc))}\n\n    {self.metadata.source_type}\n        Time series indices: {get_abbreviated_list_string(self.metadata.ts_indices[self.metadata.ts_id_name])}; use 'get_available_ts_indices' for full list\n        Features with default values: {self.metadata.default_values}\n\n        Additional data: {list(self.metadata.additional_data.keys())}\n        '''\n\n        print(to_display)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.summary","title":"summary","text":"<pre><code>summary(display_type: Literal['text', 'diagram']) -&gt; None\n</code></pre> <p>Used to display used configurations. Can be displayed as interactive html diagram or text summary.</p> <p>Parameters:</p> Name Type Description Default <code>display_type</code> <code>Literal['text', 'diagram']</code> <p>Whether configuration should be display as diagram or text summary.</p> required Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def summary(self, display_type: Literal[\"text\", \"diagram\"]) -&gt; None:\n    \"\"\"Used to display used configurations. Can be displayed as interactive html diagram or text summary.\n\n    Parameters:\n        display_type: Whether configuration should be display as diagram or text summary.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to display summary.\")\n\n    display_type = DisplayType(display_type)\n\n    if display_type == DisplayType.TEXT:\n        print(self.dataset_config)\n    elif display_type == DisplayType.DIAGRAM:\n        steps = self.dataset_config._get_summary_steps()\n        return css_utils.display_summary_diagram(steps)\n    else:\n        raise NotImplementedError()\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.save_summary_diagram_as_html","title":"save_summary_diagram_as_html","text":"<pre><code>save_summary_diagram_as_html(path: str)\n</code></pre> <p>Saves diagram produces from <code>summary</code> method as html file to specified path.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def save_summary_diagram_as_html(self, path: str):\n    \"\"\"Saves diagram produces from `summary` method as html file to specified path. \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to save summary diagram.\")\n\n    steps = self.dataset_config._get_summary_steps()\n    html = css_utils.get_summary_diagram(steps)\n\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        f.write(html)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_feature_names","title":"get_feature_names","text":"<pre><code>get_feature_names() -&gt; list[str]\n</code></pre> <p>Returns a list of all available feature names in the dataset.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_feature_names(self) -&gt; list[str]:\n    \"\"\"Returns a list of all available feature names in the dataset. \"\"\"\n\n    return list(self.metadata.features.keys())\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_available_ts_indices","title":"get_available_ts_indices","text":"<pre><code>get_available_ts_indices() -&gt; np.ndarray\n</code></pre> <p>Returns the available time series indices in this dataset.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_available_ts_indices(self) -&gt; np.ndarray:\n    \"\"\"Returns the available time series indices in this dataset. \"\"\"\n    return self.metadata.ts_indices\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_additional_data","title":"get_additional_data","text":"<pre><code>get_additional_data(data_name: str) -&gt; pd.DataFrame\n</code></pre> <p>Create a Pandas <code>DataFrame</code> of additional data of <code>data_name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data_name</code> <code>str</code> <p>Name of additional data to return.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Dataframe of additional data of <code>data_name</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_additional_data(self, data_name: str) -&gt; pd.DataFrame:\n    \"\"\"Create a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) of additional data of `data_name`.\n\n    Parameters:\n        data_name: Name of additional data to return.\n\n    Returns:\n        Dataframe of additional data of `data_name`.\n    \"\"\"\n\n    if data_name not in self.metadata.additional_data:\n        self.logger.error(\"%s is not available for this dataset.\", data_name)\n        raise ValueError(f\"{data_name} is not available for this dataset.\", f\"Possible options are: {self.metadata.additional_data}\")\n\n    data = get_additional_data(self.metadata.dataset_path, data_name)\n    data_df = pd.DataFrame(data)\n\n    for column, column_type in self.metadata.additional_data[data_name]:\n        if column_type == datetime:\n            data_df[column] = data_df[column].apply(lambda x: datetime.fromtimestamp(x, tz=timezone.utc))\n        else:\n            data_df[column] = data_df[column].astype(column_type)\n\n    return data_df\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.plot","title":"plot","text":"<pre><code>plot(ts_id: int, plot_type: Literal['scatter', 'line'], features: list[str] | str | Literal['config'] = 'config', feature_per_plot: bool = True, time_format: TimeFormat | Literal['config', 'id_time', 'datetime', 'unix_time', 'shifted_unix_time'] = 'config', is_interactive: bool = True) -&gt; None\n</code></pre> <p>Displays a graph for the selected <code>ts_id</code> and its <code>features</code>.</p> <p>The plotting is done using the <code>Plotly</code> library, which provides interactive graphs.</p> <p>Parameters:</p> Name Type Description Default <code>ts_id</code> <code>int</code> <p>The ID of the time series to display.</p> required <code>plot_type</code> <code>Literal['scatter', 'line']</code> <p>The type of graph to plot.</p> required <code>features</code> <code>list[str] | str | Literal['config']</code> <p>The features to display in the plot. <code>Defaults: \"config\"</code>.</p> <code>'config'</code> <code>feature_per_plot</code> <code>bool</code> <p>Whether each feature should be displayed in a separate plot or combined into one. <code>Defaults: True</code>.</p> <code>True</code> <code>time_format</code> <code>TimeFormat | Literal['config', 'id_time', 'datetime', 'unix_time', 'shifted_unix_time']</code> <p>The time format to use for the x-axis. <code>Defaults: \"config\"</code>.</p> <code>'config'</code> <code>is_interactive</code> <code>bool</code> <p>Whether the plot should be interactive (e.g., zoom, hover). <code>Defaults: True</code>.</p> <code>True</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def plot(self, ts_id: int, plot_type: Literal[\"scatter\", \"line\"], features: list[str] | str | Literal[\"config\"] = \"config\", feature_per_plot: bool = True,\n         time_format: TimeFormat | Literal[\"config\", \"id_time\", \"datetime\", \"unix_time\", \"shifted_unix_time\"] = \"config\", is_interactive: bool = True) -&gt; None:\n    \"\"\"\n    Displays a graph for the selected `ts_id` and its `features`.\n\n    The plotting is done using the [`Plotly`](https://plotly.com/python/) library, which provides interactive graphs.\n\n    Parameters:\n        ts_id: The ID of the time series to display.\n        plot_type: The type of graph to plot.\n        features: The features to display in the plot. `Defaults: \"config\"`.\n        feature_per_plot: Whether each feature should be displayed in a separate plot or combined into one. `Defaults: True`.\n        time_format: The time format to use for the x-axis. `Defaults: \"config\"`.\n        is_interactive: Whether the plot should be interactive (e.g., zoom, hover). `Defaults: True`.\n    \"\"\"\n\n    if time_format == \"config\":\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to plot.\")\n\n        time_format = self.dataset_config.time_format\n        self.logger.debug(\"Using time format from dataset configuration: %s\", time_format)\n    else:\n        time_format = TimeFormat(time_format)\n        self.logger.debug(\"Using specified time format: %s\", time_format)\n\n    time_series, times, features = self.__get_data_for_plot(ts_id, features, time_format)\n    self.logger.debug(\"Received data for plotting. Time series, times, and features are ready.\")\n\n    plots = []\n\n    if feature_per_plot:\n        self.logger.debug(\"Creating individual plots for each feature.\")\n        fig = make_subplots(rows=len(features), cols=1, shared_xaxes=False, x_title=time_format.value)\n\n        for i, feature in enumerate(features):\n            if plot_type == \"scatter\":\n                plot = go.Scatter(x=times, y=time_series[:, i], mode=\"markers\", name=feature, legendgroup=feature)\n                self.logger.debug(\"Creating scatter plot for feature: %s\", feature)\n            elif plot_type == \"line\":\n                plot = go.Scatter(x=times, y=time_series[:, i], mode=\"lines\", name=feature)\n                self.logger.debug(\"Creating line plot for feature: %s\", feature)\n            else:\n                raise ValueError(\"Invalid plot type.\")\n\n            fig.add_traces(plot, rows=i + 1, cols=1)\n\n        fig.update_layout(height=200 + 120 * len(features), width=2000, autosize=len(features) == 1, showlegend=True)\n        self.logger.debug(\"Created subplots for features: %s.\", features)\n    else:\n        self.logger.debug(\"Creating a combined plot for all features.\")\n        for i, feature in enumerate(features):\n            if plot_type == \"scatter\":\n                plot = go.Scatter(x=times, y=time_series[:, i], mode=\"markers\", name=feature)\n                self.logger.debug(\"Creating scatter plot for feature: %s\", feature)\n            elif plot_type == \"line\":\n                plot = go.Scatter(x=times, y=time_series[:, i], mode=\"lines\", name=feature)\n                self.logger.debug(\"Creating line plot for feature: %s\", feature)\n            else:\n                raise ValueError(\"Invalid plot type.\")\n            plots.append(plot)\n\n        fig = go.Figure(data=plots)\n        fig.update_layout(xaxis_title=time_format.value, showlegend=True, height=200 + 120 * 2)\n        self.logger.debug(\"Created combined plot for features: %s.\", features)\n\n    if not is_interactive:\n        self.logger.debug(\"Disabling interactivity for the plot.\")\n        fig.update_layout(updatemenus=[], dragmode=False, hovermode=False)\n\n    self.logger.debug(\"Displaying the plot.\")\n    fig.show()\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.add_annotation","title":"add_annotation","text":"<pre><code>add_annotation(annotation: str, annotation_group: str, ts_id: int | None, id_time: int | None, enforce_ids: bool = True) -&gt; None\n</code></pre> <p>Adds an annotation to the specified <code>annotation_group</code>.</p> <ul> <li>If the provided <code>annotation_group</code> does not exist, it will be created.</li> <li>At least one of <code>ts_id</code> or <code>id_time</code> must be provided to associate the annotation with time series or/and time point.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>annotation</code> <code>str</code> <p>The annotation to be added.</p> required <code>annotation_group</code> <code>str</code> <p>The group to which the annotation should be added.</p> required <code>ts_id</code> <code>int | None</code> <p>The time series ID to which the annotation should be added.</p> required <code>id_time</code> <code>int | None</code> <p>The time ID to which the annotation should be added.</p> required <code>enforce_ids</code> <code>bool</code> <p>Flag indicating whether the <code>ts_id</code> and <code>id_time</code> must belong to this dataset. <code>Default: True</code></p> <code>True</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def add_annotation(self, annotation: str, annotation_group: str, ts_id: int | None, id_time: int | None, enforce_ids: bool = True) -&gt; None:\n    \"\"\" \n    Adds an annotation to the specified `annotation_group`.\n\n    - If the provided `annotation_group` does not exist, it will be created.\n    - At least one of `ts_id` or `id_time` must be provided to associate the annotation with time series or/and time point.\n\n    Parameters:\n        annotation: The annotation to be added.\n        annotation_group: The group to which the annotation should be added.\n        ts_id: The time series ID to which the annotation should be added.\n        id_time: The time ID to which the annotation should be added.\n        enforce_ids: Flag indicating whether the `ts_id` and `id_time` must belong to this dataset. `Default: True`  \n    \"\"\"\n\n    if enforce_ids:\n        self._validate_annotation_ids(ts_id, id_time)\n    self.annotations.add_annotation(annotation, annotation_group, ts_id, id_time)\n\n    if ts_id is not None and id_time is not None:\n        self._update_annotations_imported_status(AnnotationType.BOTH, None)\n    elif ts_id is not None and id_time is None:\n        self._update_annotations_imported_status(AnnotationType.TS_ID, None)\n    elif ts_id is None and id_time is not None:\n        self._update_annotations_imported_status(AnnotationType.ID_TIME, None)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.remove_annotation","title":"remove_annotation","text":"<pre><code>remove_annotation(annotation_group: str, ts_id: int | None, id_time: int | None) -&gt; None\n</code></pre> <p>Removes an annotation from the specified <code>annotation_group</code>.</p> <ul> <li>At least one of <code>ts_id</code> or <code>id_time</code> must be provided to associate the annotation with time series or/and time point.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>annotation_group</code> <code>str</code> <p>The annotation group from which the annotation should be removed.</p> required <code>ts_id</code> <code>int | None</code> <p>The time series ID from which the annotation should be removed.</p> required <code>id_time</code> <code>int | None</code> <p>The time ID from which the annotation should be removed.</p> required Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def remove_annotation(self, annotation_group: str, ts_id: int | None, id_time: int | None) -&gt; None:\n    \"\"\"  \n    Removes an annotation from the specified `annotation_group`.\n\n    - At least one of `ts_id` or `id_time` must be provided to associate the annotation with time series or/and time point.\n\n    Parameters:\n        annotation_group: The annotation group from which the annotation should be removed.\n        ts_id: The time series ID from which the annotation should be removed.\n        id_time: The time ID from which the annotation should be removed. \n    \"\"\"\n\n    self.annotations.remove_annotation(annotation_group, ts_id, id_time, False)\n\n    if ts_id is not None and id_time is not None:\n        self._update_annotations_imported_status(AnnotationType.BOTH, None)\n    elif ts_id is not None and id_time is None:\n        self._update_annotations_imported_status(AnnotationType.TS_ID, None)\n    elif ts_id is None and id_time is not None:\n        self._update_annotations_imported_status(AnnotationType.ID_TIME, None)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.add_annotation_group","title":"add_annotation_group","text":"<pre><code>add_annotation_group(annotation_group: str, on: AnnotationType | Literal['id_time', 'ts_id', 'both'])\n</code></pre> <p>Adds a new <code>annotation_group</code>.</p> <p>Parameters:</p> Name Type Description Default <code>annotation_group</code> <code>str</code> <p>The name of the annotation group to be added.</p> required <code>on</code> <code>AnnotationType | Literal['id_time', 'ts_id', 'both']</code> <p>Specifies which part of the data should be annotated. If set to <code>\"both\"</code>, annotations will be applied as if <code>id_time</code> and <code>ts_id</code> were both set.</p> required Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def add_annotation_group(self, annotation_group: str, on: AnnotationType | Literal[\"id_time\", \"ts_id\", \"both\"]):\n    \"\"\" \n    Adds a new `annotation_group`.\n\n    Parameters:\n        annotation_group: The name of the annotation group to be added.\n        on: Specifies which part of the data should be annotated. If set to `\"both\"`, annotations will be applied as if `id_time` and `ts_id` were both set.\n    \"\"\"\n    on = AnnotationType(on)\n\n    self.annotations.add_annotation_group(annotation_group, on, False)\n\n    self._update_annotations_imported_status(on, None)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.remove_annotation_group","title":"remove_annotation_group","text":"<pre><code>remove_annotation_group(annotation_group: str, on: AnnotationType | Literal['id_time', 'ts_id', 'both'])\n</code></pre> <p>Removes the specified <code>annotation_group</code>.</p> <p>Parameters:</p> Name Type Description Default <code>annotation_group</code> <code>str</code> <p>The name of the annotation group to be removed.</p> required <code>on</code> <code>AnnotationType | Literal['id_time', 'ts_id', 'both']</code> <p>Specifies which part of the data the <code>annotation_group</code> should be removed from. If set to <code>\"both\"</code>, annotations will be applied as if <code>id_time</code> and <code>ts_id</code> were both set.</p> required Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def remove_annotation_group(self, annotation_group: str, on: AnnotationType | Literal[\"id_time\", \"ts_id\", \"both\"]):\n    \"\"\" \n    Removes the specified `annotation_group`.\n\n    Parameters:\n        annotation_group: The name of the annotation group to be removed.\n        on: Specifies which part of the data the `annotation_group` should be removed from. If set to `\"both\"`, annotations will be applied as if `id_time` and `ts_id` were both set.        \n    \"\"\"\n    on = AnnotationType(on)\n\n    self.annotations.remove_annotation_group(annotation_group, on, False)\n\n    self._update_annotations_imported_status(on, None)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_annotations","title":"get_annotations","text":"<pre><code>get_annotations(on: AnnotationType | Literal['id_time', 'ts_id', 'both']) -&gt; pd.DataFrame\n</code></pre> <p>Returns the annotations as a Pandas <code>DataFrame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>on</code> <code>AnnotationType | Literal['id_time', 'ts_id', 'both']</code> <p>Specifies which annotations to return. If set to <code>\"both\"</code>, annotations will be applied as if <code>id_time</code> and <code>ts_id</code> were both set.         </p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A Pandas DataFrame containing the selected annotations.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_annotations(self, on: AnnotationType | Literal[\"id_time\", \"ts_id\", \"both\"]) -&gt; pd.DataFrame:\n    \"\"\" \n    Returns the annotations as a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html).\n\n    Parameters:\n        on: Specifies which annotations to return. If set to `\"both\"`, annotations will be applied as if `id_time` and `ts_id` were both set.         \n\n    Returns:\n        A Pandas DataFrame containing the selected annotations.      \n    \"\"\"\n    on = AnnotationType(on)\n\n    return self.annotations.get_annotations(on, self.metadata.ts_id_name)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.import_annotations","title":"import_annotations","text":"<pre><code>import_annotations(identifier: str, enforce_ids: bool = True) -&gt; None\n</code></pre> <p>Imports annotations from a CSV file.</p> <p>First, it attempts to load the built-in annotations, if no built-in annotations with such an identifier exists, it attempts to load a custom annotations from the <code>\"data_root\"/tszoo/annotations/</code> directory.</p> <p><code>data_root</code> is specified when the dataset is created.     </p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The name of the CSV file.     </p> required <code>enforce_ids</code> <code>bool</code> <p>Flag indicating whether the <code>ts_id</code> and <code>id_time</code> must belong to this dataset. <code>Default: True</code></p> <code>True</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def import_annotations(self, identifier: str, enforce_ids: bool = True) -&gt; None:\n    \"\"\" \n    Imports annotations from a CSV file.\n\n    First, it attempts to load the built-in annotations, if no built-in annotations with such an identifier exists, it attempts to load a custom annotations from the `\"data_root\"/tszoo/annotations/` directory.\n\n    `data_root` is specified when the dataset is created.     \n\n    Parameters:\n        identifier: The name of the CSV file.     \n        enforce_ids: Flag indicating whether the `ts_id` and `id_time` must belong to this dataset. `Default: True`                \n    \"\"\"\n\n    annotations_file_path, is_built_in = get_annotations_path_and_whether_it_is_built_in(identifier, self.metadata.annotations_root, self.logger)\n\n    if is_built_in:\n        self.logger.info(\"Built-in annotations found: %s.\", identifier)\n        if not os.path.exists(annotations_file_path):\n            self.logger.info(\"Downloading annotations with identifier: %s\", identifier)\n            annotations_url = f\"{ANNOTATIONS_DOWNLOAD_BUCKET}&amp;file={identifier}\"  # probably will change annotations bucket... placeholder\n            resumable_download(url=annotations_url, file_path=annotations_file_path, silent=False)\n\n        self.logger.debug(\"Loading annotations from %s\", annotations_file_path)\n        temp_df = pd.read_csv(annotations_file_path)\n        self.logger.debug(\"Created DataFrame from file: %s\", annotations_file_path)\n    else:\n        self.logger.info(\"Custom annotations found: %s.\", identifier)\n        self.logger.debug(\"Loading annotations from %s\", annotations_file_path)\n        temp_df = pd.read_csv(annotations_file_path)\n        self.logger.debug(\"Created DataFrame from file: %s\", annotations_file_path)\n\n    ts_id_index = None\n    time_id_index = None\n    on = None\n\n    # Check the columns of the DataFrame to identify the type of annotation\n    if self.metadata.ts_id_name in temp_df.columns and ID_TIME_COLUMN_NAME in temp_df.columns:\n        self.annotations.clear_time_in_time_series()\n        time_id_index = temp_df.columns.tolist().index(ID_TIME_COLUMN_NAME)\n        ts_id_index = temp_df.columns.tolist().index(self.metadata.ts_id_name)\n        on = AnnotationType.BOTH\n        self.logger.info(\"Annotations detected as %s (both %s and id_time)\", AnnotationType.BOTH, self.metadata.ts_id_name)\n\n    elif self.metadata.ts_id_name in temp_df.columns:\n        self.annotations.clear_time_series()\n        ts_id_index = temp_df.columns.tolist().index(self.metadata.ts_id_name)\n        on = AnnotationType.TS_ID\n        self.logger.info(\"Annotations detected as %s (%s only)\", AnnotationType.TS_ID, self.metadata.ts_id_name)\n\n    elif ID_TIME_COLUMN_NAME in temp_df.columns:\n        self.annotations.clear_time()\n        time_id_index = temp_df.columns.tolist().index(ID_TIME_COLUMN_NAME)\n        on = AnnotationType.ID_TIME\n        self.logger.info(\"Annotations detected as %s (%s only)\", AnnotationType.ID_TIME, ID_TIME_COLUMN_NAME)\n\n    else:\n        raise ValueError(f\"Could not find {self.metadata.ts_id_name} and {ID_TIME_COLUMN_NAME} in the imported CSV.\")\n\n    # Process each row in the DataFrame and add annotations\n    for row in temp_df.itertuples(False):\n        for i, _ in enumerate(temp_df.columns):\n            if i == time_id_index or i == ts_id_index:\n                continue\n\n            ts_id = None\n            if ts_id_index is not None:\n                ts_id = row[ts_id_index]\n\n            id_time = None\n            if time_id_index is not None:\n                id_time = row[time_id_index]\n\n            self.add_annotation(row[i], temp_df.columns[i], ts_id, id_time, enforce_ids)\n\n    self._update_annotations_imported_status(on, identifier)\n    self.logger.info(\"Successfully imported annotations from %s\", annotations_file_path)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.import_config","title":"import_config","text":"<pre><code>import_config(identifier: str, display_config_details: Optional[Literal['text', 'diagram']] = 'text', workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Import the dataset_config from a pickle file and initializes the dataset. Config type must correspond to dataset type.</p> <p>First, it attempts to load the built-in config, if no built-in config with such an identifier exists, it attempts to load a custom config from the <code>\"data_root\"/tszoo/configs/</code> directory.</p> <p><code>data_root</code> is specified when the dataset is created.       </p> <p>The following configuration attributes are used during initialization:</p> Dataset config Description <code>init_workers</code> Specifies the number of workers to use for initialization. Applied when <code>workers</code> = \"config\". <code>partial_fit_initialized_transformers</code> Determines whether initialized transformers should be partially fitted on the training data. <code>nan_threshold</code> Filters out time series with missing values exceeding the specified threshold. <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>Name of the pickle file.</p> required <code>display_config_details</code> <code>Optional[Literal['text', 'diagram']]</code> <p>Flag indicating whether to display the configuration values after initialization. <code>Default: True</code> </p> <code>'text'</code> <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use during initialization. <code>Default: \"config\"</code></p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def import_config(self, identifier: str, display_config_details: Optional[Literal[\"text\", \"diagram\"]] = \"text\", workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\" \n    Import the dataset_config from a pickle file and initializes the dataset. Config type must correspond to dataset type.\n\n    First, it attempts to load the built-in config, if no built-in config with such an identifier exists, it attempts to load a custom config from the `\"data_root\"/tszoo/configs/` directory.\n\n    `data_root` is specified when the dataset is created.       \n\n    The following configuration attributes are used during initialization:\n\n    Dataset config | Description\n    -------------- | -----------\n    `init_workers` | Specifies the number of workers to use for initialization. Applied when `workers` = \"config\".\n    `partial_fit_initialized_transformers` | Determines whether initialized transformers should be partially fitted on the training data.\n    `nan_threshold` | Filters out time series with missing values exceeding the specified threshold.\n\n    Parameters:\n        identifier: Name of the pickle file.\n        display_config_details: Flag indicating whether to display the configuration values after initialization. `Default: True` \n        workers: The number of workers to use during initialization. `Default: \"config\"`  \n    \"\"\"\n\n    if display_config_details is not None:\n        display_config_details = DisplayType(display_config_details)\n\n    # Load config\n    config = load_config(identifier, self.metadata.configs_root, self.metadata.database_name, self.metadata.source_type, self.metadata.aggregation, self.logger)\n\n    self.logger.info(\"Initializing dataset configuration with the imported config.\")\n    self.set_dataset_config_and_initialize(config, display_config_details, workers)\n\n    self._update_config_imported_status(identifier)\n    self.logger.info(\"Successfully used config with identifier %s\", identifier)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.save_annotations","title":"save_annotations","text":"<pre><code>save_annotations(identifier: str, on: AnnotationType | Literal['id_time', 'ts_id', 'both'], force_write: bool = False) -&gt; None\n</code></pre> <p>Saves the annotations as a CSV file.</p> <p>The file will be saved to a path determined by the <code>data_root</code> specified when the dataset was created.</p> <p>The annotations will be saved under the directory <code>data_root/tszoo/annotations/</code>.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The name of the CSV file.</p> required <code>on</code> <code>AnnotationType | Literal['id_time', 'ts_id', 'both']</code> <p>What annotation type should be saved. If set to <code>\"both\"</code>, annotations will be applied as if <code>id_time</code> and <code>ts_id</code> were both set.   </p> required <code>force_write</code> <code>bool</code> <p>If set to <code>True</code>, will overwrite any existing files with the same name. <code>Default: False</code></p> <code>False</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def save_annotations(self, identifier: str, on: AnnotationType | Literal[\"id_time\", \"ts_id\", \"both\"], force_write: bool = False) -&gt; None:\n    \"\"\" \n    Saves the annotations as a CSV file.\n\n    The file will be saved to a path determined by the `data_root` specified when the dataset was created.\n\n    The annotations will be saved under the directory `data_root/tszoo/annotations/`.\n\n    Parameters:\n        identifier: The name of the CSV file.\n        on: What annotation type should be saved. If set to `\"both\"`, annotations will be applied as if `id_time` and `ts_id` were both set.   \n        force_write: If set to `True`, will overwrite any existing files with the same name. `Default: False`               \n    \"\"\"\n\n    if exists_built_in_annotations(identifier):\n        raise ValueError(\"Built-in annotations with this identifier already exists. Choose another identifier.\")\n\n    on = AnnotationType(on)\n\n    temp_df = self.get_annotations(on)\n\n    # Ensure the annotations root directory exists, creating it if necessary\n    if not os.path.exists(self.metadata.annotations_root):\n        os.makedirs(self.metadata.annotations_root)\n        self.logger.info(\"Created annotations directory at %s\", self.metadata.annotations_root)\n\n    path = os.path.join(self.metadata.annotations_root, f\"{identifier}.csv\")\n\n    if os.path.exists(path) and not force_write:\n        raise ValueError(f\"Annotations already exist at {path}. Set force_write=True to overwrite.\")\n    self.logger.debug(\"Annotations CSV file path: %s\", path)\n\n    temp_df.to_csv(path, index=False)\n\n    self._update_annotations_imported_status(on, identifier)\n    self.logger.info(\"Annotations successfully saved to %s\", path)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.save_config","title":"save_config","text":"<pre><code>save_config(identifier: str, create_with_details_file: bool = True, force_write: bool = False, **kwargs) -&gt; None\n</code></pre> <p>Saves the config as a pickle file.</p> <p>The file will be saved to a path determined by the <code>data_root</code> specified when the dataset was created.  The config will be saved under the directory <code>data_root/tszoo/configs/</code>.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The name of the pickle file.</p> required <code>create_with_details_file</code> <code>bool</code> <p>Whether to export the config along with a readable text file that provides details. <code>Defaults: True</code>. </p> <code>True</code> <code>force_write</code> <code>bool</code> <p>If set to <code>True</code>, will overwrite any existing files with the same name. <code>Default: False</code></p> <code>False</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def save_config(self, identifier: str, create_with_details_file: bool = True, force_write: bool = False, **kwargs) -&gt; None:\n    \"\"\" \n    Saves the config as a pickle file.\n\n    The file will be saved to a path determined by the `data_root` specified when the dataset was created. \n    The config will be saved under the directory `data_root/tszoo/configs/`.\n\n    Parameters:\n        identifier: The name of the pickle file.\n        create_with_details_file: Whether to export the config along with a readable text file that provides details. `Defaults: True`. \n        force_write: If set to `True`, will overwrite any existing files with the same name. `Default: False`            \n    \"\"\"\n\n    default_kwargs = {'hard_force': False}\n    kwargs = {**default_kwargs, **kwargs}\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to save config.\")\n\n    if not kwargs[\"hard_force\"] and exists_built_in_config(identifier):\n        raise ValueError(\"Built-in config with this identifier already exists. Choose another identifier.\")\n\n    # Ensure the config directory exists\n    if not os.path.exists(self.metadata.configs_root):\n        os.makedirs(self.metadata.configs_root)\n        self.logger.info(\"Created config directory at %s\", self.metadata.configs_root)\n\n    path_pickle = os.path.join(self.metadata.configs_root, f\"{identifier}.pickle\")\n    path_details = os.path.join(self.metadata.configs_root, f\"{identifier}.txt\")\n\n    if os.path.exists(path_pickle) and not force_write:\n        raise ValueError(f\"Config at path {path_pickle} already exists. Set force_write=True to overwrite.\")\n    self.logger.debug(\"Config pickle path: %s\", path_pickle)\n\n    if create_with_details_file:\n        if os.path.exists(path_details) and not force_write:\n            raise ValueError(f\"Config details at path {path_details} already exists. Set force_write=True to overwrite.\")\n        self.logger.debug(\"Config details path: %s\", path_details)\n\n    if not self.dataset_config.filler_factory.creates_built_in:\n        self.logger.warning(\"You are using a custom filler. Ensure the config is distributed with the source code of the filler.\")\n\n    if not self.dataset_config.anomaly_handler_factory.creates_built_in:\n        self.logger.warning(\"You are using a custom anomaly handler. Ensure the config is distributed with the source code of the anomaly handler.\")\n\n    if not self.dataset_config.transformer_factory.creates_built_in:\n        self.logger.warning(\"You are using a custom transformer. Ensure the config is distributed with the source code of the transformer.\")\n\n    if len(self.dataset_config.preprocess_order) != len(MANDATORY_PREPROCESSES_ORDER):\n        self.logger.warning(\"You are using at least one custom handler. Ensure the config is distributed with the source code of every custom handler.\")\n\n    pickle_dump(self._export_config_copy, path_pickle)\n    self.logger.info(\"Config pickle saved to %s\", path_pickle)\n\n    if create_with_details_file:\n        with open(path_details, \"w\", encoding=\"utf-8\") as file:\n            file.write(str(self.dataset_config))\n        self.logger.info(\"Config details saved to %s\", path_details)\n\n    self._update_config_imported_status(identifier)\n    self.dataset_config.export_update_needed = False\n    self.logger.info(\"Config successfully saved\")\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.save_benchmark","title":"save_benchmark","text":"<pre><code>save_benchmark(identifier: str, force_write: bool = False, **kwargs) -&gt; None\n</code></pre> <p>Saves the benchmark as a YAML file.</p> <p>The benchmark, along with any associated annotations and config files, will be saved in a path determined by the <code>data_root</code> specified when creating the dataset.  The default save path for benchmark is <code>\"data_root/tszoo/benchmarks/\"</code>.</p> <p>If you are using imported <code>annotations</code> or <code>config</code> (whether custom or built-in), their file names will be set in the <code>benchmark</code> file.  If new <code>annotations</code> or <code>config</code> are created during the process, their filenames will be derived from the provided <code>identifier</code> and set in the <code>benchmark</code> file.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The name of the YAML file.</p> required <code>force_write</code> <code>bool</code> <p>If set to <code>True</code>, will overwrite any existing files with the same name. <code>Default: False</code></p> <code>False</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def save_benchmark(self, identifier: str, force_write: bool = False, **kwargs) -&gt; None:\n    \"\"\" \n    Saves the benchmark as a YAML file.\n\n    The benchmark, along with any associated annotations and config files, will be saved in a path determined by the `data_root` specified when creating the dataset. \n    The default save path for benchmark is `\"data_root/tszoo/benchmarks/\"`.\n\n    If you are using imported `annotations` or `config` (whether custom or built-in), their file names will be set in the `benchmark` file. \n    If new `annotations` or `config` are created during the process, their filenames will be derived from the provided `identifier` and set in the `benchmark` file.\n\n    Parameters:\n        identifier: The name of the YAML file.\n        force_write: If set to `True`, will overwrite any existing files with the same name. `Default: False`            \n    \"\"\"\n\n    default_kwargs = {'hard_force': False}\n    kwargs = {**default_kwargs, **kwargs}\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to save benchmark.\")\n\n    if not kwargs[\"hard_force\"] and exists_built_in_benchmark(identifier):\n        raise ValueError(\"Built-in benchmark with this identifier already exists. Choose another identifier.\")\n\n    # Determine annotation names based on the available annotations and whether the annotations were imported\n    if len(self.annotations.time_series_annotations) &gt; 0:\n        annotations_ts_name = self.imported_annotations_ts_identifier if self.imported_annotations_ts_identifier is not None else f\"{identifier}_{AnnotationType.TS_ID.value}\"\n    else:\n        annotations_ts_name = None\n\n    if len(self.annotations.time_annotations) &gt; 0:\n        annotations_time_name = self.imported_annotations_time_identifier if self.imported_annotations_time_identifier is not None else f\"{identifier}_{AnnotationType.ID_TIME.value}\"\n    else:\n        annotations_time_name = None\n\n    if len(self.annotations.time_in_series_annotations) &gt; 0:\n        annotations_both_name = self.imported_annotations_both_identifier if self.imported_annotations_both_identifier is not None else f\"{identifier}_{AnnotationType.BOTH.value}\"\n    else:\n        annotations_both_name = None\n\n    # Use the imported identifier if available and update is not necessary, otherwise default to the current identifier\n    config_name = self.dataset_config.import_identifier if (self.dataset_config.import_identifier is not None and not self.dataset_config.export_update_needed) else identifier\n\n    export_benchmark = ExportBenchmark(self.metadata.database_name,\n                                       self.metadata.source_type.value,\n                                       self.metadata.aggregation.value,\n                                       self.metadata.dataset_type.value,\n                                       config_name,\n                                       annotations_ts_name,\n                                       annotations_time_name,\n                                       annotations_both_name,\n                                       related_results_identifier=self.related_to,\n                                       version=version.config_and_benchmarks_current_version)\n\n    # If the config was not imported, save it\n    if self.dataset_config.import_identifier is None or self.dataset_config.export_update_needed:\n        self.save_config(export_benchmark.config_identifier, force_write=force_write, hard_force=kwargs[\"hard_force\"])\n    else:\n        self.logger.info(\"Using already existing config with identifier: %s\", self.dataset_config.import_identifier)\n\n    # Save ts_id annotations if available and not previously imported\n    if self.imported_annotations_ts_identifier is None and len(self.annotations.time_series_annotations) &gt; 0:\n        self.save_annotations(export_benchmark.annotations_ts_identifier, AnnotationType.TS_ID, force_write=force_write)\n    elif self.imported_annotations_ts_identifier is not None:\n        self.logger.info(\"Using already existing annotations with identifier: %s; type: %s\", self.imported_annotations_ts_identifier, AnnotationType.TS_ID)\n\n    # Save id_time annotations if available and not previously imported\n    if self.imported_annotations_time_identifier is None and len(self.annotations.time_annotations) &gt; 0:\n        self.save_annotations(export_benchmark.annotations_time_identifier, AnnotationType.ID_TIME, force_write=force_write)\n    elif self.imported_annotations_time_identifier is not None:\n        self.logger.info(\"Using already existing annotations with identifier: %s; type: %s\", self.imported_annotations_time_identifier, AnnotationType.ID_TIME)\n\n    # Save both annotations if available and not previously imported\n    if self.imported_annotations_both_identifier is None and len(self.annotations.time_in_series_annotations) &gt; 0:\n        self.save_annotations(export_benchmark.annotations_both_identifier, AnnotationType.BOTH, force_write=force_write)\n    elif self.imported_annotations_both_identifier is not None:\n        self.logger.info(\"Using already existing annotations with identifier: %s; type: %s\", self.imported_annotations_both_identifier, AnnotationType.BOTH)\n\n    # Ensure the benchmark directory exists\n    if not os.path.exists(self.metadata.benchmarks_root):\n        os.makedirs(self.metadata.benchmarks_root)\n        self.logger.info(\"Created benchmarks directory at %s\", self.metadata.benchmarks_root)\n\n    benchmark_path = os.path.join(self.metadata.benchmarks_root, f\"{identifier}.yaml\")\n\n    if os.path.exists(benchmark_path) and not force_write:\n        self.logger.error(\"Benchmark file already exists at %s\", benchmark_path)\n        raise ValueError(f\"Benchmark at path {benchmark_path} already exists. Set force_write=True to overwrite.\")\n    self.logger.debug(\"Benchmark YAML file path: %s\", benchmark_path)\n\n    yaml_dump(export_benchmark.to_dict(), benchmark_path)\n    self.logger.info(\"Benchmark successfully saved to %s\", benchmark_path)\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.get_transformers","title":"get_transformers","text":"<pre><code>get_transformers() -&gt; np.ndarray[Transformer] | Transformer | None\n</code></pre> <p>Returns used transformers from config.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_transformers(self) -&gt; np.ndarray[Transformer] | Transformer | None:\n    \"\"\"Returns used transformers from config. \"\"\"\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting get transformers.\")\n\n    for i, preprocess_type in enumerate(self.dataset_config.preprocess_order):\n        if preprocess_type == PreprocessType.TRANSFORMING:\n            holder: TransformerHolder = self.dataset_config.train_preprocess_order[i].holder\n            return holder.transformers\n\n    return None\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.check_errors","title":"check_errors","text":"<pre><code>check_errors() -&gt; None\n</code></pre> <p>Validates whether the dataset is corrupted. </p> <p>Raises an exception if corrupted.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def check_errors(self) -&gt; None:\n    \"\"\"\n    Validates whether the dataset is corrupted. \n\n    Raises an exception if corrupted.\n    \"\"\"\n\n    dataset, _ = load_database(self.metadata.dataset_path)\n\n    try:\n        node_iter = dataset.walk_nodes()\n\n        # Process each node in the dataset\n        for node in node_iter:\n            if isinstance(node, tb.Table):\n\n                iter_by = min(LOADING_WARNING_THRESHOLD, len(node))\n                iters_done = 0\n\n                # Process the node in chunks to avoid memory issues\n                while iters_done &lt; len(node):\n                    iter_by = min(LOADING_WARNING_THRESHOLD, len(node) - iters_done)\n                    _ = node[iters_done: iters_done + iter_by]  # Fetch the data in chunks\n                    iters_done += iter_by\n\n                self.logger.info(\"Table '%s' checked successfully. (%d rows processed)\", node._v_pathname, len(node))\n\n        self.logger.info(\"Dataset check completed with no errors found.\")\n\n    except Exception as e:\n        self.logger.error(\"Error encountered during dataset check: %s\", str(e))\n\n    finally:\n        dataset.close()\n        self.logger.debug(\"Dataset connection closed.\")\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset.__get_data_for_plot","title":"__get_data_for_plot","text":"<pre><code>__get_data_for_plot(ts_id: int, features: list[str] | str, time_format: TimeFormat) -&gt; tuple[np.ndarray, np.ndarray, list[str]]\n</code></pre> <p>Returns prepared data for plotting.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def __get_data_for_plot(self, ts_id: int, features: list[str] | str, time_format: TimeFormat) -&gt; tuple[np.ndarray, np.ndarray, list[str]]:\n    \"\"\"Returns prepared data for plotting. \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before getting data for plotting.\")\n\n    features_indices = []\n\n    if features == \"config\":\n        features = deepcopy(self.dataset_config.features_to_take_without_ids)\n        features_indices = np.arange(len(features))\n        self.logger.debug(\"Features set from dataset config: %s\", features)\n    else:\n        if isinstance(features, str):\n            features = [features]\n\n        if len(features) == 0:\n            raise ValueError(\"No features specified to plot. Please provide valid features.\")\n        if len(set(features)) != len(features):\n            raise ValueError(\"Duplicate features detected. All features must be unique.\")\n\n        for feature in features:\n            if feature not in self.dataset_config.features_to_take_without_ids:\n                raise ValueError(f\"Feature '{feature}' is not valid. It is not present in the dataset configuration.\", self.dataset_config.features_to_take_without_ids)\n\n            index_in_config_features = self.dataset_config.features_to_take_without_ids.index(feature)\n            features_indices.append(index_in_config_features)\n\n    real_feature_indices = np.array(self.dataset_config.indices_of_features_to_take_no_ids)[features_indices]\n    real_feature_indices = real_feature_indices.astype(int)\n\n    time_series, time_period = self._get_data_for_plot(ts_id, real_feature_indices, time_format)\n    self.logger.debug(\"Time series data and corresponding time values retrieved.\")\n\n    return time_series, time_period, features\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset._validate_annotation_ids","title":"_validate_annotation_ids","text":"<pre><code>_validate_annotation_ids(ts_id: int | None, id_time: int | None) -&gt; None\n</code></pre> <p>Validates whether the <code>ts_id</code> and <code>id_time</code> belong to this dataset.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def _validate_annotation_ids(self, ts_id: int | None, id_time: int | None) -&gt; None:\n    \"\"\"Validates whether the `ts_id` and `id_time` belong to this dataset. \"\"\"\n\n    assert ts_id is not None or id_time is not None, \"Either ts_id or id_time must be provided.\"\n\n    # Handle when id_time is provided\n    if id_time is not None:\n        time_indices = self.metadata.time_indices\n        if id_time &lt; time_indices[ID_TIME_COLUMN_NAME][0] or id_time &gt; time_indices[ID_TIME_COLUMN_NAME][-1]:\n            valid_range = range(time_indices[ID_TIME_COLUMN_NAME][0], time_indices[ID_TIME_COLUMN_NAME][-1])\n            raise ValueError(f\"id_time {id_time} does not fall within the valid range for {self.metadata.aggregation}. \"\n                             f\"Valid id_time range: {valid_range}.\")\n\n    # Handle when ts_id is provided\n    if ts_id is not None:\n        ts_indices = self.metadata.ts_indices[self.metadata.ts_id_name]\n\n        if ts_id not in ts_indices:\n            valid_ts_range = self.metadata.ts_indices[self.metadata.ts_id_name]\n            raise ValueError(f\"ts_id {ts_id} does not exist in the available range for {self.metadata.source_type}. \"\n                             f\"Valid ts_id values: {valid_ts_range}.\")\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset._get_df","title":"_get_df","text":"<pre><code>_get_df(dataloader: DataLoader, as_single_dataframe: bool, ts_ids: ndarray, time_period: ndarray) -&gt; pd.DataFrame\n</code></pre> <p>Returns all data from the DataLoader as a Pandas <code>DataFrame</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def _get_df(self, dataloader: DataLoader, as_single_dataframe: bool, ts_ids: np.ndarray, time_period: np.ndarray) -&gt; pd.DataFrame:\n    \"\"\"Returns all data from the DataLoader as a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html). \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before getting DataFrame.\")\n\n    total_samples = len(ts_ids) * len(time_period)\n    if total_samples &gt;= LOADING_WARNING_THRESHOLD:\n        self.logger.warning(\"The dataset contains %d samples (%d time series \u00d7 %d times). Consider using get_*_dataloader() for batch loading.\", total_samples, len(ts_ids), len(time_period))\n\n    if as_single_dataframe:\n        self.logger.debug(\"Returning a single DataFrame with all features for all time series.\")\n        return dataset_loaders.create_single_df_from_dataloader(\n            dataloader,\n            ts_ids,\n            self.dataset_config.features_to_take,\n            self.dataset_config.time_format,\n            self.dataset_config.include_ts_id,\n            self.dataset_config.include_time,\n            self.dataset_config.dataset_type,\n            True\n        )\n    else:\n        self.logger.debug(\"Returning multiple DataFrames, one per time series.\")\n        return dataset_loaders.create_multiple_df_from_dataloader(\n            dataloader,\n            ts_ids,\n            self.dataset_config.features_to_take,\n            self.dataset_config.time_format,\n            self.dataset_config.include_ts_id,\n            self.dataset_config.include_time,\n            self.dataset_config.dataset_type,\n            True\n        )\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset._get_numpy","title":"_get_numpy","text":"<pre><code>_get_numpy(dataloader: DataLoader, ts_ids: ndarray, time_period: ndarray) -&gt; np.ndarray\n</code></pre> <p>Returns all data from the DataLoader as a NumPy <code>ndarray</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def _get_numpy(self, dataloader: DataLoader, ts_ids: np.ndarray, time_period: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Returns all data from the DataLoader as a NumPy `ndarray`. \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before getting Numpy array.\")\n\n    total_samples = len(ts_ids) * len(time_period)\n    if total_samples &gt;= LOADING_WARNING_THRESHOLD:\n        self.logger.warning(\"The dataset contains %d samples (%d time series \u00d7 %d times). Consider using get_*_dataloader() for batch loading.\", total_samples, len(ts_ids), len(time_period))\n\n    self.logger.debug(\"Creating numpy array from dataloader.\")\n    return dataset_loaders.create_numpy_from_dataloader(\n        dataloader,\n        ts_ids,\n        self.dataset_config.time_format,\n        self.dataset_config.include_time,\n        self.dataset_config.dataset_type,\n        True\n    )\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset._clear","title":"_clear","text":"<pre><code>_clear() -&gt; None\n</code></pre> <p>Clears set data. Mainly called when initializing new config.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def _clear(self) -&gt; None:\n    \"\"\"Clears set data. Mainly called when initializing new config. \"\"\"\n    self.train_dataset = None\n    self.train_dataloader = None\n    self.val_dataset = None\n    self.val_dataloader = None\n    self.test_dataset = None\n    self.test_dataloader = None\n    self.all_dataset = None\n    self.all_dataloader = None\n    self.dataset_config = None\n    self.logger.debug(\"Dataset attributes had been cleared. \")\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset._update_annotations_imported_status","title":"_update_annotations_imported_status","text":"<pre><code>_update_annotations_imported_status(on: AnnotationType, identifier: str)\n</code></pre> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def _update_annotations_imported_status(self, on: AnnotationType, identifier: str):\n    if on == AnnotationType.TS_ID:\n        self.imported_annotations_ts_identifier = identifier\n    elif on == AnnotationType.ID_TIME:\n        self.imported_annotations_time_identifier = identifier\n    elif on == AnnotationType.BOTH:\n        self.imported_annotations_both_identifier = identifier\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset._update_config_imported_status","title":"_update_config_imported_status","text":"<pre><code>_update_config_imported_status(identifier: str) -&gt; None\n</code></pre> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def _update_config_imported_status(self, identifier: str) -&gt; None:\n    self.dataset_config.import_identifier = identifier\n    self._export_config_copy.import_identifier = identifier\n</code></pre>"},{"location":"reference_series_based_cesnet_dataset/#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset._validate_config_for_dataset","title":"_validate_config_for_dataset","text":"<pre><code>_validate_config_for_dataset(config: DatasetConfig) -&gt; bool\n</code></pre> <p>Validates whether config is supposed to be used for this dataset.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def _validate_config_for_dataset(self, config: DatasetConfig) -&gt; bool:\n    \"\"\"Validates whether config is supposed to be used for this dataset. \"\"\"\n\n    if config.database_name != self.metadata.database_name:\n        self.logger.error(\"This config is not compatible with the current dataset. Difference in database name between config and this dataset.\")\n        raise ValueError(\"This config is not compatible with the current dataset.\", f\"config.database_name == {config.database_name} and dataset.database_name == {self.metadata.database_name}\")\n\n    if config.dataset_type != self.metadata.dataset_type:\n        self.logger.error(\"This config is not compatible with the current dataset. Difference in is_series_based between config and this dataset.\")\n        raise ValueError(\"This config is not compatible with the current dataset.\", f\"config.dataset_type == {config.dataset_type} and dataset.dataset_type == {self.metadata.dataset_type}\")\n\n    if config.aggregation != self.metadata.aggregation:\n        self.logger.error(\"This config is not compatible with the current dataset. Difference in aggregation type between config and this dataset.\")\n        raise ValueError(\"This config is not compatible with the current dataset.\", f\"config.aggregation == {config.aggregation} and dataset.aggregation == {self.metadata.aggregation}\")\n\n    if config.source_type != self.metadata.source_type:\n        self.logger.error(\"This config is not compatible with the current dataset. Difference in source type between config and this dataset.\")\n        raise ValueError(\"This config is not compatible with the current dataset.\", f\"config.source_type == {config.source_type} and dataset.source_type == {self.metadata.source_type}\")\n</code></pre>"},{"location":"reference_series_based_config/","title":"Series-based config class","text":""},{"location":"reference_series_based_config/#cesnet_tszoo.configs.series_based_config.SeriesBasedConfig","title":"<code>cesnet_tszoo.configs.series_based_config.SeriesBasedConfig</code>","text":"<p>               Bases: <code>SeriesBasedHandler</code>, <code>DatasetConfig</code></p> <p>This class is used for configuring the <code>SeriesBasedCesnetDataset</code>.</p> <p>Used to configure the following:</p> <ul> <li>Train, validation, test, all sets (time period, sizes, features)</li> <li>Handling missing values (default values, <code>fillers</code>)</li> <li>Handling anomalies (<code>anomaly handlers</code>)</li> <li>Data transformation using <code>transformers</code></li> <li>Applying custom handlers (<code>custom handlers</code>)</li> <li>Changing order of preprocesses</li> <li>Dataloader options (train/val/test/all/init workers, batch size, train loading order)</li> <li>Plotting</li> </ul> <p>Important Notes:</p> <ul> <li>Custom fillers must inherit from the <code>fillers</code> base class.</li> <li>Custom anomaly handlers must inherit from the <code>anomaly handlers</code> base class.</li> <li>Selected anomaly handler is only used for train set.    </li> <li>It is recommended to use the <code>transformers</code> base class, though this is not mandatory as long as it meets the required methods.<ul> <li>If a transformer is already initialized and <code>partial_fit_initialized_transformers</code> is <code>False</code>, the transformer does not require <code>partial_fit</code>.</li> <li>Otherwise, the transformer must support <code>partial_fit</code>.</li> <li>Transformers must implement <code>transform</code> method.</li> <li>Both <code>partial_fit</code> and <code>transform</code> methods must accept an input of type <code>np.ndarray</code> with shape <code>(times, features)</code>.</li> </ul> </li> <li>Custom handlers must be derived from one of the built-in <code>custom handler</code> classes </li> <li><code>train_ts</code>, <code>val_ts</code>, and <code>test_ts</code> must not contain any overlapping time series IDs.</li> </ul> Source code in <code>cesnet_tszoo\\configs\\series_based_config.py</code> <pre><code>class SeriesBasedConfig(SeriesBasedHandler, DatasetConfig):\n    \"\"\"\n    This class is used for configuring the [`SeriesBasedCesnetDataset`](reference_series_based_cesnet_dataset.md#cesnet_tszoo.datasets.series_based_cesnet_dataset.SeriesBasedCesnetDataset).\n\n    Used to configure the following:\n\n    - Train, validation, test, all sets (time period, sizes, features)\n    - Handling missing values (default values, [`fillers`](reference_fillers.md#cesnet_tszoo.utils.filler.filler))\n    - Handling anomalies ([`anomaly handlers`](reference_anomaly_handlers.md#cesnet_tszoo.utils.anomaly_handler.anomaly_handler))\n    - Data transformation using [`transformers`](reference_transformers.md#cesnet_tszoo.utils.transformer.transformer)\n    - Applying custom handlers ([`custom handlers`](reference_custom_handlers.md#cesnet_tszoo.utils.custom_handler.custom_handler))\n    - Changing order of preprocesses\n    - Dataloader options (train/val/test/all/init workers, batch size, train loading order)\n    - Plotting\n\n    **Important Notes:**\n\n    - Custom fillers must inherit from the [`fillers`](reference_fillers.md#cesnet_tszoo.utils.filler.filler.Filler) base class.\n    - Custom anomaly handlers must inherit from the [`anomaly handlers`](reference_anomaly_handlers.md#cesnet_tszoo.utils.anomaly_handler.anomaly_handler.AnomalyHandler) base class.\n    - Selected anomaly handler is only used for train set.    \n    - It is recommended to use the [`transformers`](reference_transformers.md#cesnet_tszoo.utils.transformer.transformer.Transformer) base class, though this is not mandatory as long as it meets the required methods.\n        - If a transformer is already initialized and `partial_fit_initialized_transformers` is `False`, the transformer does not require `partial_fit`.\n        - Otherwise, the transformer must support `partial_fit`.\n        - Transformers must implement `transform` method.\n        - Both `partial_fit` and `transform` methods must accept an input of type `np.ndarray` with shape `(times, features)`.\n    - Custom handlers must be derived from one of the built-in [`custom handler`](reference_custom_handlers.md#cesnet_tszoo.utils.custom_handler.custom_handler) classes \n    - `train_ts`, `val_ts`, and `test_ts` must not contain any overlapping time series IDs.\n\n    Attributes:\n        used_train_workers: Tracks the number of train workers in use. Helps determine if the train dataloader should be recreated based on worker changes.\n        used_val_workers: Tracks the number of validation workers in use. Helps determine if the validation dataloader should be recreated based on worker changes.\n        used_test_workers: Tracks the number of test workers in use. Helps determine if the test dataloader should be recreated based on worker changes.\n        used_all_workers: Tracks the total number of all workers in use. Helps determine if the all dataloader should be recreated based on worker changes.\n        uses_all_ts: Whether all time series set should be used.\n        import_identifier: Tracks the name of the config upon import. None if not imported.\n        filler_factory: Represents factory used to create passed Filler type.\n        anomaly_handler_factory: Represents factory used to create passed Anomaly Handler type.\n        transformer_factory: Represents factory used to create passed Transformer type.\n        can_fit_fillers: Whether fillers in this config, can be fitted.                \n        logger: Logger for displaying information.   \n        all_ts: If no specific sets (train/val/test) are provided, all time series IDs are used. When any set is defined, only the time series IDs in defined sets are used.\n        train_ts_row_ranges: Initialized when `train_ts` is set. Contains time series IDs in train set with their respective time ID ranges.\n        val_ts_row_ranges: Initialized when `val_ts` is set. Contains time series IDs in validation set with their respective time ID ranges.\n        test_ts_row_ranges: Initialized when `test_ts` is set. Contains time series IDs in test set with their respective time ID ranges.\n        all_ts_row_ranges: Initialized when `all_ts` is set. Contains time series IDs in all set with their respective time ID ranges.\n        display_time_period: Used to display the configured value of `time_period`.\n        aggregation: The aggregation period used for the data.\n        source_type: The source type of the data.\n        database_name: Specifies which database this config applies to.\n        features_to_take_without_ids: Features to be returned, excluding time or time series IDs.\n        indices_of_features_to_take_no_ids: Indices of non-ID features in `features_to_take`.\n        ts_id_name: Name of the time series ID, dependent on `source_type`.\n        used_singular_train_time_series: Currently used singular train set time series for dataloader.\n        used_singular_val_time_series: Currently used singular validation set time series for dataloader.\n        used_singular_test_time_series: Currently used singular test set time series for dataloader.\n        used_singular_all_time_series: Currently used singular all set time series for dataloader.     \n        train_preprocess_order: All preprocesses used for train set. \n        val_preprocess_order: All preprocesses used for val set. \n        test_preprocess_order: All preprocesses used for test set. \n        all_preprocess_order: All preprocesses used for all set.                  \n        is_initialized: Flag indicating if the configuration has already been initialized. If true, config initialization will be skipped.  \n        version: Version of cesnet-tszoo this config was made in.\n        export_update_needed: Whether config was updated to newer version and should be exported.      \n        time_period: Defines the time period for returning data from `train/val/test/all`. Can be a range of time IDs, a tuple of datetime objects or a float. Float value is equivalent to percentage of available times from start.\n        train_ts: Defines which time series IDs are used in the training set. Can be a list of IDs, or an integer/float to specify a random selection. An `int` specifies the number of random time series, and a `float` specifies the proportion of available time series. \n                  `int` and `float` must be greater than 0, and a float should be smaller or equal to 1.0. Using `int` or `float` guarantees that no time series from other sets will be used.\n        val_ts: Defines which time series IDs are used in the validation set. Same as `train_ts` but for the validation set.\n        test_ts: Defines which time series IDs are used in the test set. Same as `train_ts` but for the test set.         \n        features_to_take: Defines which features are used.\n        default_values: Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature.\n        train_batch_size: Batch size for the train dataloader. Affects number of returned time series in one batch.\n        val_batch_size: Batch size for the validation dataloader. Affects number of returned time series in one batch.\n        test_batch_size: Batch size for the test dataloader. Affects number of returned time series in one batch.\n        all_batch_size: Batch size for the all dataloader. Affects number of returned time series in one batch.\n        preprocess_order: Defines in which order preprocesses are used. Also can add to order a type of `AllSeriesCustomHandler` or `NoFitCustomHandler`.            \n        partial_fit_initialized_transformer: If `True`, partial fitting on train set is performed when using initiliazed transformer.\n        include_time: If `True`, time data is included in the returned values.\n        include_ts_id: If `True`, time series IDs are included in the returned values.\n        time_format: Format for the returned time data. When using TimeFormat.DATETIME, time will be returned as separate list along rest of the values.\n        train_workers: Number of workers for loading training data. `0` means that the data will be loaded in the main process.\n        val_workers: Number of workers for loading validation data. `0` means that the data will be loaded in the main process.\n        test_workers: Number of workers for loading test data. `0` means that the data will be loaded in the main process.\n        all_workers: Number of workers for loading all data. `0` means that the data will be loaded in the main process.\n        init_workers: Number of workers for initial dataset processing during configuration. `0` means that the data will be loaded in the main process.\n        nan_threshold: Maximum allowable percentage of missing data. Time series exceeding this threshold are excluded. Time series over the threshold will not be used. Used for `train/val/test/all` separately.\n        train_dataloader_order: Defines the order of data returned by the training dataloader.\n        random_state: Fixes randomness for reproducibility during configuration and dataset initialization.               \n    \"\"\"\n\n    def __init__(self,\n                 time_period: tuple[datetime, datetime] | range | float | Literal[\"all\"],\n                 train_ts: list[int] | npt.NDArray[np.int_] | float | int | None = None,\n                 val_ts: list[int] | npt.NDArray[np.int_] | float | int | None = None,\n                 test_ts: list[int] | npt.NDArray[np.int_] | float | int | None = None,\n                 features_to_take: list[str] | Literal[\"all\"] = \"all\",\n                 default_values: list[Number] | npt.NDArray[np.number] | dict[str, Number] | Number | Literal[\"default\"] | None = \"default\",\n                 train_batch_size: int = 32,\n                 val_batch_size: int = 64,\n                 test_batch_size: int = 128,\n                 all_batch_size: int = 128,\n                 preprocess_order: list[str, type] = [\"filling_gaps\", \"handling_anomalies\", \"transforming\"],\n                 fill_missing_with: type | FillerType | Literal[\"mean_filler\", \"forward_filler\", \"linear_interpolation_filler\"] | None = None,\n                 transform_with: type | TransformerType | Transformer | Literal[\"min_max_scaler\", \"standard_scaler\", \"max_abs_scaler\", \"log_transformer\", \"l2_normalizer\"] | None = None,\n                 handle_anomalies_with: type | AnomalyHandlerType | Literal[\"z-score\", \"interquartile_range\"] | None = None,\n                 partial_fit_initialized_transformer: bool = False,\n                 include_time: bool = True,\n                 include_ts_id: bool = True,\n                 time_format: TimeFormat | Literal[\"id_time\", \"datetime\", \"unix_time\", \"shifted_unix_time\"] = TimeFormat.ID_TIME,\n                 train_workers: int = 4,\n                 val_workers: int = 3,\n                 test_workers: int = 2,\n                 all_workers: int = 4,\n                 init_workers: int = 4,\n                 nan_threshold: float = 1.0,\n                 train_dataloader_order: DataloaderOrder | Literal[\"random\", \"sequential\"] = DataloaderOrder.SEQUENTIAL,\n                 random_state: int | None = None):\n        \"\"\"\n        Parameters:\n            time_period: Defines the time period for returning data from `train/val/test/all`. Can be a range of time IDs, a tuple of datetime objects or a float. Float value is equivalent to percentage of available times from start.\n            train_ts: Defines which time series IDs are used in the training set. Can be a list of IDs, or an integer/float to specify a random selection. An `int` specifies the number of random time series, and a `float` specifies the proportion of available time series. \n                    `int` and `float` must be greater than 0, and a float should be smaller or equal to 1.0. Using `int` or `float` guarantees that no time series from other sets will be used. `Default: None`\n            val_ts: Defines which time series IDs are used in the validation set. Same as `train_ts` but for the validation set. `Default: None`\n            test_ts: Defines which time series IDs are used in the test set. Same as `train_ts` but for the test set. `Default: None`           \n            features_to_take: Defines which features are used. `Default: \"all\"`\n            default_values: Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature. `Default: \"default\"`\n            train_batch_size: Batch size for the train dataloader. Affects number of returned time series in one batch. `Default: 32`\n            val_batch_size: Batch size for the validation dataloader. Affects number of returned time series in one batch. `Default: 64`\n            test_batch_size: Batch size for the test dataloader. Affects number of returned time series in one batch. `Default: 128`\n            all_batch_size: Batch size for the all dataloader. Affects number of returned time series in one batch. `Default: 128`   \n            preprocess_order: Defines in which order preprocesses are used. Also can add to order a type of `AllSeriesCustomHandler` or `NoFitCustomHandler`. `Default: [\"handling_anomalies\", \"filling_gaps\", \"transforming\"]`              \n            fill_missing_with: Defines how to fill missing values in the dataset. Can pass enum `FillerType` for built-in filler or pass a type of custom filler that must derive from `Filler` base class. `Default: None`\n            transform_with: Defines the transformer used to transform the dataset. Can pass enum `TransformerType` for built-in transformer, pass a type of custom transformer or instance of already fitted transformer. `Default: None`\n            handle_anomalies_with: Defines the anomaly handler for handling anomalies in the train set. Can pass enum `AnomalyHandlerType` for built-in anomaly handler or a type of custom anomaly handler. `Default: None`\n            partial_fit_initialized_transformer: If `True`, partial fitting on train set is performed when using initiliazed transformer. `Default: False`\n            include_time: If `True`, time data is included in the returned values. `Default: True`\n            include_ts_id: If `True`, time series IDs are included in the returned values. `Default: True`\n            time_format: Format for the returned time data. When using TimeFormat.DATETIME, time will be returned as separate list along rest of the values. `Default: TimeFormat.ID_TIME`\n            train_workers: Number of workers for loading training data. `0` means that the data will be loaded in the main process. `Default: 4`\n            val_workers: Number of workers for loading validation data. `0` means that the data will be loaded in the main process. `Default: 3`\n            test_workers: Number of workers for loading test data. `0` means that the data will be loaded in the main process. `Default: 2`\n            all_workers: Number of workers for loading all data. `0` means that the data will be loaded in the main process. `Default: 4`\n            init_workers: Number of workers for initial dataset processing during configuration. `0` means that the data will be loaded in the main process. `Default: 4`\n            nan_threshold: Maximum allowable percentage of missing data. Time series exceeding this threshold are excluded. Time series over the threshold will not be used. Used for `train/val/test/all` separately. `Default: 1.0`\n            train_dataloader_order: Defines the order of data returned by the training dataloader. `Default: DataloaderOrder.SEQUENTIAL`\n            random_state: Fixes randomness for reproducibility during configuration and dataset initialization. `Default: None`       \n        \"\"\"\n\n        self.time_period: np.ndarray = time_period\n\n        self.display_time_period: Optional[range] = None\n\n        self.logger = logging.getLogger(\"series_config\")\n\n        SeriesBasedHandler.__init__(self, self.logger, True, train_ts, val_ts, test_ts)\n        DatasetConfig.__init__(self, features_to_take, default_values, train_batch_size, val_batch_size, test_batch_size, all_batch_size, preprocess_order, fill_missing_with, transform_with, handle_anomalies_with, partial_fit_initialized_transformer, include_time, include_ts_id, time_format,\n                               train_workers, val_workers, test_workers, all_workers, init_workers, nan_threshold, False, DatasetType.SERIES_BASED, train_dataloader_order, random_state, False, self.logger)\n\n    def _validate_construction(self) -&gt; None:\n        \"\"\"Performs basic parameter validation to ensure correct configuration. More comprehensive validation, which requires dataset-specific data, is handled in [`_dataset_init`][cesnet_tszoo.configs.series_based_config.SeriesBasedConfig._dataset_init]. \"\"\"\n\n        DatasetConfig._validate_construction(self)\n\n        if isinstance(self.time_period, (float, int)):\n            self.time_period = float(self.time_period)\n            assert self.time_period &gt; 0.0, \"time_period must be greater than 0\"\n            assert self.time_period &lt;= 1.0, \"time_period must be lower or equal to 1.0\"\n\n        self._validate_ts_init()\n\n        self.logger.debug(\"Series-based configuration validated successfully.\")\n\n    def _get_train(self) -&gt; tuple[np.ndarray, np.ndarray] | tuple[None, None]:\n        \"\"\"Returns the indices corresponding to the training set. \"\"\"\n        return self.train_ts, self.time_period\n\n    def _get_val(self) -&gt; tuple[np.ndarray, np.ndarray] | tuple[None, None]:\n        \"\"\"Returns the indices corresponding to the validation set. \"\"\"\n        return self.val_ts, self.time_period\n\n    def _get_test(self) -&gt; tuple[np.ndarray, np.ndarray] | tuple[None, None]:\n        \"\"\"Returns the indices corresponding to the test set. \"\"\"\n        return self.test_ts, self.time_period\n\n    def _get_all(self) -&gt; tuple[np.ndarray, np.ndarray] | tuple[None, None]:\n        \"\"\"Returns the indices corresponding to the all set. \"\"\"\n        return self.all_ts, self.time_period\n\n    def has_train(self) -&gt; bool:\n        \"\"\"Returns whether training set is used. \"\"\"\n        return self.train_ts is not None\n\n    def has_val(self) -&gt; bool:\n        \"\"\"Returns whether validation set is used. \"\"\"\n        return self.val_ts is not None\n\n    def has_test(self) -&gt; bool:\n        \"\"\"Returns whether test set is used. \"\"\"\n        return self.test_ts is not None\n\n    def has_all(self) -&gt; bool:\n        \"\"\"Returns whether all set is used. \"\"\"\n        return self.all_ts is not None\n\n    def _set_time_period(self, all_time_ids: np.ndarray) -&gt; None:\n        \"\"\"Validates and filters the input time period based on the dataset and aggregation. \"\"\"\n\n        if self.time_period == \"all\":\n            self.time_period = range(len(all_time_ids))\n            self.logger.debug(\"Time period set to 'all'. Using all available time IDs, range: %s\", self.time_period)\n        elif isinstance(self.time_period, float):\n            self.time_period = range(int(self.time_period * len(all_time_ids)))\n            self.logger.debug(\"Time period set with float value. Using range: %s\", self.time_period)\n\n        self.time_period, self.display_time_period = TimeBasedHandler._process_time_period(self.time_period, all_time_ids, self.logger, self.time_format)\n        self.logger.debug(\"Processed time_period: %s, display_time_period: %s\", self.time_period, self.display_time_period)\n\n    def _set_ts(self, all_ts_ids: np.ndarray, all_ts_row_ranges: np.ndarray, rd: np.random.RandomState) -&gt; None:\n        \"\"\"Validates and filters the input time series IDs based on the `dataset` and `source_type`. Handles random split.\"\"\"\n\n        self._prepare_and_set_ts_sets(all_ts_ids, all_ts_row_ranges, self.ts_id_name, self.random_state, rd)\n\n    def _get_feature_transformers(self) -&gt; Transformer:\n        \"\"\"Creates transformer with `transformer_factory`. \"\"\"\n\n        if self.transformer_factory.has_already_initialized:\n            if not self.has_train() and self.partial_fit_initialized_transformers:\n                self.partial_fit_initialized_transformers = False\n                self.logger.warning(\"partial_fit_initialized_transformers will be ignored because train set is not used.\")\n\n            transformers = self.transformer_factory.get_already_initialized_transformers()\n            self.logger.debug(\"Using already initialized transformer %s.\", self.transformer_factory.name)\n        else:\n            if not self.has_train() and not self.transformer_factory.is_empty_factory:\n                self.transformer_factory = transformer_factories.get_transformer_factory(None, self.create_transformer_per_time_series, self.partial_fit_initialized_transformers)\n                self.logger.warning(\"No transformer will be used because train set is not used.\")\n\n            transformers = self.transformer_factory.create_transformer()\n            self.logger.debug(\"Using transformer %s.\", self.transformer_factory.name)\n\n        return transformers\n\n    def _set_no_fit_custom_handler(self, factory: NoFitCustomHandlerFactory):\n\n        train_handlers = np.array([factory.create_handler() for _ in self.train_ts]) if self.has_train() else None\n        self.train_preprocess_order.append(PreprocessNote(factory.preprocess_enum_type, False, False, factory.can_apply_to_train and self.has_train(), True, NoFitCustomHandlerHolder(train_handlers)))\n\n        val_handlers = np.array([factory.create_handler() for _ in self.val_ts]) if self.has_val() else None\n        self.val_preprocess_order.append(PreprocessNote(factory.preprocess_enum_type, False, False, factory.can_apply_to_val and self.has_val(), True, NoFitCustomHandlerHolder(val_handlers)))\n\n        test_handlers = np.array([factory.create_handler() for _ in self.test_ts]) if self.has_test() else None\n        self.test_preprocess_order.append(PreprocessNote(factory.preprocess_enum_type, False, False, factory.can_apply_to_test and self.has_test(), True, NoFitCustomHandlerHolder(test_handlers)))\n\n        all_handlers = np.array([factory.create_handler() for _ in self.all_ts]) if self.has_all() else None\n        self.all_preprocess_order.append(PreprocessNote(factory.preprocess_enum_type, False, False, factory.can_apply_to_all and self.has_all(), True, NoFitCustomHandlerHolder(all_handlers)))\n\n    def _get_fillers(self) -&gt; tuple:\n        \"\"\"Creates fillers with `filler_factory`. \"\"\"\n\n        train_fillers = None\n        # Set the fillers for the training set\n        if self.has_train():\n            train_fillers = np.array([self.filler_factory.create_filler(self.features_to_take_without_ids) for _ in self.train_ts])\n            self.logger.debug(\"Fillers for training set are set.\")\n\n        val_fillers = None\n        # Set the fillers for the validation set\n        if self.has_val():\n            val_fillers = np.array([self.filler_factory.create_filler(self.features_to_take_without_ids) for _ in self.val_ts])\n            self.logger.debug(\"Fillers for validation set are set.\")\n\n        test_fillers = None\n        # Set the fillers for the test set\n        if self.has_test():\n            test_fillers = np.array([self.filler_factory.create_filler(self.features_to_take_without_ids) for _ in self.test_ts])\n            self.logger.debug(\"Fillers for test set are set.\")\n\n        all_fillers = None\n        # Set the fillers for the all set\n        if self.has_all():\n            all_fillers = np.array([self.filler_factory.create_filler(self.features_to_take_without_ids) for _ in self.all_ts])\n            self.logger.debug(\"Fillers for all set are set.\")\n\n        self.logger.debug(\"Using filler %s\", self.filler_factory.name)\n\n        return train_fillers, val_fillers, test_fillers, all_fillers\n\n    def _get_anomaly_handlers(self) -&gt; np.ndarray:\n        \"\"\"Creates anomaly handlers with `anomaly_handler_factory`. \"\"\"\n\n        if not self.has_train() and not self.anomaly_handler_factory.is_empty_factory:\n            self.anomaly_handler_factory = anomaly_handler_factories.get_anomaly_handler_factory(None)\n            self.logger.warning(\"No anomaly handler will be used because train set is not used.\")\n\n        anomaly_handlers = np.array([])\n        if self.has_train():\n            anomaly_handlers = np.array([self.anomaly_handler_factory.create_anomaly_handler() for _ in self.train_ts])\n\n        self.logger.debug(\"Using anomaly handler %s\", self.anomaly_handler_factory.name)\n\n        return anomaly_handlers\n\n    def _set_per_series_custom_handler(self, factory: PerSeriesCustomHandlerFactory):\n        raise ValueError(f\"Cannot use {factory.name} CustomHandler, because PerSeriesCustomHandler is not supported for {self.dataset_type}. Use AllSeriesCustomHandler or NoFitCustomHandler instead. \")\n\n    def _get_summary_filter_time_series(self) -&gt; css_utils.SummaryDiagramStep:\n        attributes = [css_utils.StepAttribute(\"Train time series IDs\", get_abbreviated_list_string(self.train_ts)),\n                      css_utils.StepAttribute(\"Val time series IDs\", get_abbreviated_list_string(self.val_ts)),\n                      css_utils.StepAttribute(\"Test time series IDs\", get_abbreviated_list_string(self.test_ts)),\n                      css_utils.StepAttribute(\"All time series IDs\", get_abbreviated_list_string(self.all_ts)),\n                      css_utils.StepAttribute(\"Time periods\", self.display_time_period),\n                      css_utils.StepAttribute(\"Nan threshold\", self.nan_threshold)]\n\n        return css_utils.SummaryDiagramStep(\"Filter time series\", attributes)\n\n    def _get_summary_loader(self) -&gt; list[css_utils.SummaryDiagramStep]:\n\n        steps = []\n\n        attributes = [css_utils.StepAttribute(\"Train batch size\", self.train_batch_size),\n                      css_utils.StepAttribute(\"Val batch size\", self.val_batch_size),\n                      css_utils.StepAttribute(\"Test batch size\", self.test_batch_size),\n                      css_utils.StepAttribute(\"All batch size\", self.all_batch_size),\n                      css_utils.StepAttribute(\"Train dataloader order\", self.train_dataloader_order),]\n\n        steps.append(css_utils.SummaryDiagramStep(\"Transform into specific format\", attributes))\n\n        return steps\n\n    def _validate_finalization(self) -&gt; None:\n        \"\"\"Performs final validation of the configuration. \"\"\"\n\n        self._validate_ts_overlap()\n\n    def __str__(self) -&gt; str:\n\n        if self.transformer_factory.is_empty_factory:\n            transformer_part = f\"Transformer type: {self.transformer_factory.name}\"\n        else:\n            transformer_part = f'''Transformer type: {self.transformer_factory.name}\n        Are transformers premade: {self.transformer_factory.has_already_initialized}\n        Are premade transformers partial_fitted: {self.partial_fit_initialized_transformers}'''\n\n        if self.include_time:\n            time_part = f'''Time included: {str(self.include_time)}    \n        Time format: {str(self.time_format)}'''\n        else:\n            time_part = f\"Time included: {str(self.include_time)}\"\n\n        return f'''\nConfig Details:\n    Used for database: {self.database_name}\n    Aggregation: {str(self.aggregation)}\n    Source: {str(self.source_type)}\n\n    Time series\n        Train time series IDS: {get_abbreviated_list_string(self.train_ts)}\n        Val time series IDS: {get_abbreviated_list_string(self.val_ts)}\n        Test time series IDS {get_abbreviated_list_string(self.test_ts)}\n        All time series IDS {get_abbreviated_list_string(self.all_ts)}\n    Time periods\n        Time period: {str(self.display_time_period)}\n    Features\n        Taken features: {str(self.features_to_take_without_ids)}\n        Default values: {self.default_values}\n        Time series ID included: {str(self.include_ts_id)}\n        {time_part}\n    Fillers         \n        Filler type: {self.filler_factory.name}\n    Transformers\n        {transformer_part}\n    Anomaly handler\n        Anomaly handler type (train set): {self.anomaly_handler_factory.name}   \n    Batch sizes\n        Train batch size: {self.train_batch_size}\n        Val batch size: {self.val_batch_size}\n        Test batch size: {self.test_batch_size}\n        All batch size: {self.all_batch_size}\n    Default workers\n        Train worker count: {str(self.train_workers)}\n        Val worker count: {str(self.val_workers)}\n        Test worker count: {str(self.test_workers)}\n        All worker count: {str(self.all_workers)}\n        Init worker count: {str(self.init_workers)}\n    Other\n        Preprocess order: {normalize_display_list(self.preprocess_order)}\n        Nan threshold: {str(self.nan_threshold)}\n        Random state: {self.random_state}\n        Train dataloader order: {str(self.train_dataloader_order)}\n        Version: {self.version}\n                '''\n</code></pre>"},{"location":"reference_series_based_config/#configuration-options","title":"Configuration options","text":"<p>Parameters:</p> Name Type Description Default <code>time_period</code> <code>tuple[datetime, datetime] | range | float | Literal['all']</code> <p>Defines the time period for returning data from <code>train/val/test/all</code>. Can be a range of time IDs, a tuple of datetime objects or a float. Float value is equivalent to percentage of available times from start.</p> required <code>train_ts</code> <code>list[int] | NDArray[int_] | float | int | None</code> <p>Defines which time series IDs are used in the training set. Can be a list of IDs, or an integer/float to specify a random selection. An <code>int</code> specifies the number of random time series, and a <code>float</code> specifies the proportion of available time series.      <code>int</code> and <code>float</code> must be greater than 0, and a float should be smaller or equal to 1.0. Using <code>int</code> or <code>float</code> guarantees that no time series from other sets will be used. <code>Default: None</code></p> <code>None</code> <code>val_ts</code> <code>list[int] | NDArray[int_] | float | int | None</code> <p>Defines which time series IDs are used in the validation set. Same as <code>train_ts</code> but for the validation set. <code>Default: None</code></p> <code>None</code> <code>test_ts</code> <code>list[int] | NDArray[int_] | float | int | None</code> <p>Defines which time series IDs are used in the test set. Same as <code>train_ts</code> but for the test set. <code>Default: None</code> </p> <code>None</code> <code>features_to_take</code> <code>list[str] | Literal['all']</code> <p>Defines which features are used. <code>Default: \"all\"</code></p> <code>'all'</code> <code>default_values</code> <code>list[Number] | NDArray[number] | dict[str, Number] | Number | Literal['default'] | None</code> <p>Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature. <code>Default: \"default\"</code></p> <code>'default'</code> <code>train_batch_size</code> <code>int</code> <p>Batch size for the train dataloader. Affects number of returned time series in one batch. <code>Default: 32</code></p> <code>32</code> <code>val_batch_size</code> <code>int</code> <p>Batch size for the validation dataloader. Affects number of returned time series in one batch. <code>Default: 64</code></p> <code>64</code> <code>test_batch_size</code> <code>int</code> <p>Batch size for the test dataloader. Affects number of returned time series in one batch. <code>Default: 128</code></p> <code>128</code> <code>all_batch_size</code> <code>int</code> <p>Batch size for the all dataloader. Affects number of returned time series in one batch. <code>Default: 128</code> </p> <code>128</code> <code>preprocess_order</code> <code>list[str, type]</code> <p>Defines in which order preprocesses are used. Also can add to order a type of <code>AllSeriesCustomHandler</code> or <code>NoFitCustomHandler</code>. <code>Default: [\"handling_anomalies\", \"filling_gaps\", \"transforming\"]</code> </p> <code>['filling_gaps', 'handling_anomalies', 'transforming']</code> <code>fill_missing_with</code> <code>type | FillerType | Literal['mean_filler', 'forward_filler', 'linear_interpolation_filler'] | None</code> <p>Defines how to fill missing values in the dataset. Can pass enum <code>FillerType</code> for built-in filler or pass a type of custom filler that must derive from <code>Filler</code> base class. <code>Default: None</code></p> <code>None</code> <code>transform_with</code> <code>type | TransformerType | Transformer | Literal['min_max_scaler', 'standard_scaler', 'max_abs_scaler', 'log_transformer', 'l2_normalizer'] | None</code> <p>Defines the transformer used to transform the dataset. Can pass enum <code>TransformerType</code> for built-in transformer, pass a type of custom transformer or instance of already fitted transformer. <code>Default: None</code></p> <code>None</code> <code>handle_anomalies_with</code> <code>type | AnomalyHandlerType | Literal['z-score', 'interquartile_range'] | None</code> <p>Defines the anomaly handler for handling anomalies in the train set. Can pass enum <code>AnomalyHandlerType</code> for built-in anomaly handler or a type of custom anomaly handler. <code>Default: None</code></p> <code>None</code> <code>partial_fit_initialized_transformer</code> <code>bool</code> <p>If <code>True</code>, partial fitting on train set is performed when using initiliazed transformer. <code>Default: False</code></p> <code>False</code> <code>include_time</code> <code>bool</code> <p>If <code>True</code>, time data is included in the returned values. <code>Default: True</code></p> <code>True</code> <code>include_ts_id</code> <code>bool</code> <p>If <code>True</code>, time series IDs are included in the returned values. <code>Default: True</code></p> <code>True</code> <code>time_format</code> <code>TimeFormat | Literal['id_time', 'datetime', 'unix_time', 'shifted_unix_time']</code> <p>Format for the returned time data. When using TimeFormat.DATETIME, time will be returned as separate list along rest of the values. <code>Default: TimeFormat.ID_TIME</code></p> <code>ID_TIME</code> <code>train_workers</code> <code>int</code> <p>Number of workers for loading training data. <code>0</code> means that the data will be loaded in the main process. <code>Default: 4</code></p> <code>4</code> <code>val_workers</code> <code>int</code> <p>Number of workers for loading validation data. <code>0</code> means that the data will be loaded in the main process. <code>Default: 3</code></p> <code>3</code> <code>test_workers</code> <code>int</code> <p>Number of workers for loading test data. <code>0</code> means that the data will be loaded in the main process. <code>Default: 2</code></p> <code>2</code> <code>all_workers</code> <code>int</code> <p>Number of workers for loading all data. <code>0</code> means that the data will be loaded in the main process. <code>Default: 4</code></p> <code>4</code> <code>init_workers</code> <code>int</code> <p>Number of workers for initial dataset processing during configuration. <code>0</code> means that the data will be loaded in the main process. <code>Default: 4</code></p> <code>4</code> <code>nan_threshold</code> <code>float</code> <p>Maximum allowable percentage of missing data. Time series exceeding this threshold are excluded. Time series over the threshold will not be used. Used for <code>train/val/test/all</code> separately. <code>Default: 1.0</code></p> <code>1.0</code> <code>train_dataloader_order</code> <code>DataloaderOrder | Literal['random', 'sequential']</code> <p>Defines the order of data returned by the training dataloader. <code>Default: DataloaderOrder.SEQUENTIAL</code></p> <code>SEQUENTIAL</code> <code>random_state</code> <code>int | None</code> <p>Fixes randomness for reproducibility during configuration and dataset initialization. <code>Default: None</code></p> <code>None</code>"},{"location":"reference_series_based_config/#config-attributes","title":"Config attributes","text":"<p>Attributes:</p> Name Type Description <code>used_train_workers</code> <code>Optional[int]</code> <p>Tracks the number of train workers in use. Helps determine if the train dataloader should be recreated based on worker changes.</p> <code>used_val_workers</code> <code>Optional[int]</code> <p>Tracks the number of validation workers in use. Helps determine if the validation dataloader should be recreated based on worker changes.</p> <code>used_test_workers</code> <code>Optional[int]</code> <p>Tracks the number of test workers in use. Helps determine if the test dataloader should be recreated based on worker changes.</p> <code>used_all_workers</code> <code>Optional[int]</code> <p>Tracks the total number of all workers in use. Helps determine if the all dataloader should be recreated based on worker changes.</p> <code>uses_all_ts</code> <code>bool</code> <p>Whether all time series set should be used.</p> <code>import_identifier</code> <code>Optional[str]</code> <p>Tracks the name of the config upon import. None if not imported.</p> <code>filler_factory</code> <code>FillerFactory</code> <p>Represents factory used to create passed Filler type.</p> <code>anomaly_handler_factory</code> <code>AnomalyHandlerFactory</code> <p>Represents factory used to create passed Anomaly Handler type.</p> <code>transformer_factory</code> <code>TransformerFactory</code> <p>Represents factory used to create passed Transformer type.</p> <code>can_fit_fillers</code> <code>bool</code> <p>Whether fillers in this config, can be fitted.                </p> <code>logger</code> <p>Logger for displaying information.   </p> <code>all_ts</code> <code>Optional[ndarray]</code> <p>If no specific sets (train/val/test) are provided, all time series IDs are used. When any set is defined, only the time series IDs in defined sets are used.</p> <code>train_ts_row_ranges</code> <code>Optional[ndarray]</code> <p>Initialized when <code>train_ts</code> is set. Contains time series IDs in train set with their respective time ID ranges.</p> <code>val_ts_row_ranges</code> <code>Optional[ndarray]</code> <p>Initialized when <code>val_ts</code> is set. Contains time series IDs in validation set with their respective time ID ranges.</p> <code>test_ts_row_ranges</code> <code>Optional[ndarray]</code> <p>Initialized when <code>test_ts</code> is set. Contains time series IDs in test set with their respective time ID ranges.</p> <code>all_ts_row_ranges</code> <code>Optional[ndarray]</code> <p>Initialized when <code>all_ts</code> is set. Contains time series IDs in all set with their respective time ID ranges.</p> <code>display_time_period</code> <code>Optional[range]</code> <p>Used to display the configured value of <code>time_period</code>.</p> <code>aggregation</code> <code>Optional[AgreggationType]</code> <p>The aggregation period used for the data.</p> <code>source_type</code> <code>Optional[SourceType]</code> <p>The source type of the data.</p> <code>database_name</code> <code>Optional[str]</code> <p>Specifies which database this config applies to.</p> <code>features_to_take_without_ids</code> <code>Optional[ndarray]</code> <p>Features to be returned, excluding time or time series IDs.</p> <code>indices_of_features_to_take_no_ids</code> <code>Optional[ndarray]</code> <p>Indices of non-ID features in <code>features_to_take</code>.</p> <code>ts_id_name</code> <code>Optional[str]</code> <p>Name of the time series ID, dependent on <code>source_type</code>.</p> <code>used_singular_train_time_series</code> <code>Optional[int]</code> <p>Currently used singular train set time series for dataloader.</p> <code>used_singular_val_time_series</code> <code>Optional[int]</code> <p>Currently used singular validation set time series for dataloader.</p> <code>used_singular_test_time_series</code> <code>Optional[int]</code> <p>Currently used singular test set time series for dataloader.</p> <code>used_singular_all_time_series</code> <code>Optional[int]</code> <p>Currently used singular all set time series for dataloader.     </p> <code>train_preprocess_order</code> <code>list[PreprocessNote]</code> <p>All preprocesses used for train set. </p> <code>val_preprocess_order</code> <code>list[PreprocessNote]</code> <p>All preprocesses used for val set. </p> <code>test_preprocess_order</code> <code>list[PreprocessNote]</code> <p>All preprocesses used for test set. </p> <code>all_preprocess_order</code> <code>list[PreprocessNote]</code> <p>All preprocesses used for all set.                  </p> <code>is_initialized</code> <code>bool</code> <p>Flag indicating if the configuration has already been initialized. If true, config initialization will be skipped.  </p> <code>version</code> <code>str</code> <p>Version of cesnet-tszoo this config was made in.</p> <code>export_update_needed</code> <code>bool</code> <p>Whether config was updated to newer version and should be exported.      </p> <code>time_period</code> <code>ndarray</code> <p>Defines the time period for returning data from <code>train/val/test/all</code>. Can be a range of time IDs, a tuple of datetime objects or a float. Float value is equivalent to percentage of available times from start.</p> <code>train_ts</code> <code>Optional[ndarray]</code> <p>Defines which time series IDs are used in the training set. Can be a list of IDs, or an integer/float to specify a random selection. An <code>int</code> specifies the number of random time series, and a <code>float</code> specifies the proportion of available time series.        <code>int</code> and <code>float</code> must be greater than 0, and a float should be smaller or equal to 1.0. Using <code>int</code> or <code>float</code> guarantees that no time series from other sets will be used.</p> <code>val_ts</code> <code>Optional[ndarray]</code> <p>Defines which time series IDs are used in the validation set. Same as <code>train_ts</code> but for the validation set.</p> <code>test_ts</code> <code>Optional[ndarray]</code> <p>Defines which time series IDs are used in the test set. Same as <code>train_ts</code> but for the test set.         </p> <code>features_to_take</code> <code>list[str]</code> <p>Defines which features are used.</p> <code>default_values</code> <code>ndarray</code> <p>Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature.</p> <code>train_batch_size</code> <code>int</code> <p>Batch size for the train dataloader. Affects number of returned time series in one batch.</p> <code>val_batch_size</code> <code>int</code> <p>Batch size for the validation dataloader. Affects number of returned time series in one batch.</p> <code>test_batch_size</code> <code>int</code> <p>Batch size for the test dataloader. Affects number of returned time series in one batch.</p> <code>all_batch_size</code> <code>int</code> <p>Batch size for the all dataloader. Affects number of returned time series in one batch.</p> <code>preprocess_order</code> <code>list[PreprocessType]</code> <p>Defines in which order preprocesses are used. Also can add to order a type of <code>AllSeriesCustomHandler</code> or <code>NoFitCustomHandler</code>.            </p> <code>partial_fit_initialized_transformer</code> <code>list[PreprocessType]</code> <p>If <code>True</code>, partial fitting on train set is performed when using initiliazed transformer.</p> <code>include_time</code> <code>bool</code> <p>If <code>True</code>, time data is included in the returned values.</p> <code>include_ts_id</code> <code>bool</code> <p>If <code>True</code>, time series IDs are included in the returned values.</p> <code>time_format</code> <code>TimeFormat</code> <p>Format for the returned time data. When using TimeFormat.DATETIME, time will be returned as separate list along rest of the values.</p> <code>train_workers</code> <code>int</code> <p>Number of workers for loading training data. <code>0</code> means that the data will be loaded in the main process.</p> <code>val_workers</code> <code>int</code> <p>Number of workers for loading validation data. <code>0</code> means that the data will be loaded in the main process.</p> <code>test_workers</code> <code>int</code> <p>Number of workers for loading test data. <code>0</code> means that the data will be loaded in the main process.</p> <code>all_workers</code> <code>int</code> <p>Number of workers for loading all data. <code>0</code> means that the data will be loaded in the main process.</p> <code>init_workers</code> <code>int</code> <p>Number of workers for initial dataset processing during configuration. <code>0</code> means that the data will be loaded in the main process.</p> <code>nan_threshold</code> <code>float</code> <p>Maximum allowable percentage of missing data. Time series exceeding this threshold are excluded. Time series over the threshold will not be used. Used for <code>train/val/test/all</code> separately.</p> <code>train_dataloader_order</code> <code>DataloaderOrder</code> <p>Defines the order of data returned by the training dataloader.</p> <code>random_state</code> <code>Optional[int]</code> <p>Fixes randomness for reproducibility during configuration and dataset initialization.</p>"},{"location":"reference_time_based_cesnet_dataset/","title":"Time-based dataset class","text":""},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset","title":"cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset  <code>dataclass</code>","text":"<p>               Bases: <code>CesnetDataset</code></p> <p>This class is used for time-based returning of data. Can be created by using <code>get_dataset</code> with parameter <code>dataset_type</code> = <code>DatasetType.TIME_BASED</code>.</p> <p>Time-based means batch size affects number of returned times in one batch and all sets have the same time series. Which time series are returned does not change. Additionally it supports sliding window.</p> <p>The dataset provides multiple ways to access the data:</p> <ul> <li>Iterable PyTorch DataLoader: For batch processing.</li> <li>Pandas DataFrame: For loading the entire training, validation, test or all set at once.</li> <li>Numpy array: For loading the entire training, validation, test or all set at once. </li> <li>See loading data for more details.</li> </ul> <p>The dataset is stored in a PyTables database. The internal <code>TimeBasedDataset</code>, <code>SplittedDataset</code>, <code>TimeBasedInitializerDataset</code> classes (used only when calling <code>set_dataset_config_and_initialize</code>) act as wrappers that implement the PyTorch <code>Dataset</code>  interface. These wrappers are compatible with PyTorch\u2019s <code>DataLoader</code>, providing efficient parallel data loading. </p> <p>The dataset configuration is done through the <code>TimeBasedConfig</code> class.       </p> <p>Intended usage:</p> <ol> <li>Create an instance of the dataset with the desired data root by calling <code>get_dataset</code>. This will download the dataset if it has not been previously downloaded and return instance of dataset.</li> <li>Create an instance of <code>TimeBasedConfig</code> and set it using <code>set_dataset_config_and_initialize</code>.     This initializes the dataset, including data splitting (train/validation/test), fitting transformers (if needed), selecting features, and more. This is cached for later use.</li> <li>Use <code>get_train_dataloader</code>/<code>get_train_df</code>/<code>get_train_numpy</code> to get training data for chosen model.</li> <li>Validate the model and perform the hyperparameter optimalization on <code>get_val_dataloader</code>/<code>get_val_df</code>/<code>get_val_numpy</code>.</li> <li>Evaluate the model on <code>get_test_dataloader</code>/<code>get_test_df</code>/<code>get_test_numpy</code>.  </li> </ol> <p>Alternatively you can use <code>load_benchmark</code></p> <ol> <li>Call <code>load_benchmark</code> with the desired benchmark. You can use your own saved benchmark or you can use already built-in one. This will download the dataset and annotations (if available) if they have not been previously downloaded.</li> <li>Retrieve the initialized dataset using <code>get_initialized_dataset</code>. This will provide a dataset that is ready to use.</li> <li>Use <code>get_train_dataloader</code>/<code>get_train_df</code>/<code>get_train_numpy</code> to get training data for chosen model.</li> <li>Validate the model and perform the hyperparameter optimalization on <code>get_val_dataloader</code>/<code>get_val_df</code>/<code>get_val_numpy</code>.</li> <li>Evaluate the model on <code>get_test_dataloader</code>/<code>get_test_df</code>/<code>get_test_numpy</code>.</li> </ol> Source code in <code>cesnet_tszoo\\datasets\\time_based_cesnet_dataset.py</code> <pre><code>@dataclass\nclass TimeBasedCesnetDataset(CesnetDataset):\n    \"\"\"This class is used for time-based returning of data. Can be created by using [`get_dataset`](reference_cesnet_database.md#cesnet_tszoo.datasets.databases.cesnet_database.CesnetDatabase.get_dataset) with parameter `dataset_type` = `DatasetType.TIME_BASED`.\n\n    Time-based means batch size affects number of returned times in one batch and all sets have the same time series. Which time series are returned does not change. Additionally it supports sliding window.\n\n    The dataset provides multiple ways to access the data:\n\n    - **Iterable PyTorch DataLoader**: For batch processing.\n    - **Pandas DataFrame**: For loading the entire training, validation, test or all set at once.\n    - **Numpy array**: For loading the entire training, validation, test or all set at once. \n    - See [loading data][loading-data] for more details.\n\n    The dataset is stored in a [PyTables](https://www.pytables.org/) database. The internal `TimeBasedDataset`, `SplittedDataset`, `TimeBasedInitializerDataset` classes (used only when calling [`set_dataset_config_and_initialize`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.set_dataset_config_and_initialize)) act as wrappers that implement the PyTorch [`Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) \n    interface. These wrappers are compatible with PyTorch\u2019s [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader), providing efficient parallel data loading. \n\n    The dataset configuration is done through the [`TimeBasedConfig`](reference_time_based_config.md#references.TimeBasedConfig) class.       \n\n    **Intended usage:**\n\n    1. Create an instance of the dataset with the desired data root by calling [`get_dataset`](reference_cesnet_database.md#cesnet_tszoo.datasets.databases.cesnet_database.CesnetDatabase.get_dataset). This will download the dataset if it has not been previously downloaded and return instance of dataset.\n    2. Create an instance of [`TimeBasedConfig`](reference_time_based_config.md#references.TimeBasedConfig) and set it using [`set_dataset_config_and_initialize`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.set_dataset_config_and_initialize). \n       This initializes the dataset, including data splitting (train/validation/test), fitting transformers (if needed), selecting features, and more. This is cached for later use.\n    3. Use [`get_train_dataloader`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset)/[`get_train_df`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_train_df)/[`get_train_numpy`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_train_numpy) to get training data for chosen model.\n    4. Validate the model and perform the hyperparameter optimalization on [`get_val_dataloader`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_val_dataloader)/[`get_val_df`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_val_df)/[`get_val_numpy`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_val_numpy).\n    5. Evaluate the model on [`get_test_dataloader`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_dataloader)/[`get_test_df`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_df)/[`get_test_numpy`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_numpy).  \n\n    Alternatively you can use [`load_benchmark`][cesnet_tszoo.benchmarks.load_benchmark]\n\n    1. Call [`load_benchmark`][cesnet_tszoo.benchmarks.load_benchmark] with the desired benchmark. You can use your own saved benchmark or you can use already built-in one. This will download the dataset and annotations (if available) if they have not been previously downloaded.\n    2. Retrieve the initialized dataset using [`get_initialized_dataset`](reference_benchmarks.md#cesnet_tszoo.benchmarks.Benchmark.get_initialized_dataset). This will provide a dataset that is ready to use.\n    3. Use [`get_train_dataloader`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset)/[`get_train_df`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_train_df)/[`get_train_numpy`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_train_numpy) to get training data for chosen model.\n    4. Validate the model and perform the hyperparameter optimalization on [`get_val_dataloader`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_val_dataloader)/[`get_val_df`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_val_df)/[`get_val_numpy`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_val_numpy).\n    5. Evaluate the model on [`get_test_dataloader`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_dataloader)/[`get_test_df`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_df)/[`get_test_numpy`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_numpy).  \n    \"\"\"\n\n    dataset_config: Optional[TimeBasedConfig] = field(default=None, init=False)\n    \"\"\"Configuration of the dataset.\"\"\"\n\n    train_dataset: Optional[TimeBasedSplittedDataset] = field(default=None, init=False)\n    \"\"\"Training set as a `SplittedDataset` instance wrapping multiple `TimeBasedDataset` that wrap the PyTables database.\"\"\"\n\n    val_dataset: Optional[TimeBasedSplittedDataset] = field(default=None, init=False)\n    \"\"\"Validation set as a `SplittedDataset` instance wrapping multiple `TimeBasedDataset` that wrap the PyTables database.\"\"\"\n\n    test_dataset: Optional[TimeBasedSplittedDataset] = field(default=None, init=False)\n    \"\"\"Test set as a `SplittedDataset` instance wrapping multiple `TimeBasedDataset` that wrap the PyTables database.  \"\"\"\n\n    all_dataset: Optional[TimeBasedSplittedDataset] = field(default=None, init=False)\n    \"\"\"All set as a `SplittedDataset` instance wrapping multiple `TimeBasedDataset` that wrap the PyTables database.   \"\"\"\n\n    train_dataloader: Optional[TimeBasedDataloader] = field(default=None, init=False)\n    \"\"\"Iterable PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for training set.\"\"\"\n    val_dataloader: Optional[TimeBasedDataloader] = field(default=None, init=False)\n    \"\"\"Iterable PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for validation set.\"\"\"\n    test_dataloader: Optional[TimeBasedDataloader] = field(default=None, init=False)\n    \"\"\"Iterable PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for test set.   \"\"\"\n    all_dataloader: Optional[TimeBasedDataloader] = field(default=None, init=False)\n    \"\"\"Iterable PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for all set.   \"\"\"\n\n    dataloader_factory: TimeBasedDataloaderFactory = field(default=TimeBasedDataloaderFactory(), init=False)\n    \"\"\"Factory used to create TimeBasedDataloader.  \"\"\"\n\n    dataset_type: DatasetType = field(default=DatasetType.TIME_BASED, init=False)\n\n    _export_config_copy: Optional[TimeBasedConfig] = field(default=None, init=False)\n\n    def __post_init__(self):\n        super().__post_init__()\n\n        self.logger.info(\"Dataset is time-based. Use cesnet_tszoo.configs.TimeBasedConfig\")\n\n    def set_dataset_config_and_initialize(self, dataset_config: TimeBasedConfig, display_config_details: Optional[Literal[\"text\", \"diagram\"]] = \"text\", workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n        \"\"\"\n        Initialize training set, validation set, test set etc.. This method must be called before any data can be accessed. It is required for the final initialization of [`dataset_config`][references.TimeBasedConfig].\n\n        The following configuration attributes are used during initialization:\n\n        Dataset config | Description\n        -------------- | -----------\n        `init_workers` | Specifies the number of workers to use for initialization. Applied when `workers` = \"config\".\n        `partial_fit_initialized_transformers` | Determines whether initialized transformers should be partially fitted on the training data.\n        `nan_threshold` | Filters out time series with missing values exceeding the specified threshold.\n\n        Parameters:\n            dataset_config: Desired configuration of the dataset.\n            display_config_details: Flag indicating whether and how to display the configuration values after initialization. `Default: text`  \n            workers: The number of workers to use during initialization. `Default: \"config\"`  \n        \"\"\"\n\n        assert dataset_config is not None, \"Used dataset_config cannot be None.\"\n        assert isinstance(dataset_config, TimeBasedConfig), f\"This config is used for dataset of type '{dataset_config.dataset_type}'. Meanwhile this dataset is of type '{self.metadata.dataset_type}'.\"\n\n        super(TimeBasedCesnetDataset, self).set_dataset_config_and_initialize(dataset_config, display_config_details, workers)\n\n    def update_dataset_config_and_initialize(self,\n                                             default_values: list[Number] | npt.NDArray[np.number] | dict[str, Number] | Number | Literal[\"default\"] | None | Literal[\"config\"] = \"config\",\n                                             sliding_window_size: int | None | Literal[\"config\"] = \"config\",\n                                             sliding_window_prediction_size: int | None | Literal[\"config\"] = \"config\",\n                                             sliding_window_step: int | Literal[\"config\"] = \"config\",\n                                             set_shared_size: float | int | Literal[\"config\"] = \"config\",\n                                             train_batch_size: int | Literal[\"config\"] = \"config\",\n                                             val_batch_size: int | Literal[\"config\"] = \"config\",\n                                             test_batch_size: int | Literal[\"config\"] = \"config\",\n                                             all_batch_size: int | Literal[\"config\"] = \"config\",\n                                             preprocess_order: list[str, type] | Literal[\"config\"] = \"config\",\n                                             fill_missing_with: type | FillerType | Literal[\"mean_filler\", \"forward_filler\", \"linear_interpolation_filler\"] | None | Literal[\"config\"] = \"config\",\n                                             transform_with: type | list[Transformer] | np.ndarray[Transformer] | TransformerType | Transformer | Literal[\"min_max_scaler\", \"standard_scaler\", \"max_abs_scaler\", \"log_transformer\", \"robust_scaler\", \"power_transformer\", \"quantile_transformer\", \"l2_normalizer\"] | None | Literal[\"config\"] = \"config\",\n                                             handle_anomalies_with: type | AnomalyHandlerType | Literal[\"z-score\", \"interquartile_range\"] | None | Literal[\"config\"] = \"config\",\n                                             create_transformer_per_time_series: bool | Literal[\"config\"] = \"config\",\n                                             partial_fit_initialized_transformers: bool | Literal[\"config\"] = \"config\",\n                                             train_workers: int | Literal[\"config\"] = \"config\",\n                                             val_workers: int | Literal[\"config\"] = \"config\",\n                                             test_workers: int | Literal[\"config\"] = \"config\",\n                                             all_workers: int | Literal[\"config\"] = \"config\",\n                                             init_workers: int | Literal[\"config\"] = \"config\",\n                                             workers: int | Literal[\"config\"] = \"config\",\n                                             display_config_details: Optional[Literal[\"text\", \"diagram\"]] = None):\n        \"\"\"Used for updating selected configurations set in config.\n        Set parameter to `config` to keep it as it is config.\n        If exception is thrown during set, no changes are made.\n\n        Can affect following configuration:\n\n        Dataset config | Description\n        -------------- | -----------\n        `default_values` | Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature.\n        `sliding_window_size` | Number of times in one window. Impacts dataloader behavior. Refer to relevant config for details.\n        `sliding_window_prediction_size` | Number of times to predict from sliding_window_size. Refer to relevant config for details.\n        `sliding_window_step` | Number of times to move by after each window. Refer to relevant config for details.\n        `set_shared_size` | How much times should time periods share. Order of sharing is training set &lt; validation set &lt; test set. Refer to relevant config for details.\n        `train_batch_size` | Number of samples per batch for train set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n        `val_batch_size` | Number of samples per batch for val set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n        `test_batch_size` | Number of samples per batch for test set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n        `all_batch_size` | Number of samples per batch for all set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n        `preprocess_order` | Used order of when preprocesses are applied. Can be also used to add/remove custom handlers.\n        `fill_missing_with` | Defines how to fill missing values in the dataset.\n        `transform_with` | Defines the transformer to transform the dataset.\n        `handle_anomalies_with` | Defines the anomaly handler to handle anomalies in the dataset.\n        `create_transformer_per_time_series` | If `True`, a separate transformer is created for each time series. Not used when using already initialized transformers.\n        `partial_fit_initialized_transformers` | If `True`, partial fitting on train set is performed when using initialized transformers.\n        `train_workers` | Number of workers for loading training data.\n        `val_workers` | Number of workers for loading validation data.\n        `test_workers` | Number of workers for loading test data.\n        `all_workers` | Number of workers for loading all data.\n        `init_workers` | Number of workers for dataset configuration.\n\n\n        Parameters:\n            default_values: Default values for missing data, applied before fillers. `Defaults: config`.  \n            sliding_window_size: Number of times in one window. `Defaults: config`.\n            sliding_window_prediction_size: Number of times to predict from sliding_window_size. `Defaults: config`.\n            sliding_window_step: Number of times to move by after each window. `Defaults: config`.\n            set_shared_size: How much times should time periods share. `Defaults: config`.            \n            train_batch_size: Number of samples per batch for train set. `Defaults: config`.\n            val_batch_size: Number of samples per batch for val set. `Defaults: config`.\n            test_batch_size: Number of samples per batch for test set. `Defaults: config`.\n            all_batch_size: Number of samples per batch for all set. `Defaults: config`.      \n            preprocess_order: Used order of when preprocesses are applied. Can be also used to add/remove custom handlers. `Defaults: config`.               \n            fill_missing_with: Defines how to fill missing values in the dataset. `Defaults: config`. \n            transform_with: Defines the transformer to transform the dataset. `Defaults: config`.  \n            handle_anomalies_with: Defines the anomaly handler to handle anomalies in the dataset. `Defaults: config`.  \n            create_transformer_per_time_series: If `True`, a separate transformer is created for each time series. Not used when using already initialized transformers. `Defaults: config`.  \n            partial_fit_initialized_transformers: If `True`, partial fitting on train set is performed when using initiliazed transformers. `Defaults: config`.    \n            train_workers: Number of workers for loading training data. `Defaults: config`.\n            val_workers: Number of workers for loading validation data. `Defaults: config`.\n            test_workers: Number of workers for loading test data. `Defaults: config`.\n            all_workers: Number of workers for loading all data.  `Defaults: config`.\n            init_workers: Number of workers for dataset configuration. `Defaults: config`.                          \n            workers: How many workers to use when updating configuration. `Defaults: config`.  \n            display_config_details: Whether config details should be displayed after configuration. `Defaults: False`. \n        \"\"\"\n\n        config_editor = TimeBasedConfigEditor(self._export_config_copy,\n                                              default_values,\n                                              train_batch_size,\n                                              val_batch_size,\n                                              test_batch_size,\n                                              all_batch_size,\n                                              preprocess_order,\n                                              fill_missing_with,\n                                              transform_with,\n                                              handle_anomalies_with,\n                                              create_transformer_per_time_series,\n                                              partial_fit_initialized_transformers,\n                                              train_workers,\n                                              val_workers,\n                                              test_workers,\n                                              all_workers,\n                                              init_workers,\n                                              sliding_window_size,\n                                              sliding_window_prediction_size,\n                                              sliding_window_step,\n                                              set_shared_size\n                                              )\n\n        self._update_dataset_config_and_initialize(config_editor, workers, display_config_details)\n\n    def get_data_about_set(self, about: SplitType | Literal[\"train\", \"val\", \"test\", \"all\"]) -&gt; dict:\n        \"\"\"\n        Retrieve data related to the specified set.\n\n        Parameters:\n            about: Specifies the set to retrieve data about.\n\n        Returned dictionary contains:\n\n        - **ts_ids:** Ids of time series in `about` set.\n        - **TimeFormat.ID_TIME:** Times in `about` set, where time format is `TimeFormat.ID_TIME`.\n        - **TimeFormat.DATETIME:** Times in `about` set, where time format is `TimeFormat.DATETIME`.\n        - **TimeFormat.UNIX_TIME:** Times in `about` set, where time format is `TimeFormat.UNIX_TIME`.\n        - **TimeFormat.SHIFTED_UNIX_TIME:** Times in `about` set, where time format is `TimeFormat.SHIFTED_UNIX_TIME`.\n\n        Returns:\n            Returns dictionary with details about set.\n        \"\"\"\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before getting data about set.\")\n\n        about = SplitType(about)\n\n        time_period = None\n\n        result = {}\n\n        if about == SplitType.TRAIN:\n            if not self.dataset_config.has_train():\n                raise ValueError(\"Train split is not used.\")\n            time_period = self.dataset_config.train_time_period\n        elif about == SplitType.VAL:\n            if not self.dataset_config.has_val():\n                raise ValueError(\"Val split is not used.\")\n            time_period = self.dataset_config.val_time_period\n        elif about == SplitType.TEST:\n            if not self.dataset_config.has_test():\n                raise ValueError(\"Test split is not used.\")\n            time_period = self.dataset_config.test_time_period\n        elif about == SplitType.ALL:\n            if not self.dataset_config.has_all():\n                raise ValueError(\"All split is not used.\")\n\n            time_period = self.dataset_config.all_time_period\n        else:\n            raise ValueError(\"Invalid split type!\")\n\n        datetime_temp = np.array([datetime.fromtimestamp(time, timezone.utc) for time in self.metadata.time_indices[TIME_COLUMN_NAME][time_period[ID_TIME_COLUMN_NAME]]])\n\n        result[\"ts_ids\"] = self.dataset_config.ts_ids.copy()\n        result[TimeFormat.ID_TIME] = time_period[ID_TIME_COLUMN_NAME].copy()\n        result[TimeFormat.DATETIME] = datetime_temp.copy()\n        result[TimeFormat.UNIX_TIME] = self.metadata.time_indices[TIME_COLUMN_NAME][time_period[ID_TIME_COLUMN_NAME]].copy()\n        result[TimeFormat.SHIFTED_UNIX_TIME] = self.metadata.time_indices[TIME_COLUMN_NAME][time_period[ID_TIME_COLUMN_NAME]] - self.metadata.time_indices[TIME_COLUMN_NAME][0]\n\n        return result\n\n    def set_sliding_window(self, sliding_window_size: int | None | Literal[\"config\"] = \"config\", sliding_window_prediction_size: int | None | Literal[\"config\"] = \"config\",\n                           sliding_window_step: int | None | Literal[\"config\"] = \"config\", set_shared_size: float | int | Literal[\"config\"] = \"config\", workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n        \"\"\"Used for updating sliding window related values set in config.\n        Set parameter to `config` to keep it as it is config.\n        If exception is thrown during set, no changes are made.\n\n        Affects following configuration:\n\n        Dataset config | Description\n        -------------- | -----------\n        `sliding_window_size` | Number of times in one window. Impacts dataloader behavior. Refer to relevant config for details.\n        `sliding_window_prediction_size` | Number of times to predict from sliding_window_size. Refer to relevant config for details.\n        `sliding_window_step` | Number of times to move by after each window. Refer to relevant config for details.\n        `set_shared_size` | How much times should time periods share. Order of sharing is training set &lt; validation set &lt; test set. Refer to relevant config for details.\n\n        Parameters:\n            sliding_window_size: Number of times in one window. `Defaults: config`.\n            sliding_window_prediction_size: Number of times to predict from sliding_window_size. `Defaults: config`.\n            sliding_window_step: Number of times to move by after each window. `Defaults: config`.\n            set_shared_size: How much times should time periods share. `Defaults: config`.\n            workers: How many workers to use when setting new sliding window values. `Defaults: config`.  \n        \"\"\"\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating sliding window values.\")\n\n        self.update_dataset_config_and_initialize(sliding_window_size=sliding_window_size, sliding_window_prediction_size=sliding_window_prediction_size, sliding_window_step=sliding_window_step, set_shared_size=set_shared_size, workers=workers)\n        self.logger.info(\"Sliding window values has been changed successfuly.\")\n\n    def _initialize_datasets(self) -&gt; None:\n        \"\"\"Called in [`set_dataset_config_and_initialize`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.set_dataset_config_and_initialize), this method initializes the set datasets (train, validation, test and all). \"\"\"\n\n        if self.dataset_config.has_train():\n            load_config = TimeLoadConfig(self.dataset_config, SplitType.TRAIN)\n            self.train_dataset = TimeBasedSplittedDataset(self.metadata.dataset_path, self.metadata.data_table_path, load_config, self.dataset_config.train_workers)\n\n            self.logger.debug(\"train_dataset initiliazed.\")\n\n        if self.dataset_config.has_val():\n            load_config = TimeLoadConfig(self.dataset_config, SplitType.VAL)\n            self.val_dataset = TimeBasedSplittedDataset(self.metadata.dataset_path, self.metadata.data_table_path, load_config, self.dataset_config.val_workers)\n\n            self.logger.debug(\"val_dataset initiliazed.\")\n\n        if self.dataset_config.has_test():\n            load_config = TimeLoadConfig(self.dataset_config, SplitType.TEST)\n            self.test_dataset = TimeBasedSplittedDataset(self.metadata.dataset_path, self.metadata.data_table_path, load_config, self.dataset_config.test_workers)\n\n            self.logger.debug(\"test_dataset initiliazed.\")\n\n        if self.dataset_config.has_all():\n            load_config = TimeLoadConfig(self.dataset_config, SplitType.ALL)\n            self.all_dataset = TimeBasedSplittedDataset(self.metadata.dataset_path, self.metadata.data_table_path, load_config, self.dataset_config.all_workers)\n\n            self.logger.debug(\"all_dataset initiliazed.\")\n\n    def _initialize_transformers_and_details(self, workers: int) -&gt; None:\n        \"\"\"\n        Called in [`set_dataset_config_and_initialize`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.set_dataset_config_and_initialize). \n\n        Goes through data to validate time series against `nan_threshold`, fit/partial fit `transformers`, `anomaly handlers` and prepare `fillers`.\n        \"\"\"\n\n        self.logger.info(\"Updating config on train/val/test/all and selected time series.\")\n\n        is_first_cycle = True\n\n        train_groups = self.dataset_config._get_train_preprocess_init_order_groups()\n        val_groups = self.dataset_config._get_val_preprocess_init_order_groups()\n        test_groups = self.dataset_config._get_test_preprocess_init_order_groups()\n\n        grouped = list(zip(train_groups, val_groups, test_groups))\n        ts_ids_ignore = np.zeros_like(self.dataset_config.ts_row_ranges, dtype=np.bool)\n        ts_ids_to_take = []\n\n        for i, groups in enumerate(grouped):\n            ts_ids_to_take = []\n            train_group, val_group, test_group = groups\n\n            self.logger.info(\"Starting fitting cycle %s/%s.\", i + 1, len(grouped))\n\n            init_config = TimeDatasetInitConfig(self.dataset_config, ts_ids_ignore, train_group, val_group, test_group)\n            init_dataset = TimeBasedInitializerDataset(self.metadata.dataset_path, self.metadata.data_table_path, init_config)\n\n            sampler = SequentialSampler(init_dataset)\n            dataloader = DataLoader(init_dataset, num_workers=workers, collate_fn=self._collate_fn, worker_init_fn=TimeBasedInitializerDataset.worker_init_fn, persistent_workers=False, sampler=sampler)\n\n            if workers == 0:\n                init_dataset.pytables_worker_init()\n\n            for ts_id, data in enumerate(tqdm(dataloader, total=len(self.dataset_config.ts_row_ranges))):\n\n                if ts_ids_ignore[ts_id]:\n                    continue\n\n                train_return: InitDatasetReturn\n                val_return: InitDatasetReturn\n                test_return: InitDatasetReturn\n                all_return: InitDatasetReturn\n                train_return, val_return, test_return, all_return = data[0]\n\n                if train_return.is_under_nan_threshold and val_return.is_under_nan_threshold and test_return.is_under_nan_threshold and all_return.is_under_nan_threshold:\n                    ts_ids_to_take.append(ts_id)\n\n                    if self.dataset_config.has_train():\n                        self.__update_based_on_train_init_return(train_return, train_group, ts_id)\n\n                    if self.dataset_config.has_val() or self.dataset_config.has_test():\n                        self.__update_based_on_non_fit_returns(val_return, test_return, val_group, test_group, ts_id)\n\n            if workers == 0:\n                init_dataset.cleanup()\n\n            if is_first_cycle:\n\n                if len(ts_ids_to_take) == 0:\n                    raise ValueError(\"No valid time series left in ts_ids after applying nan_threshold.\")\n\n                ts_ids_ignore = np.ones_like(self.dataset_config.ts_row_ranges, dtype=np.bool)\n                ts_ids_ignore[ts_ids_to_take] = False\n                self.logger.debug(\"invalid ts_ids flagged: %s time series left.\", len(ts_ids_to_take))\n\n                is_first_cycle = False\n\n        # Update config based on filtered time series\n        self.dataset_config.ts_row_ranges = self.dataset_config.ts_row_ranges[ts_ids_to_take]\n        self.dataset_config.ts_ids = self.dataset_config.ts_ids[ts_ids_to_take]\n\n        if self.dataset_config.has_train():\n            self.dataset_config._update_preprocess_order_supported_ids(self.dataset_config.train_preprocess_order, ts_ids_to_take)\n        if self.dataset_config.has_val():\n            self.dataset_config._update_preprocess_order_supported_ids(self.dataset_config.val_preprocess_order, ts_ids_to_take)\n        if self.dataset_config.has_test():\n            self.dataset_config._update_preprocess_order_supported_ids(self.dataset_config.test_preprocess_order, ts_ids_to_take)\n        if self.dataset_config.has_all():\n            self.dataset_config._update_preprocess_order_supported_ids(self.dataset_config.all_preprocess_order, ts_ids_to_take)\n\n    def __update_based_on_train_init_return(self, train_return: InitDatasetReturn, train_group: PreprocessOrderGroup, ts_id: int):\n        fitted_inner_index = 0\n        for inner_preprocess_order in train_group.preprocess_inner_orders:\n            if inner_preprocess_order.should_be_fitted:\n                inner_preprocess_order.holder.update_instance(train_return.preprocess_fitted_instances[fitted_inner_index].instance, ts_id)\n                fitted_inner_index += 1\n\n        # updates outer preprocessors based on passed train data from InitDataset\n        for outer_preprocess_order in train_group.preprocess_outer_orders:\n            if outer_preprocess_order.should_be_fitted:\n                outer_preprocess_order.holder.fit(train_return.train_data, ts_id)\n\n            if outer_preprocess_order.can_be_applied:\n                train_return.train_data = outer_preprocess_order.holder.apply(train_return.train_data, ts_id)\n\n    def __update_based_on_non_fit_returns(self, val_return: InitDatasetReturn, test_return: InitDatasetReturn, val_group: PreprocessOrderGroup, test_group: PreprocessOrderGroup, ts_id: int):\n\n        if self.dataset_config.has_val():\n            fitted_inner_index = 0\n            for inner_preprocess_order in val_group.preprocess_inner_orders:\n                if inner_preprocess_order.should_be_fitted:\n                    inner_preprocess_order.holder.update_instance(val_return.preprocess_fitted_instances[fitted_inner_index].instance, ts_id)\n                    fitted_inner_index += 1\n\n        if self.dataset_config.has_test():\n            fitted_inner_index = 0\n            for inner_preprocess_order in test_group.preprocess_inner_orders:\n                if inner_preprocess_order.should_be_fitted:\n                    inner_preprocess_order.holder.update_instance(test_return.preprocess_fitted_instances[fitted_inner_index].instance, ts_id)\n                    fitted_inner_index += 1\n\n    def _update_export_config_copy(self) -&gt; None:\n        \"\"\"\n        Called at the end of [`set_dataset_config_and_initialize`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.set_dataset_config_and_initialize) or when changing config values. \n\n        Updates values of config used for saving config.\n        \"\"\"\n        self._export_config_copy.database_name = self.metadata.database_name\n\n        if self.dataset_config.ts_ids is not None:\n            self._export_config_copy.ts_ids = self.dataset_config.ts_ids.copy()\n            self.logger.debug(\"Updated ts_ids of _export_config_copy.\")\n        else:\n            self._export_config_copy.ts_ids = None\n            self.logger.debug(\"Updated ts_ids of _export_config_copy.\")\n\n        self._export_config_copy.sliding_window_size = self.dataset_config.sliding_window_size\n        self._export_config_copy.sliding_window_prediction_size = self.dataset_config.sliding_window_prediction_size\n        self._export_config_copy.sliding_window_step = self.dataset_config.sliding_window_step\n        self._export_config_copy.set_shared_size = self.dataset_config.set_shared_size\n\n        super(TimeBasedCesnetDataset, self)._update_export_config_copy()\n\n    def _get_singular_time_series_dataset(self, parent_dataset: TimeBasedSplittedDataset, ts_id: int) -&gt; TimeBasedSplittedDataset:\n        \"\"\"Returns dataset for single time series \"\"\"\n\n        temp = np.where(np.isin(parent_dataset.load_config.ts_row_ranges[self.metadata.ts_id_name], [ts_id]))[0]\n\n        if len(temp) == 0:\n            raise ValueError(f\"ts_id {ts_id} was not found in valid time series for this set. Available time series are: {parent_dataset.ts_row_ranges[self.metadata.ts_id_name]}\")\n\n        time_series_position = temp[0]\n\n        split_load_config = parent_dataset.load_config.create_split_copy(slice(time_series_position, time_series_position + 1))\n\n        dataset = TimeBasedSplittedDataset(self.metadata.dataset_path, self.metadata.data_table_path, split_load_config, 0)\n        self.logger.debug(\"Singular time series dataset initiliazed.\")\n\n        return dataset\n\n    def _get_data_for_plot(self, ts_id: int, feature_indices: np.ndarray[int], time_format: TimeFormat) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"Dataset type specific retrieval of data. \"\"\"\n\n        # Validate the time series ID (ts_id)\n        id_result = np.argwhere(np.isin(self.dataset_config.ts_ids, ts_id)).ravel()\n\n        if len(id_result) == 0:\n            raise ValueError(f\"Invalid ts_id '{ts_id}'. The provided ts_id is not found in the available time series IDs.\", self.dataset_config.ts_ids)\n        else:\n            id_result = id_result[0]\n            self.logger.debug(\"Valid ts_id found: %d\", id_result)\n\n        data = None\n\n        if self.dataset_config.has_train():\n            data = self.__update_data_for_plot(self.train_dataset, ts_id, feature_indices, data)\n\n        if self.dataset_config.has_val():\n            data = self.__update_data_for_plot(self.val_dataset, ts_id, feature_indices, data)\n\n        if self.dataset_config.has_test():\n            data = self.__update_data_for_plot(self.test_dataset, ts_id, feature_indices, data)\n\n        return data, self.get_data_about_set(SplitType.ALL)[time_format]\n\n    def __update_data_for_plot(self, dataset: TimeBasedSplittedDataset, ts_id: int, feature_indices: list[int], previous_data: Optional[np.ndarray]):\n        dataset = self._get_singular_time_series_dataset(dataset, ts_id)\n\n        dataloader = self.dataloader_factory.create_dataloader(dataset, self.dataset_config, 0, True, None)\n\n        temp_data = dataset_loaders.create_numpy_from_dataloader(dataloader, np.array([ts_id]), dataset.load_config.time_format, dataset.load_config.include_time, DatasetType.TIME_BASED, True)\n\n        if (dataset.load_config.time_format == TimeFormat.DATETIME and dataset.load_config.include_time):\n            temp_data = temp_data[0]\n\n        temp_data = temp_data[0][:, feature_indices]\n\n        if previous_data is None:\n            return temp_data\n\n        return np.concatenate([previous_data, temp_data])\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.dataset_config","title":"dataset_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dataset_config: Optional[TimeBasedConfig] = field(default=None, init=False)\n</code></pre> <p>Configuration of the dataset.</p>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.train_dataset","title":"train_dataset  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>train_dataset: Optional[TimeBasedSplittedDataset] = field(default=None, init=False)\n</code></pre> <p>Training set as a <code>SplittedDataset</code> instance wrapping multiple <code>TimeBasedDataset</code> that wrap the PyTables database.</p>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.val_dataset","title":"val_dataset  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>val_dataset: Optional[TimeBasedSplittedDataset] = field(default=None, init=False)\n</code></pre> <p>Validation set as a <code>SplittedDataset</code> instance wrapping multiple <code>TimeBasedDataset</code> that wrap the PyTables database.</p>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.test_dataset","title":"test_dataset  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>test_dataset: Optional[TimeBasedSplittedDataset] = field(default=None, init=False)\n</code></pre> <p>Test set as a <code>SplittedDataset</code> instance wrapping multiple <code>TimeBasedDataset</code> that wrap the PyTables database.</p>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.all_dataset","title":"all_dataset  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>all_dataset: Optional[TimeBasedSplittedDataset] = field(default=None, init=False)\n</code></pre> <p>All set as a <code>SplittedDataset</code> instance wrapping multiple <code>TimeBasedDataset</code> that wrap the PyTables database.</p>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.train_dataloader","title":"train_dataloader  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>train_dataloader: Optional[TimeBasedDataloader] = field(default=None, init=False)\n</code></pre> <p>Iterable PyTorch <code>DataLoader</code> for training set.</p>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.val_dataloader","title":"val_dataloader  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>val_dataloader: Optional[TimeBasedDataloader] = field(default=None, init=False)\n</code></pre> <p>Iterable PyTorch <code>DataLoader</code> for validation set.</p>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.test_dataloader","title":"test_dataloader  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>test_dataloader: Optional[TimeBasedDataloader] = field(default=None, init=False)\n</code></pre> <p>Iterable PyTorch <code>DataLoader</code> for test set.</p>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.all_dataloader","title":"all_dataloader  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>all_dataloader: Optional[TimeBasedDataloader] = field(default=None, init=False)\n</code></pre> <p>Iterable PyTorch <code>DataLoader</code> for all set.</p>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.dataloader_factory","title":"dataloader_factory  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dataloader_factory: TimeBasedDataloaderFactory = field(default=TimeBasedDataloaderFactory(), init=False)\n</code></pre> <p>Factory used to create TimeBasedDataloader.</p>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.dataset_type","title":"dataset_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dataset_type: DatasetType = field(default=TIME_BASED, init=False)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset._export_config_copy","title":"_export_config_copy  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>_export_config_copy: Optional[TimeBasedConfig] = field(default=None, init=False)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata: DatasetMetadata\n</code></pre> <p>Holds various metadata used in dataset for its creation, loading data and similar.</p>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.related_to","title":"related_to  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>related_to: Optional[str] = field(default=None, init=False)\n</code></pre> <p>Name of file with relevant results to used benchmark.</p>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset._collate_fn","title":"_collate_fn  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>_collate_fn: Optional[Callable] = field(default=None, init=False)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.__init__","title":"__init__","text":"<pre><code>__init__(metadata: DatasetMetadata) -&gt; None\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> Source code in <code>cesnet_tszoo\\datasets\\time_based_cesnet_dataset.py</code> <pre><code>def __post_init__(self):\n    super().__post_init__()\n\n    self.logger.info(\"Dataset is time-based. Use cesnet_tszoo.configs.TimeBasedConfig\")\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.set_dataset_config_and_initialize","title":"set_dataset_config_and_initialize","text":"<pre><code>set_dataset_config_and_initialize(dataset_config: TimeBasedConfig, display_config_details: Optional[Literal['text', 'diagram']] = 'text', workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Initialize training set, validation set, test set etc.. This method must be called before any data can be accessed. It is required for the final initialization of <code>dataset_config</code>.</p> <p>The following configuration attributes are used during initialization:</p> Dataset config Description <code>init_workers</code> Specifies the number of workers to use for initialization. Applied when <code>workers</code> = \"config\". <code>partial_fit_initialized_transformers</code> Determines whether initialized transformers should be partially fitted on the training data. <code>nan_threshold</code> Filters out time series with missing values exceeding the specified threshold. <p>Parameters:</p> Name Type Description Default <code>dataset_config</code> <code>TimeBasedConfig</code> <p>Desired configuration of the dataset.</p> required <code>display_config_details</code> <code>Optional[Literal['text', 'diagram']]</code> <p>Flag indicating whether and how to display the configuration values after initialization. <code>Default: text</code> </p> <code>'text'</code> <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use during initialization. <code>Default: \"config\"</code></p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\time_based_cesnet_dataset.py</code> <pre><code>def set_dataset_config_and_initialize(self, dataset_config: TimeBasedConfig, display_config_details: Optional[Literal[\"text\", \"diagram\"]] = \"text\", workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"\n    Initialize training set, validation set, test set etc.. This method must be called before any data can be accessed. It is required for the final initialization of [`dataset_config`][references.TimeBasedConfig].\n\n    The following configuration attributes are used during initialization:\n\n    Dataset config | Description\n    -------------- | -----------\n    `init_workers` | Specifies the number of workers to use for initialization. Applied when `workers` = \"config\".\n    `partial_fit_initialized_transformers` | Determines whether initialized transformers should be partially fitted on the training data.\n    `nan_threshold` | Filters out time series with missing values exceeding the specified threshold.\n\n    Parameters:\n        dataset_config: Desired configuration of the dataset.\n        display_config_details: Flag indicating whether and how to display the configuration values after initialization. `Default: text`  \n        workers: The number of workers to use during initialization. `Default: \"config\"`  \n    \"\"\"\n\n    assert dataset_config is not None, \"Used dataset_config cannot be None.\"\n    assert isinstance(dataset_config, TimeBasedConfig), f\"This config is used for dataset of type '{dataset_config.dataset_type}'. Meanwhile this dataset is of type '{self.metadata.dataset_type}'.\"\n\n    super(TimeBasedCesnetDataset, self).set_dataset_config_and_initialize(dataset_config, display_config_details, workers)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.update_dataset_config_and_initialize","title":"update_dataset_config_and_initialize","text":"<pre><code>update_dataset_config_and_initialize(default_values: list[Number] | NDArray[number] | dict[str, Number] | Number | Literal['default'] | None | Literal['config'] = 'config', sliding_window_size: int | None | Literal['config'] = 'config', sliding_window_prediction_size: int | None | Literal['config'] = 'config', sliding_window_step: int | Literal['config'] = 'config', set_shared_size: float | int | Literal['config'] = 'config', train_batch_size: int | Literal['config'] = 'config', val_batch_size: int | Literal['config'] = 'config', test_batch_size: int | Literal['config'] = 'config', all_batch_size: int | Literal['config'] = 'config', preprocess_order: list[str, type] | Literal['config'] = 'config', fill_missing_with: type | FillerType | Literal['mean_filler', 'forward_filler', 'linear_interpolation_filler'] | None | Literal['config'] = 'config', transform_with: type | list[Transformer] | ndarray[Transformer] | TransformerType | Transformer | Literal['min_max_scaler', 'standard_scaler', 'max_abs_scaler', 'log_transformer', 'robust_scaler', 'power_transformer', 'quantile_transformer', 'l2_normalizer'] | None | Literal['config'] = 'config', handle_anomalies_with: type | AnomalyHandlerType | Literal['z-score', 'interquartile_range'] | None | Literal['config'] = 'config', create_transformer_per_time_series: bool | Literal['config'] = 'config', partial_fit_initialized_transformers: bool | Literal['config'] = 'config', train_workers: int | Literal['config'] = 'config', val_workers: int | Literal['config'] = 'config', test_workers: int | Literal['config'] = 'config', all_workers: int | Literal['config'] = 'config', init_workers: int | Literal['config'] = 'config', workers: int | Literal['config'] = 'config', display_config_details: Optional[Literal['text', 'diagram']] = None)\n</code></pre> <p>Used for updating selected configurations set in config. Set parameter to <code>config</code> to keep it as it is config. If exception is thrown during set, no changes are made.</p> <p>Can affect following configuration:</p> Dataset config Description <code>default_values</code> Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature. <code>sliding_window_size</code> Number of times in one window. Impacts dataloader behavior. Refer to relevant config for details. <code>sliding_window_prediction_size</code> Number of times to predict from sliding_window_size. Refer to relevant config for details. <code>sliding_window_step</code> Number of times to move by after each window. Refer to relevant config for details. <code>set_shared_size</code> How much times should time periods share. Order of sharing is training set &lt; validation set &lt; test set. Refer to relevant config for details. <code>train_batch_size</code> Number of samples per batch for train set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>val_batch_size</code> Number of samples per batch for val set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>test_batch_size</code> Number of samples per batch for test set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>all_batch_size</code> Number of samples per batch for all set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>preprocess_order</code> Used order of when preprocesses are applied. Can be also used to add/remove custom handlers. <code>fill_missing_with</code> Defines how to fill missing values in the dataset. <code>transform_with</code> Defines the transformer to transform the dataset. <code>handle_anomalies_with</code> Defines the anomaly handler to handle anomalies in the dataset. <code>create_transformer_per_time_series</code> If <code>True</code>, a separate transformer is created for each time series. Not used when using already initialized transformers. <code>partial_fit_initialized_transformers</code> If <code>True</code>, partial fitting on train set is performed when using initialized transformers. <code>train_workers</code> Number of workers for loading training data. <code>val_workers</code> Number of workers for loading validation data. <code>test_workers</code> Number of workers for loading test data. <code>all_workers</code> Number of workers for loading all data. <code>init_workers</code> Number of workers for dataset configuration. <p>Parameters:</p> Name Type Description Default <code>default_values</code> <code>list[Number] | NDArray[number] | dict[str, Number] | Number | Literal['default'] | None | Literal['config']</code> <p>Default values for missing data, applied before fillers. <code>Defaults: config</code>.  </p> <code>'config'</code> <code>sliding_window_size</code> <code>int | None | Literal['config']</code> <p>Number of times in one window. <code>Defaults: config</code>.</p> <code>'config'</code> <code>sliding_window_prediction_size</code> <code>int | None | Literal['config']</code> <p>Number of times to predict from sliding_window_size. <code>Defaults: config</code>.</p> <code>'config'</code> <code>sliding_window_step</code> <code>int | Literal['config']</code> <p>Number of times to move by after each window. <code>Defaults: config</code>.</p> <code>'config'</code> <code>set_shared_size</code> <code>float | int | Literal['config']</code> <p>How much times should time periods share. <code>Defaults: config</code>.            </p> <code>'config'</code> <code>train_batch_size</code> <code>int | Literal['config']</code> <p>Number of samples per batch for train set. <code>Defaults: config</code>.</p> <code>'config'</code> <code>val_batch_size</code> <code>int | Literal['config']</code> <p>Number of samples per batch for val set. <code>Defaults: config</code>.</p> <code>'config'</code> <code>test_batch_size</code> <code>int | Literal['config']</code> <p>Number of samples per batch for test set. <code>Defaults: config</code>.</p> <code>'config'</code> <code>all_batch_size</code> <code>int | Literal['config']</code> <p>Number of samples per batch for all set. <code>Defaults: config</code>.      </p> <code>'config'</code> <code>preprocess_order</code> <code>list[str, type] | Literal['config']</code> <p>Used order of when preprocesses are applied. Can be also used to add/remove custom handlers. <code>Defaults: config</code>.               </p> <code>'config'</code> <code>fill_missing_with</code> <code>type | FillerType | Literal['mean_filler', 'forward_filler', 'linear_interpolation_filler'] | None | Literal['config']</code> <p>Defines how to fill missing values in the dataset. <code>Defaults: config</code>. </p> <code>'config'</code> <code>transform_with</code> <code>type | list[Transformer] | ndarray[Transformer] | TransformerType | Transformer | Literal['min_max_scaler', 'standard_scaler', 'max_abs_scaler', 'log_transformer', 'robust_scaler', 'power_transformer', 'quantile_transformer', 'l2_normalizer'] | None | Literal['config']</code> <p>Defines the transformer to transform the dataset. <code>Defaults: config</code>.  </p> <code>'config'</code> <code>handle_anomalies_with</code> <code>type | AnomalyHandlerType | Literal['z-score', 'interquartile_range'] | None | Literal['config']</code> <p>Defines the anomaly handler to handle anomalies in the dataset. <code>Defaults: config</code>.  </p> <code>'config'</code> <code>create_transformer_per_time_series</code> <code>bool | Literal['config']</code> <p>If <code>True</code>, a separate transformer is created for each time series. Not used when using already initialized transformers. <code>Defaults: config</code>.  </p> <code>'config'</code> <code>partial_fit_initialized_transformers</code> <code>bool | Literal['config']</code> <p>If <code>True</code>, partial fitting on train set is performed when using initiliazed transformers. <code>Defaults: config</code>.    </p> <code>'config'</code> <code>train_workers</code> <code>int | Literal['config']</code> <p>Number of workers for loading training data. <code>Defaults: config</code>.</p> <code>'config'</code> <code>val_workers</code> <code>int | Literal['config']</code> <p>Number of workers for loading validation data. <code>Defaults: config</code>.</p> <code>'config'</code> <code>test_workers</code> <code>int | Literal['config']</code> <p>Number of workers for loading test data. <code>Defaults: config</code>.</p> <code>'config'</code> <code>all_workers</code> <code>int | Literal['config']</code> <p>Number of workers for loading all data.  <code>Defaults: config</code>.</p> <code>'config'</code> <code>init_workers</code> <code>int | Literal['config']</code> <p>Number of workers for dataset configuration. <code>Defaults: config</code>.                          </p> <code>'config'</code> <code>workers</code> <code>int | Literal['config']</code> <p>How many workers to use when updating configuration. <code>Defaults: config</code>.  </p> <code>'config'</code> <code>display_config_details</code> <code>Optional[Literal['text', 'diagram']]</code> <p>Whether config details should be displayed after configuration. <code>Defaults: False</code>.</p> <code>None</code> Source code in <code>cesnet_tszoo\\datasets\\time_based_cesnet_dataset.py</code> <pre><code>def update_dataset_config_and_initialize(self,\n                                         default_values: list[Number] | npt.NDArray[np.number] | dict[str, Number] | Number | Literal[\"default\"] | None | Literal[\"config\"] = \"config\",\n                                         sliding_window_size: int | None | Literal[\"config\"] = \"config\",\n                                         sliding_window_prediction_size: int | None | Literal[\"config\"] = \"config\",\n                                         sliding_window_step: int | Literal[\"config\"] = \"config\",\n                                         set_shared_size: float | int | Literal[\"config\"] = \"config\",\n                                         train_batch_size: int | Literal[\"config\"] = \"config\",\n                                         val_batch_size: int | Literal[\"config\"] = \"config\",\n                                         test_batch_size: int | Literal[\"config\"] = \"config\",\n                                         all_batch_size: int | Literal[\"config\"] = \"config\",\n                                         preprocess_order: list[str, type] | Literal[\"config\"] = \"config\",\n                                         fill_missing_with: type | FillerType | Literal[\"mean_filler\", \"forward_filler\", \"linear_interpolation_filler\"] | None | Literal[\"config\"] = \"config\",\n                                         transform_with: type | list[Transformer] | np.ndarray[Transformer] | TransformerType | Transformer | Literal[\"min_max_scaler\", \"standard_scaler\", \"max_abs_scaler\", \"log_transformer\", \"robust_scaler\", \"power_transformer\", \"quantile_transformer\", \"l2_normalizer\"] | None | Literal[\"config\"] = \"config\",\n                                         handle_anomalies_with: type | AnomalyHandlerType | Literal[\"z-score\", \"interquartile_range\"] | None | Literal[\"config\"] = \"config\",\n                                         create_transformer_per_time_series: bool | Literal[\"config\"] = \"config\",\n                                         partial_fit_initialized_transformers: bool | Literal[\"config\"] = \"config\",\n                                         train_workers: int | Literal[\"config\"] = \"config\",\n                                         val_workers: int | Literal[\"config\"] = \"config\",\n                                         test_workers: int | Literal[\"config\"] = \"config\",\n                                         all_workers: int | Literal[\"config\"] = \"config\",\n                                         init_workers: int | Literal[\"config\"] = \"config\",\n                                         workers: int | Literal[\"config\"] = \"config\",\n                                         display_config_details: Optional[Literal[\"text\", \"diagram\"]] = None):\n    \"\"\"Used for updating selected configurations set in config.\n    Set parameter to `config` to keep it as it is config.\n    If exception is thrown during set, no changes are made.\n\n    Can affect following configuration:\n\n    Dataset config | Description\n    -------------- | -----------\n    `default_values` | Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature.\n    `sliding_window_size` | Number of times in one window. Impacts dataloader behavior. Refer to relevant config for details.\n    `sliding_window_prediction_size` | Number of times to predict from sliding_window_size. Refer to relevant config for details.\n    `sliding_window_step` | Number of times to move by after each window. Refer to relevant config for details.\n    `set_shared_size` | How much times should time periods share. Order of sharing is training set &lt; validation set &lt; test set. Refer to relevant config for details.\n    `train_batch_size` | Number of samples per batch for train set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `val_batch_size` | Number of samples per batch for val set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `test_batch_size` | Number of samples per batch for test set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `all_batch_size` | Number of samples per batch for all set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `preprocess_order` | Used order of when preprocesses are applied. Can be also used to add/remove custom handlers.\n    `fill_missing_with` | Defines how to fill missing values in the dataset.\n    `transform_with` | Defines the transformer to transform the dataset.\n    `handle_anomalies_with` | Defines the anomaly handler to handle anomalies in the dataset.\n    `create_transformer_per_time_series` | If `True`, a separate transformer is created for each time series. Not used when using already initialized transformers.\n    `partial_fit_initialized_transformers` | If `True`, partial fitting on train set is performed when using initialized transformers.\n    `train_workers` | Number of workers for loading training data.\n    `val_workers` | Number of workers for loading validation data.\n    `test_workers` | Number of workers for loading test data.\n    `all_workers` | Number of workers for loading all data.\n    `init_workers` | Number of workers for dataset configuration.\n\n\n    Parameters:\n        default_values: Default values for missing data, applied before fillers. `Defaults: config`.  \n        sliding_window_size: Number of times in one window. `Defaults: config`.\n        sliding_window_prediction_size: Number of times to predict from sliding_window_size. `Defaults: config`.\n        sliding_window_step: Number of times to move by after each window. `Defaults: config`.\n        set_shared_size: How much times should time periods share. `Defaults: config`.            \n        train_batch_size: Number of samples per batch for train set. `Defaults: config`.\n        val_batch_size: Number of samples per batch for val set. `Defaults: config`.\n        test_batch_size: Number of samples per batch for test set. `Defaults: config`.\n        all_batch_size: Number of samples per batch for all set. `Defaults: config`.      \n        preprocess_order: Used order of when preprocesses are applied. Can be also used to add/remove custom handlers. `Defaults: config`.               \n        fill_missing_with: Defines how to fill missing values in the dataset. `Defaults: config`. \n        transform_with: Defines the transformer to transform the dataset. `Defaults: config`.  \n        handle_anomalies_with: Defines the anomaly handler to handle anomalies in the dataset. `Defaults: config`.  \n        create_transformer_per_time_series: If `True`, a separate transformer is created for each time series. Not used when using already initialized transformers. `Defaults: config`.  \n        partial_fit_initialized_transformers: If `True`, partial fitting on train set is performed when using initiliazed transformers. `Defaults: config`.    \n        train_workers: Number of workers for loading training data. `Defaults: config`.\n        val_workers: Number of workers for loading validation data. `Defaults: config`.\n        test_workers: Number of workers for loading test data. `Defaults: config`.\n        all_workers: Number of workers for loading all data.  `Defaults: config`.\n        init_workers: Number of workers for dataset configuration. `Defaults: config`.                          \n        workers: How many workers to use when updating configuration. `Defaults: config`.  \n        display_config_details: Whether config details should be displayed after configuration. `Defaults: False`. \n    \"\"\"\n\n    config_editor = TimeBasedConfigEditor(self._export_config_copy,\n                                          default_values,\n                                          train_batch_size,\n                                          val_batch_size,\n                                          test_batch_size,\n                                          all_batch_size,\n                                          preprocess_order,\n                                          fill_missing_with,\n                                          transform_with,\n                                          handle_anomalies_with,\n                                          create_transformer_per_time_series,\n                                          partial_fit_initialized_transformers,\n                                          train_workers,\n                                          val_workers,\n                                          test_workers,\n                                          all_workers,\n                                          init_workers,\n                                          sliding_window_size,\n                                          sliding_window_prediction_size,\n                                          sliding_window_step,\n                                          set_shared_size\n                                          )\n\n    self._update_dataset_config_and_initialize(config_editor, workers, display_config_details)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_data_about_set","title":"get_data_about_set","text":"<pre><code>get_data_about_set(about: SplitType | Literal['train', 'val', 'test', 'all']) -&gt; dict\n</code></pre> <p>Retrieve data related to the specified set.</p> <p>Parameters:</p> Name Type Description Default <code>about</code> <code>SplitType | Literal['train', 'val', 'test', 'all']</code> <p>Specifies the set to retrieve data about.</p> required <p>Returned dictionary contains:</p> <ul> <li>ts_ids: Ids of time series in <code>about</code> set.</li> <li>TimeFormat.ID_TIME: Times in <code>about</code> set, where time format is <code>TimeFormat.ID_TIME</code>.</li> <li>TimeFormat.DATETIME: Times in <code>about</code> set, where time format is <code>TimeFormat.DATETIME</code>.</li> <li>TimeFormat.UNIX_TIME: Times in <code>about</code> set, where time format is <code>TimeFormat.UNIX_TIME</code>.</li> <li>TimeFormat.SHIFTED_UNIX_TIME: Times in <code>about</code> set, where time format is <code>TimeFormat.SHIFTED_UNIX_TIME</code>.</li> </ul> <p>Returns:</p> Type Description <code>dict</code> <p>Returns dictionary with details about set.</p> Source code in <code>cesnet_tszoo\\datasets\\time_based_cesnet_dataset.py</code> <pre><code>def get_data_about_set(self, about: SplitType | Literal[\"train\", \"val\", \"test\", \"all\"]) -&gt; dict:\n    \"\"\"\n    Retrieve data related to the specified set.\n\n    Parameters:\n        about: Specifies the set to retrieve data about.\n\n    Returned dictionary contains:\n\n    - **ts_ids:** Ids of time series in `about` set.\n    - **TimeFormat.ID_TIME:** Times in `about` set, where time format is `TimeFormat.ID_TIME`.\n    - **TimeFormat.DATETIME:** Times in `about` set, where time format is `TimeFormat.DATETIME`.\n    - **TimeFormat.UNIX_TIME:** Times in `about` set, where time format is `TimeFormat.UNIX_TIME`.\n    - **TimeFormat.SHIFTED_UNIX_TIME:** Times in `about` set, where time format is `TimeFormat.SHIFTED_UNIX_TIME`.\n\n    Returns:\n        Returns dictionary with details about set.\n    \"\"\"\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before getting data about set.\")\n\n    about = SplitType(about)\n\n    time_period = None\n\n    result = {}\n\n    if about == SplitType.TRAIN:\n        if not self.dataset_config.has_train():\n            raise ValueError(\"Train split is not used.\")\n        time_period = self.dataset_config.train_time_period\n    elif about == SplitType.VAL:\n        if not self.dataset_config.has_val():\n            raise ValueError(\"Val split is not used.\")\n        time_period = self.dataset_config.val_time_period\n    elif about == SplitType.TEST:\n        if not self.dataset_config.has_test():\n            raise ValueError(\"Test split is not used.\")\n        time_period = self.dataset_config.test_time_period\n    elif about == SplitType.ALL:\n        if not self.dataset_config.has_all():\n            raise ValueError(\"All split is not used.\")\n\n        time_period = self.dataset_config.all_time_period\n    else:\n        raise ValueError(\"Invalid split type!\")\n\n    datetime_temp = np.array([datetime.fromtimestamp(time, timezone.utc) for time in self.metadata.time_indices[TIME_COLUMN_NAME][time_period[ID_TIME_COLUMN_NAME]]])\n\n    result[\"ts_ids\"] = self.dataset_config.ts_ids.copy()\n    result[TimeFormat.ID_TIME] = time_period[ID_TIME_COLUMN_NAME].copy()\n    result[TimeFormat.DATETIME] = datetime_temp.copy()\n    result[TimeFormat.UNIX_TIME] = self.metadata.time_indices[TIME_COLUMN_NAME][time_period[ID_TIME_COLUMN_NAME]].copy()\n    result[TimeFormat.SHIFTED_UNIX_TIME] = self.metadata.time_indices[TIME_COLUMN_NAME][time_period[ID_TIME_COLUMN_NAME]] - self.metadata.time_indices[TIME_COLUMN_NAME][0]\n\n    return result\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.set_sliding_window","title":"set_sliding_window","text":"<pre><code>set_sliding_window(sliding_window_size: int | None | Literal['config'] = 'config', sliding_window_prediction_size: int | None | Literal['config'] = 'config', sliding_window_step: int | None | Literal['config'] = 'config', set_shared_size: float | int | Literal['config'] = 'config', workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Used for updating sliding window related values set in config. Set parameter to <code>config</code> to keep it as it is config. If exception is thrown during set, no changes are made.</p> <p>Affects following configuration:</p> Dataset config Description <code>sliding_window_size</code> Number of times in one window. Impacts dataloader behavior. Refer to relevant config for details. <code>sliding_window_prediction_size</code> Number of times to predict from sliding_window_size. Refer to relevant config for details. <code>sliding_window_step</code> Number of times to move by after each window. Refer to relevant config for details. <code>set_shared_size</code> How much times should time periods share. Order of sharing is training set &lt; validation set &lt; test set. Refer to relevant config for details. <p>Parameters:</p> Name Type Description Default <code>sliding_window_size</code> <code>int | None | Literal['config']</code> <p>Number of times in one window. <code>Defaults: config</code>.</p> <code>'config'</code> <code>sliding_window_prediction_size</code> <code>int | None | Literal['config']</code> <p>Number of times to predict from sliding_window_size. <code>Defaults: config</code>.</p> <code>'config'</code> <code>sliding_window_step</code> <code>int | None | Literal['config']</code> <p>Number of times to move by after each window. <code>Defaults: config</code>.</p> <code>'config'</code> <code>set_shared_size</code> <code>float | int | Literal['config']</code> <p>How much times should time periods share. <code>Defaults: config</code>.</p> <code>'config'</code> <code>workers</code> <code>int | Literal['config']</code> <p>How many workers to use when setting new sliding window values. <code>Defaults: config</code>.</p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\time_based_cesnet_dataset.py</code> <pre><code>def set_sliding_window(self, sliding_window_size: int | None | Literal[\"config\"] = \"config\", sliding_window_prediction_size: int | None | Literal[\"config\"] = \"config\",\n                       sliding_window_step: int | None | Literal[\"config\"] = \"config\", set_shared_size: float | int | Literal[\"config\"] = \"config\", workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"Used for updating sliding window related values set in config.\n    Set parameter to `config` to keep it as it is config.\n    If exception is thrown during set, no changes are made.\n\n    Affects following configuration:\n\n    Dataset config | Description\n    -------------- | -----------\n    `sliding_window_size` | Number of times in one window. Impacts dataloader behavior. Refer to relevant config for details.\n    `sliding_window_prediction_size` | Number of times to predict from sliding_window_size. Refer to relevant config for details.\n    `sliding_window_step` | Number of times to move by after each window. Refer to relevant config for details.\n    `set_shared_size` | How much times should time periods share. Order of sharing is training set &lt; validation set &lt; test set. Refer to relevant config for details.\n\n    Parameters:\n        sliding_window_size: Number of times in one window. `Defaults: config`.\n        sliding_window_prediction_size: Number of times to predict from sliding_window_size. `Defaults: config`.\n        sliding_window_step: Number of times to move by after each window. `Defaults: config`.\n        set_shared_size: How much times should time periods share. `Defaults: config`.\n        workers: How many workers to use when setting new sliding window values. `Defaults: config`.  \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating sliding window values.\")\n\n    self.update_dataset_config_and_initialize(sliding_window_size=sliding_window_size, sliding_window_prediction_size=sliding_window_prediction_size, sliding_window_step=sliding_window_step, set_shared_size=set_shared_size, workers=workers)\n    self.logger.info(\"Sliding window values has been changed successfuly.\")\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset._initialize_datasets","title":"_initialize_datasets","text":"<pre><code>_initialize_datasets() -&gt; None\n</code></pre> <p>Called in <code>set_dataset_config_and_initialize</code>, this method initializes the set datasets (train, validation, test and all).</p> Source code in <code>cesnet_tszoo\\datasets\\time_based_cesnet_dataset.py</code> <pre><code>def _initialize_datasets(self) -&gt; None:\n    \"\"\"Called in [`set_dataset_config_and_initialize`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.set_dataset_config_and_initialize), this method initializes the set datasets (train, validation, test and all). \"\"\"\n\n    if self.dataset_config.has_train():\n        load_config = TimeLoadConfig(self.dataset_config, SplitType.TRAIN)\n        self.train_dataset = TimeBasedSplittedDataset(self.metadata.dataset_path, self.metadata.data_table_path, load_config, self.dataset_config.train_workers)\n\n        self.logger.debug(\"train_dataset initiliazed.\")\n\n    if self.dataset_config.has_val():\n        load_config = TimeLoadConfig(self.dataset_config, SplitType.VAL)\n        self.val_dataset = TimeBasedSplittedDataset(self.metadata.dataset_path, self.metadata.data_table_path, load_config, self.dataset_config.val_workers)\n\n        self.logger.debug(\"val_dataset initiliazed.\")\n\n    if self.dataset_config.has_test():\n        load_config = TimeLoadConfig(self.dataset_config, SplitType.TEST)\n        self.test_dataset = TimeBasedSplittedDataset(self.metadata.dataset_path, self.metadata.data_table_path, load_config, self.dataset_config.test_workers)\n\n        self.logger.debug(\"test_dataset initiliazed.\")\n\n    if self.dataset_config.has_all():\n        load_config = TimeLoadConfig(self.dataset_config, SplitType.ALL)\n        self.all_dataset = TimeBasedSplittedDataset(self.metadata.dataset_path, self.metadata.data_table_path, load_config, self.dataset_config.all_workers)\n\n        self.logger.debug(\"all_dataset initiliazed.\")\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset._initialize_transformers_and_details","title":"_initialize_transformers_and_details","text":"<pre><code>_initialize_transformers_and_details(workers: int) -&gt; None\n</code></pre> <p>Called in <code>set_dataset_config_and_initialize</code>. </p> <p>Goes through data to validate time series against <code>nan_threshold</code>, fit/partial fit <code>transformers</code>, <code>anomaly handlers</code> and prepare <code>fillers</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\time_based_cesnet_dataset.py</code> <pre><code>def _initialize_transformers_and_details(self, workers: int) -&gt; None:\n    \"\"\"\n    Called in [`set_dataset_config_and_initialize`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.set_dataset_config_and_initialize). \n\n    Goes through data to validate time series against `nan_threshold`, fit/partial fit `transformers`, `anomaly handlers` and prepare `fillers`.\n    \"\"\"\n\n    self.logger.info(\"Updating config on train/val/test/all and selected time series.\")\n\n    is_first_cycle = True\n\n    train_groups = self.dataset_config._get_train_preprocess_init_order_groups()\n    val_groups = self.dataset_config._get_val_preprocess_init_order_groups()\n    test_groups = self.dataset_config._get_test_preprocess_init_order_groups()\n\n    grouped = list(zip(train_groups, val_groups, test_groups))\n    ts_ids_ignore = np.zeros_like(self.dataset_config.ts_row_ranges, dtype=np.bool)\n    ts_ids_to_take = []\n\n    for i, groups in enumerate(grouped):\n        ts_ids_to_take = []\n        train_group, val_group, test_group = groups\n\n        self.logger.info(\"Starting fitting cycle %s/%s.\", i + 1, len(grouped))\n\n        init_config = TimeDatasetInitConfig(self.dataset_config, ts_ids_ignore, train_group, val_group, test_group)\n        init_dataset = TimeBasedInitializerDataset(self.metadata.dataset_path, self.metadata.data_table_path, init_config)\n\n        sampler = SequentialSampler(init_dataset)\n        dataloader = DataLoader(init_dataset, num_workers=workers, collate_fn=self._collate_fn, worker_init_fn=TimeBasedInitializerDataset.worker_init_fn, persistent_workers=False, sampler=sampler)\n\n        if workers == 0:\n            init_dataset.pytables_worker_init()\n\n        for ts_id, data in enumerate(tqdm(dataloader, total=len(self.dataset_config.ts_row_ranges))):\n\n            if ts_ids_ignore[ts_id]:\n                continue\n\n            train_return: InitDatasetReturn\n            val_return: InitDatasetReturn\n            test_return: InitDatasetReturn\n            all_return: InitDatasetReturn\n            train_return, val_return, test_return, all_return = data[0]\n\n            if train_return.is_under_nan_threshold and val_return.is_under_nan_threshold and test_return.is_under_nan_threshold and all_return.is_under_nan_threshold:\n                ts_ids_to_take.append(ts_id)\n\n                if self.dataset_config.has_train():\n                    self.__update_based_on_train_init_return(train_return, train_group, ts_id)\n\n                if self.dataset_config.has_val() or self.dataset_config.has_test():\n                    self.__update_based_on_non_fit_returns(val_return, test_return, val_group, test_group, ts_id)\n\n        if workers == 0:\n            init_dataset.cleanup()\n\n        if is_first_cycle:\n\n            if len(ts_ids_to_take) == 0:\n                raise ValueError(\"No valid time series left in ts_ids after applying nan_threshold.\")\n\n            ts_ids_ignore = np.ones_like(self.dataset_config.ts_row_ranges, dtype=np.bool)\n            ts_ids_ignore[ts_ids_to_take] = False\n            self.logger.debug(\"invalid ts_ids flagged: %s time series left.\", len(ts_ids_to_take))\n\n            is_first_cycle = False\n\n    # Update config based on filtered time series\n    self.dataset_config.ts_row_ranges = self.dataset_config.ts_row_ranges[ts_ids_to_take]\n    self.dataset_config.ts_ids = self.dataset_config.ts_ids[ts_ids_to_take]\n\n    if self.dataset_config.has_train():\n        self.dataset_config._update_preprocess_order_supported_ids(self.dataset_config.train_preprocess_order, ts_ids_to_take)\n    if self.dataset_config.has_val():\n        self.dataset_config._update_preprocess_order_supported_ids(self.dataset_config.val_preprocess_order, ts_ids_to_take)\n    if self.dataset_config.has_test():\n        self.dataset_config._update_preprocess_order_supported_ids(self.dataset_config.test_preprocess_order, ts_ids_to_take)\n    if self.dataset_config.has_all():\n        self.dataset_config._update_preprocess_order_supported_ids(self.dataset_config.all_preprocess_order, ts_ids_to_take)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.__update_based_on_train_init_return","title":"__update_based_on_train_init_return","text":"<pre><code>__update_based_on_train_init_return(train_return: InitDatasetReturn, train_group: PreprocessOrderGroup, ts_id: int)\n</code></pre> Source code in <code>cesnet_tszoo\\datasets\\time_based_cesnet_dataset.py</code> <pre><code>def __update_based_on_train_init_return(self, train_return: InitDatasetReturn, train_group: PreprocessOrderGroup, ts_id: int):\n    fitted_inner_index = 0\n    for inner_preprocess_order in train_group.preprocess_inner_orders:\n        if inner_preprocess_order.should_be_fitted:\n            inner_preprocess_order.holder.update_instance(train_return.preprocess_fitted_instances[fitted_inner_index].instance, ts_id)\n            fitted_inner_index += 1\n\n    # updates outer preprocessors based on passed train data from InitDataset\n    for outer_preprocess_order in train_group.preprocess_outer_orders:\n        if outer_preprocess_order.should_be_fitted:\n            outer_preprocess_order.holder.fit(train_return.train_data, ts_id)\n\n        if outer_preprocess_order.can_be_applied:\n            train_return.train_data = outer_preprocess_order.holder.apply(train_return.train_data, ts_id)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.__update_based_on_non_fit_returns","title":"__update_based_on_non_fit_returns","text":"<pre><code>__update_based_on_non_fit_returns(val_return: InitDatasetReturn, test_return: InitDatasetReturn, val_group: PreprocessOrderGroup, test_group: PreprocessOrderGroup, ts_id: int)\n</code></pre> Source code in <code>cesnet_tszoo\\datasets\\time_based_cesnet_dataset.py</code> <pre><code>def __update_based_on_non_fit_returns(self, val_return: InitDatasetReturn, test_return: InitDatasetReturn, val_group: PreprocessOrderGroup, test_group: PreprocessOrderGroup, ts_id: int):\n\n    if self.dataset_config.has_val():\n        fitted_inner_index = 0\n        for inner_preprocess_order in val_group.preprocess_inner_orders:\n            if inner_preprocess_order.should_be_fitted:\n                inner_preprocess_order.holder.update_instance(val_return.preprocess_fitted_instances[fitted_inner_index].instance, ts_id)\n                fitted_inner_index += 1\n\n    if self.dataset_config.has_test():\n        fitted_inner_index = 0\n        for inner_preprocess_order in test_group.preprocess_inner_orders:\n            if inner_preprocess_order.should_be_fitted:\n                inner_preprocess_order.holder.update_instance(test_return.preprocess_fitted_instances[fitted_inner_index].instance, ts_id)\n                fitted_inner_index += 1\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset._update_export_config_copy","title":"_update_export_config_copy","text":"<pre><code>_update_export_config_copy() -&gt; None\n</code></pre> <p>Called at the end of <code>set_dataset_config_and_initialize</code> or when changing config values. </p> <p>Updates values of config used for saving config.</p> Source code in <code>cesnet_tszoo\\datasets\\time_based_cesnet_dataset.py</code> <pre><code>def _update_export_config_copy(self) -&gt; None:\n    \"\"\"\n    Called at the end of [`set_dataset_config_and_initialize`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.set_dataset_config_and_initialize) or when changing config values. \n\n    Updates values of config used for saving config.\n    \"\"\"\n    self._export_config_copy.database_name = self.metadata.database_name\n\n    if self.dataset_config.ts_ids is not None:\n        self._export_config_copy.ts_ids = self.dataset_config.ts_ids.copy()\n        self.logger.debug(\"Updated ts_ids of _export_config_copy.\")\n    else:\n        self._export_config_copy.ts_ids = None\n        self.logger.debug(\"Updated ts_ids of _export_config_copy.\")\n\n    self._export_config_copy.sliding_window_size = self.dataset_config.sliding_window_size\n    self._export_config_copy.sliding_window_prediction_size = self.dataset_config.sliding_window_prediction_size\n    self._export_config_copy.sliding_window_step = self.dataset_config.sliding_window_step\n    self._export_config_copy.set_shared_size = self.dataset_config.set_shared_size\n\n    super(TimeBasedCesnetDataset, self)._update_export_config_copy()\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset._get_singular_time_series_dataset","title":"_get_singular_time_series_dataset","text":"<pre><code>_get_singular_time_series_dataset(parent_dataset: TimeBasedSplittedDataset, ts_id: int) -&gt; TimeBasedSplittedDataset\n</code></pre> <p>Returns dataset for single time series</p> Source code in <code>cesnet_tszoo\\datasets\\time_based_cesnet_dataset.py</code> <pre><code>def _get_singular_time_series_dataset(self, parent_dataset: TimeBasedSplittedDataset, ts_id: int) -&gt; TimeBasedSplittedDataset:\n    \"\"\"Returns dataset for single time series \"\"\"\n\n    temp = np.where(np.isin(parent_dataset.load_config.ts_row_ranges[self.metadata.ts_id_name], [ts_id]))[0]\n\n    if len(temp) == 0:\n        raise ValueError(f\"ts_id {ts_id} was not found in valid time series for this set. Available time series are: {parent_dataset.ts_row_ranges[self.metadata.ts_id_name]}\")\n\n    time_series_position = temp[0]\n\n    split_load_config = parent_dataset.load_config.create_split_copy(slice(time_series_position, time_series_position + 1))\n\n    dataset = TimeBasedSplittedDataset(self.metadata.dataset_path, self.metadata.data_table_path, split_load_config, 0)\n    self.logger.debug(\"Singular time series dataset initiliazed.\")\n\n    return dataset\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset._get_data_for_plot","title":"_get_data_for_plot","text":"<pre><code>_get_data_for_plot(ts_id: int, feature_indices: ndarray[int], time_format: TimeFormat) -&gt; tuple[np.ndarray, np.ndarray]\n</code></pre> <p>Dataset type specific retrieval of data.</p> Source code in <code>cesnet_tszoo\\datasets\\time_based_cesnet_dataset.py</code> <pre><code>def _get_data_for_plot(self, ts_id: int, feature_indices: np.ndarray[int], time_format: TimeFormat) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Dataset type specific retrieval of data. \"\"\"\n\n    # Validate the time series ID (ts_id)\n    id_result = np.argwhere(np.isin(self.dataset_config.ts_ids, ts_id)).ravel()\n\n    if len(id_result) == 0:\n        raise ValueError(f\"Invalid ts_id '{ts_id}'. The provided ts_id is not found in the available time series IDs.\", self.dataset_config.ts_ids)\n    else:\n        id_result = id_result[0]\n        self.logger.debug(\"Valid ts_id found: %d\", id_result)\n\n    data = None\n\n    if self.dataset_config.has_train():\n        data = self.__update_data_for_plot(self.train_dataset, ts_id, feature_indices, data)\n\n    if self.dataset_config.has_val():\n        data = self.__update_data_for_plot(self.val_dataset, ts_id, feature_indices, data)\n\n    if self.dataset_config.has_test():\n        data = self.__update_data_for_plot(self.test_dataset, ts_id, feature_indices, data)\n\n    return data, self.get_data_about_set(SplitType.ALL)[time_format]\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.__update_data_for_plot","title":"__update_data_for_plot","text":"<pre><code>__update_data_for_plot(dataset: TimeBasedSplittedDataset, ts_id: int, feature_indices: list[int], previous_data: Optional[ndarray])\n</code></pre> Source code in <code>cesnet_tszoo\\datasets\\time_based_cesnet_dataset.py</code> <pre><code>def __update_data_for_plot(self, dataset: TimeBasedSplittedDataset, ts_id: int, feature_indices: list[int], previous_data: Optional[np.ndarray]):\n    dataset = self._get_singular_time_series_dataset(dataset, ts_id)\n\n    dataloader = self.dataloader_factory.create_dataloader(dataset, self.dataset_config, 0, True, None)\n\n    temp_data = dataset_loaders.create_numpy_from_dataloader(dataloader, np.array([ts_id]), dataset.load_config.time_format, dataset.load_config.include_time, DatasetType.TIME_BASED, True)\n\n    if (dataset.load_config.time_format == TimeFormat.DATETIME and dataset.load_config.include_time):\n        temp_data = temp_data[0]\n\n    temp_data = temp_data[0][:, feature_indices]\n\n    if previous_data is None:\n        return temp_data\n\n    return np.concatenate([previous_data, temp_data])\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_train_dataloader","title":"get_train_dataloader","text":"<pre><code>get_train_dataloader(ts_id: int | None = None, workers: int | Literal['config'] = 'config', **kwargs) -&gt; DataLoader\n</code></pre> <p>Returns a PyTorch <code>DataLoader</code> for training set.</p> <p>The <code>DataLoader</code> is created on the first call and cached for subsequent use.  The cached dataloader is cleared when either <code>get_train_df</code> or <code>get_train_numpy</code> is called.</p> <p>The structure of the returned batch depends on the <code>time_format</code> and whether <code>sliding_window_size</code> is used:</p> <ul> <li>When <code>sliding_window_size</code> is used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> <li><code>np.ndarray</code> of times with shape <code>(times - 1)</code></li> <li><code>np.ndarray</code> of time with shape <code>(1)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> </ul> </li> </ul> </li> <li>When <code>sliding_window_size</code> is not used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> <li><code>np.ndarray</code> of time with shape <code>(times)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> </ul> </li> </ul> </li> </ul> <p>The <code>DataLoader</code> is configured with the following config attributes:</p> Dataset config Description <code>train_batch_size</code> Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>sliding_window_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_prediction_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_step</code> Available only for time-based datasets. Number of times to move by after each window. <code>train_workers</code> Specifies the number of workers to use for loading train data. Applied when <code>workers</code> = \"config\". <code>train_dataloader_order</code> Available only for series-based datasets. Whether to load train data in sequential or random order. <code>random_state</code> Seed for loading train data in random order. <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading train data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>ts_id</code> <code>int | None</code> <p>Specifies time series to take. If None returns all time series as normal. <code>Default: \"None\"</code></p> <code>None</code> <p>Returns:</p> Type Description <code>DataLoader</code> <p>An iterable <code>DataLoader</code> containing data from training set.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_train_dataloader(self, ts_id: int | None = None, workers: int | Literal[\"config\"] = \"config\", **kwargs) -&gt; DataLoader:\n    \"\"\"\n    Returns a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for training set.\n\n    The `DataLoader` is created on the first call and cached for subsequent use. &lt;br/&gt;\n    The cached dataloader is cleared when either [`get_train_df`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_train_df) or [`get_train_numpy`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_train_numpy) is called.\n\n    The structure of the returned batch depends on the `time_format` and whether `sliding_window_size` is used:\n\n    - When `sliding_window_size` is used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n            - `np.ndarray` of times with shape `(times - 1)`\n            - `np.ndarray` of time with shape `(1)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n    - When `sliding_window_size` is not used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n            - `np.ndarray` of time with shape `(times)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n\n    The `DataLoader` is configured with the following config attributes:\n\n    Dataset config | Description\n    -------------- | -----------\n    `train_batch_size` | Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `sliding_window_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n    `sliding_window_prediction_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n    `sliding_window_step` | Available only for time-based datasets. Number of times to move by after each window.\n    `train_workers` | Specifies the number of workers to use for loading train data. Applied when `workers` = \"config\".\n    `train_dataloader_order` | Available only for series-based datasets. Whether to load train data in sequential or random order.\n    `random_state` | Seed for loading train data in random order.\n\n    Parameters:\n        workers: The number of workers to use for loading train data. `Default: \"config\"` \n        ts_id: Specifies time series to take. If None returns all time series as normal. `Default: \"None\"`\n\n    Returns:\n        An iterable `DataLoader` containing data from training set.          \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access train_dataloader.\")\n\n    if not self.dataset_config.has_train():\n        raise ValueError(\"Dataloader for training set is not available in the dataset configuration.\")\n\n    assert self.train_dataset is not None, \"The train_dataset must be initialized before accessing data from training set.\"\n\n    default_kwargs = {'take_all': False, \"cache_loader\": True}\n    kwargs = {**default_kwargs, **kwargs}\n\n    if ts_id is not None:\n\n        if ts_id == self.dataset_config.used_singular_train_time_series and self.train_dataloader is not None:\n            self.logger.debug(\"Returning cached train_dataloader.\")\n            return self.train_dataloader\n\n        dataset = self._get_singular_time_series_dataset(self.train_dataset, ts_id)\n        self.dataset_config.used_singular_train_time_series = ts_id\n        if self.train_dataloader:\n            del self.train_dataloader\n            self.train_dataloader = None\n            self.logger.info(\"Destroyed previous cached train_dataloader.\")\n\n        self.dataset_config.used_train_workers = 0\n        self.train_dataloader = self.dataloader_factory.create_dataloader(dataset, self.dataset_config, 0, False, self.dataset_config.train_batch_size)\n        self.logger.info(\"Created new cached train_dataloader.\")\n        return self.train_dataloader\n    elif self.dataset_config.used_singular_train_time_series is not None and self.train_dataloader is not None:\n        del self.train_dataloader\n        self.train_dataloader = None\n        self.dataset_config.used_singular_train_time_series = None\n        self.logger.info(\"Destroyed previous cached train_dataloader.\")\n\n    if workers == \"config\":\n        workers = self.dataset_config.train_workers\n\n    # If the dataloader is cached and number of used workers did not change, return the cached dataloader\n    if self.train_dataloader and kwargs[\"cache_loader\"] and workers == self.dataset_config.used_train_workers:\n        self.logger.debug(\"Returning cached train_dataloader.\")\n        return self.train_dataloader\n\n    # Update the used workers count\n    self.dataset_config.used_train_workers = workers\n\n    # If there's a previously cached dataloader, destroy it\n    if self.train_dataloader:\n        del self.train_dataloader\n        self.train_dataloader = None\n        self.logger.info(\"Destroyed previous cached train_dataloader.\")\n\n    # If caching is enabled, create a new cached dataloader\n    if kwargs[\"cache_loader\"]:\n        self.train_dataloader = self.dataloader_factory.create_dataloader(self.train_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.train_batch_size, order=self.dataset_config.train_dataloader_order)\n        self.logger.info(\"Created new cached train_dataloader.\")\n        return self.train_dataloader\n\n    # If caching is disabled, create a new uncached dataloader\n    self.logger.debug(\"Created new uncached train_dataloader.\")\n    return self.dataloader_factory.create_dataloader(self.train_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.train_batch_size, order=self.dataset_config.train_dataloader_order)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_val_dataloader","title":"get_val_dataloader","text":"<pre><code>get_val_dataloader(ts_id: int | None = None, workers: int | Literal['config'] = 'config', **kwargs) -&gt; DataLoader\n</code></pre> <p>Returns a PyTorch <code>DataLoader</code> for validation set.</p> <p>The <code>DataLoader</code> is created on the first call and cached for subsequent use.  The cached dataloader is cleared when either <code>get_val_df</code> or <code>get_val_numpy</code> is called.</p> <p>The structure of the returned batch depends on the <code>time_format</code> and whether <code>sliding_window_size</code> is used:</p> <ul> <li>When <code>sliding_window_size</code> is used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> <li><code>np.ndarray</code> of times with shape <code>(times - 1)</code></li> <li><code>np.ndarray</code> of time with shape <code>(1)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> </ul> </li> </ul> </li> <li>When <code>sliding_window_size</code> is not used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> <li><code>np.ndarray</code> of time with shape <code>(times)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> </ul> </li> </ul> </li> </ul> <p>The <code>DataLoader</code> is configured with the following config attributes:</p> Dataset config Description <code>val_batch_size</code> Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>sliding_window_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_prediction_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_step</code> Available only for time-based datasets. Number of times to move by after each window. <code>val_workers</code> Specifies the number of workers to use for loading validation data. Applied when <code>workers</code> = \"config\". <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading validation data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>ts_id</code> <code>int | None</code> <p>Specifies time series to take. If None returns all time series as normal. <code>Default: \"None\"</code></p> <code>None</code> <p>Returns:</p> Type Description <code>DataLoader</code> <p>An iterable <code>DataLoader</code> containing data from validation set.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_val_dataloader(self, ts_id: int | None = None, workers: int | Literal[\"config\"] = \"config\", **kwargs) -&gt; DataLoader:\n    \"\"\"\n    Returns a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for validation set.\n\n    The `DataLoader` is created on the first call and cached for subsequent use. &lt;br/&gt;\n    The cached dataloader is cleared when either [`get_val_df`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_val_df) or [`get_val_numpy`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_val_numpy) is called.\n\n    The structure of the returned batch depends on the `time_format` and whether `sliding_window_size` is used:\n\n    - When `sliding_window_size` is used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n            - `np.ndarray` of times with shape `(times - 1)`\n            - `np.ndarray` of time with shape `(1)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n    - When `sliding_window_size` is not used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n            - `np.ndarray` of time with shape `(times)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n\n    The `DataLoader` is configured with the following config attributes:\n\n    Dataset config | Description\n    -------------- | -----------\n    `val_batch_size` | Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `sliding_window_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n    `sliding_window_prediction_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n    `sliding_window_step` | Available only for time-based datasets. Number of times to move by after each window.\n    `val_workers` | Specifies the number of workers to use for loading validation data. Applied when `workers` = \"config\".\n\n\n    Parameters:\n        workers: The number of workers to use for loading validation data. `Default: \"config\"`  \n        ts_id: Specifies time series to take. If None returns all time series as normal. `Default: \"None\"`\n\n    Returns:\n        An iterable `DataLoader` containing data from validation set.        \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access val_dataloader.\")\n\n    if not self.dataset_config.has_val():\n        raise ValueError(\"Dataloader for validation set is not available in the dataset configuration.\")\n\n    assert self.val_dataset is not None, \"The val_dataset must be initialized before accessing data from validation set.\"\n\n    default_kwargs = {'take_all': False, \"cache_loader\": True}\n    kwargs = {**default_kwargs, **kwargs}\n\n    if ts_id is not None:\n\n        if ts_id == self.dataset_config.used_singular_val_time_series and self.val_dataloader is not None:\n            self.logger.debug(\"Returning cached val_dataloader.\")\n            return self.val_dataloader\n\n        dataset = self._get_singular_time_series_dataset(self.val_dataset, ts_id)\n        self.dataset_config.used_singular_val_time_series = ts_id\n        if self.val_dataloader:\n            del self.val_dataloader\n            self.val_dataloader = None\n            self.logger.info(\"Destroyed previous cached val_dataloader.\")\n\n        self.dataset_config.used_val_workers = 0\n        self.val_dataloader = self.dataloader_factory.create_dataloader(dataset, self.dataset_config, 0, False, self.dataset_config.val_batch_size)\n        self.logger.info(\"Created new cached val_dataloader.\")\n        return self.val_dataloader\n    elif self.dataset_config.used_singular_val_time_series is not None and self.val_dataloader is not None:\n        del self.val_dataloader\n        self.val_dataloader = None\n        self.dataset_config.used_singular_val_time_series = None\n        self.logger.info(\"Destroyed previous cached val_dataloader.\")\n\n    if workers == \"config\":\n        workers = self.dataset_config.val_workers\n\n    # If the dataloader is cached and number of used workers did not change, return the cached dataloader\n    if self.val_dataloader and kwargs[\"cache_loader\"] and workers == self.dataset_config.used_val_workers:\n        self.logger.debug(\"Returning cached val_dataloader.\")\n        return self.val_dataloader\n\n    # Update the used workers count\n    self.dataset_config.used_val_workers = workers\n\n    # If there's a previously cached dataloader, destroy it\n    if self.val_dataloader:\n        del self.val_dataloader\n        self.val_dataloader = None\n        self.logger.info(\"Destroyed previous cached val_dataloader.\")\n\n    # If caching is enabled, create a new cached dataloader\n    if kwargs[\"cache_loader\"]:\n        self.val_dataloader = self.dataloader_factory.create_dataloader(self.val_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.val_batch_size)\n        self.logger.info(\"Created new cached val_dataloader.\")\n        return self.val_dataloader\n\n    # If caching is disabled, create a new uncached dataloader\n    self.logger.debug(\"Created new uncached val_dataloader.\")\n    return self.dataloader_factory.create_dataloader(self.val_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.val_batch_size)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_dataloader","title":"get_test_dataloader","text":"<pre><code>get_test_dataloader(ts_id: int | None = None, workers: int | Literal['config'] = 'config', **kwargs) -&gt; DataLoader\n</code></pre> <p>Returns a PyTorch <code>DataLoader</code> for test set.</p> <p>The <code>DataLoader</code> is created on the first call and cached for subsequent use.  The cached dataloader is cleared when either <code>get_test_df</code> or <code>get_test_numpy</code> is called.</p> <p>The structure of the returned batch depends on the <code>time_format</code> and whether <code>sliding_window_size</code> is used:</p> <ul> <li>When <code>sliding_window_size</code> is used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> <li><code>np.ndarray</code> of times with shape <code>(times - 1)</code></li> <li><code>np.ndarray</code> of time with shape <code>(1)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> </ul> </li> </ul> </li> <li>When <code>sliding_window_size</code> is not used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> <li><code>np.ndarray</code> of time with shape <code>(times)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> </ul> </li> </ul> </li> </ul> <p>The <code>DataLoader</code> is configured with the following config attributes:</p> Dataset config Description <code>test_batch_size</code> Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>sliding_window_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_prediction_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_step</code> Available only for time-based datasets. Number of times to move by after each window. <code>test_workers</code> Specifies the number of workers to use for loading test data. Applied when <code>workers</code> = \"config\". <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading test data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>ts_id</code> <code>int | None</code> <p>Specifies time series to take. If None returns all time series as normal. <code>Default: \"None\"</code></p> <code>None</code> <p>Returns:</p> Type Description <code>DataLoader</code> <p>An iterable <code>DataLoader</code> containing data from test set.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_test_dataloader(self, ts_id: int | None = None, workers: int | Literal[\"config\"] = \"config\", **kwargs) -&gt; DataLoader:\n    \"\"\"\n    Returns a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for test set.\n\n    The `DataLoader` is created on the first call and cached for subsequent use. &lt;br/&gt;\n    The cached dataloader is cleared when either [`get_test_df`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_test_df) or [`get_test_numpy`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_test_numpy) is called.\n\n    The structure of the returned batch depends on the `time_format` and whether `sliding_window_size` is used:\n\n    - When `sliding_window_size` is used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n            - `np.ndarray` of times with shape `(times - 1)`\n            - `np.ndarray` of time with shape `(1)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n    - When `sliding_window_size` is not used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n            - `np.ndarray` of time with shape `(times)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n\n    The `DataLoader` is configured with the following config attributes:\n\n    Dataset config | Description\n    -------------- | -----------\n    `test_batch_size` | Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `sliding_window_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n    `sliding_window_prediction_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n    `sliding_window_step` | Available only for time-based datasets. Number of times to move by after each window.\n    `test_workers` | Specifies the number of workers to use for loading test data. Applied when `workers` = \"config\".\n\n\n    Parameters:\n        workers: The number of workers to use for loading test data. `Default: \"config\"`  \n        ts_id: Specifies time series to take. If None returns all time series as normal. `Default: \"None\"`\n\n    Returns:\n        An iterable `DataLoader` containing data from test set.        \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access test_dataloader.\")\n\n    if not self.dataset_config.has_test():\n        raise ValueError(\"Dataloader for test set is not available in the dataset configuration.\")\n\n    assert self.test_dataset is not None, \"The test_dataset must be initialized before accessing data from test set.\"\n\n    default_kwargs = {'take_all': False, \"cache_loader\": True}\n    kwargs = {**default_kwargs, **kwargs}\n\n    if ts_id is not None:\n\n        if ts_id == self.dataset_config.used_singular_test_time_series and self.test_dataloader is not None:\n            self.logger.debug(\"Returning cached test_dataloader.\")\n            return self.test_dataloader\n\n        dataset = self._get_singular_time_series_dataset(self.test_dataset, ts_id)\n        self.dataset_config.used_singular_test_time_series = ts_id\n        if self.test_dataloader:\n            del self.test_dataloader\n            self.test_dataloader = None\n            self.logger.info(\"Destroyed previous cached test_dataloader.\")\n\n        self.dataset_config.used_test_workers = 0\n        self.test_dataloader = self.dataloader_factory.create_dataloader(dataset, self.dataset_config, 0, False, self.dataset_config.test_batch_size)\n        self.logger.info(\"Created new cached test_dataloader.\")\n        return self.test_dataloader\n    elif self.dataset_config.used_singular_test_time_series is not None and self.test_dataloader is not None:\n        del self.test_dataloader\n        self.test_dataloader = None\n        self.dataset_config.used_singular_test_time_series = None\n        self.logger.info(\"Destroyed previous cached test_dataloader.\")\n\n    if workers == \"config\":\n        workers = self.dataset_config.test_workers\n\n    # If the dataloader is cached and number of used workers did not change, return the cached dataloader\n    if self.test_dataloader and kwargs[\"cache_loader\"] and workers == self.dataset_config.used_test_workers:\n        self.logger.debug(\"Returning cached test_dataloader.\")\n        return self.test_dataloader\n\n    # Update the used workers count\n    self.dataset_config.used_test_workers = workers\n\n    # If there's a previously cached dataloader, destroy it\n    if self.test_dataloader:\n        del self.test_dataloader\n        self.test_dataloader = None\n        self.logger.info(\"Destroyed previous cached test_dataloader.\")\n\n    # If caching is enabled, create a new cached dataloader\n    if kwargs[\"cache_loader\"]:\n        self.test_dataloader = self.dataloader_factory.create_dataloader(self.test_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.test_batch_size)\n        self.logger.info(\"Created new cached test_dataloader.\")\n        return self.test_dataloader\n\n    # If caching is disabled, create a new uncached dataloader\n    self.logger.debug(\"Created new uncached test_dataloader.\")\n    return self.dataloader_factory.create_dataloader(self.test_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.test_batch_size)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_all_dataloader","title":"get_all_dataloader","text":"<pre><code>get_all_dataloader(ts_id: int | None = None, workers: int | Literal['config'] = 'config', **kwargs) -&gt; DataLoader\n</code></pre> <p>Returns a PyTorch <code>DataLoader</code> for all set.</p> <p>The <code>DataLoader</code> is created on the first call and cached for subsequent use.  The cached dataloader is cleared when either <code>get_all_df</code> or <code>get_all_numpy</code> is called.</p> <p>The structure of the returned batch depends on the <code>time_format</code> and whether <code>sliding_window_size</code> is used:</p> <ul> <li>When <code>sliding_window_size</code> is used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> <li><code>np.ndarray</code> of times with shape <code>(times - 1)</code></li> <li><code>np.ndarray</code> of time with shape <code>(1)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times - 1, features)</code></li> <li><code>np.ndarray</code> of shape <code>(num_time_series, 1, features)</code></li> </ul> </li> </ul> </li> <li>When <code>sliding_window_size</code> is not used:<ul> <li>With <code>time_format</code> == TimeFormat.DATETIME and included time:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> <li><code>np.ndarray</code> of time with shape <code>(times)</code></li> </ul> </li> <li>When <code>time_format</code> != TimeFormat.DATETIME or time is not included:<ul> <li><code>np.ndarray</code> of shape <code>(num_time_series, times, features)</code></li> </ul> </li> </ul> </li> </ul> <p>The <code>DataLoader</code> is configured with the following config attributes:</p> Dataset config Description <code>all_batch_size</code> Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>sliding_window_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_prediction_size</code> Available only for time-based datasets. Modifies the shape of the returned data. <code>sliding_window_step</code> Available only for time-based datasets. Number of times to move by after each window. <code>all_workers</code> Specifies the number of workers to use for loading all data. Applied when <code>workers</code> = \"config\". <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading all data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>ts_id</code> <code>int | None</code> <p>Specifies time series to take. If None returns all time series as normal. <code>Default: \"None\"</code></p> <code>None</code> <p>Returns:</p> Type Description <code>DataLoader</code> <p>An iterable <code>DataLoader</code> containing data from all set.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_all_dataloader(self, ts_id: int | None = None, workers: int | Literal[\"config\"] = \"config\", **kwargs) -&gt; DataLoader:\n    \"\"\"\n    Returns a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for all set.\n\n    The `DataLoader` is created on the first call and cached for subsequent use. &lt;br/&gt;\n    The cached dataloader is cleared when either [`get_all_df`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_all_df) or [`get_all_numpy`](reference_cesnet_dataset.md#cesnet_tszoo.datasets.cesnet_dataset.CesnetDataset.get_all_numpy) is called.\n\n    The structure of the returned batch depends on the `time_format` and whether `sliding_window_size` is used:\n\n    - When `sliding_window_size` is used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n            - `np.ndarray` of times with shape `(times - 1)`\n            - `np.ndarray` of time with shape `(1)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times - 1, features)`\n            - `np.ndarray` of shape `(num_time_series, 1, features)`\n    - When `sliding_window_size` is not used:\n        - With `time_format` == TimeFormat.DATETIME and included time:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n            - `np.ndarray` of time with shape `(times)`\n        - When `time_format` != TimeFormat.DATETIME or time is not included:\n            - `np.ndarray` of shape `(num_time_series, times, features)`\n\n    The `DataLoader` is configured with the following config attributes:\n\n    Dataset config | Description\n    -------------- | -----------\n    `all_batch_size` | Number of samples per batch. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `sliding_window_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n    `sliding_window_prediction_size` | Available only for time-based datasets. Modifies the shape of the returned data.\n    `sliding_window_step` | Available only for time-based datasets. Number of times to move by after each window.\n    `all_workers` | Specifies the number of workers to use for loading all data. Applied when `workers` = \"config\".\n\n\n    Parameters:\n        workers: The number of workers to use for loading all data. `Default: \"config\"`  \n        ts_id: Specifies time series to take. If None returns all time series as normal. `Default: \"None\"`\n\n    Returns:\n        An iterable `DataLoader` containing data from all set.       \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access all_dataloader.\")\n\n    if not self.dataset_config.has_all():\n        raise ValueError(\"Dataloader for all set is not available in the dataset configuration.\")\n\n    assert self.all_dataset is not None, \"The all_dataset must be initialized before accessing data from all set.\"\n\n    default_kwargs = {'take_all': False, \"cache_loader\": True}\n    kwargs = {**default_kwargs, **kwargs}\n\n    if ts_id is not None:\n\n        if ts_id == self.dataset_config.used_singular_all_time_series and self.all_dataloader is not None:\n            self.logger.debug(\"Returning cached all_dataloader.\")\n            return self.all_dataloader\n\n        dataset = self._get_singular_time_series_dataset(self.all_dataset, ts_id)\n        self.dataset_config.used_singular_all_time_series = ts_id\n        if self.all_dataloader:\n            del self.all_dataloader\n            self.all_dataloader = None\n            self.logger.info(\"Destroyed previous cached all_dataloader.\")\n\n        self.dataset_config.used_all_workers = 0\n        self.all_dataloader = self.dataloader_factory.create_dataloader(dataset, self.dataset_config, 0, False, self.dataset_config.all_batch_size)\n        self.logger.info(\"Created new cached all_dataloader.\")\n        return self.all_dataloader\n    elif self.dataset_config.used_singular_all_time_series is not None and self.all_dataloader is not None:\n        del self.all_dataloader\n        self.all_dataloader = None\n        self.dataset_config.used_singular_all_time_series = None\n        self.logger.info(\"Destroyed previous cached all_dataloader.\")\n\n    if workers == \"config\":\n        workers = self.dataset_config.all_workers\n\n    # If the dataloader is cached and number of used workers did not change, return the cached dataloader\n    if self.all_dataloader and kwargs[\"cache_loader\"] and workers == self.dataset_config.used_all_workers:\n        self.logger.debug(\"Returning cached all_dataloader.\")\n        return self.all_dataloader\n\n    # Update the used workers count\n    self.dataset_config.used_all_workers = workers\n\n    # If there's a previously cached dataloader, destroy it\n    if self.all_dataloader:\n        del self.all_dataloader\n        self.all_dataloader = None\n        self.logger.info(\"Destroyed previous cached all_dataloader.\")\n\n    # If caching is enabled, create a new cached dataloader\n    if kwargs[\"cache_loader\"]:\n        self.all_dataloader = self.dataloader_factory.create_dataloader(self.all_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.all_batch_size)\n        self.logger.info(\"Created new cached all_dataloader.\")\n        return self.all_dataloader\n\n    # If caching is disabled, create a new uncached dataloader\n    self.logger.debug(\"Creating new uncached all_dataloader.\")\n    return self.dataloader_factory.create_dataloader(self.all_dataset, self.dataset_config, workers, kwargs['take_all'], self.dataset_config.all_batch_size)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_train_df","title":"get_train_df","text":"<pre><code>get_train_df(workers: int | Literal['config'] = 'config', as_single_dataframe: bool = True) -&gt; pd.DataFrame\n</code></pre> <p>Creates a Pandas <code>DataFrame</code> containing all the data from training set grouped by time series.</p> <p>This method uses the <code>train_dataloader</code> with a batch size set to the total number of data in the training set. The cached <code>train_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire training set is loaded into memory, which may lead to high memory usage. If working with large training set, consider using <code>get_train_dataloader</code> instead to handle data in batches.</p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading train data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>as_single_dataframe</code> <code>bool</code> <p>Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. <code>Default: True</code> </p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A single Pandas DataFrame containing all data from training set, or a list of DataFrames (one per time series).</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_train_df(self, workers: int | Literal[\"config\"] = \"config\", as_single_dataframe: bool = True) -&gt; pd.DataFrame:\n    \"\"\"\n    Creates a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) containing all the data from training set grouped by time series.\n\n    This method uses the `train_dataloader` with a batch size set to the total number of data in the training set. The cached `train_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire training set is loaded into memory, which may lead to high memory usage. If working with large training set, consider using `get_train_dataloader` instead to handle data in batches.\n\n    Parameters:\n        workers: The number of workers to use for loading train data. `Default: \"config\"`  \n        as_single_dataframe: Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. `Default: True` \n\n    Returns:\n        A single Pandas DataFrame containing all data from training set, or a list of DataFrames (one per time series).\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access train_dataloader.\")\n\n    if not self.dataset_config.has_train():\n        raise ValueError(\"Dataloader for training set is not available in the dataset configuration.\")\n\n    assert self.train_dataset is not None, \"The train_dataset must be initialized before accessing data from training set.\"\n\n    ts_ids, time_period = self.dataset_config._get_train()\n\n    should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n    dataloader = self.get_train_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n    return self._get_df(dataloader, as_single_dataframe, ts_ids, time_period)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_val_df","title":"get_val_df","text":"<pre><code>get_val_df(workers: int | Literal['config'] = 'config', as_single_dataframe: bool = True) -&gt; pd.DataFrame\n</code></pre> <p>Create a Pandas <code>DataFrame</code> containing all the data from validation set grouped by time series.</p> <p>This method uses the <code>val_dataloader</code> with a batch size set to the total number of data in the validation set. The cached <code>val_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire validation set is loaded into memory, which may lead to high memory usage. If working with large validation set, consider using <code>get_val_dataloader</code> instead to handle data in batches.</p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading validation data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>as_single_dataframe</code> <code>bool</code> <p>Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. <code>Default: True</code> </p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A single Pandas DataFrame containing all data from validation set, or a list of DataFrames (one per time series).</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_val_df(self, workers: int | Literal[\"config\"] = \"config\", as_single_dataframe: bool = True) -&gt; pd.DataFrame:\n    \"\"\"\n    Create a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) containing all the data from validation set grouped by time series.\n\n    This method uses the `val_dataloader` with a batch size set to the total number of data in the validation set. The cached `val_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire validation set is loaded into memory, which may lead to high memory usage. If working with large validation set, consider using `get_val_dataloader` instead to handle data in batches.\n\n    Parameters:\n        workers: The number of workers to use for loading validation data. `Default: \"config\"`  \n        as_single_dataframe: Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. `Default: True` \n\n    Returns:\n        A single Pandas DataFrame containing all data from validation set, or a list of DataFrames (one per time series).\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access val_dataloader.\")\n\n    if not self.dataset_config.has_val():\n        raise ValueError(\"Dataloader for validation set is not available in the dataset configuration.\")\n\n    assert self.val_dataset is not None, \"The val_dataset must be initialized before accessing data from validation set.\"\n\n    ts_ids, time_period = self.dataset_config._get_val()\n\n    should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n    dataloader = self.get_val_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n    return self._get_df(dataloader, as_single_dataframe, ts_ids, time_period)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_df","title":"get_test_df","text":"<pre><code>get_test_df(workers: int | Literal['config'] = 'config', as_single_dataframe: bool = True) -&gt; pd.DataFrame\n</code></pre> <p>Creates a Pandas <code>DataFrame</code> containing all the data from test set grouped by time series.</p> <p>This method uses the <code>test_dataloader</code> with a batch size set to the total number of data in the test set. The cached <code>test_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire test set is loaded into memory, which may lead to high memory usage. If working with large test set, consider using <code>get_test_dataloader</code> instead to handle data in batches.</p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading test data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>as_single_dataframe</code> <code>bool</code> <p>Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. <code>Default: True</code> </p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A single Pandas DataFrame containing all data from test set, or a list of DataFrames (one per time series).</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_test_df(self, workers: int | Literal[\"config\"] = \"config\", as_single_dataframe: bool = True) -&gt; pd.DataFrame:\n    \"\"\"\n    Creates a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) containing all the data from test set grouped by time series.\n\n    This method uses the `test_dataloader` with a batch size set to the total number of data in the test set. The cached `test_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire test set is loaded into memory, which may lead to high memory usage. If working with large test set, consider using `get_test_dataloader` instead to handle data in batches.\n\n    Parameters:\n        workers: The number of workers to use for loading test data. `Default: \"config\"`  \n        as_single_dataframe: Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. `Default: True` \n\n    Returns:\n        A single Pandas DataFrame containing all data from test set, or a list of DataFrames (one per time series).\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access test_dataloader.\")\n\n    if not self.dataset_config.has_test():\n        raise ValueError(\"Dataloader for test set is not available in the dataset configuration.\")\n\n    assert self.test_dataset is not None, \"The test_dataset must be initialized before accessing data from test set.\"\n\n    ts_ids, time_period = self.dataset_config._get_test()\n\n    should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n    dataloader = self.get_test_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n    return self._get_df(dataloader, as_single_dataframe, ts_ids, time_period)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_all_df","title":"get_all_df","text":"<pre><code>get_all_df(workers: int | Literal['config'] = 'config', as_single_dataframe: bool = True) -&gt; pd.DataFrame\n</code></pre> <p>Creates a Pandas <code>DataFrame</code> containing all the data from all set grouped by time series.</p> <p>This method uses the <code>all_dataloader</code> with a batch size set to the total number of data in the all set. The cached <code>all_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire all set is loaded into memory, which may lead to high memory usage. If working with large all set, consider using <code>get_all_dataloader</code> instead to handle data in batches.</p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading all data. <code>Default: \"config\"</code> </p> <code>'config'</code> <code>as_single_dataframe</code> <code>bool</code> <p>Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. <code>Default: True</code> </p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A single Pandas DataFrame containing all data from all set, or a list of DataFrames (one per time series).</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_all_df(self, workers: int | Literal[\"config\"] = \"config\", as_single_dataframe: bool = True) -&gt; pd.DataFrame:\n    \"\"\"\n    Creates a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) containing all the data from all set grouped by time series.\n\n    This method uses the `all_dataloader` with a batch size set to the total number of data in the all set. The cached `all_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire all set is loaded into memory, which may lead to high memory usage. If working with large all set, consider using `get_all_dataloader` instead to handle data in batches.\n\n    Parameters:\n        workers: The number of workers to use for loading all data. `Default: \"config\"`  \n        as_single_dataframe: Whether to return a single dataframe with all time series combined, or to create separate dataframes for each time series. `Default: True` \n\n    Returns:\n        A single Pandas DataFrame containing all data from all set, or a list of DataFrames (one per time series).\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access all_dataloader.\")\n\n    if not self.dataset_config.has_all():\n        raise ValueError(\"Dataloader for all set is not available in the dataset configuration.\")\n\n    assert self.all_dataset is not None, \"The all_dataset must be initialized before accessing data from all set.\"\n\n    ts_ids, time_period = self.dataset_config._get_all()\n\n    should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n    dataloader = self.get_all_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n    return self._get_df(dataloader, as_single_dataframe, ts_ids, time_period)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_train_numpy","title":"get_train_numpy","text":"<pre><code>get_train_numpy(workers: int | Literal['config'] = 'config') -&gt; np.ndarray\n</code></pre> <p>Creates a NumPy array containing all the data from training set grouped by time series, with the shape <code>(num_time_series, num_times, num_features)</code>.</p> <p>This method uses the <code>train_dataloader</code> with a batch size set to the total number of data in the training set. The cached <code>train_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire training set is loaded into memory, which may lead to high memory usage. If working with large training set, consider using <code>get_train_dataloader</code> instead to handle data in batches.        </p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading train data. <code>Default: \"config\"</code> </p> <code>'config'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A NumPy array containing all the data in training set with the shape <code>(num_time_series, num_times, num_features)</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_train_numpy(self, workers: int | Literal[\"config\"] = \"config\") -&gt; np.ndarray:\n    \"\"\"\n    Creates a NumPy array containing all the data from training set grouped by time series, with the shape `(num_time_series, num_times, num_features)`.\n\n    This method uses the `train_dataloader` with a batch size set to the total number of data in the training set. The cached `train_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire training set is loaded into memory, which may lead to high memory usage. If working with large training set, consider using `get_train_dataloader` instead to handle data in batches.        \n\n    Parameters:\n        workers: The number of workers to use for loading train data. `Default: \"config\"`  \n\n    Returns:\n        A NumPy array containing all the data in training set with the shape `(num_time_series, num_times, num_features)`.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access train_dataloader.\")\n\n    if not self.dataset_config.has_train():\n        raise ValueError(\"Dataloader for training set is not available in the dataset configuration.\")\n\n    assert self.train_dataset is not None, \"The train_dataset must be initialized before accessing data from training set.\"\n\n    ts_ids, time_period = self.dataset_config._get_train()\n\n    should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n    dataloader = self.get_train_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n    return self._get_numpy(dataloader, ts_ids, time_period)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_val_numpy","title":"get_val_numpy","text":"<pre><code>get_val_numpy(workers: int | Literal['config'] = 'config') -&gt; np.ndarray\n</code></pre> <p>Creates a NumPy array containing all the data from validation set grouped by time series, with the shape <code>(num_time_series, num_times, num_features)</code>.</p> <p>This method uses the <code>val_dataloader</code> with a batch size set to the total number of data in the validation set. The cached <code>val_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire validation set is loaded into memory, which may lead to high memory usage. If working with large validation set, consider using <code>get_val_dataloader</code> instead to handle data in batches.        </p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading validation data. <code>Default: \"config\"</code> </p> <code>'config'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A NumPy array containing all the data in validation set with the shape <code>(num_time_series, num_times, num_features)</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_val_numpy(self, workers: int | Literal[\"config\"] = \"config\") -&gt; np.ndarray:\n    \"\"\"\n    Creates a NumPy array containing all the data from validation set grouped by time series, with the shape `(num_time_series, num_times, num_features)`.\n\n    This method uses the `val_dataloader` with a batch size set to the total number of data in the validation set. The cached `val_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire validation set is loaded into memory, which may lead to high memory usage. If working with large validation set, consider using `get_val_dataloader` instead to handle data in batches.        \n\n    Parameters:\n        workers: The number of workers to use for loading validation data. `Default: \"config\"`  \n\n    Returns:\n        A NumPy array containing all the data in validation set with the shape `(num_time_series, num_times, num_features)`.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access val_dataloader.\")\n\n    if not self.dataset_config.has_val():\n        raise ValueError(\"Dataloader for validation set is not available in the dataset configuration.\")\n\n    assert self.val_dataset is not None, \"The val_dataset must be initialized before accessing data from validation set.\"\n\n    ts_ids, time_period = self.dataset_config._get_val()\n\n    should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n    dataloader = self.get_val_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n    return self._get_numpy(dataloader, ts_ids, time_period)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_test_numpy","title":"get_test_numpy","text":"<pre><code>get_test_numpy(workers: int | Literal['config'] = 'config') -&gt; np.ndarray\n</code></pre> <p>Creates a NumPy array containing all the data from test set grouped by time series, with the shape <code>(num_time_series, num_times, num_features)</code>.</p> <p>This method uses the <code>test_dataloader</code> with a batch size set to the total number of data in the test set. The cached <code>test_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire test set is loaded into memory, which may lead to high memory usage. If working with large test set, consider using <code>get_test_dataloader</code> instead to handle data in batches.        </p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading test data. <code>Default: \"config\"</code> </p> <code>'config'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A NumPy array containing all the data in test set with the shape <code>(num_time_series, num_times, num_features)</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_test_numpy(self, workers: int | Literal[\"config\"] = \"config\") -&gt; np.ndarray:\n    \"\"\"\n    Creates a NumPy array containing all the data from test set grouped by time series, with the shape `(num_time_series, num_times, num_features)`.\n\n    This method uses the `test_dataloader` with a batch size set to the total number of data in the test set. The cached `test_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire test set is loaded into memory, which may lead to high memory usage. If working with large test set, consider using `get_test_dataloader` instead to handle data in batches.        \n\n    Parameters:\n        workers: The number of workers to use for loading test data. `Default: \"config\"`  \n\n    Returns:\n        A NumPy array containing all the data in test set with the shape `(num_time_series, num_times, num_features)`.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access test_dataloader.\")\n\n    if not self.dataset_config.has_test():\n        raise ValueError(\"Dataloader for test set is not available in the dataset configuration.\")\n\n    assert self.test_dataset is not None, \"The test_dataset must be initialized before accessing data from test set.\"\n\n    ts_ids, time_period = self.dataset_config._get_test()\n\n    should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n    dataloader = self.get_test_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n    return self._get_numpy(dataloader, ts_ids, time_period)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_all_numpy","title":"get_all_numpy","text":"<pre><code>get_all_numpy(workers: int | Literal['config'] = 'config') -&gt; np.ndarray\n</code></pre> <p>Creates a NumPy array containing all the data from all set grouped by time series, with the shape <code>(num_time_series, num_times, num_features)</code>.</p> <p>This method uses the <code>all_dataloader</code> with a batch size set to the total number of data in the all set. The cached <code>all_dataloader</code> is cleared during this operation.</p> <p>Memory usage</p> <p>The entire all set is loaded into memory, which may lead to high memory usage. If working with large all set, consider using <code>get_all_dataloader</code> instead to handle data in batches.        </p> <p>Parameters:</p> Name Type Description Default <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use for loading all data. <code>Default: \"config\"</code> </p> <code>'config'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A NumPy array containing all the data in all set with the shape <code>(num_time_series, num_times, num_features)</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_all_numpy(self, workers: int | Literal[\"config\"] = \"config\") -&gt; np.ndarray:\n    \"\"\"\n    Creates a NumPy array containing all the data from all set grouped by time series, with the shape `(num_time_series, num_times, num_features)`.\n\n    This method uses the `all_dataloader` with a batch size set to the total number of data in the all set. The cached `all_dataloader` is cleared during this operation.\n\n    !!! warning \"Memory usage\"\n        The entire all set is loaded into memory, which may lead to high memory usage. If working with large all set, consider using `get_all_dataloader` instead to handle data in batches.        \n\n    Parameters:\n        workers: The number of workers to use for loading all data. `Default: \"config\"`  \n\n    Returns:\n        A NumPy array containing all the data in all set with the shape `(num_time_series, num_times, num_features)`.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to access all_dataloader.\")\n\n    if not self.dataset_config.has_all():\n        raise ValueError(\"Dataloader for all set is not available in the dataset configuration.\")\n\n    assert self.all_dataset is not None, \"The all_dataset must be initialized before accessing data from all set.\"\n\n    ts_ids, time_period = self.dataset_config._get_all()\n\n    should_take_all = self.dataset_config.dataset_type != DatasetType.SERIES_BASED\n\n    dataloader = self.get_all_dataloader(workers=workers, take_all=should_take_all, cache_loader=False)\n    return self._get_numpy(dataloader, ts_ids, time_period)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset._update_dataset_config_and_initialize","title":"_update_dataset_config_and_initialize","text":"<pre><code>_update_dataset_config_and_initialize(config_editor: ConfigEditor, workers: int | Literal['config'] = 'config', display_config_details: Optional[Literal['test', 'diagram']] = None)\n</code></pre> <p>Updates config via passed config editor.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def _update_dataset_config_and_initialize(self, config_editor: ConfigEditor, workers: int | Literal[\"config\"] = \"config\", display_config_details: Optional[Literal[\"test\", \"diagram\"]] = None):\n    \"\"\"Updates config via passed config editor. \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating dataset configuration.\")\n\n    if display_config_details is not None:\n        display_config_details = DisplayType(display_config_details)\n\n    original_config = deepcopy(self.dataset_config)\n    original_export_config = deepcopy(self._export_config_copy)\n\n    try:\n        if config_editor.requires_init:\n            self.logger.info(\"Re-initialization is required.\")\n            config_editor.modify_dataset_config(self._export_config_copy, self.metadata)\n            self.set_dataset_config_and_initialize(self._export_config_copy, None, workers)\n\n        else:\n            config_editor.modify_dataset_config(self.dataset_config, self.metadata)\n\n    except Exception:\n        self.dataset_config = original_config\n        self._export_config_copy = original_export_config\n        self.logger.error(\"Error occured, reverting changes.\")\n        raise\n\n    if self.train_dataloader is not None:\n        del self.train_dataloader\n        self.train_dataloader = None\n        self.logger.info(\"Destroyed cached train_dataloader.\")\n\n    if self.val_dataloader is not None:\n        del self.val_dataloader\n        self.val_dataloader = None\n        self.logger.info(\"Destroyed cached val_dataloader.\")\n\n    if self.test_dataloader is not None:\n        del self.test_dataloader\n        self.test_dataloader = None\n        self.logger.info(\"Destroyed cached test_dataloader.\")\n\n    if self.all_dataloader is not None:\n        del self.all_dataloader\n        self.all_dataloader = None\n        self.logger.info(\"Destroyed cached all_dataloader.\")\n\n    self._update_config_imported_status(None)\n    self._update_export_config_copy()\n\n    self.logger.info(\"Configuration has been changed successfuly.\")\n\n    if display_config_details is not None:\n        self.summary(display_config_details)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.apply_filler","title":"apply_filler","text":"<pre><code>apply_filler(fill_missing_with: type | FillerType | Literal['mean_filler', 'forward_filler', 'linear_interpolation_filler'] | None, workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Used for updating filler set in config. Set parameter to <code>config</code> to keep it as it is config. If exception is thrown during set, no changes are made.</p> <p>Affects following configuration: </p> Dataset config Description <code>fill_missing_with</code> Defines how to fill missing values in the dataset. <p>Parameters:</p> Name Type Description Default <code>fill_missing_with</code> <code>type | FillerType | Literal['mean_filler', 'forward_filler', 'linear_interpolation_filler'] | None</code> <p>Defines how to fill missing values in the dataset. <code>Defaults: config</code>.  </p> required <code>workers</code> <code>int | Literal['config']</code> <p>How many workers to use when setting new filler. <code>Defaults: config</code>.</p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def apply_filler(self, fill_missing_with: type | FillerType | Literal[\"mean_filler\", \"forward_filler\", \"linear_interpolation_filler\"] | None, workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"Used for updating filler set in config.\n    Set parameter to `config` to keep it as it is config.\n    If exception is thrown during set, no changes are made.\n\n    Affects following configuration: \n\n    Dataset config | Description\n    -------------- | -----------\n    `fill_missing_with` | Defines how to fill missing values in the dataset.\n\n    Parameters:\n        fill_missing_with: Defines how to fill missing values in the dataset. `Defaults: config`.  \n        workers: How many workers to use when setting new filler. `Defaults: config`.      \n    \"\"\"\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating filler.\")\n\n    self.update_dataset_config_and_initialize(fill_missing_with=fill_missing_with, workers=workers)\n    self.logger.info(\"Filler has been changed successfuly.\")\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.apply_anomaly_handler","title":"apply_anomaly_handler","text":"<pre><code>apply_anomaly_handler(handle_anomalies_with: type | AnomalyHandlerType | Literal['z-score', 'interquartile_range'] | None | Literal['config'], workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Used for updating anomaly handler set in config.</p> <p>Set parameter to <code>config</code> to keep it as it is config.</p> <p>If exception is thrown during set, no changes are made.</p> <p>Affects following configuration:</p> Dataset config Description <code>handle_anomalies_with</code> Defines the anomaly handler to handle anomalies in the dataset. <p>Parameters:</p> Name Type Description Default <code>handle_anomalies_with</code> <code>type | AnomalyHandlerType | Literal['z-score', 'interquartile_range'] | None | Literal['config']</code> <p>Defines the anomaly handler to handle anomalies in the dataset. <code>Defaults: config</code>.  </p> required <code>workers</code> <code>int | Literal['config']</code> <p>How many workers to use when setting new filler. <code>Defaults: config</code>.</p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def apply_anomaly_handler(self, handle_anomalies_with: type | AnomalyHandlerType | Literal[\"z-score\", \"interquartile_range\"] | None | Literal[\"config\"], workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"Used for updating anomaly handler set in config.\n\n    Set parameter to `config` to keep it as it is config.\n\n    If exception is thrown during set, no changes are made.\n\n    Affects following configuration:\n\n    Dataset config | Description\n    -------------- | -----------\n    `handle_anomalies_with` | Defines the anomaly handler to handle anomalies in the dataset.\n\n    Parameters:\n        handle_anomalies_with: Defines the anomaly handler to handle anomalies in the dataset. `Defaults: config`.  \n        workers: How many workers to use when setting new filler. `Defaults: config`.      \n    \"\"\"\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating anomaly handler.\")\n\n    self.update_dataset_config_and_initialize(handle_anomalies_with=handle_anomalies_with, workers=workers)\n    self.logger.info(\"Anomaly handler has been changed successfuly.\")\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.apply_transformer","title":"apply_transformer","text":"<pre><code>apply_transformer(transform_with: type | list[Transformer] | ndarray[Transformer] | TransformerType | Transformer | Literal['min_max_scaler', 'standard_scaler', 'max_abs_scaler', 'log_transformer', 'robust_scaler', 'power_transformer', 'quantile_transformer', 'l2_normalizer'] | None | Literal['config'] = 'config', create_transformer_per_time_series: bool | Literal['config'] = 'config', partial_fit_initialized_transformers: bool | Literal['config'] = 'config', workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Used for updating transformer and relevenat configurations set in config. Set parameter to <code>config</code> to keep it as it is config. If exception is thrown during set, no changes are made.</p> <p>Affects following configuration:</p> Dataset config Description <code>transform_with</code> Defines the transformer to transform the dataset. <code>create_transformer_per_time_series</code> If <code>True</code>, a separate transformer is created for each time series. Not used when using already initialized transformers. <code>partial_fit_initialized_transformers</code> If <code>True</code>, partial fitting on train set is performed when using initialized transformers. <p>Parameters:</p> Name Type Description Default <code>transform_with</code> <code>type | list[Transformer] | ndarray[Transformer] | TransformerType | Transformer | Literal['min_max_scaler', 'standard_scaler', 'max_abs_scaler', 'log_transformer', 'robust_scaler', 'power_transformer', 'quantile_transformer', 'l2_normalizer'] | None | Literal['config']</code> <p>Defines the transformer to transform the dataset. <code>Defaults: config</code>.  </p> <code>'config'</code> <code>create_transformer_per_time_series</code> <code>bool | Literal['config']</code> <p>If <code>True</code>, a separate transformer is created for each time series. Not used when using already initialized transformers. <code>Defaults: config</code>.  </p> <code>'config'</code> <code>partial_fit_initialized_transformers</code> <code>bool | Literal['config']</code> <p>If <code>True</code>, partial fitting on train set is performed when using initiliazed transformers. <code>Defaults: config</code>.  </p> <code>'config'</code> <code>workers</code> <code>int | Literal['config']</code> <p>How many workers to use when setting new transformer. <code>Defaults: config</code>.</p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def apply_transformer(self, transform_with: type | list[Transformer] | np.ndarray[Transformer] | TransformerType | Transformer | Literal[\"min_max_scaler\", \"standard_scaler\", \"max_abs_scaler\", \"log_transformer\", \"robust_scaler\", \"power_transformer\", \"quantile_transformer\", \"l2_normalizer\"] | None | Literal[\"config\"] = \"config\",\n                      create_transformer_per_time_series: bool | Literal[\"config\"] = \"config\", partial_fit_initialized_transformers: bool | Literal[\"config\"] = \"config\", workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"Used for updating transformer and relevenat configurations set in config.\n    Set parameter to `config` to keep it as it is config.\n    If exception is thrown during set, no changes are made.\n\n    Affects following configuration:\n\n    Dataset config | Description\n    -------------- | -----------\n    `transform_with` | Defines the transformer to transform the dataset.\n    `create_transformer_per_time_series` | If `True`, a separate transformer is created for each time series. Not used when using already initialized transformers.\n    `partial_fit_initialized_transformers` | If `True`, partial fitting on train set is performed when using initialized transformers.\n\n    Parameters:\n        transform_with: Defines the transformer to transform the dataset. `Defaults: config`.  \n        create_transformer_per_time_series: If `True`, a separate transformer is created for each time series. Not used when using already initialized transformers. `Defaults: config`.  \n        partial_fit_initialized_transformers: If `True`, partial fitting on train set is performed when using initiliazed transformers. `Defaults: config`.  \n        workers: How many workers to use when setting new transformer. `Defaults: config`.      \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating transformer values.\")\n\n    self.update_dataset_config_and_initialize(transform_with=transform_with, create_transformer_per_time_series=create_transformer_per_time_series, partial_fit_initialized_transformers=partial_fit_initialized_transformers, workers=workers)\n    self.logger.info(\"Transformer configuration has been changed successfuly.\")\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.set_default_values","title":"set_default_values","text":"<pre><code>set_default_values(default_values: list[Number] | NDArray[number] | dict[str, Number] | Number | Literal['default'] | None, workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Used for updating default values set in config. Set parameter to <code>config</code> to keep it as it is config. If exception is thrown during set, no changes are made.</p> <p>Affects following configuration:</p> Dataset config Description <code>default_values</code> Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature. <p>Parameters:</p> Name Type Description Default <code>default_values</code> <code>list[Number] | NDArray[number] | dict[str, Number] | Number | Literal['default'] | None</code> <p>Default values for missing data, applied before fillers. <code>Defaults: config</code>.  </p> required <code>workers</code> <code>int | Literal['config']</code> <p>How many workers to use when setting new default values. <code>Defaults: config</code>.</p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def set_default_values(self, default_values: list[Number] | npt.NDArray[np.number] | dict[str, Number] | Number | Literal[\"default\"] | None, workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"Used for updating default values set in config.\n    Set parameter to `config` to keep it as it is config.\n    If exception is thrown during set, no changes are made.\n\n    Affects following configuration:\n\n    Dataset config | Description\n    -------------- | -----------\n    `default_values` | Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature.\n\n    Parameters:\n        default_values: Default values for missing data, applied before fillers. `Defaults: config`.  \n        workers: How many workers to use when setting new default values. `Defaults: config`.      \n    \"\"\"\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating default values.\")\n\n    self.update_dataset_config_and_initialize(default_values=default_values, workers=workers)\n    self.logger.info(\"Default values has been changed successfuly.\")\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.set_preprocess_order","title":"set_preprocess_order","text":"<pre><code>set_preprocess_order(preprocess_order: list[str, type] | Literal['config'] = 'config', workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Used for updating preprocess_order set in config. Set parameter to <code>config</code> to keep it as it is config. If exception is thrown during set, no changes are made.</p> <p>Affects following configuration: </p> Dataset config Description <code>preprocess_order</code> Used order of when preprocesses are applied. Can be also used to add/remove custom handlers. <p>Parameters:</p> Name Type Description Default <code>preprocess_order</code> <code>list[str, type] | Literal['config']</code> <p>Used order of when preprocesses are applied. Can be also used to add/remove custom handlers. <code>Defaults: config</code>.  </p> <code>'config'</code> <code>workers</code> <code>int | Literal['config']</code> <p>How many workers to use when setting new default values. <code>Defaults: config</code>.</p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def set_preprocess_order(self, preprocess_order: list[str, type] | Literal[\"config\"] = \"config\", workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"Used for updating preprocess_order set in config.\n    Set parameter to `config` to keep it as it is config.\n    If exception is thrown during set, no changes are made.\n\n    Affects following configuration: \n\n    Dataset config | Description\n    -------------- | -----------\n    `preprocess_order` | Used order of when preprocesses are applied. Can be also used to add/remove custom handlers.\n\n    Parameters:\n        preprocess_order: Used order of when preprocesses are applied. Can be also used to add/remove custom handlers. `Defaults: config`.  \n        workers: How many workers to use when setting new default values. `Defaults: config`.      \n    \"\"\"\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating preprocess order.\")\n\n    self.update_dataset_config_and_initialize(preprocess_order=preprocess_order, workers=workers)\n    self.logger.info(\"Preprocess order has been changed successfuly.\")\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.set_workers","title":"set_workers","text":"<pre><code>set_workers(train_workers: int | Literal['config'] = 'config', val_workers: int | Literal['config'] = 'config', test_workers: int | Literal['config'] = 'config', all_workers: int | Literal['config'] = 'config', init_workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Used for updating workers set in config. Set parameter to <code>config</code> to keep it as it is config. If exception is thrown during set, no changes are made.</p> <p>Affects following configuration:</p> Dataset config Description <code>train_workers</code> Number of workers for loading training data. <code>val_workers</code> Number of workers for loading validation data. <code>test_workers</code> Number of workers for loading test data. <code>all_workers</code> Number of workers for loading all data. <code>init_workers</code> Number of workers for dataset configuration. <p>Parameters:</p> Name Type Description Default <code>train_workers</code> <code>int | Literal['config']</code> <p>Number of workers for loading training data. <code>Defaults: config</code>.</p> <code>'config'</code> <code>val_workers</code> <code>int | Literal['config']</code> <p>Number of workers for loading validation data. <code>Defaults: config</code>.</p> <code>'config'</code> <code>test_workers</code> <code>int | Literal['config']</code> <p>Number of workers for loading test data. <code>Defaults: config</code>.</p> <code>'config'</code> <code>all_workers</code> <code>int | Literal['config']</code> <p>Number of workers for loading all data.  <code>Defaults: config</code>.</p> <code>'config'</code> <code>init_workers</code> <code>int | Literal['config']</code> <p>Number of workers for dataset configuration. <code>Defaults: config</code>.</p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def set_workers(self, train_workers: int | Literal[\"config\"] = \"config\", val_workers: int | Literal[\"config\"] = \"config\",\n                test_workers: int | Literal[\"config\"] = \"config\", all_workers: int | Literal[\"config\"] = \"config\", init_workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"Used for updating workers set in config.\n    Set parameter to `config` to keep it as it is config.\n    If exception is thrown during set, no changes are made.\n\n    Affects following configuration:\n\n    Dataset config | Description\n    -------------- | -----------\n    `train_workers` | Number of workers for loading training data.\n    `val_workers` | Number of workers for loading validation data.\n    `test_workers` | Number of workers for loading test data.\n    `all_workers` | Number of workers for loading all data.\n    `init_workers` | Number of workers for dataset configuration.\n\n    Parameters:\n        train_workers: Number of workers for loading training data. `Defaults: config`.\n        val_workers: Number of workers for loading validation data. `Defaults: config`.\n        test_workers: Number of workers for loading test data. `Defaults: config`.\n        all_workers: Number of workers for loading all data.  `Defaults: config`.\n        init_workers: Number of workers for dataset configuration. `Defaults: config`.            \n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating workers.\")\n\n    self.update_dataset_config_and_initialize(train_workers=train_workers, val_workers=val_workers, test_workers=test_workers, all_workers=all_workers, init_workers=init_workers, workers=\"config\")\n    self.logger.info(\"Workers has been changed successfuly.\")\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.set_batch_sizes","title":"set_batch_sizes","text":"<pre><code>set_batch_sizes(train_batch_size: int | Literal['config'] = 'config', val_batch_size: int | Literal['config'] = 'config', test_batch_size: int | Literal['config'] = 'config', all_batch_size: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Used for updating batch sizes set in config. Set parameter to <code>config</code> to keep it as it is config. If exception is thrown during set, no changes are made.</p> <p>Affects following configuration:</p> Dataset config Description <code>train_batch_size</code> Number of samples per batch for train set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>val_batch_size</code> Number of samples per batch for val set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>test_batch_size</code> Number of samples per batch for test set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <code>all_batch_size</code> Number of samples per batch for all set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details. <p>Parameters:</p> Name Type Description Default <code>train_batch_size</code> <code>int | Literal['config']</code> <p>Number of samples per batch for train set. <code>Defaults: config</code>.</p> <code>'config'</code> <code>val_batch_size</code> <code>int | Literal['config']</code> <p>Number of samples per batch for val set. <code>Defaults: config</code>.</p> <code>'config'</code> <code>test_batch_size</code> <code>int | Literal['config']</code> <p>Number of samples per batch for test set. <code>Defaults: config</code>.</p> <code>'config'</code> <code>all_batch_size</code> <code>int | Literal['config']</code> <p>Number of samples per batch for all set. <code>Defaults: config</code>.</p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def set_batch_sizes(self, train_batch_size: int | Literal[\"config\"] = \"config\", val_batch_size: int | Literal[\"config\"] = \"config\",\n                    test_batch_size: int | Literal[\"config\"] = \"config\", all_batch_size: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\"Used for updating batch sizes set in config.\n    Set parameter to `config` to keep it as it is config.\n    If exception is thrown during set, no changes are made.\n\n    Affects following configuration:\n\n    Dataset config | Description\n    -------------- | -----------\n    `train_batch_size` | Number of samples per batch for train set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `val_batch_size` | Number of samples per batch for val set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `test_batch_size` | Number of samples per batch for test set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n    `all_batch_size` | Number of samples per batch for all set. Affected by whether the dataset is series-based or time-based. Refer to relevant config for details.\n\n    Parameters:\n        train_batch_size: Number of samples per batch for train set. `Defaults: config`.\n        val_batch_size: Number of samples per batch for val set. `Defaults: config`.\n        test_batch_size: Number of samples per batch for test set. `Defaults: config`.\n        all_batch_size: Number of samples per batch for all set. `Defaults: config`.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized, use set_dataset_config_and_initialize() before updating batch sizes.\")\n\n    self.update_dataset_config_and_initialize(train_batch_size=train_batch_size, val_batch_size=val_batch_size, test_batch_size=test_batch_size, all_batch_size=all_batch_size, workers=\"config\")\n    self.logger.info(\"Batch sizes has been changed successfuly.\")\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.display_dataset_details","title":"display_dataset_details","text":"<pre><code>display_dataset_details() -&gt; None\n</code></pre> <p>Display information about the contents of the dataset.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>    def display_dataset_details(self) -&gt; None:\n        \"\"\"Display information about the contents of the dataset.  \"\"\"\n\n        to_display = f'''\nDataset details:\n\n    {self.metadata.aggregation}\n        Time indices: {range(self.metadata.time_indices[ID_TIME_COLUMN_NAME][0], self.metadata.time_indices[ID_TIME_COLUMN_NAME][-1])}\n        Datetime: {(datetime.fromtimestamp(self.metadata.time_indices['time'][0], tz=timezone.utc), datetime.fromtimestamp(self.metadata.time_indices['time'][-1], timezone.utc))}\n\n    {self.metadata.source_type}\n        Time series indices: {get_abbreviated_list_string(self.metadata.ts_indices[self.metadata.ts_id_name])}; use 'get_available_ts_indices' for full list\n        Features with default values: {self.metadata.default_values}\n\n        Additional data: {list(self.metadata.additional_data.keys())}\n        '''\n\n        print(to_display)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.summary","title":"summary","text":"<pre><code>summary(display_type: Literal['text', 'diagram']) -&gt; None\n</code></pre> <p>Used to display used configurations. Can be displayed as interactive html diagram or text summary.</p> <p>Parameters:</p> Name Type Description Default <code>display_type</code> <code>Literal['text', 'diagram']</code> <p>Whether configuration should be display as diagram or text summary.</p> required Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def summary(self, display_type: Literal[\"text\", \"diagram\"]) -&gt; None:\n    \"\"\"Used to display used configurations. Can be displayed as interactive html diagram or text summary.\n\n    Parameters:\n        display_type: Whether configuration should be display as diagram or text summary.\n    \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to display summary.\")\n\n    display_type = DisplayType(display_type)\n\n    if display_type == DisplayType.TEXT:\n        print(self.dataset_config)\n    elif display_type == DisplayType.DIAGRAM:\n        steps = self.dataset_config._get_summary_steps()\n        return css_utils.display_summary_diagram(steps)\n    else:\n        raise NotImplementedError()\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.save_summary_diagram_as_html","title":"save_summary_diagram_as_html","text":"<pre><code>save_summary_diagram_as_html(path: str)\n</code></pre> <p>Saves diagram produces from <code>summary</code> method as html file to specified path.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def save_summary_diagram_as_html(self, path: str):\n    \"\"\"Saves diagram produces from `summary` method as html file to specified path. \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to save summary diagram.\")\n\n    steps = self.dataset_config._get_summary_steps()\n    html = css_utils.get_summary_diagram(steps)\n\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        f.write(html)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_feature_names","title":"get_feature_names","text":"<pre><code>get_feature_names() -&gt; list[str]\n</code></pre> <p>Returns a list of all available feature names in the dataset.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_feature_names(self) -&gt; list[str]:\n    \"\"\"Returns a list of all available feature names in the dataset. \"\"\"\n\n    return list(self.metadata.features.keys())\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_available_ts_indices","title":"get_available_ts_indices","text":"<pre><code>get_available_ts_indices() -&gt; np.ndarray\n</code></pre> <p>Returns the available time series indices in this dataset.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_available_ts_indices(self) -&gt; np.ndarray:\n    \"\"\"Returns the available time series indices in this dataset. \"\"\"\n    return self.metadata.ts_indices\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_additional_data","title":"get_additional_data","text":"<pre><code>get_additional_data(data_name: str) -&gt; pd.DataFrame\n</code></pre> <p>Create a Pandas <code>DataFrame</code> of additional data of <code>data_name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data_name</code> <code>str</code> <p>Name of additional data to return.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Dataframe of additional data of <code>data_name</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_additional_data(self, data_name: str) -&gt; pd.DataFrame:\n    \"\"\"Create a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) of additional data of `data_name`.\n\n    Parameters:\n        data_name: Name of additional data to return.\n\n    Returns:\n        Dataframe of additional data of `data_name`.\n    \"\"\"\n\n    if data_name not in self.metadata.additional_data:\n        self.logger.error(\"%s is not available for this dataset.\", data_name)\n        raise ValueError(f\"{data_name} is not available for this dataset.\", f\"Possible options are: {self.metadata.additional_data}\")\n\n    data = get_additional_data(self.metadata.dataset_path, data_name)\n    data_df = pd.DataFrame(data)\n\n    for column, column_type in self.metadata.additional_data[data_name]:\n        if column_type == datetime:\n            data_df[column] = data_df[column].apply(lambda x: datetime.fromtimestamp(x, tz=timezone.utc))\n        else:\n            data_df[column] = data_df[column].astype(column_type)\n\n    return data_df\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.plot","title":"plot","text":"<pre><code>plot(ts_id: int, plot_type: Literal['scatter', 'line'], features: list[str] | str | Literal['config'] = 'config', feature_per_plot: bool = True, time_format: TimeFormat | Literal['config', 'id_time', 'datetime', 'unix_time', 'shifted_unix_time'] = 'config', is_interactive: bool = True) -&gt; None\n</code></pre> <p>Displays a graph for the selected <code>ts_id</code> and its <code>features</code>.</p> <p>The plotting is done using the <code>Plotly</code> library, which provides interactive graphs.</p> <p>Parameters:</p> Name Type Description Default <code>ts_id</code> <code>int</code> <p>The ID of the time series to display.</p> required <code>plot_type</code> <code>Literal['scatter', 'line']</code> <p>The type of graph to plot.</p> required <code>features</code> <code>list[str] | str | Literal['config']</code> <p>The features to display in the plot. <code>Defaults: \"config\"</code>.</p> <code>'config'</code> <code>feature_per_plot</code> <code>bool</code> <p>Whether each feature should be displayed in a separate plot or combined into one. <code>Defaults: True</code>.</p> <code>True</code> <code>time_format</code> <code>TimeFormat | Literal['config', 'id_time', 'datetime', 'unix_time', 'shifted_unix_time']</code> <p>The time format to use for the x-axis. <code>Defaults: \"config\"</code>.</p> <code>'config'</code> <code>is_interactive</code> <code>bool</code> <p>Whether the plot should be interactive (e.g., zoom, hover). <code>Defaults: True</code>.</p> <code>True</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def plot(self, ts_id: int, plot_type: Literal[\"scatter\", \"line\"], features: list[str] | str | Literal[\"config\"] = \"config\", feature_per_plot: bool = True,\n         time_format: TimeFormat | Literal[\"config\", \"id_time\", \"datetime\", \"unix_time\", \"shifted_unix_time\"] = \"config\", is_interactive: bool = True) -&gt; None:\n    \"\"\"\n    Displays a graph for the selected `ts_id` and its `features`.\n\n    The plotting is done using the [`Plotly`](https://plotly.com/python/) library, which provides interactive graphs.\n\n    Parameters:\n        ts_id: The ID of the time series to display.\n        plot_type: The type of graph to plot.\n        features: The features to display in the plot. `Defaults: \"config\"`.\n        feature_per_plot: Whether each feature should be displayed in a separate plot or combined into one. `Defaults: True`.\n        time_format: The time format to use for the x-axis. `Defaults: \"config\"`.\n        is_interactive: Whether the plot should be interactive (e.g., zoom, hover). `Defaults: True`.\n    \"\"\"\n\n    if time_format == \"config\":\n\n        if self.dataset_config is None or not self.dataset_config.is_initialized:\n            raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to plot.\")\n\n        time_format = self.dataset_config.time_format\n        self.logger.debug(\"Using time format from dataset configuration: %s\", time_format)\n    else:\n        time_format = TimeFormat(time_format)\n        self.logger.debug(\"Using specified time format: %s\", time_format)\n\n    time_series, times, features = self.__get_data_for_plot(ts_id, features, time_format)\n    self.logger.debug(\"Received data for plotting. Time series, times, and features are ready.\")\n\n    plots = []\n\n    if feature_per_plot:\n        self.logger.debug(\"Creating individual plots for each feature.\")\n        fig = make_subplots(rows=len(features), cols=1, shared_xaxes=False, x_title=time_format.value)\n\n        for i, feature in enumerate(features):\n            if plot_type == \"scatter\":\n                plot = go.Scatter(x=times, y=time_series[:, i], mode=\"markers\", name=feature, legendgroup=feature)\n                self.logger.debug(\"Creating scatter plot for feature: %s\", feature)\n            elif plot_type == \"line\":\n                plot = go.Scatter(x=times, y=time_series[:, i], mode=\"lines\", name=feature)\n                self.logger.debug(\"Creating line plot for feature: %s\", feature)\n            else:\n                raise ValueError(\"Invalid plot type.\")\n\n            fig.add_traces(plot, rows=i + 1, cols=1)\n\n        fig.update_layout(height=200 + 120 * len(features), width=2000, autosize=len(features) == 1, showlegend=True)\n        self.logger.debug(\"Created subplots for features: %s.\", features)\n    else:\n        self.logger.debug(\"Creating a combined plot for all features.\")\n        for i, feature in enumerate(features):\n            if plot_type == \"scatter\":\n                plot = go.Scatter(x=times, y=time_series[:, i], mode=\"markers\", name=feature)\n                self.logger.debug(\"Creating scatter plot for feature: %s\", feature)\n            elif plot_type == \"line\":\n                plot = go.Scatter(x=times, y=time_series[:, i], mode=\"lines\", name=feature)\n                self.logger.debug(\"Creating line plot for feature: %s\", feature)\n            else:\n                raise ValueError(\"Invalid plot type.\")\n            plots.append(plot)\n\n        fig = go.Figure(data=plots)\n        fig.update_layout(xaxis_title=time_format.value, showlegend=True, height=200 + 120 * 2)\n        self.logger.debug(\"Created combined plot for features: %s.\", features)\n\n    if not is_interactive:\n        self.logger.debug(\"Disabling interactivity for the plot.\")\n        fig.update_layout(updatemenus=[], dragmode=False, hovermode=False)\n\n    self.logger.debug(\"Displaying the plot.\")\n    fig.show()\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.add_annotation","title":"add_annotation","text":"<pre><code>add_annotation(annotation: str, annotation_group: str, ts_id: int | None, id_time: int | None, enforce_ids: bool = True) -&gt; None\n</code></pre> <p>Adds an annotation to the specified <code>annotation_group</code>.</p> <ul> <li>If the provided <code>annotation_group</code> does not exist, it will be created.</li> <li>At least one of <code>ts_id</code> or <code>id_time</code> must be provided to associate the annotation with time series or/and time point.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>annotation</code> <code>str</code> <p>The annotation to be added.</p> required <code>annotation_group</code> <code>str</code> <p>The group to which the annotation should be added.</p> required <code>ts_id</code> <code>int | None</code> <p>The time series ID to which the annotation should be added.</p> required <code>id_time</code> <code>int | None</code> <p>The time ID to which the annotation should be added.</p> required <code>enforce_ids</code> <code>bool</code> <p>Flag indicating whether the <code>ts_id</code> and <code>id_time</code> must belong to this dataset. <code>Default: True</code></p> <code>True</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def add_annotation(self, annotation: str, annotation_group: str, ts_id: int | None, id_time: int | None, enforce_ids: bool = True) -&gt; None:\n    \"\"\" \n    Adds an annotation to the specified `annotation_group`.\n\n    - If the provided `annotation_group` does not exist, it will be created.\n    - At least one of `ts_id` or `id_time` must be provided to associate the annotation with time series or/and time point.\n\n    Parameters:\n        annotation: The annotation to be added.\n        annotation_group: The group to which the annotation should be added.\n        ts_id: The time series ID to which the annotation should be added.\n        id_time: The time ID to which the annotation should be added.\n        enforce_ids: Flag indicating whether the `ts_id` and `id_time` must belong to this dataset. `Default: True`  \n    \"\"\"\n\n    if enforce_ids:\n        self._validate_annotation_ids(ts_id, id_time)\n    self.annotations.add_annotation(annotation, annotation_group, ts_id, id_time)\n\n    if ts_id is not None and id_time is not None:\n        self._update_annotations_imported_status(AnnotationType.BOTH, None)\n    elif ts_id is not None and id_time is None:\n        self._update_annotations_imported_status(AnnotationType.TS_ID, None)\n    elif ts_id is None and id_time is not None:\n        self._update_annotations_imported_status(AnnotationType.ID_TIME, None)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.remove_annotation","title":"remove_annotation","text":"<pre><code>remove_annotation(annotation_group: str, ts_id: int | None, id_time: int | None) -&gt; None\n</code></pre> <p>Removes an annotation from the specified <code>annotation_group</code>.</p> <ul> <li>At least one of <code>ts_id</code> or <code>id_time</code> must be provided to associate the annotation with time series or/and time point.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>annotation_group</code> <code>str</code> <p>The annotation group from which the annotation should be removed.</p> required <code>ts_id</code> <code>int | None</code> <p>The time series ID from which the annotation should be removed.</p> required <code>id_time</code> <code>int | None</code> <p>The time ID from which the annotation should be removed.</p> required Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def remove_annotation(self, annotation_group: str, ts_id: int | None, id_time: int | None) -&gt; None:\n    \"\"\"  \n    Removes an annotation from the specified `annotation_group`.\n\n    - At least one of `ts_id` or `id_time` must be provided to associate the annotation with time series or/and time point.\n\n    Parameters:\n        annotation_group: The annotation group from which the annotation should be removed.\n        ts_id: The time series ID from which the annotation should be removed.\n        id_time: The time ID from which the annotation should be removed. \n    \"\"\"\n\n    self.annotations.remove_annotation(annotation_group, ts_id, id_time, False)\n\n    if ts_id is not None and id_time is not None:\n        self._update_annotations_imported_status(AnnotationType.BOTH, None)\n    elif ts_id is not None and id_time is None:\n        self._update_annotations_imported_status(AnnotationType.TS_ID, None)\n    elif ts_id is None and id_time is not None:\n        self._update_annotations_imported_status(AnnotationType.ID_TIME, None)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.add_annotation_group","title":"add_annotation_group","text":"<pre><code>add_annotation_group(annotation_group: str, on: AnnotationType | Literal['id_time', 'ts_id', 'both'])\n</code></pre> <p>Adds a new <code>annotation_group</code>.</p> <p>Parameters:</p> Name Type Description Default <code>annotation_group</code> <code>str</code> <p>The name of the annotation group to be added.</p> required <code>on</code> <code>AnnotationType | Literal['id_time', 'ts_id', 'both']</code> <p>Specifies which part of the data should be annotated. If set to <code>\"both\"</code>, annotations will be applied as if <code>id_time</code> and <code>ts_id</code> were both set.</p> required Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def add_annotation_group(self, annotation_group: str, on: AnnotationType | Literal[\"id_time\", \"ts_id\", \"both\"]):\n    \"\"\" \n    Adds a new `annotation_group`.\n\n    Parameters:\n        annotation_group: The name of the annotation group to be added.\n        on: Specifies which part of the data should be annotated. If set to `\"both\"`, annotations will be applied as if `id_time` and `ts_id` were both set.\n    \"\"\"\n    on = AnnotationType(on)\n\n    self.annotations.add_annotation_group(annotation_group, on, False)\n\n    self._update_annotations_imported_status(on, None)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.remove_annotation_group","title":"remove_annotation_group","text":"<pre><code>remove_annotation_group(annotation_group: str, on: AnnotationType | Literal['id_time', 'ts_id', 'both'])\n</code></pre> <p>Removes the specified <code>annotation_group</code>.</p> <p>Parameters:</p> Name Type Description Default <code>annotation_group</code> <code>str</code> <p>The name of the annotation group to be removed.</p> required <code>on</code> <code>AnnotationType | Literal['id_time', 'ts_id', 'both']</code> <p>Specifies which part of the data the <code>annotation_group</code> should be removed from. If set to <code>\"both\"</code>, annotations will be applied as if <code>id_time</code> and <code>ts_id</code> were both set.</p> required Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def remove_annotation_group(self, annotation_group: str, on: AnnotationType | Literal[\"id_time\", \"ts_id\", \"both\"]):\n    \"\"\" \n    Removes the specified `annotation_group`.\n\n    Parameters:\n        annotation_group: The name of the annotation group to be removed.\n        on: Specifies which part of the data the `annotation_group` should be removed from. If set to `\"both\"`, annotations will be applied as if `id_time` and `ts_id` were both set.        \n    \"\"\"\n    on = AnnotationType(on)\n\n    self.annotations.remove_annotation_group(annotation_group, on, False)\n\n    self._update_annotations_imported_status(on, None)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_annotations","title":"get_annotations","text":"<pre><code>get_annotations(on: AnnotationType | Literal['id_time', 'ts_id', 'both']) -&gt; pd.DataFrame\n</code></pre> <p>Returns the annotations as a Pandas <code>DataFrame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>on</code> <code>AnnotationType | Literal['id_time', 'ts_id', 'both']</code> <p>Specifies which annotations to return. If set to <code>\"both\"</code>, annotations will be applied as if <code>id_time</code> and <code>ts_id</code> were both set.         </p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A Pandas DataFrame containing the selected annotations.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_annotations(self, on: AnnotationType | Literal[\"id_time\", \"ts_id\", \"both\"]) -&gt; pd.DataFrame:\n    \"\"\" \n    Returns the annotations as a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html).\n\n    Parameters:\n        on: Specifies which annotations to return. If set to `\"both\"`, annotations will be applied as if `id_time` and `ts_id` were both set.         \n\n    Returns:\n        A Pandas DataFrame containing the selected annotations.      \n    \"\"\"\n    on = AnnotationType(on)\n\n    return self.annotations.get_annotations(on, self.metadata.ts_id_name)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.import_annotations","title":"import_annotations","text":"<pre><code>import_annotations(identifier: str, enforce_ids: bool = True) -&gt; None\n</code></pre> <p>Imports annotations from a CSV file.</p> <p>First, it attempts to load the built-in annotations, if no built-in annotations with such an identifier exists, it attempts to load a custom annotations from the <code>\"data_root\"/tszoo/annotations/</code> directory.</p> <p><code>data_root</code> is specified when the dataset is created.     </p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The name of the CSV file.     </p> required <code>enforce_ids</code> <code>bool</code> <p>Flag indicating whether the <code>ts_id</code> and <code>id_time</code> must belong to this dataset. <code>Default: True</code></p> <code>True</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def import_annotations(self, identifier: str, enforce_ids: bool = True) -&gt; None:\n    \"\"\" \n    Imports annotations from a CSV file.\n\n    First, it attempts to load the built-in annotations, if no built-in annotations with such an identifier exists, it attempts to load a custom annotations from the `\"data_root\"/tszoo/annotations/` directory.\n\n    `data_root` is specified when the dataset is created.     \n\n    Parameters:\n        identifier: The name of the CSV file.     \n        enforce_ids: Flag indicating whether the `ts_id` and `id_time` must belong to this dataset. `Default: True`                \n    \"\"\"\n\n    annotations_file_path, is_built_in = get_annotations_path_and_whether_it_is_built_in(identifier, self.metadata.annotations_root, self.logger)\n\n    if is_built_in:\n        self.logger.info(\"Built-in annotations found: %s.\", identifier)\n        if not os.path.exists(annotations_file_path):\n            self.logger.info(\"Downloading annotations with identifier: %s\", identifier)\n            annotations_url = f\"{ANNOTATIONS_DOWNLOAD_BUCKET}&amp;file={identifier}\"  # probably will change annotations bucket... placeholder\n            resumable_download(url=annotations_url, file_path=annotations_file_path, silent=False)\n\n        self.logger.debug(\"Loading annotations from %s\", annotations_file_path)\n        temp_df = pd.read_csv(annotations_file_path)\n        self.logger.debug(\"Created DataFrame from file: %s\", annotations_file_path)\n    else:\n        self.logger.info(\"Custom annotations found: %s.\", identifier)\n        self.logger.debug(\"Loading annotations from %s\", annotations_file_path)\n        temp_df = pd.read_csv(annotations_file_path)\n        self.logger.debug(\"Created DataFrame from file: %s\", annotations_file_path)\n\n    ts_id_index = None\n    time_id_index = None\n    on = None\n\n    # Check the columns of the DataFrame to identify the type of annotation\n    if self.metadata.ts_id_name in temp_df.columns and ID_TIME_COLUMN_NAME in temp_df.columns:\n        self.annotations.clear_time_in_time_series()\n        time_id_index = temp_df.columns.tolist().index(ID_TIME_COLUMN_NAME)\n        ts_id_index = temp_df.columns.tolist().index(self.metadata.ts_id_name)\n        on = AnnotationType.BOTH\n        self.logger.info(\"Annotations detected as %s (both %s and id_time)\", AnnotationType.BOTH, self.metadata.ts_id_name)\n\n    elif self.metadata.ts_id_name in temp_df.columns:\n        self.annotations.clear_time_series()\n        ts_id_index = temp_df.columns.tolist().index(self.metadata.ts_id_name)\n        on = AnnotationType.TS_ID\n        self.logger.info(\"Annotations detected as %s (%s only)\", AnnotationType.TS_ID, self.metadata.ts_id_name)\n\n    elif ID_TIME_COLUMN_NAME in temp_df.columns:\n        self.annotations.clear_time()\n        time_id_index = temp_df.columns.tolist().index(ID_TIME_COLUMN_NAME)\n        on = AnnotationType.ID_TIME\n        self.logger.info(\"Annotations detected as %s (%s only)\", AnnotationType.ID_TIME, ID_TIME_COLUMN_NAME)\n\n    else:\n        raise ValueError(f\"Could not find {self.metadata.ts_id_name} and {ID_TIME_COLUMN_NAME} in the imported CSV.\")\n\n    # Process each row in the DataFrame and add annotations\n    for row in temp_df.itertuples(False):\n        for i, _ in enumerate(temp_df.columns):\n            if i == time_id_index or i == ts_id_index:\n                continue\n\n            ts_id = None\n            if ts_id_index is not None:\n                ts_id = row[ts_id_index]\n\n            id_time = None\n            if time_id_index is not None:\n                id_time = row[time_id_index]\n\n            self.add_annotation(row[i], temp_df.columns[i], ts_id, id_time, enforce_ids)\n\n    self._update_annotations_imported_status(on, identifier)\n    self.logger.info(\"Successfully imported annotations from %s\", annotations_file_path)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.import_config","title":"import_config","text":"<pre><code>import_config(identifier: str, display_config_details: Optional[Literal['text', 'diagram']] = 'text', workers: int | Literal['config'] = 'config') -&gt; None\n</code></pre> <p>Import the dataset_config from a pickle file and initializes the dataset. Config type must correspond to dataset type.</p> <p>First, it attempts to load the built-in config, if no built-in config with such an identifier exists, it attempts to load a custom config from the <code>\"data_root\"/tszoo/configs/</code> directory.</p> <p><code>data_root</code> is specified when the dataset is created.       </p> <p>The following configuration attributes are used during initialization:</p> Dataset config Description <code>init_workers</code> Specifies the number of workers to use for initialization. Applied when <code>workers</code> = \"config\". <code>partial_fit_initialized_transformers</code> Determines whether initialized transformers should be partially fitted on the training data. <code>nan_threshold</code> Filters out time series with missing values exceeding the specified threshold. <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>Name of the pickle file.</p> required <code>display_config_details</code> <code>Optional[Literal['text', 'diagram']]</code> <p>Flag indicating whether to display the configuration values after initialization. <code>Default: True</code> </p> <code>'text'</code> <code>workers</code> <code>int | Literal['config']</code> <p>The number of workers to use during initialization. <code>Default: \"config\"</code></p> <code>'config'</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def import_config(self, identifier: str, display_config_details: Optional[Literal[\"text\", \"diagram\"]] = \"text\", workers: int | Literal[\"config\"] = \"config\") -&gt; None:\n    \"\"\" \n    Import the dataset_config from a pickle file and initializes the dataset. Config type must correspond to dataset type.\n\n    First, it attempts to load the built-in config, if no built-in config with such an identifier exists, it attempts to load a custom config from the `\"data_root\"/tszoo/configs/` directory.\n\n    `data_root` is specified when the dataset is created.       \n\n    The following configuration attributes are used during initialization:\n\n    Dataset config | Description\n    -------------- | -----------\n    `init_workers` | Specifies the number of workers to use for initialization. Applied when `workers` = \"config\".\n    `partial_fit_initialized_transformers` | Determines whether initialized transformers should be partially fitted on the training data.\n    `nan_threshold` | Filters out time series with missing values exceeding the specified threshold.\n\n    Parameters:\n        identifier: Name of the pickle file.\n        display_config_details: Flag indicating whether to display the configuration values after initialization. `Default: True` \n        workers: The number of workers to use during initialization. `Default: \"config\"`  \n    \"\"\"\n\n    if display_config_details is not None:\n        display_config_details = DisplayType(display_config_details)\n\n    # Load config\n    config = load_config(identifier, self.metadata.configs_root, self.metadata.database_name, self.metadata.source_type, self.metadata.aggregation, self.logger)\n\n    self.logger.info(\"Initializing dataset configuration with the imported config.\")\n    self.set_dataset_config_and_initialize(config, display_config_details, workers)\n\n    self._update_config_imported_status(identifier)\n    self.logger.info(\"Successfully used config with identifier %s\", identifier)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.save_annotations","title":"save_annotations","text":"<pre><code>save_annotations(identifier: str, on: AnnotationType | Literal['id_time', 'ts_id', 'both'], force_write: bool = False) -&gt; None\n</code></pre> <p>Saves the annotations as a CSV file.</p> <p>The file will be saved to a path determined by the <code>data_root</code> specified when the dataset was created.</p> <p>The annotations will be saved under the directory <code>data_root/tszoo/annotations/</code>.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The name of the CSV file.</p> required <code>on</code> <code>AnnotationType | Literal['id_time', 'ts_id', 'both']</code> <p>What annotation type should be saved. If set to <code>\"both\"</code>, annotations will be applied as if <code>id_time</code> and <code>ts_id</code> were both set.   </p> required <code>force_write</code> <code>bool</code> <p>If set to <code>True</code>, will overwrite any existing files with the same name. <code>Default: False</code></p> <code>False</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def save_annotations(self, identifier: str, on: AnnotationType | Literal[\"id_time\", \"ts_id\", \"both\"], force_write: bool = False) -&gt; None:\n    \"\"\" \n    Saves the annotations as a CSV file.\n\n    The file will be saved to a path determined by the `data_root` specified when the dataset was created.\n\n    The annotations will be saved under the directory `data_root/tszoo/annotations/`.\n\n    Parameters:\n        identifier: The name of the CSV file.\n        on: What annotation type should be saved. If set to `\"both\"`, annotations will be applied as if `id_time` and `ts_id` were both set.   \n        force_write: If set to `True`, will overwrite any existing files with the same name. `Default: False`               \n    \"\"\"\n\n    if exists_built_in_annotations(identifier):\n        raise ValueError(\"Built-in annotations with this identifier already exists. Choose another identifier.\")\n\n    on = AnnotationType(on)\n\n    temp_df = self.get_annotations(on)\n\n    # Ensure the annotations root directory exists, creating it if necessary\n    if not os.path.exists(self.metadata.annotations_root):\n        os.makedirs(self.metadata.annotations_root)\n        self.logger.info(\"Created annotations directory at %s\", self.metadata.annotations_root)\n\n    path = os.path.join(self.metadata.annotations_root, f\"{identifier}.csv\")\n\n    if os.path.exists(path) and not force_write:\n        raise ValueError(f\"Annotations already exist at {path}. Set force_write=True to overwrite.\")\n    self.logger.debug(\"Annotations CSV file path: %s\", path)\n\n    temp_df.to_csv(path, index=False)\n\n    self._update_annotations_imported_status(on, identifier)\n    self.logger.info(\"Annotations successfully saved to %s\", path)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.save_config","title":"save_config","text":"<pre><code>save_config(identifier: str, create_with_details_file: bool = True, force_write: bool = False, **kwargs) -&gt; None\n</code></pre> <p>Saves the config as a pickle file.</p> <p>The file will be saved to a path determined by the <code>data_root</code> specified when the dataset was created.  The config will be saved under the directory <code>data_root/tszoo/configs/</code>.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The name of the pickle file.</p> required <code>create_with_details_file</code> <code>bool</code> <p>Whether to export the config along with a readable text file that provides details. <code>Defaults: True</code>. </p> <code>True</code> <code>force_write</code> <code>bool</code> <p>If set to <code>True</code>, will overwrite any existing files with the same name. <code>Default: False</code></p> <code>False</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def save_config(self, identifier: str, create_with_details_file: bool = True, force_write: bool = False, **kwargs) -&gt; None:\n    \"\"\" \n    Saves the config as a pickle file.\n\n    The file will be saved to a path determined by the `data_root` specified when the dataset was created. \n    The config will be saved under the directory `data_root/tszoo/configs/`.\n\n    Parameters:\n        identifier: The name of the pickle file.\n        create_with_details_file: Whether to export the config along with a readable text file that provides details. `Defaults: True`. \n        force_write: If set to `True`, will overwrite any existing files with the same name. `Default: False`            \n    \"\"\"\n\n    default_kwargs = {'hard_force': False}\n    kwargs = {**default_kwargs, **kwargs}\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to save config.\")\n\n    if not kwargs[\"hard_force\"] and exists_built_in_config(identifier):\n        raise ValueError(\"Built-in config with this identifier already exists. Choose another identifier.\")\n\n    # Ensure the config directory exists\n    if not os.path.exists(self.metadata.configs_root):\n        os.makedirs(self.metadata.configs_root)\n        self.logger.info(\"Created config directory at %s\", self.metadata.configs_root)\n\n    path_pickle = os.path.join(self.metadata.configs_root, f\"{identifier}.pickle\")\n    path_details = os.path.join(self.metadata.configs_root, f\"{identifier}.txt\")\n\n    if os.path.exists(path_pickle) and not force_write:\n        raise ValueError(f\"Config at path {path_pickle} already exists. Set force_write=True to overwrite.\")\n    self.logger.debug(\"Config pickle path: %s\", path_pickle)\n\n    if create_with_details_file:\n        if os.path.exists(path_details) and not force_write:\n            raise ValueError(f\"Config details at path {path_details} already exists. Set force_write=True to overwrite.\")\n        self.logger.debug(\"Config details path: %s\", path_details)\n\n    if not self.dataset_config.filler_factory.creates_built_in:\n        self.logger.warning(\"You are using a custom filler. Ensure the config is distributed with the source code of the filler.\")\n\n    if not self.dataset_config.anomaly_handler_factory.creates_built_in:\n        self.logger.warning(\"You are using a custom anomaly handler. Ensure the config is distributed with the source code of the anomaly handler.\")\n\n    if not self.dataset_config.transformer_factory.creates_built_in:\n        self.logger.warning(\"You are using a custom transformer. Ensure the config is distributed with the source code of the transformer.\")\n\n    if len(self.dataset_config.preprocess_order) != len(MANDATORY_PREPROCESSES_ORDER):\n        self.logger.warning(\"You are using at least one custom handler. Ensure the config is distributed with the source code of every custom handler.\")\n\n    pickle_dump(self._export_config_copy, path_pickle)\n    self.logger.info(\"Config pickle saved to %s\", path_pickle)\n\n    if create_with_details_file:\n        with open(path_details, \"w\", encoding=\"utf-8\") as file:\n            file.write(str(self.dataset_config))\n        self.logger.info(\"Config details saved to %s\", path_details)\n\n    self._update_config_imported_status(identifier)\n    self.dataset_config.export_update_needed = False\n    self.logger.info(\"Config successfully saved\")\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.save_benchmark","title":"save_benchmark","text":"<pre><code>save_benchmark(identifier: str, force_write: bool = False, **kwargs) -&gt; None\n</code></pre> <p>Saves the benchmark as a YAML file.</p> <p>The benchmark, along with any associated annotations and config files, will be saved in a path determined by the <code>data_root</code> specified when creating the dataset.  The default save path for benchmark is <code>\"data_root/tszoo/benchmarks/\"</code>.</p> <p>If you are using imported <code>annotations</code> or <code>config</code> (whether custom or built-in), their file names will be set in the <code>benchmark</code> file.  If new <code>annotations</code> or <code>config</code> are created during the process, their filenames will be derived from the provided <code>identifier</code> and set in the <code>benchmark</code> file.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The name of the YAML file.</p> required <code>force_write</code> <code>bool</code> <p>If set to <code>True</code>, will overwrite any existing files with the same name. <code>Default: False</code></p> <code>False</code> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def save_benchmark(self, identifier: str, force_write: bool = False, **kwargs) -&gt; None:\n    \"\"\" \n    Saves the benchmark as a YAML file.\n\n    The benchmark, along with any associated annotations and config files, will be saved in a path determined by the `data_root` specified when creating the dataset. \n    The default save path for benchmark is `\"data_root/tszoo/benchmarks/\"`.\n\n    If you are using imported `annotations` or `config` (whether custom or built-in), their file names will be set in the `benchmark` file. \n    If new `annotations` or `config` are created during the process, their filenames will be derived from the provided `identifier` and set in the `benchmark` file.\n\n    Parameters:\n        identifier: The name of the YAML file.\n        force_write: If set to `True`, will overwrite any existing files with the same name. `Default: False`            \n    \"\"\"\n\n    default_kwargs = {'hard_force': False}\n    kwargs = {**default_kwargs, **kwargs}\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting to save benchmark.\")\n\n    if not kwargs[\"hard_force\"] and exists_built_in_benchmark(identifier):\n        raise ValueError(\"Built-in benchmark with this identifier already exists. Choose another identifier.\")\n\n    # Determine annotation names based on the available annotations and whether the annotations were imported\n    if len(self.annotations.time_series_annotations) &gt; 0:\n        annotations_ts_name = self.imported_annotations_ts_identifier if self.imported_annotations_ts_identifier is not None else f\"{identifier}_{AnnotationType.TS_ID.value}\"\n    else:\n        annotations_ts_name = None\n\n    if len(self.annotations.time_annotations) &gt; 0:\n        annotations_time_name = self.imported_annotations_time_identifier if self.imported_annotations_time_identifier is not None else f\"{identifier}_{AnnotationType.ID_TIME.value}\"\n    else:\n        annotations_time_name = None\n\n    if len(self.annotations.time_in_series_annotations) &gt; 0:\n        annotations_both_name = self.imported_annotations_both_identifier if self.imported_annotations_both_identifier is not None else f\"{identifier}_{AnnotationType.BOTH.value}\"\n    else:\n        annotations_both_name = None\n\n    # Use the imported identifier if available and update is not necessary, otherwise default to the current identifier\n    config_name = self.dataset_config.import_identifier if (self.dataset_config.import_identifier is not None and not self.dataset_config.export_update_needed) else identifier\n\n    export_benchmark = ExportBenchmark(self.metadata.database_name,\n                                       self.metadata.source_type.value,\n                                       self.metadata.aggregation.value,\n                                       self.metadata.dataset_type.value,\n                                       config_name,\n                                       annotations_ts_name,\n                                       annotations_time_name,\n                                       annotations_both_name,\n                                       related_results_identifier=self.related_to,\n                                       version=version.config_and_benchmarks_current_version)\n\n    # If the config was not imported, save it\n    if self.dataset_config.import_identifier is None or self.dataset_config.export_update_needed:\n        self.save_config(export_benchmark.config_identifier, force_write=force_write, hard_force=kwargs[\"hard_force\"])\n    else:\n        self.logger.info(\"Using already existing config with identifier: %s\", self.dataset_config.import_identifier)\n\n    # Save ts_id annotations if available and not previously imported\n    if self.imported_annotations_ts_identifier is None and len(self.annotations.time_series_annotations) &gt; 0:\n        self.save_annotations(export_benchmark.annotations_ts_identifier, AnnotationType.TS_ID, force_write=force_write)\n    elif self.imported_annotations_ts_identifier is not None:\n        self.logger.info(\"Using already existing annotations with identifier: %s; type: %s\", self.imported_annotations_ts_identifier, AnnotationType.TS_ID)\n\n    # Save id_time annotations if available and not previously imported\n    if self.imported_annotations_time_identifier is None and len(self.annotations.time_annotations) &gt; 0:\n        self.save_annotations(export_benchmark.annotations_time_identifier, AnnotationType.ID_TIME, force_write=force_write)\n    elif self.imported_annotations_time_identifier is not None:\n        self.logger.info(\"Using already existing annotations with identifier: %s; type: %s\", self.imported_annotations_time_identifier, AnnotationType.ID_TIME)\n\n    # Save both annotations if available and not previously imported\n    if self.imported_annotations_both_identifier is None and len(self.annotations.time_in_series_annotations) &gt; 0:\n        self.save_annotations(export_benchmark.annotations_both_identifier, AnnotationType.BOTH, force_write=force_write)\n    elif self.imported_annotations_both_identifier is not None:\n        self.logger.info(\"Using already existing annotations with identifier: %s; type: %s\", self.imported_annotations_both_identifier, AnnotationType.BOTH)\n\n    # Ensure the benchmark directory exists\n    if not os.path.exists(self.metadata.benchmarks_root):\n        os.makedirs(self.metadata.benchmarks_root)\n        self.logger.info(\"Created benchmarks directory at %s\", self.metadata.benchmarks_root)\n\n    benchmark_path = os.path.join(self.metadata.benchmarks_root, f\"{identifier}.yaml\")\n\n    if os.path.exists(benchmark_path) and not force_write:\n        self.logger.error(\"Benchmark file already exists at %s\", benchmark_path)\n        raise ValueError(f\"Benchmark at path {benchmark_path} already exists. Set force_write=True to overwrite.\")\n    self.logger.debug(\"Benchmark YAML file path: %s\", benchmark_path)\n\n    yaml_dump(export_benchmark.to_dict(), benchmark_path)\n    self.logger.info(\"Benchmark successfully saved to %s\", benchmark_path)\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.get_transformers","title":"get_transformers","text":"<pre><code>get_transformers() -&gt; np.ndarray[Transformer] | Transformer | None\n</code></pre> <p>Returns used transformers from config.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def get_transformers(self) -&gt; np.ndarray[Transformer] | Transformer | None:\n    \"\"\"Returns used transformers from config. \"\"\"\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before attempting get transformers.\")\n\n    for i, preprocess_type in enumerate(self.dataset_config.preprocess_order):\n        if preprocess_type == PreprocessType.TRANSFORMING:\n            holder: TransformerHolder = self.dataset_config.train_preprocess_order[i].holder\n            return holder.transformers\n\n    return None\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.check_errors","title":"check_errors","text":"<pre><code>check_errors() -&gt; None\n</code></pre> <p>Validates whether the dataset is corrupted. </p> <p>Raises an exception if corrupted.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def check_errors(self) -&gt; None:\n    \"\"\"\n    Validates whether the dataset is corrupted. \n\n    Raises an exception if corrupted.\n    \"\"\"\n\n    dataset, _ = load_database(self.metadata.dataset_path)\n\n    try:\n        node_iter = dataset.walk_nodes()\n\n        # Process each node in the dataset\n        for node in node_iter:\n            if isinstance(node, tb.Table):\n\n                iter_by = min(LOADING_WARNING_THRESHOLD, len(node))\n                iters_done = 0\n\n                # Process the node in chunks to avoid memory issues\n                while iters_done &lt; len(node):\n                    iter_by = min(LOADING_WARNING_THRESHOLD, len(node) - iters_done)\n                    _ = node[iters_done: iters_done + iter_by]  # Fetch the data in chunks\n                    iters_done += iter_by\n\n                self.logger.info(\"Table '%s' checked successfully. (%d rows processed)\", node._v_pathname, len(node))\n\n        self.logger.info(\"Dataset check completed with no errors found.\")\n\n    except Exception as e:\n        self.logger.error(\"Error encountered during dataset check: %s\", str(e))\n\n    finally:\n        dataset.close()\n        self.logger.debug(\"Dataset connection closed.\")\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset.__get_data_for_plot","title":"__get_data_for_plot","text":"<pre><code>__get_data_for_plot(ts_id: int, features: list[str] | str, time_format: TimeFormat) -&gt; tuple[np.ndarray, np.ndarray, list[str]]\n</code></pre> <p>Returns prepared data for plotting.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def __get_data_for_plot(self, ts_id: int, features: list[str] | str, time_format: TimeFormat) -&gt; tuple[np.ndarray, np.ndarray, list[str]]:\n    \"\"\"Returns prepared data for plotting. \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before getting data for plotting.\")\n\n    features_indices = []\n\n    if features == \"config\":\n        features = deepcopy(self.dataset_config.features_to_take_without_ids)\n        features_indices = np.arange(len(features))\n        self.logger.debug(\"Features set from dataset config: %s\", features)\n    else:\n        if isinstance(features, str):\n            features = [features]\n\n        if len(features) == 0:\n            raise ValueError(\"No features specified to plot. Please provide valid features.\")\n        if len(set(features)) != len(features):\n            raise ValueError(\"Duplicate features detected. All features must be unique.\")\n\n        for feature in features:\n            if feature not in self.dataset_config.features_to_take_without_ids:\n                raise ValueError(f\"Feature '{feature}' is not valid. It is not present in the dataset configuration.\", self.dataset_config.features_to_take_without_ids)\n\n            index_in_config_features = self.dataset_config.features_to_take_without_ids.index(feature)\n            features_indices.append(index_in_config_features)\n\n    real_feature_indices = np.array(self.dataset_config.indices_of_features_to_take_no_ids)[features_indices]\n    real_feature_indices = real_feature_indices.astype(int)\n\n    time_series, time_period = self._get_data_for_plot(ts_id, real_feature_indices, time_format)\n    self.logger.debug(\"Time series data and corresponding time values retrieved.\")\n\n    return time_series, time_period, features\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset._validate_annotation_ids","title":"_validate_annotation_ids","text":"<pre><code>_validate_annotation_ids(ts_id: int | None, id_time: int | None) -&gt; None\n</code></pre> <p>Validates whether the <code>ts_id</code> and <code>id_time</code> belong to this dataset.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def _validate_annotation_ids(self, ts_id: int | None, id_time: int | None) -&gt; None:\n    \"\"\"Validates whether the `ts_id` and `id_time` belong to this dataset. \"\"\"\n\n    assert ts_id is not None or id_time is not None, \"Either ts_id or id_time must be provided.\"\n\n    # Handle when id_time is provided\n    if id_time is not None:\n        time_indices = self.metadata.time_indices\n        if id_time &lt; time_indices[ID_TIME_COLUMN_NAME][0] or id_time &gt; time_indices[ID_TIME_COLUMN_NAME][-1]:\n            valid_range = range(time_indices[ID_TIME_COLUMN_NAME][0], time_indices[ID_TIME_COLUMN_NAME][-1])\n            raise ValueError(f\"id_time {id_time} does not fall within the valid range for {self.metadata.aggregation}. \"\n                             f\"Valid id_time range: {valid_range}.\")\n\n    # Handle when ts_id is provided\n    if ts_id is not None:\n        ts_indices = self.metadata.ts_indices[self.metadata.ts_id_name]\n\n        if ts_id not in ts_indices:\n            valid_ts_range = self.metadata.ts_indices[self.metadata.ts_id_name]\n            raise ValueError(f\"ts_id {ts_id} does not exist in the available range for {self.metadata.source_type}. \"\n                             f\"Valid ts_id values: {valid_ts_range}.\")\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset._get_df","title":"_get_df","text":"<pre><code>_get_df(dataloader: DataLoader, as_single_dataframe: bool, ts_ids: ndarray, time_period: ndarray) -&gt; pd.DataFrame\n</code></pre> <p>Returns all data from the DataLoader as a Pandas <code>DataFrame</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def _get_df(self, dataloader: DataLoader, as_single_dataframe: bool, ts_ids: np.ndarray, time_period: np.ndarray) -&gt; pd.DataFrame:\n    \"\"\"Returns all data from the DataLoader as a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html). \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before getting DataFrame.\")\n\n    total_samples = len(ts_ids) * len(time_period)\n    if total_samples &gt;= LOADING_WARNING_THRESHOLD:\n        self.logger.warning(\"The dataset contains %d samples (%d time series \u00d7 %d times). Consider using get_*_dataloader() for batch loading.\", total_samples, len(ts_ids), len(time_period))\n\n    if as_single_dataframe:\n        self.logger.debug(\"Returning a single DataFrame with all features for all time series.\")\n        return dataset_loaders.create_single_df_from_dataloader(\n            dataloader,\n            ts_ids,\n            self.dataset_config.features_to_take,\n            self.dataset_config.time_format,\n            self.dataset_config.include_ts_id,\n            self.dataset_config.include_time,\n            self.dataset_config.dataset_type,\n            True\n        )\n    else:\n        self.logger.debug(\"Returning multiple DataFrames, one per time series.\")\n        return dataset_loaders.create_multiple_df_from_dataloader(\n            dataloader,\n            ts_ids,\n            self.dataset_config.features_to_take,\n            self.dataset_config.time_format,\n            self.dataset_config.include_ts_id,\n            self.dataset_config.include_time,\n            self.dataset_config.dataset_type,\n            True\n        )\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset._get_numpy","title":"_get_numpy","text":"<pre><code>_get_numpy(dataloader: DataLoader, ts_ids: ndarray, time_period: ndarray) -&gt; np.ndarray\n</code></pre> <p>Returns all data from the DataLoader as a NumPy <code>ndarray</code>.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def _get_numpy(self, dataloader: DataLoader, ts_ids: np.ndarray, time_period: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Returns all data from the DataLoader as a NumPy `ndarray`. \"\"\"\n\n    if self.dataset_config is None or not self.dataset_config.is_initialized:\n        raise ValueError(\"Dataset is not initialized. Please call set_dataset_config_and_initialize() before getting Numpy array.\")\n\n    total_samples = len(ts_ids) * len(time_period)\n    if total_samples &gt;= LOADING_WARNING_THRESHOLD:\n        self.logger.warning(\"The dataset contains %d samples (%d time series \u00d7 %d times). Consider using get_*_dataloader() for batch loading.\", total_samples, len(ts_ids), len(time_period))\n\n    self.logger.debug(\"Creating numpy array from dataloader.\")\n    return dataset_loaders.create_numpy_from_dataloader(\n        dataloader,\n        ts_ids,\n        self.dataset_config.time_format,\n        self.dataset_config.include_time,\n        self.dataset_config.dataset_type,\n        True\n    )\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset._clear","title":"_clear","text":"<pre><code>_clear() -&gt; None\n</code></pre> <p>Clears set data. Mainly called when initializing new config.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def _clear(self) -&gt; None:\n    \"\"\"Clears set data. Mainly called when initializing new config. \"\"\"\n    self.train_dataset = None\n    self.train_dataloader = None\n    self.val_dataset = None\n    self.val_dataloader = None\n    self.test_dataset = None\n    self.test_dataloader = None\n    self.all_dataset = None\n    self.all_dataloader = None\n    self.dataset_config = None\n    self.logger.debug(\"Dataset attributes had been cleared. \")\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset._update_annotations_imported_status","title":"_update_annotations_imported_status","text":"<pre><code>_update_annotations_imported_status(on: AnnotationType, identifier: str)\n</code></pre> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def _update_annotations_imported_status(self, on: AnnotationType, identifier: str):\n    if on == AnnotationType.TS_ID:\n        self.imported_annotations_ts_identifier = identifier\n    elif on == AnnotationType.ID_TIME:\n        self.imported_annotations_time_identifier = identifier\n    elif on == AnnotationType.BOTH:\n        self.imported_annotations_both_identifier = identifier\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset._update_config_imported_status","title":"_update_config_imported_status","text":"<pre><code>_update_config_imported_status(identifier: str) -&gt; None\n</code></pre> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def _update_config_imported_status(self, identifier: str) -&gt; None:\n    self.dataset_config.import_identifier = identifier\n    self._export_config_copy.import_identifier = identifier\n</code></pre>"},{"location":"reference_time_based_cesnet_dataset/#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset._validate_config_for_dataset","title":"_validate_config_for_dataset","text":"<pre><code>_validate_config_for_dataset(config: DatasetConfig) -&gt; bool\n</code></pre> <p>Validates whether config is supposed to be used for this dataset.</p> Source code in <code>cesnet_tszoo\\datasets\\cesnet_dataset.py</code> <pre><code>def _validate_config_for_dataset(self, config: DatasetConfig) -&gt; bool:\n    \"\"\"Validates whether config is supposed to be used for this dataset. \"\"\"\n\n    if config.database_name != self.metadata.database_name:\n        self.logger.error(\"This config is not compatible with the current dataset. Difference in database name between config and this dataset.\")\n        raise ValueError(\"This config is not compatible with the current dataset.\", f\"config.database_name == {config.database_name} and dataset.database_name == {self.metadata.database_name}\")\n\n    if config.dataset_type != self.metadata.dataset_type:\n        self.logger.error(\"This config is not compatible with the current dataset. Difference in is_series_based between config and this dataset.\")\n        raise ValueError(\"This config is not compatible with the current dataset.\", f\"config.dataset_type == {config.dataset_type} and dataset.dataset_type == {self.metadata.dataset_type}\")\n\n    if config.aggregation != self.metadata.aggregation:\n        self.logger.error(\"This config is not compatible with the current dataset. Difference in aggregation type between config and this dataset.\")\n        raise ValueError(\"This config is not compatible with the current dataset.\", f\"config.aggregation == {config.aggregation} and dataset.aggregation == {self.metadata.aggregation}\")\n\n    if config.source_type != self.metadata.source_type:\n        self.logger.error(\"This config is not compatible with the current dataset. Difference in source type between config and this dataset.\")\n        raise ValueError(\"This config is not compatible with the current dataset.\", f\"config.source_type == {config.source_type} and dataset.source_type == {self.metadata.source_type}\")\n</code></pre>"},{"location":"reference_time_based_config/","title":"Time-based config class","text":""},{"location":"reference_time_based_config/#cesnet_tszoo.configs.time_based_config.TimeBasedConfig","title":"<code>cesnet_tszoo.configs.time_based_config.TimeBasedConfig</code>","text":"<p>               Bases: <code>TimeBasedHandler</code>, <code>DatasetConfig</code></p> <p>This class is used for configuring the <code>TimeBasedCesnetDataset</code>.</p> <p>Used to configure the following:</p> <ul> <li>Train, validation, test, all sets (time period, sizes, features, window size)</li> <li>Handling missing values (default values, <code>fillers</code>)</li> <li>Handling anomalies (<code>anomaly handlers</code>)</li> <li>Data transformation using <code>transformers</code></li> <li>Applying custom handlers (<code>custom handlers</code>)</li> <li>Changing order of preprocesses</li> <li>Dataloader options (train/val/test/all/init workers, batch sizes)</li> <li>Plotting</li> </ul> <p>Important Notes:</p> <ul> <li>Custom fillers must inherit from the <code>fillers</code> base class.</li> <li>Fillers can carry over values from the train set to the validation and test sets. For example, <code>ForwardFiller</code> can carry over values from previous sets.   </li> <li>Custom anomaly handlers must inherit from the <code>anomaly handlers</code> base class.</li> <li>It is recommended to use the <code>transformers</code> base class, though this is not mandatory as long as it meets the required methods.<ul> <li>If transformers are already initialized and <code>create_transformer_per_time_series</code> is <code>True</code> and <code>partial_fit_initialized_transformers</code> is <code>True</code> then transformers must support <code>partial_fit</code>.</li> <li>If <code>create_transformer_per_time_series</code> is <code>True</code>, transformers must have a <code>fit</code> method and <code>transform_with</code> should be a list of transformers.</li> <li>If <code>create_transformer_per_time_series</code> is <code>False</code>, transformers must support <code>partial_fit</code>.</li> <li>Transformers must implement the <code>transform</code> method.</li> <li>The <code>fit/partial_fit</code> and <code>transform</code> methods must accept an input of type <code>np.ndarray</code> with shape <code>(times, features)</code>.</li> </ul> </li> <li>Custom handlers must be derived from one of the built-in <code>custom handler</code> classes </li> <li><code>train_time_period</code>, <code>val_time_period</code>, <code>test_time_period</code> can overlap, but they should keep order of <code>train_time_period</code> &lt; <code>val_time_period</code> &lt; <code>test_time_period</code></li> </ul> Source code in <code>cesnet_tszoo\\configs\\time_based_config.py</code> <pre><code>class TimeBasedConfig(TimeBasedHandler, DatasetConfig):\n    \"\"\"\n    This class is used for configuring the [`TimeBasedCesnetDataset`](reference_time_based_cesnet_dataset.md#cesnet_tszoo.datasets.time_based_cesnet_dataset.TimeBasedCesnetDataset).\n\n    Used to configure the following:\n\n    - Train, validation, test, all sets (time period, sizes, features, window size)\n    - Handling missing values (default values, [`fillers`](reference_fillers.md#cesnet_tszoo.utils.filler.filler))\n    - Handling anomalies ([`anomaly handlers`](reference_anomaly_handlers.md#cesnet_tszoo.utils.anomaly_handler.anomaly_handler))\n    - Data transformation using [`transformers`](reference_transformers.md#cesnet_tszoo.utils.transformer.transformer)\n    - Applying custom handlers ([`custom handlers`](reference_custom_handlers.md#cesnet_tszoo.utils.custom_handler.custom_handler))\n    - Changing order of preprocesses\n    - Dataloader options (train/val/test/all/init workers, batch sizes)\n    - Plotting\n\n    **Important Notes:**\n\n    - Custom fillers must inherit from the [`fillers`](reference_fillers.md#cesnet_tszoo.utils.filler.filler.Filler) base class.\n    - Fillers can carry over values from the train set to the validation and test sets. For example, [`ForwardFiller`](reference_fillers.md#cesnet_tszoo.utils.filler.filler.ForwardFiller) can carry over values from previous sets.   \n    - Custom anomaly handlers must inherit from the [`anomaly handlers`](reference_anomaly_handlers.md#cesnet_tszoo.utils.anomaly_handler.anomaly_handler.AnomalyHandler) base class.\n    - It is recommended to use the [`transformers`](reference_transformers.md#cesnet_tszoo.utils.transformer.transformer.Transformer) base class, though this is not mandatory as long as it meets the required methods.\n        - If transformers are already initialized and `create_transformer_per_time_series` is `True` and `partial_fit_initialized_transformers` is `True` then transformers must support `partial_fit`.\n        - If `create_transformer_per_time_series` is `True`, transformers must have a `fit` method and `transform_with` should be a list of transformers.\n        - If `create_transformer_per_time_series` is `False`, transformers must support `partial_fit`.\n        - Transformers must implement the `transform` method.\n        - The `fit/partial_fit` and `transform` methods must accept an input of type `np.ndarray` with shape `(times, features)`.\n    - Custom handlers must be derived from one of the built-in [`custom handler`](reference_custom_handlers.md#cesnet_tszoo.utils.custom_handler.custom_handler) classes \n    - `train_time_period`, `val_time_period`, `test_time_period` can overlap, but they should keep order of `train_time_period` &lt; `val_time_period` &lt; `test_time_period`\n\n    Attributes:\n        used_train_workers: Tracks the number of train workers in use. Helps determine if the train dataloader should be recreated based on worker changes.\n        used_val_workers: Tracks the number of validation workers in use. Helps determine if the validation dataloader should be recreated based on worker changes.\n        used_test_workers: Tracks the number of test workers in use. Helps determine if the test dataloader should be recreated based on worker changes.\n        used_all_workers: Tracks the total number of all workers in use. Helps determine if the all dataloader should be recreated based on worker changes.\n        uses_all_time_period: Whether all time period set should be used.\n        import_identifier: Tracks the name of the config upon import. None if not imported.\n        filler_factory: Represents factory used to create passed Filler type.\n        anomaly_handler_factory: Represents factory used to create passed Anomaly Handler type.\n        transformer_factory: Represents factory used to create passed Transformer type.\n        can_fit_fillers: Whether fillers in this config, can be fitted.           \n        logger: Logger for displaying information.     \n        display_train_time_period: Used to display the configured value of `train_time_period`.\n        display_val_time_period: Used to display the configured value of `val_time_period`.\n        display_test_time_period: Used to display the configured value of `test_time_period`.\n        display_all_time_period: Used to display the configured value of `all_time_period`.\n        all_time_period: If no specific sets (train/val/test) are provided, all time IDs are used. When any set is defined, only the time IDs in defined sets are used.\n        ts_row_ranges: Initialized when `ts_ids` is set. Contains time series IDs in `ts_ids` with their respective time ID ranges (same as `all_time_period`).\n        aggregation: The aggregation period used for the data.\n        source_type: The source type of the data.\n        database_name: Specifies which database this config applies to.\n        features_to_take_without_ids: Features to be returned, excluding time or time series IDs.\n        indices_of_features_to_take_no_ids: Indices of non-ID features in `features_to_take`.\n        ts_id_name: Name of the time series ID, dependent on `source_type`.\n        used_singular_train_time_series: Currently used singular train set time series for dataloader.\n        used_singular_val_time_series: Currently used singular validation set time series for dataloader.\n        used_singular_test_time_series: Currently used singular test set time series for dataloader.\n        used_singular_all_time_series: Currently used singular all set time series for dataloader.        \n        train_preprocess_order: All preprocesses used for train set. \n        val_preprocess_order: All preprocesses used for val set. \n        test_preprocess_order: All preprocesses used for test set.      \n        all_preprocess_order: All preprocesses used for all set.        \n        is_initialized: Flag indicating if the configuration has already been initialized. If true, config initialization will be skipped.  \n        version: Version of cesnet-tszoo this config was made in.\n        export_update_needed: Whether config was updated to newer version and should be exported.     \n        ts_ids: Defines which time series IDs are used for train/val/test/all. Can be a list of IDs, or an integer/float to specify a random selection. An `int` specifies the number of random time series, and a `float` specifies the proportion of available time series. \n                `int` and `float` must be greater than 0, and a float should be smaller or equal to 1.0.  \n        train_time_period: Defines the time period for training set. Can be a range of time IDs or a tuple of datetime objects. Float value is equivalent to percentage of available times with offseted position from previous used set.\n        val_time_period: Defines the time period for validation set. Can be a range of time IDs or a tuple of datetime objects. Float value is equivalent to percentage of available times with offseted position from previous used set.\n        test_time_period: Defines the time period for test set. Can be a range of time IDs or a tuple of datetime objects.\n        features_to_take: Defines which features are used.                \n        default_values: Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature.\n        sliding_window_size: Number of times in one window. Impacts dataloader behavior. Batch sizes affects how much data will be cached for creating windows.\n        sliding_window_prediction_size: Number of times to predict from sliding_window_size. Impacts dataloader behavior. Batch sizes affects how much data will be cached for creating windows.\n        sliding_window_step: Number of times to move by after each window.\n        set_shared_size: How much times should time periods share. Order of sharing is training set &lt; validation set &lt; test set. Only in effect if sets share less values than set_shared_size. Use float value for percentage of total times or int for count.\n        train_batch_size: Batch size for the train dataloader. Affects number of returned times in one batch.\n        val_batch_size: Batch size for the validation dataloader. Affects number of returned times in one batch.\n        test_batch_size: Batch size for the test dataloader. Affects number of returned times in one batch.\n        all_batch_size: Batch size for the all dataloader. Affects number of returned times in one batch.\n        preprocess_order: Defines in which order preprocesses are used. Also can add to order a type of `PerSeriesCustomHandler`, `AllSeriesCustomHandler` or `NoFitCustomHandler`.\n        create_transformer_per_time_series: If `True`, a separate transformer is created for each time series. Not used when using already initialized transformers.\n        partial_fit_initialized_transformers: If `True`, partial fitting on train set is performed when using initiliazed transformers.\n        include_time: If `True`, time data is included in the returned values.\n        include_ts_id: If `True`, time series IDs are included in the returned values.\n        time_format: Format for the returned time data. When using TimeFormat.DATETIME, time will be returned as separate list along rest of the values.\n        train_workers: Number of workers for loading training data. `0` means that the data will be loaded in the main process.\n        val_workers: Number of workers for loading validation data. `0` means that the data will be loaded in the main process.\n        test_workers: Number of workers for loading test data. `0` means that the data will be loaded in the main process.\n        all_workers: Number of workers for loading all data. `0` means that the data will be loaded in the main process.\n        init_workers: Number of workers for initial dataset processing during configuration. `0` means that the data will be loaded in the main process.\n        nan_threshold: Maximum allowable percentage of missing data. Time series exceeding this threshold are excluded. Time series over the threshold will not be used. Used for `train/val/test/all` separately.\n        random_state: Fixes randomness for reproducibility during configuration and dataset initialization.               \n    \"\"\"\n\n    def __init__(self,\n                 ts_ids: list[int] | npt.NDArray[np.int_] | float | int,\n                 train_time_period: tuple[datetime, datetime] | range | float | None = None,\n                 val_time_period: tuple[datetime, datetime] | range | float | None = None,\n                 test_time_period: tuple[datetime, datetime] | range | float | None = None,\n                 features_to_take: list[str] | Literal[\"all\"] = \"all\",\n                 default_values: list[Number] | npt.NDArray[np.number] | dict[str, Number] | Number | Literal[\"default\"] | None = \"default\",\n                 sliding_window_size: int | None = None,\n                 sliding_window_prediction_size: int | None = None,\n                 sliding_window_step: int = 1,\n                 set_shared_size: float | int = 0,\n                 train_batch_size: int = 32,\n                 val_batch_size: int = 64,\n                 test_batch_size: int = 128,\n                 all_batch_size: int = 128,\n                 preprocess_order: list[str, type] = [\"handling_anomalies\", \"filling_gaps\", \"transforming\"],\n                 fill_missing_with: type | FillerType | Literal[\"mean_filler\", \"forward_filler\", \"linear_interpolation_filler\"] | None = None,\n                 transform_with: type | list[Transformer] | np.ndarray[Transformer] | TransformerType | Transformer | Literal[\"min_max_scaler\", \"standard_scaler\", \"max_abs_scaler\", \"log_transformer\", \"robust_scaler\", \"power_transformer\", \"quantile_transformer\", \"l2_normalizer\"] | None = None,\n                 handle_anomalies_with: type | AnomalyHandlerType | Literal[\"z-score\", \"interquartile_range\"] | None = None,\n                 create_transformer_per_time_series: bool = True,\n                 partial_fit_initialized_transformers: bool = False,\n                 include_time: bool = True,\n                 include_ts_id: bool = True,\n                 time_format: TimeFormat | Literal[\"id_time\", \"datetime\", \"unix_time\", \"shifted_unix_time\"] = TimeFormat.ID_TIME,\n                 train_workers: int = 4,\n                 val_workers: int = 3,\n                 test_workers: int = 2,\n                 all_workers: int = 4,\n                 init_workers: int = 4,\n                 nan_threshold: float = 1.0,\n                 random_state: int | None = None):\n        \"\"\"\n        Parameters:\n            ts_ids: Defines which time series IDs are used for train/val/test/all. Can be a list of IDs, or an integer/float to specify a random selection. An `int` specifies the number of random time series, and a `float` specifies the proportion of available time series. \n                    `int` and `float` must be greater than 0, and a float should be smaller or equal to 1.0.  \n            train_time_period: Defines the time period for training set. Can be a range of time IDs or a tuple of datetime objects. Float value is equivalent to percentage of available times with offseted position from previous used set. `Default: None`\n            val_time_period: Defines the time period for validation set. Can be a range of time IDs or a tuple of datetime objects. Float value is equivalent to percentage of available times with offseted position from previous used set. `Default: None`\n            test_time_period: Defines the time period for test set. Can be a range of time IDs or a tuple of datetime objects. `Default: None`\n            features_to_take: Defines which features are used. `Default: \"all\"`                  \n            default_values: Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature. `Default: \"default\"`\n            sliding_window_size: Number of times in one window. Impacts dataloader behavior. Batch sizes affects how much data will be cached for creating windows. `Default: None`\n            sliding_window_prediction_size: Number of times to predict from sliding_window_size. Impacts dataloader behavior. Batch sizes affects how much data will be cached for creating windows. `Default: None`\n            sliding_window_step: Number of times to move by after each window. `Default: 1`\n            set_shared_size: How much times should time periods share. Order of sharing is training set &lt; validation set &lt; test set. Only in effect if sets share less values than set_shared_size. Use float value for percentage of total times or int for count. `Default: 0`\n            train_batch_size: Batch size for the train dataloader. Affects number of returned times in one batch. `Default: 32`\n            val_batch_size: Batch size for the validation dataloader. Affects number of returned times in one batch. `Default: 64`\n            test_batch_size: Batch size for the test dataloader. Affects number of returned times in one batch. `Default: 128`\n            all_batch_size: Batch size for the all dataloader. Affects number of returned times in one batch. `Default: 128`   \n            preprocess_order: Defines in which order preprocesses are used. Also can add to order a type of `PerSeriesCustomHandler`, `AllSeriesCustomHandler` or `NoFitCustomHandler`. `Default: [\"handling_anomalies\", \"filling_gaps\", \"transforming\"]`\n            fill_missing_with: Defines how to fill missing values in the dataset. Can pass enum `FillerType` for built-in filler or pass a type of custom filler that must derive from `Filler` base class. `Default: None`\n            transform_with: Defines the transformer used to transform the dataset. Can pass enum `TransformerType` for built-in transformer, pass a type of custom transformer or instance of already fitted transformer(s). `Default: None`\n            handle_anomalies_with: Defines the anomaly handler for handling anomalies in the train set. Can pass enum `AnomalyHandlerType` for built-in anomaly handler or a type of custom anomaly handler. `Default: None`\n            create_transformer_per_time_series: If `True`, a separate transformer is created for each time series. Not used when using already initialized transformers. `Default: True`\n            partial_fit_initialized_transformers: If `True`, partial fitting on train set is performed when using initiliazed transformers. `Default: False`\n            include_time: If `True`, time data is included in the returned values. `Default: True`\n            include_ts_id: If `True`, time series IDs are included in the returned values. `Default: True`\n            time_format: Format for the returned time data. When using TimeFormat.DATETIME, time will be returned as separate list along rest of the values. `Default: TimeFormat.ID_TIME`\n            train_workers: Number of workers for loading training data. `0` means that the data will be loaded in the main process. `Default: 4`\n            val_workers: Number of workers for loading validation data. `0` means that the data will be loaded in the main process. `Default: 3`\n            test_workers: Number of workers for loading test data. `0` means that the data will be loaded in the main process. `Default: 2`\n            all_workers: Number of workers for loading all data. `0` means that the data will be loaded in the main process. `Default: 4`\n            init_workers: Number of workers for initial dataset processing during configuration. `0` means that the data will be loaded in the main process. `Default: 4`\n            nan_threshold: Maximum allowable percentage of missing data. Time series exceeding this threshold are excluded. Time series over the threshold will not be used. Used for `train/val/test/all` separately. `Default: 1.0`\n            random_state: Fixes randomness for reproducibility during configuration and dataset initialization. `Default: None`         \n        \"\"\"\n\n        self.ts_ids: np.ndarray = ts_ids\n\n        self.ts_row_ranges: Optional[np.ndarray] = None\n\n        self.logger: logging.Logger = logging.getLogger(\"time_config\")\n\n        TimeBasedHandler.__init__(self, self.logger, train_batch_size, val_batch_size, test_batch_size, all_batch_size, True, sliding_window_size, sliding_window_prediction_size, sliding_window_step, set_shared_size, train_time_period, val_time_period, test_time_period)\n        DatasetConfig.__init__(self, features_to_take, default_values, train_batch_size, val_batch_size, test_batch_size, all_batch_size, preprocess_order, fill_missing_with, transform_with, handle_anomalies_with, partial_fit_initialized_transformers, include_time, include_ts_id, time_format,\n                               train_workers, val_workers, test_workers, all_workers, init_workers, nan_threshold, create_transformer_per_time_series, DatasetType.TIME_BASED, DataloaderOrder.SEQUENTIAL, random_state, True, self.logger)\n\n    def _validate_construction(self) -&gt; None:\n        \"\"\"Performs basic parameter validation to ensure correct configuration. More comprehensive validation, which requires dataset-specific data, is handled in [`_dataset_init`][cesnet_tszoo.configs.time_based_config.TimeBasedConfig._dataset_init]. \"\"\"\n\n        DatasetConfig._validate_construction(self)\n\n        self._validate_set_shared_size_init()\n        self._validate_sliding_window_init()\n        self._update_batch_sizes(self.train_batch_size, self.val_batch_size, self.test_batch_size, self.all_batch_size)\n\n        assert self.ts_ids is not None, \"ts_ids must not be None\"\n\n        split_float_total = 0\n\n        if isinstance(self.ts_ids, (float, int)):\n            assert self.ts_ids &gt; 0, \"ts_ids must be greater than 0\"\n            if isinstance(self.ts_ids, float):\n                split_float_total += self.ts_ids\n\n        # Check if the total of float splits exceeds 1.0\n        if split_float_total &gt; 1.0:\n            self.logger.error(\"The total of the float split sizes is greater than 1.0. Current total: %s\", split_float_total)\n            raise ValueError(\"Total value of used float split sizes can't be greater than 1.0.\")\n\n        self._validate_time_periods_init()\n\n        self.logger.debug(\"Time-based configuration validated successfully.\")\n\n    def _update_batch_sizes(self, train_batch_size: int, val_batch_size: int, test_batch_size: int, all_batch_size: int) -&gt; None:\n\n        # Adjust batch sizes based on sliding_window_size\n        if self.sliding_window_size is not None:\n\n            if self.sliding_window_step &lt;= 0:\n                raise ValueError(\"sliding_window_step must be greater or equal to 1.\")\n\n            total_window_size = self.sliding_window_size + self.sliding_window_prediction_size\n\n            if isinstance(self.train_batch_size, int) and total_window_size &gt; self.train_batch_size:\n                train_batch_size = self.sliding_window_size + self.sliding_window_prediction_size\n                self.logger.info(\"train_batch_size adjusted to %s as it should be greater than or equal to sliding_window_size + sliding_window_prediction_size.\", total_window_size)\n            if isinstance(self.val_batch_size, int) and total_window_size &gt; self.val_batch_size:\n                val_batch_size = self.sliding_window_size + self.sliding_window_prediction_size\n                self.logger.info(\"val_batch_size adjusted to %s as it should be greater than or equal to sliding_window_size + sliding_window_prediction_size.\", total_window_size)\n            if isinstance(self.test_batch_size, int) and total_window_size &gt; self.test_batch_size:\n                test_batch_size = self.sliding_window_size + self.sliding_window_prediction_size\n                self.logger.info(\"test_batch_size adjusted to %s as it should be greater than or equal to sliding_window_size + sliding_window_prediction_size.\", total_window_size)\n            if isinstance(self.all_batch_size, int) and total_window_size &gt; self.all_batch_size:\n                all_batch_size = self.sliding_window_size + self.sliding_window_prediction_size\n                self.logger.info(\"all_batch_size adjusted to %s as it should be greater than or equal to sliding_window_size + sliding_window_prediction_size.\", total_window_size)\n\n        DatasetConfig._update_batch_sizes(self, train_batch_size, val_batch_size, test_batch_size, all_batch_size)\n\n    def _update_sliding_window(self, sliding_window_size: int | None, sliding_window_prediction_size: int | None, sliding_window_step: int | None, set_shared_size: float | int, all_time_ids: np.ndarray):\n        \"\"\"Updates values related to sliding window. \"\"\"\n        TimeBasedHandler._update_sliding_window(self, sliding_window_size, sliding_window_prediction_size, sliding_window_step, set_shared_size, all_time_ids, self.has_train(), self.has_val(), self.has_test(), self.has_all())\n\n    def _get_train(self) -&gt; tuple[np.ndarray, np.ndarray] | tuple[None, None]:\n        \"\"\"Returns the indices corresponding to the training set. \"\"\"\n        return self.ts_ids, self.train_time_period\n\n    def _get_val(self) -&gt; tuple[np.ndarray, np.ndarray] | tuple[None, None]:\n        \"\"\"Returns the indices corresponding to the validation set. \"\"\"\n        return self.ts_ids, self.val_time_period\n\n    def _get_test(self) -&gt; tuple[np.ndarray, np.ndarray] | tuple[None, None]:\n        \"\"\"Returns the indices corresponding to the test set. \"\"\"\n        return self.ts_ids, self.test_time_period\n\n    def _get_all(self) -&gt; tuple[np.ndarray, np.ndarray] | tuple[None, None]:\n        \"\"\"Returns the indices corresponding to the all set. \"\"\"\n        return self.ts_ids, self.all_time_period\n\n    def has_train(self) -&gt; bool:\n        \"\"\"Returns whether training set is used. \"\"\"\n        return self.train_time_period is not None\n\n    def has_val(self) -&gt; bool:\n        \"\"\"Returns whether validation set is used. \"\"\"\n        return self.val_time_period is not None\n\n    def has_test(self) -&gt; bool:\n        \"\"\"Returns whether test set is used. \"\"\"\n        return self.test_time_period is not None\n\n    def has_all(self) -&gt; bool:\n        \"\"\"Returns whether all set is used. \"\"\"\n        return self.all_time_period is not None\n\n    def _set_time_period(self, all_time_ids: np.ndarray) -&gt; None:\n        \"\"\"Validates and filters `train_time_period`, `val_time_period`, `test_time_period` and `all_time_period` based on `dataset` and `aggregation`. \"\"\"\n\n        self._prepare_and_set_time_period_sets(all_time_ids, self.time_format)\n\n    def _set_ts(self, all_ts_ids: np.ndarray, all_ts_row_ranges: np.ndarray, rd: np.random.RandomState) -&gt; None:\n        \"\"\" Validates and filters inputted time series id from `ts_ids` based on `dataset` and `source_type`. Handles random set.\"\"\"\n\n        random_ts_ids = all_ts_ids[self.ts_id_name]\n        random_indices = np.arange(len(all_ts_ids))\n\n        # Process ts_ids if it was specified with times series ids\n        if not isinstance(self.ts_ids, (float, int)):\n            self.ts_ids, self.ts_row_ranges, _ = SeriesBasedHandler._process_ts_ids(self.ts_ids, all_ts_ids, all_ts_row_ranges, None, None, self.logger, self.ts_id_name, self.random_state, rd)\n\n            mask = np.isin(random_ts_ids, self.ts_ids, invert=True)\n            random_ts_ids = random_ts_ids[mask]\n            random_indices = random_indices[mask]\n\n            self.logger.debug(\"ts_ids set: %s\", self.ts_ids)\n\n        # Convert proportions to total values\n        if isinstance(self.ts_ids, float):\n            self.ts_ids = int(self.ts_ids * len(random_ts_ids))\n            self.logger.debug(\"ts_ids converted to total values: %s\", self.ts_ids)\n\n        # Process random ts_ids if it is to be randomly made\n        if isinstance(self.ts_ids, int):\n            self.ts_ids, self.ts_row_ranges, random_indices = SeriesBasedHandler._process_ts_ids(None, all_ts_ids, all_ts_row_ranges, self.ts_ids, random_indices, self.logger, self.ts_id_name, self.random_state, rd)\n            self.logger.debug(\"Random ts_ids set with %s time series.\", self.ts_ids)\n\n    def _get_feature_transformers(self) -&gt; np.ndarray[Transformer] | Transformer:\n        \"\"\"Creates transformer/s with `transformer_factory`. \"\"\"\n\n        if self.transformer_factory.has_already_initialized:\n            if not self.has_train() and self.partial_fit_initialized_transformers:\n                self.partial_fit_initialized_transformers = False\n                self.logger.warning(\"partial_fit_initialized_transformers will be ignored because train set is not used.\")\n\n            transformers = self.transformer_factory.get_already_initialized_transformers()\n\n            if self.transformer_factory.has_single_initialized:\n                self.logger.debug(\"Using already initialized transformer %s.\", self.transformer_factory.name)\n            else:\n                assert len(transformers) == len(self.ts_ids), \"Number of time series in ts_ids does not match with number of provided transformers.\"\n                self.create_transformer_per_time_series = True\n                self.logger.debug(\"Using list of initialized transformers of %s\", self.transformer_factory.name)\n        else:\n            if not self.has_train() and not self.transformer_factory.is_empty_factory:\n                self.transformer_factory = transformer_factories.get_transformer_factory(None, self.create_transformer_per_time_series, self.partial_fit_initialized_transformers)\n                self.logger.warning(\"No transformer will be used because train set is not used.\")\n\n            if self.create_transformer_per_time_series:\n                transformers = np.array([self.transformer_factory.create_transformer() for _ in self.ts_ids])\n                self.logger.debug(\"Using list of transformers of %s\", self.transformer_factory.name)\n            else:\n                transformers = self.transformer_factory.create_transformer()\n                self.logger.debug(\"Using transformer %s\", self.transformer_factory.name)\n\n        return transformers\n\n    def _get_fillers(self) -&gt; tuple:\n        \"\"\"Creates fillers with `filler_factory`. \"\"\"\n\n        train_fillers = None\n        # Set the fillers for the training set\n        if self.has_train():\n            train_fillers = np.array([self.filler_factory.create_filler(self.features_to_take_without_ids) for _ in self.ts_ids])\n            self.logger.debug(\"Fillers for training set are set.\")\n\n        val_fillers = None\n        # Set the fillers for the validation set\n        if self.has_val():\n            val_fillers = np.array([self.filler_factory.create_filler(self.features_to_take_without_ids) for _ in self.ts_ids])\n            self.logger.debug(\"Fillers for validation set are set.\")\n\n        test_fillers = None\n        # Set the fillers for the test set\n        if self.has_test():\n            test_fillers = np.array([self.filler_factory.create_filler(self.features_to_take_without_ids) for _ in self.ts_ids])\n            self.logger.debug(\"Fillers for test set are set.\")\n\n        all_fillers = None\n        # Set the fillers for the all set\n        if self.has_all():\n            all_fillers = np.array([self.filler_factory.create_filler(self.features_to_take_without_ids) for _ in self.ts_ids])\n            self.logger.debug(\"Fillers for all set are set.\")\n\n        self.logger.debug(\"Using filler %s\", self.filler_factory.name)\n\n        return train_fillers, val_fillers, test_fillers, all_fillers\n\n    def _get_anomaly_handlers(self) -&gt; np.ndarray:\n        \"\"\"Creates anomaly handlers with `anomaly_handler_factory`. \"\"\"\n\n        if not self.has_train() and not self.anomaly_handler_factory.is_empty_factory:\n            self.anomaly_handler_factory = anomaly_handler_factories.get_anomaly_handler_factory(None)\n            self.logger.warning(\"No anomaly handler will be used because train set is not used.\")\n\n        anomaly_handlers = np.array([])\n        if self.has_train():\n            anomaly_handlers = np.array([self.anomaly_handler_factory.create_anomaly_handler() for _ in self.ts_ids])\n\n        self.logger.debug(\"Using anomaly handler %s\", self.anomaly_handler_factory.name)\n\n        return anomaly_handlers\n\n    def _set_per_series_custom_handler(self, factory: PerSeriesCustomHandlerFactory):\n\n        if not self.has_train():\n            raise ValueError(\"To use PerSeriesCustomHandler you need to use train set.\")\n\n        handlers = np.array([factory.create_handler() for _ in self.ts_ids])\n\n        self.train_preprocess_order.append(PreprocessNote(factory.preprocess_enum_type, False, True, factory.can_apply_to_train, True, PerSeriesCustomHandlerHolder(handlers)))\n        self.val_preprocess_order.append(PreprocessNote(factory.preprocess_enum_type, True, False, factory.can_apply_to_val and self.has_val(), True, PerSeriesCustomHandlerHolder(handlers)))\n        self.test_preprocess_order.append(PreprocessNote(factory.preprocess_enum_type, True, False, factory.can_apply_to_test and self.has_test(), True, PerSeriesCustomHandlerHolder(handlers)))\n        self.all_preprocess_order.append(PreprocessNote(factory.preprocess_enum_type, True, False, factory.can_apply_to_all and self.has_all(), True, PerSeriesCustomHandlerHolder(handlers)))\n\n    def _set_no_fit_custom_handler(self, factory: NoFitCustomHandlerFactory):\n\n        train_handlers = np.array([factory.create_handler() for _ in self.ts_ids]) if self.has_train() else None\n        self.train_preprocess_order.append(PreprocessNote(factory.preprocess_enum_type, False, False, factory.can_apply_to_train and self.has_train(), True, NoFitCustomHandlerHolder(train_handlers)))\n\n        val_handlers = np.array([factory.create_handler() for _ in self.ts_ids]) if self.has_val() else None\n        self.val_preprocess_order.append(PreprocessNote(factory.preprocess_enum_type, False, False, factory.can_apply_to_val and self.has_val(), True, NoFitCustomHandlerHolder(val_handlers)))\n\n        test_handlers = np.array([factory.create_handler() for _ in self.ts_ids]) if self.has_test() else None\n        self.test_preprocess_order.append(PreprocessNote(factory.preprocess_enum_type, False, False, factory.can_apply_to_test and self.has_test(), True, NoFitCustomHandlerHolder(test_handlers)))\n\n        all_handlers = np.array([factory.create_handler() for _ in self.ts_ids]) if self.has_all() else None\n        self.all_preprocess_order.append(PreprocessNote(factory.preprocess_enum_type, False, False, factory.can_apply_to_all and self.has_all(), True, NoFitCustomHandlerHolder(all_handlers)))\n\n    def _validate_finalization(self) -&gt; None:\n        \"\"\" Performs final validation of the configuration. Validates whether `train/val/test` are continuos. \"\"\"\n\n        self._validate_time_periods_overlap()\n\n    def _get_summary_filter_time_series(self) -&gt; css_utils.SummaryDiagramStep:\n        attributes = [css_utils.StepAttribute(\"Time series IDs\", get_abbreviated_list_string(self.ts_ids)),\n                      css_utils.StepAttribute(\"Train time periods\", self.display_train_time_period),\n                      css_utils.StepAttribute(\"Val time periods\", self.display_val_time_period),\n                      css_utils.StepAttribute(\"Test time periods\", self.display_test_time_period),\n                      css_utils.StepAttribute(\"All time periods\", self.display_all_time_period),\n                      css_utils.StepAttribute(\"Nan threshold\", self.nan_threshold)]\n\n        return css_utils.SummaryDiagramStep(\"Filter time series\", attributes)\n\n    def _get_summary_loader(self) -&gt; list[css_utils.SummaryDiagramStep]:\n\n        steps = []\n\n        if self.sliding_window_size is not None:\n            attributes = [\n                css_utils.StepAttribute(\"Window size\", self.sliding_window_size),\n                css_utils.StepAttribute(\"Prediction size\", self.sliding_window_prediction_size),\n                css_utils.StepAttribute(\"Step\", self.sliding_window_step)\n            ]\n\n            steps.append(css_utils.SummaryDiagramStep(\"Apply sliding window\", attributes))\n\n        attributes = [css_utils.StepAttribute(\"Train batch size\", self.train_batch_size),\n                      css_utils.StepAttribute(\"Val batch size\", self.val_batch_size),\n                      css_utils.StepAttribute(\"Test batch size\", self.test_batch_size),\n                      css_utils.StepAttribute(\"All batch size\", self.all_batch_size)]\n\n        steps.append(css_utils.SummaryDiagramStep(\"Transform into specific format\", attributes))\n\n        return steps\n\n    def __str__(self) -&gt; str:\n\n        if self.transformer_factory.is_empty_factory:\n            transformer_part = f\"Transformer type: {self.transformer_factory.name}\"\n        else:\n            transformer_part = f'''Transformer type: {self.transformer_factory.name}\n        Is transformer per Time series: {self.create_transformer_per_time_series}\n        Are transformers premade: {self.transformer_factory.has_already_initialized}\n        Are premade transformers partial_fitted: {self.partial_fit_initialized_transformers}'''\n\n        if self.include_time:\n            time_part = f'''Time included: {str(self.include_time)}    \n        Time format: {str(self.time_format)}'''\n        else:\n            time_part = f\"Time included: {str(self.include_time)}\"\n\n        return f'''\nConfig Details\n    Used for database: {self.database_name}\n    Aggregation: {str(self.aggregation)}\n    Source: {str(self.source_type)}\n\n    Time series\n        Time series IDS: {get_abbreviated_list_string(self.ts_ids)}\n    Time periods\n        Train time periods: {str(self.display_train_time_period)}\n        Val time periods: {str(self.display_val_time_period)}\n        Test time periods: {str(self.display_test_time_period)}\n        All time periods: {str(self.display_all_time_period)}\n    Features\n        Taken features: {str(self.features_to_take_without_ids)}\n        Default values: {self.default_values}\n        Time series ID included: {str(self.include_ts_id)}\n        {time_part}\n    Sliding window\n        Sliding window size: {self.sliding_window_size}\n        Sliding window prediction size: {self.sliding_window_prediction_size}\n        Sliding window step size: {self.sliding_window_step}\n        Set shared size: {self.set_shared_size}\n    Fillers\n        Filler type: {self.filler_factory.name}\n    Transformers\n        {transformer_part}\n    Anomaly handler\n        Anomaly handler type: {self.anomaly_handler_factory.name}        \n    Batch sizes\n        Train batch size: {self.train_batch_size}\n        Val batch size: {self.val_batch_size}\n        Test batch size: {self.test_batch_size}\n        All batch size: {self.all_batch_size}\n    Default workers\n        Init worker count: {str(self.init_workers)}\n        Train worker count: {str(self.train_workers)}\n        Val worker count: {str(self.val_workers)}\n        Test worker count: {str(self.test_workers)}\n        All worker count: {str(self.all_workers)}\n    Other\n        Preprocess order: {normalize_display_list(self.preprocess_order)}\n        Nan threshold: {str(self.nan_threshold)}\n        Random state: {self.random_state}\n        Version: {self.version}\n                '''\n</code></pre>"},{"location":"reference_time_based_config/#configuration-options","title":"Configuration options","text":"<p>Parameters:</p> Name Type Description Default <code>ts_ids</code> <code>list[int] | NDArray[int_] | float | int</code> <p>Defines which time series IDs are used for train/val/test/all. Can be a list of IDs, or an integer/float to specify a random selection. An <code>int</code> specifies the number of random time series, and a <code>float</code> specifies the proportion of available time series.      <code>int</code> and <code>float</code> must be greater than 0, and a float should be smaller or equal to 1.0.  </p> required <code>train_time_period</code> <code>tuple[datetime, datetime] | range | float | None</code> <p>Defines the time period for training set. Can be a range of time IDs or a tuple of datetime objects. Float value is equivalent to percentage of available times with offseted position from previous used set. <code>Default: None</code></p> <code>None</code> <code>val_time_period</code> <code>tuple[datetime, datetime] | range | float | None</code> <p>Defines the time period for validation set. Can be a range of time IDs or a tuple of datetime objects. Float value is equivalent to percentage of available times with offseted position from previous used set. <code>Default: None</code></p> <code>None</code> <code>test_time_period</code> <code>tuple[datetime, datetime] | range | float | None</code> <p>Defines the time period for test set. Can be a range of time IDs or a tuple of datetime objects. <code>Default: None</code></p> <code>None</code> <code>features_to_take</code> <code>list[str] | Literal['all']</code> <p>Defines which features are used. <code>Default: \"all\"</code> </p> <code>'all'</code> <code>default_values</code> <code>list[Number] | NDArray[number] | dict[str, Number] | Number | Literal['default'] | None</code> <p>Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature. <code>Default: \"default\"</code></p> <code>'default'</code> <code>sliding_window_size</code> <code>int | None</code> <p>Number of times in one window. Impacts dataloader behavior. Batch sizes affects how much data will be cached for creating windows. <code>Default: None</code></p> <code>None</code> <code>sliding_window_prediction_size</code> <code>int | None</code> <p>Number of times to predict from sliding_window_size. Impacts dataloader behavior. Batch sizes affects how much data will be cached for creating windows. <code>Default: None</code></p> <code>None</code> <code>sliding_window_step</code> <code>int</code> <p>Number of times to move by after each window. <code>Default: 1</code></p> <code>1</code> <code>set_shared_size</code> <code>float | int</code> <p>How much times should time periods share. Order of sharing is training set &lt; validation set &lt; test set. Only in effect if sets share less values than set_shared_size. Use float value for percentage of total times or int for count. <code>Default: 0</code></p> <code>0</code> <code>train_batch_size</code> <code>int</code> <p>Batch size for the train dataloader. Affects number of returned times in one batch. <code>Default: 32</code></p> <code>32</code> <code>val_batch_size</code> <code>int</code> <p>Batch size for the validation dataloader. Affects number of returned times in one batch. <code>Default: 64</code></p> <code>64</code> <code>test_batch_size</code> <code>int</code> <p>Batch size for the test dataloader. Affects number of returned times in one batch. <code>Default: 128</code></p> <code>128</code> <code>all_batch_size</code> <code>int</code> <p>Batch size for the all dataloader. Affects number of returned times in one batch. <code>Default: 128</code> </p> <code>128</code> <code>preprocess_order</code> <code>list[str, type]</code> <p>Defines in which order preprocesses are used. Also can add to order a type of <code>PerSeriesCustomHandler</code>, <code>AllSeriesCustomHandler</code> or <code>NoFitCustomHandler</code>. <code>Default: [\"handling_anomalies\", \"filling_gaps\", \"transforming\"]</code></p> <code>['handling_anomalies', 'filling_gaps', 'transforming']</code> <code>fill_missing_with</code> <code>type | FillerType | Literal['mean_filler', 'forward_filler', 'linear_interpolation_filler'] | None</code> <p>Defines how to fill missing values in the dataset. Can pass enum <code>FillerType</code> for built-in filler or pass a type of custom filler that must derive from <code>Filler</code> base class. <code>Default: None</code></p> <code>None</code> <code>transform_with</code> <code>type | list[Transformer] | ndarray[Transformer] | TransformerType | Transformer | Literal['min_max_scaler', 'standard_scaler', 'max_abs_scaler', 'log_transformer', 'robust_scaler', 'power_transformer', 'quantile_transformer', 'l2_normalizer'] | None</code> <p>Defines the transformer used to transform the dataset. Can pass enum <code>TransformerType</code> for built-in transformer, pass a type of custom transformer or instance of already fitted transformer(s). <code>Default: None</code></p> <code>None</code> <code>handle_anomalies_with</code> <code>type | AnomalyHandlerType | Literal['z-score', 'interquartile_range'] | None</code> <p>Defines the anomaly handler for handling anomalies in the train set. Can pass enum <code>AnomalyHandlerType</code> for built-in anomaly handler or a type of custom anomaly handler. <code>Default: None</code></p> <code>None</code> <code>create_transformer_per_time_series</code> <code>bool</code> <p>If <code>True</code>, a separate transformer is created for each time series. Not used when using already initialized transformers. <code>Default: True</code></p> <code>True</code> <code>partial_fit_initialized_transformers</code> <code>bool</code> <p>If <code>True</code>, partial fitting on train set is performed when using initiliazed transformers. <code>Default: False</code></p> <code>False</code> <code>include_time</code> <code>bool</code> <p>If <code>True</code>, time data is included in the returned values. <code>Default: True</code></p> <code>True</code> <code>include_ts_id</code> <code>bool</code> <p>If <code>True</code>, time series IDs are included in the returned values. <code>Default: True</code></p> <code>True</code> <code>time_format</code> <code>TimeFormat | Literal['id_time', 'datetime', 'unix_time', 'shifted_unix_time']</code> <p>Format for the returned time data. When using TimeFormat.DATETIME, time will be returned as separate list along rest of the values. <code>Default: TimeFormat.ID_TIME</code></p> <code>ID_TIME</code> <code>train_workers</code> <code>int</code> <p>Number of workers for loading training data. <code>0</code> means that the data will be loaded in the main process. <code>Default: 4</code></p> <code>4</code> <code>val_workers</code> <code>int</code> <p>Number of workers for loading validation data. <code>0</code> means that the data will be loaded in the main process. <code>Default: 3</code></p> <code>3</code> <code>test_workers</code> <code>int</code> <p>Number of workers for loading test data. <code>0</code> means that the data will be loaded in the main process. <code>Default: 2</code></p> <code>2</code> <code>all_workers</code> <code>int</code> <p>Number of workers for loading all data. <code>0</code> means that the data will be loaded in the main process. <code>Default: 4</code></p> <code>4</code> <code>init_workers</code> <code>int</code> <p>Number of workers for initial dataset processing during configuration. <code>0</code> means that the data will be loaded in the main process. <code>Default: 4</code></p> <code>4</code> <code>nan_threshold</code> <code>float</code> <p>Maximum allowable percentage of missing data. Time series exceeding this threshold are excluded. Time series over the threshold will not be used. Used for <code>train/val/test/all</code> separately. <code>Default: 1.0</code></p> <code>1.0</code> <code>random_state</code> <code>int | None</code> <p>Fixes randomness for reproducibility during configuration and dataset initialization. <code>Default: None</code></p> <code>None</code>"},{"location":"reference_time_based_config/#config-attributes","title":"Config attributes","text":"<p>Attributes:</p> Name Type Description <code>used_train_workers</code> <code>Optional[int]</code> <p>Tracks the number of train workers in use. Helps determine if the train dataloader should be recreated based on worker changes.</p> <code>used_val_workers</code> <code>Optional[int]</code> <p>Tracks the number of validation workers in use. Helps determine if the validation dataloader should be recreated based on worker changes.</p> <code>used_test_workers</code> <code>Optional[int]</code> <p>Tracks the number of test workers in use. Helps determine if the test dataloader should be recreated based on worker changes.</p> <code>used_all_workers</code> <code>Optional[int]</code> <p>Tracks the total number of all workers in use. Helps determine if the all dataloader should be recreated based on worker changes.</p> <code>uses_all_time_period</code> <code>bool</code> <p>Whether all time period set should be used.</p> <code>import_identifier</code> <code>Optional[str]</code> <p>Tracks the name of the config upon import. None if not imported.</p> <code>filler_factory</code> <code>FillerFactory</code> <p>Represents factory used to create passed Filler type.</p> <code>anomaly_handler_factory</code> <code>AnomalyHandlerFactory</code> <p>Represents factory used to create passed Anomaly Handler type.</p> <code>transformer_factory</code> <code>TransformerFactory</code> <p>Represents factory used to create passed Transformer type.</p> <code>can_fit_fillers</code> <code>bool</code> <p>Whether fillers in this config, can be fitted.           </p> <code>logger</code> <code>Logger</code> <p>Logger for displaying information.     </p> <code>display_train_time_period</code> <code>Optional[range]</code> <p>Used to display the configured value of <code>train_time_period</code>.</p> <code>display_val_time_period</code> <code>Optional[range]</code> <p>Used to display the configured value of <code>val_time_period</code>.</p> <code>display_test_time_period</code> <code>Optional[range]</code> <p>Used to display the configured value of <code>test_time_period</code>.</p> <code>display_all_time_period</code> <code>Optional[range]</code> <p>Used to display the configured value of <code>all_time_period</code>.</p> <code>all_time_period</code> <code>Optional[ndarray]</code> <p>If no specific sets (train/val/test) are provided, all time IDs are used. When any set is defined, only the time IDs in defined sets are used.</p> <code>ts_row_ranges</code> <code>Optional[ndarray]</code> <p>Initialized when <code>ts_ids</code> is set. Contains time series IDs in <code>ts_ids</code> with their respective time ID ranges (same as <code>all_time_period</code>).</p> <code>aggregation</code> <code>Optional[AgreggationType]</code> <p>The aggregation period used for the data.</p> <code>source_type</code> <code>Optional[SourceType]</code> <p>The source type of the data.</p> <code>database_name</code> <code>Optional[str]</code> <p>Specifies which database this config applies to.</p> <code>features_to_take_without_ids</code> <code>Optional[ndarray]</code> <p>Features to be returned, excluding time or time series IDs.</p> <code>indices_of_features_to_take_no_ids</code> <code>Optional[ndarray]</code> <p>Indices of non-ID features in <code>features_to_take</code>.</p> <code>ts_id_name</code> <code>Optional[str]</code> <p>Name of the time series ID, dependent on <code>source_type</code>.</p> <code>used_singular_train_time_series</code> <code>Optional[int]</code> <p>Currently used singular train set time series for dataloader.</p> <code>used_singular_val_time_series</code> <code>Optional[int]</code> <p>Currently used singular validation set time series for dataloader.</p> <code>used_singular_test_time_series</code> <code>Optional[int]</code> <p>Currently used singular test set time series for dataloader.</p> <code>used_singular_all_time_series</code> <code>Optional[int]</code> <p>Currently used singular all set time series for dataloader.        </p> <code>train_preprocess_order</code> <code>list[PreprocessNote]</code> <p>All preprocesses used for train set. </p> <code>val_preprocess_order</code> <code>list[PreprocessNote]</code> <p>All preprocesses used for val set. </p> <code>test_preprocess_order</code> <code>list[PreprocessNote]</code> <p>All preprocesses used for test set.      </p> <code>all_preprocess_order</code> <code>list[PreprocessNote]</code> <p>All preprocesses used for all set.        </p> <code>is_initialized</code> <code>bool</code> <p>Flag indicating if the configuration has already been initialized. If true, config initialization will be skipped.  </p> <code>version</code> <code>str</code> <p>Version of cesnet-tszoo this config was made in.</p> <code>export_update_needed</code> <code>bool</code> <p>Whether config was updated to newer version and should be exported.     </p> <code>ts_ids</code> <code>ndarray</code> <p>Defines which time series IDs are used for train/val/test/all. Can be a list of IDs, or an integer/float to specify a random selection. An <code>int</code> specifies the number of random time series, and a <code>float</code> specifies the proportion of available time series.      <code>int</code> and <code>float</code> must be greater than 0, and a float should be smaller or equal to 1.0.  </p> <code>train_time_period</code> <code>Optional[ndarray]</code> <p>Defines the time period for training set. Can be a range of time IDs or a tuple of datetime objects. Float value is equivalent to percentage of available times with offseted position from previous used set.</p> <code>val_time_period</code> <code>Optional[ndarray]</code> <p>Defines the time period for validation set. Can be a range of time IDs or a tuple of datetime objects. Float value is equivalent to percentage of available times with offseted position from previous used set.</p> <code>test_time_period</code> <code>Optional[ndarray]</code> <p>Defines the time period for test set. Can be a range of time IDs or a tuple of datetime objects.</p> <code>features_to_take</code> <code>list[str]</code> <p>Defines which features are used.                </p> <code>default_values</code> <code>ndarray</code> <p>Default values for missing data, applied before fillers. Can set one value for all features or specify for each feature.</p> <code>sliding_window_size</code> <code>Optional[int]</code> <p>Number of times in one window. Impacts dataloader behavior. Batch sizes affects how much data will be cached for creating windows.</p> <code>sliding_window_prediction_size</code> <code>Optional[int]</code> <p>Number of times to predict from sliding_window_size. Impacts dataloader behavior. Batch sizes affects how much data will be cached for creating windows.</p> <code>sliding_window_step</code> <code>int</code> <p>Number of times to move by after each window.</p> <code>set_shared_size</code> <code>int | float</code> <p>How much times should time periods share. Order of sharing is training set &lt; validation set &lt; test set. Only in effect if sets share less values than set_shared_size. Use float value for percentage of total times or int for count.</p> <code>train_batch_size</code> <code>int</code> <p>Batch size for the train dataloader. Affects number of returned times in one batch.</p> <code>val_batch_size</code> <code>int</code> <p>Batch size for the validation dataloader. Affects number of returned times in one batch.</p> <code>test_batch_size</code> <code>int</code> <p>Batch size for the test dataloader. Affects number of returned times in one batch.</p> <code>all_batch_size</code> <code>int</code> <p>Batch size for the all dataloader. Affects number of returned times in one batch.</p> <code>preprocess_order</code> <code>list[PreprocessType]</code> <p>Defines in which order preprocesses are used. Also can add to order a type of <code>PerSeriesCustomHandler</code>, <code>AllSeriesCustomHandler</code> or <code>NoFitCustomHandler</code>.</p> <code>create_transformer_per_time_series</code> <code>bool</code> <p>If <code>True</code>, a separate transformer is created for each time series. Not used when using already initialized transformers.</p> <code>partial_fit_initialized_transformers</code> <code>bool</code> <p>If <code>True</code>, partial fitting on train set is performed when using initiliazed transformers.</p> <code>include_time</code> <code>bool</code> <p>If <code>True</code>, time data is included in the returned values.</p> <code>include_ts_id</code> <code>bool</code> <p>If <code>True</code>, time series IDs are included in the returned values.</p> <code>time_format</code> <code>TimeFormat</code> <p>Format for the returned time data. When using TimeFormat.DATETIME, time will be returned as separate list along rest of the values.</p> <code>train_workers</code> <code>int</code> <p>Number of workers for loading training data. <code>0</code> means that the data will be loaded in the main process.</p> <code>val_workers</code> <code>int</code> <p>Number of workers for loading validation data. <code>0</code> means that the data will be loaded in the main process.</p> <code>test_workers</code> <code>int</code> <p>Number of workers for loading test data. <code>0</code> means that the data will be loaded in the main process.</p> <code>all_workers</code> <code>int</code> <p>Number of workers for loading all data. <code>0</code> means that the data will be loaded in the main process.</p> <code>init_workers</code> <code>int</code> <p>Number of workers for initial dataset processing during configuration. <code>0</code> means that the data will be loaded in the main process.</p> <code>nan_threshold</code> <code>float</code> <p>Maximum allowable percentage of missing data. Time series exceeding this threshold are excluded. Time series over the threshold will not be used. Used for <code>train/val/test/all</code> separately.</p> <code>random_state</code> <code>Optional[int]</code> <p>Fixes randomness for reproducibility during configuration and dataset initialization.</p>"},{"location":"reference_transformers/","title":"Transformers","text":""},{"location":"reference_transformers/#cesnet_tszoo.utils.transformer.transformer","title":"cesnet_tszoo.utils.transformer.transformer","text":""},{"location":"reference_transformers/#cesnet_tszoo.utils.transformer.transformer.Transformer","title":"Transformer","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for transformers, used for transforming data.</p> <p>This class serves as the foundation for creating custom transformers. To implement a custom transformer, this class is recommended to be subclassed and extended.</p> <p>Example:</p> <pre><code>import numpy as np\n\nclass LogTransformer(Transformer):\n\n    def fit(self, data: np.ndarray):\n        ...\n\n    def partial_fit(self, data: np.ndarray) -&gt; None:\n        ...\n\n    def transform(self, data: np.ndarray):\n        log_data = np.ma.log(data)\n\n        return log_data.filled(np.nan)\n\n    def inverse_transform(self, transformed_data):\n        return np.exp(transformed_data)\n</code></pre> Source code in <code>cesnet_tszoo\\utils\\transformer\\transformer.py</code> <pre><code>class Transformer(ABC):\n    \"\"\"\n    Base class for transformers, used for transforming data.\n\n    This class serves as the foundation for creating custom transformers. To implement a custom transformer, this class is recommended to be subclassed and extended.\n\n    Example:\n\n        import numpy as np\n\n        class LogTransformer(Transformer):\n\n            def fit(self, data: np.ndarray):\n                ...\n\n            def partial_fit(self, data: np.ndarray) -&gt; None:\n                ...\n\n            def transform(self, data: np.ndarray):\n                log_data = np.ma.log(data)\n\n                return log_data.filled(np.nan)\n\n            def inverse_transform(self, transformed_data):\n                return np.exp(transformed_data)                \n    \"\"\"\n\n    @abstractmethod\n    def fit(self, data: np.ndarray) -&gt; None:\n        \"\"\"\n        Sets the transformer values for a given time series part.\n\n        This method must be implemented if using multiple transformers that have not been pre-fitted.\n\n        Parameters:\n            data: A numpy array representing data for a single time series with shape `(times, features)` excluding any identifiers.  \n        \"\"\"\n        ...\n\n    @abstractmethod\n    def partial_fit(self, data: np.ndarray) -&gt; None:\n        \"\"\"\n        Partially sets the transformer values for a given time series part.\n\n        This method must be implemented if using a single transformer that is not pre-fitted for all time series, or when using pre-fitted transformer(s) with `partial_fit_initialized_transformers` set to `True`.\n\n        Parameters:\n            data: A numpy array representing data for a single time series with shape `(times, features)` excluding any identifiers.        \n        \"\"\"\n        ...\n\n    @abstractmethod\n    def transform(self, data: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Transforms the input data for a given time series part.\n\n        This method must always be implemented.\n\n        Parameters:\n            data: A numpy array representing data for a single time series with shape `(times, features)` excluding any identifiers.  \n\n        Returns:\n            The transformed data, with the same shape as the input `(times, features)`.            \n        \"\"\"\n        ...\n\n    def inverse_transform(self, transformed_data: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Transforms the input transformed data to their original representation for a given time series part.\n\n        Parameters:\n            transformed_data: A numpy array representing data for a single time series with shape `(times, features)` excluding any identifiers.  \n\n        Returns:\n            The original representation of transformed data, with the same shape as the input `(times, features)`.            \n        \"\"\"\n        return transformed_data\n</code></pre>"},{"location":"reference_transformers/#cesnet_tszoo.utils.transformer.transformer.Transformer.fit","title":"fit  <code>abstractmethod</code>","text":"<pre><code>fit(data: ndarray) -&gt; None\n</code></pre> <p>Sets the transformer values for a given time series part.</p> <p>This method must be implemented if using multiple transformers that have not been pre-fitted.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>A numpy array representing data for a single time series with shape <code>(times, features)</code> excluding any identifiers.</p> required Source code in <code>cesnet_tszoo\\utils\\transformer\\transformer.py</code> <pre><code>@abstractmethod\ndef fit(self, data: np.ndarray) -&gt; None:\n    \"\"\"\n    Sets the transformer values for a given time series part.\n\n    This method must be implemented if using multiple transformers that have not been pre-fitted.\n\n    Parameters:\n        data: A numpy array representing data for a single time series with shape `(times, features)` excluding any identifiers.  \n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference_transformers/#cesnet_tszoo.utils.transformer.transformer.Transformer.inverse_transform","title":"inverse_transform","text":"<pre><code>inverse_transform(transformed_data: ndarray) -&gt; np.ndarray\n</code></pre> <p>Transforms the input transformed data to their original representation for a given time series part.</p> <p>Parameters:</p> Name Type Description Default <code>transformed_data</code> <code>ndarray</code> <p>A numpy array representing data for a single time series with shape <code>(times, features)</code> excluding any identifiers.  </p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The original representation of transformed data, with the same shape as the input <code>(times, features)</code>.</p> Source code in <code>cesnet_tszoo\\utils\\transformer\\transformer.py</code> <pre><code>def inverse_transform(self, transformed_data: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Transforms the input transformed data to their original representation for a given time series part.\n\n    Parameters:\n        transformed_data: A numpy array representing data for a single time series with shape `(times, features)` excluding any identifiers.  \n\n    Returns:\n        The original representation of transformed data, with the same shape as the input `(times, features)`.            \n    \"\"\"\n    return transformed_data\n</code></pre>"},{"location":"reference_transformers/#cesnet_tszoo.utils.transformer.transformer.Transformer.partial_fit","title":"partial_fit  <code>abstractmethod</code>","text":"<pre><code>partial_fit(data: ndarray) -&gt; None\n</code></pre> <p>Partially sets the transformer values for a given time series part.</p> <p>This method must be implemented if using a single transformer that is not pre-fitted for all time series, or when using pre-fitted transformer(s) with <code>partial_fit_initialized_transformers</code> set to <code>True</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>A numpy array representing data for a single time series with shape <code>(times, features)</code> excluding any identifiers.</p> required Source code in <code>cesnet_tszoo\\utils\\transformer\\transformer.py</code> <pre><code>@abstractmethod\ndef partial_fit(self, data: np.ndarray) -&gt; None:\n    \"\"\"\n    Partially sets the transformer values for a given time series part.\n\n    This method must be implemented if using a single transformer that is not pre-fitted for all time series, or when using pre-fitted transformer(s) with `partial_fit_initialized_transformers` set to `True`.\n\n    Parameters:\n        data: A numpy array representing data for a single time series with shape `(times, features)` excluding any identifiers.        \n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference_transformers/#cesnet_tszoo.utils.transformer.transformer.Transformer.transform","title":"transform  <code>abstractmethod</code>","text":"<pre><code>transform(data: ndarray) -&gt; np.ndarray\n</code></pre> <p>Transforms the input data for a given time series part.</p> <p>This method must always be implemented.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>A numpy array representing data for a single time series with shape <code>(times, features)</code> excluding any identifiers.  </p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The transformed data, with the same shape as the input <code>(times, features)</code>.</p> Source code in <code>cesnet_tszoo\\utils\\transformer\\transformer.py</code> <pre><code>@abstractmethod\ndef transform(self, data: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Transforms the input data for a given time series part.\n\n    This method must always be implemented.\n\n    Parameters:\n        data: A numpy array representing data for a single time series with shape `(times, features)` excluding any identifiers.  \n\n    Returns:\n        The transformed data, with the same shape as the input `(times, features)`.            \n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference_transformers/#cesnet_tszoo.utils.transformer.transformer.LogTransformer","title":"LogTransformer","text":"<p>               Bases: <code>Transformer</code></p> <p>Tranforms data with natural logarithm. Zero or invalid values are set to <code>np.nan</code>.</p> <p>Corresponds to enum <code>TransformerType.LOG_TRANSFORMER</code> or literal <code>log_transformer</code>.</p> Source code in <code>cesnet_tszoo\\utils\\transformer\\transformer.py</code> <pre><code>class LogTransformer(Transformer):\n    \"\"\"\n    Tranforms data with natural logarithm. Zero or invalid values are set to `np.nan`.\n\n    Corresponds to enum [`TransformerType.LOG_TRANSFORMER`][cesnet_tszoo.utils.enums.TransformerType] or literal `log_transformer`.\n    \"\"\"\n\n    def fit(self, data: np.ndarray):\n        ...\n\n    def partial_fit(self, data: np.ndarray) -&gt; None:\n        ...\n\n    def transform(self, data: np.ndarray):\n        log_data = np.ma.log(data)\n\n        return log_data.filled(np.nan)\n\n    def inverse_transform(self, transformed_data: np.ndarray):\n        return np.exp(transformed_data)\n</code></pre>"},{"location":"reference_transformers/#cesnet_tszoo.utils.transformer.transformer.L2Normalizer","title":"L2Normalizer","text":"<p>               Bases: <code>Transformer</code></p> <p>Tranforms data using Scikit <code>L2Normalizer</code>.</p> <p>Corresponds to enum <code>TransformerType.L2_NORMALIZER</code> or literal <code>l2_normalizer</code>.</p> Source code in <code>cesnet_tszoo\\utils\\transformer\\transformer.py</code> <pre><code>class L2Normalizer(Transformer):\n    \"\"\"\n    Tranforms data using Scikit [`L2Normalizer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html).\n\n    Corresponds to enum [`TransformerType.L2_NORMALIZER`][cesnet_tszoo.utils.enums.TransformerType] or literal `l2_normalizer`.\n    \"\"\"\n\n    def __init__(self):\n        self.transformer = sk.Normalizer(norm=\"l2\")\n\n    def fit(self, data: np.ndarray):\n        ...\n\n    def partial_fit(self, data: np.ndarray) -&gt; None:\n        ...\n\n    def transform(self, data: np.ndarray):\n        return self.transformer.fit_transform(data)\n\n    def inverse_transform(self, transformed_data: np.ndarray):\n        raise NotImplementedError(\"Normalizer does not support inverse_transform.\")\n</code></pre>"},{"location":"reference_transformers/#cesnet_tszoo.utils.transformer.transformer.MinMaxScaler","title":"MinMaxScaler","text":"<p>               Bases: <code>Transformer</code></p> <p>Tranforms data using Scikit <code>MinMaxScaler</code>.</p> <p>Corresponds to enum <code>TransformerType.MIN_MAX_SCALER</code> or literal <code>min_max_scaler</code>.</p> Source code in <code>cesnet_tszoo\\utils\\transformer\\transformer.py</code> <pre><code>class MinMaxScaler(Transformer):\n    \"\"\"\n    Tranforms data using Scikit [`MinMaxScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html).\n\n    Corresponds to enum [`TransformerType.MIN_MAX_SCALER`][cesnet_tszoo.utils.enums.TransformerType] or literal `min_max_scaler`.\n    \"\"\"\n\n    def __init__(self):\n        self.transformer = sk.MinMaxScaler()\n\n    def fit(self, data: np.ndarray) -&gt; None:\n        self.transformer.fit(data)\n\n    def partial_fit(self, data: np.ndarray) -&gt; None:\n        self.transformer.partial_fit(data)\n\n    def transform(self, data: np.ndarray) -&gt; np.ndarray:\n        return self.transformer.transform(data)\n\n    def inverse_transform(self, transformed_data: np.ndarray) -&gt; np.ndarray:\n        return self.transformer.inverse_transform(transformed_data)\n</code></pre>"},{"location":"reference_transformers/#cesnet_tszoo.utils.transformer.transformer.StandardScaler","title":"StandardScaler","text":"<p>               Bases: <code>Transformer</code></p> <p>Tranforms data using Scikit <code>StandardScaler</code>.</p> <p>Corresponds to enum <code>TransformerType.STANDARD_SCALER</code> or literal <code>standard_scaler</code>.</p> Source code in <code>cesnet_tszoo\\utils\\transformer\\transformer.py</code> <pre><code>class StandardScaler(Transformer):\n    \"\"\"\n    Tranforms data using Scikit [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).\n\n    Corresponds to enum [`TransformerType.STANDARD_SCALER`][cesnet_tszoo.utils.enums.TransformerType] or literal `standard_scaler`.\n    \"\"\"\n\n    def __init__(self):\n        self.transformer = sk.StandardScaler()\n\n    def fit(self, data: np.ndarray) -&gt; None:\n        self.transformer.fit(data)\n\n    def partial_fit(self, data: np.ndarray) -&gt; None:\n        self.transformer.partial_fit(data)\n\n    def transform(self, data: np.ndarray) -&gt; np.ndarray:\n        return self.transformer.transform(data)\n\n    def inverse_transform(self, transformed_data: np.ndarray):\n        return self.transformer.inverse_transform(transformed_data)\n</code></pre>"},{"location":"reference_transformers/#cesnet_tszoo.utils.transformer.transformer.MaxAbsScaler","title":"MaxAbsScaler","text":"<p>               Bases: <code>Transformer</code></p> <p>Tranforms data using Scikit <code>MaxAbsScaler</code>.</p> <p>Corresponds to enum <code>TransformerType.MAX_ABS_SCALER</code> or literal <code>max_abs_scaler</code>.</p> Source code in <code>cesnet_tszoo\\utils\\transformer\\transformer.py</code> <pre><code>class MaxAbsScaler(Transformer):\n    \"\"\"\n    Tranforms data using Scikit [`MaxAbsScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html).\n\n    Corresponds to enum [`TransformerType.MAX_ABS_SCALER`][cesnet_tszoo.utils.enums.TransformerType] or literal `max_abs_scaler`.\n    \"\"\"\n\n    def __init__(self):\n        self.transformer = sk.MaxAbsScaler()\n\n    def fit(self, data: np.ndarray):\n        self.transformer.fit(data)\n\n    def partial_fit(self, data: np.ndarray) -&gt; None:\n        self.transformer.partial_fit(data)\n\n    def transform(self, data: np.ndarray):\n        return self.transformer.transform(data)\n\n    def inverse_transform(self, transformed_data: np.ndarray):\n        return self.transformer.inverse_transform(transformed_data)\n</code></pre>"},{"location":"reference_transformers/#cesnet_tszoo.utils.transformer.transformer.PowerTransformer","title":"PowerTransformer","text":"<p>               Bases: <code>Transformer</code></p> <p>Tranforms data using Scikit <code>PowerTransformer</code>.</p> <p>Corresponds to enum <code>TransformerType.POWER_TRANSFORMER</code> or literal <code>power_transformer</code>.</p> <p>partial_fit not supported</p> <p>Because this transformer does not support partial_fit it can't be used when using one transformer that needs to be fitted for multiple time series.</p> Source code in <code>cesnet_tszoo\\utils\\transformer\\transformer.py</code> <pre><code>class PowerTransformer(Transformer):\n    \"\"\"\n    Tranforms data using Scikit [`PowerTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html).\n\n    Corresponds to enum [`TransformerType.POWER_TRANSFORMER`][cesnet_tszoo.utils.enums.TransformerType] or literal `power_transformer`.\n\n    !!! warning \"partial_fit not supported\"\n        Because this transformer does not support partial_fit it can't be used when using one transformer that needs to be fitted for multiple time series.\n    \"\"\"\n\n    def __init__(self):\n        self.transformer = sk.PowerTransformer()\n\n    def fit(self, data: np.ndarray):\n        self.transformer.fit(data)\n\n    def partial_fit(self, data: np.ndarray) -&gt; None:\n        raise NotImplementedError(\"PowerTransformer does not support partial_fit.\")\n\n    def transform(self, data: np.ndarray):\n        return self.transformer.transform(data)\n\n    def inverse_transform(self, transformed_data: np.ndarray):\n        return self.transformer.inverse_transform(transformed_data)\n</code></pre>"},{"location":"reference_transformers/#cesnet_tszoo.utils.transformer.transformer.QuantileTransformer","title":"QuantileTransformer","text":"<p>               Bases: <code>Transformer</code></p> <p>Tranforms data using Scikit <code>QuantileTransformer</code>.</p> <p>Corresponds to enum <code>TransformerType.QUANTILE_TRANSFORMER</code> or literal <code>quantile_transformer</code>.</p> <p>partial_fit not supported</p> <p>Because this transformer does not support partial_fit it can't be used when using one transformer that needs to be fitted for multiple time series.</p> Source code in <code>cesnet_tszoo\\utils\\transformer\\transformer.py</code> <pre><code>class QuantileTransformer(Transformer):\n    \"\"\"\n    Tranforms data using Scikit [`QuantileTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html).\n\n    Corresponds to enum [`TransformerType.QUANTILE_TRANSFORMER`][cesnet_tszoo.utils.enums.TransformerType] or literal `quantile_transformer`.\n\n    !!! warning \"partial_fit not supported\"\n        Because this transformer does not support partial_fit it can't be used when using one transformer that needs to be fitted for multiple time series.    \n    \"\"\"\n\n    def __init__(self):\n        self.transformer = sk.QuantileTransformer()\n\n    def fit(self, data: np.ndarray):\n        self.transformer.fit(data)\n\n    def partial_fit(self, data: np.ndarray) -&gt; None:\n        raise NotImplementedError(\"QuantileTransformer does not support partial_fit.\")\n\n    def transform(self, data: np.ndarray):\n        return self.transformer.transform(data)\n\n    def inverse_transform(self, transformed_data: np.ndarray):\n        return self.transformer.inverse_transform(transformed_data)\n</code></pre>"},{"location":"reference_transformers/#cesnet_tszoo.utils.transformer.transformer.RobustScaler","title":"RobustScaler","text":"<p>               Bases: <code>Transformer</code></p> <p>Tranforms data using Scikit <code>RobustScaler</code>.</p> <p>Corresponds to enum <code>TransformerType.ROBUST_SCALER</code> or literal <code>robust_scaler</code>.</p> <p>partial_fit not supported</p> <p>Because this transformer does not support partial_fit it can't be used when using one transformer that needs to be fitted for multiple time series.</p> Source code in <code>cesnet_tszoo\\utils\\transformer\\transformer.py</code> <pre><code>class RobustScaler(Transformer):\n    \"\"\"\n    Tranforms data using Scikit [`RobustScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html).\n\n    Corresponds to enum [`TransformerType.ROBUST_SCALER`][cesnet_tszoo.utils.enums.TransformerType] or literal `robust_scaler`.\n\n    !!! warning \"partial_fit not supported\"\n        Because this transformer does not support partial_fit it can't be used when using one transformer that needs to be fitted for multiple time series.    \n    \"\"\"\n\n    def __init__(self):\n        self.transformer = sk.RobustScaler()\n\n    def fit(self, data: np.ndarray):\n        self.transformer.fit(data)\n\n    def partial_fit(self, data: np.ndarray) -&gt; None:\n        raise NotImplementedError(\"RobustScaler does not support partial_fit.\")\n\n    def transform(self, data: np.ndarray):\n        return self.transformer.transform(data)\n\n    def inverse_transform(self, transformed_data: np.ndarray):\n        return self.transformer.inverse_transform(transformed_data)\n</code></pre>"},{"location":"reference_transformers/#cesnet_tszoo.utils.transformer.transformer.NoTransformer","title":"NoTransformer","text":"<p>               Bases: <code>Transformer</code></p> <p>Does nothing.</p> <p>Corresponds to enum <code>TransformerType.NO_TRANSFORMER</code> or literal <code>no_transformer</code>.</p> Source code in <code>cesnet_tszoo\\utils\\transformer\\transformer.py</code> <pre><code>class NoTransformer(Transformer):\n    \"\"\"\n    Does nothing.\n\n    Corresponds to enum [`TransformerType.NO_TRANSFORMER`][cesnet_tszoo.utils.enums.TransformerType] or literal `no_transformer`.\n    \"\"\"\n\n    def fit(self, data: np.ndarray):\n        ...\n\n    def partial_fit(self, data: np.ndarray) -&gt; None:\n        ...\n\n    def transform(self, data: np.ndarray) -&gt; np.ndarray:\n        return data\n\n    def inverse_transform(self, transformed_data: np.ndarray) -&gt; np.ndarray:\n        return transformed_data\n</code></pre>"},{"location":"transformers/","title":"Transformers","text":"<p>The <code>cesnet_tszoo</code> package supports various ways of using transformers to transform data. Transformer(s) can be created and fitted (on train set) when initializing dataset with config. Or already fitted transformer(s) can be passed to transform data.</p>"},{"location":"transformers/#built-in-transformers","title":"Built-in transformers","text":"<p>The <code>cesnet_tszoo</code> package comes with multiple built-in transformers. Not all of them support <code>partial_fit</code> though. To check built-in transformers refer to <code>transformers</code>.</p>"},{"location":"transformers/#custom-transformers","title":"Custom transformers","text":"<p>It is possible to create and use own transformers. It is recommended to use built-in base class <code>Transformer</code>.</p>"},{"location":"transformers/#using-transformers-on-time-based-dataset","title":"Using transformers on time-based dataset","text":"<p>Related config parameters in <code>TimeBasedConfig</code>:</p> <ul> <li><code>transform_with</code>:  Defines the transformer(s) to transform the dataset. Can pass enum <code>TransformerType</code> for built-in transformer, pass a type of custom transformer or instance of already fitted transformer(s).</li> <li><code>create_transformer_per_time_series</code>: Whether to create a separate transformer for each time series or create one transformer for all time series.</li> <li><code>partial_fit_initialized_transformers</code>: Whether to <code>partial_fit</code> already fitted transformer(s).</li> </ul> <p>fit vs partial_fit</p> <p>When <code>create_transformer_per_time_series</code> = <code>True</code> and transformers are not pre-fitted, transformers must implement <code>fit</code> method. Else if you want to fit transformers, <code>partial_fit</code> method must be implemented. Check <code>Transformer</code> for details.</p>"},{"location":"transformers/#using-transformers-on-disjoint-time-based-dataset","title":"Using transformers on disjoint-time-based dataset","text":"<p>Disjoint-time-based dataset always uses <code>create_transformer_per_time_series</code> = <code>False</code>. Related config parameters in <code>DisjointTimeBasedConfig</code>:</p> <ul> <li><code>transform_with</code>:  Defines the transformer to transform the dataset. Can pass enum <code>TransformerType</code> for built-in transformer, pass a type of custom transformer or instance of already fitted transformer.</li> <li><code>partial_fit_initialized_transformers</code>: Whether to <code>partial_fit</code> already fitted transformer.</li> </ul> <p>partial_fit</p> <p>Transformer must implement <code>partial_fit</code> method unless using already fitted transformer without fitting it on train data. Check <code>Transformer</code> for details.   </p>"},{"location":"transformers/#using-transformers-on-series-based-dataset","title":"Using transformers on series-based dataset","text":"<p>Series-based dataset always uses <code>create_transformer_per_time_series</code> = <code>False</code>. Related config parameters in <code>SeriesBasedConfig</code>:</p> <ul> <li><code>transform_with</code>:  Defines the transformer to transform the dataset. Can pass enum <code>TransformerType</code> for built-in transformer, pass a type of custom transformer or instance of already fitted transformer.</li> <li><code>partial_fit_initialized_transformers</code>: Whether to <code>partial_fit</code> already fitted transformer.</li> </ul> <p>partial_fit</p> <p>Transformer must implement <code>partial_fit</code> method unless using already fitted transformer without fitting it on train data. Check <code>Transformer</code> for details.    </p>"},{"location":"using_anomaly_handlers/","title":"Using anomaly handlers","text":"<p>This tutorial will look at some configuration options for using anomaly handlers.</p> <p>Only time-based will be used, because all methods work almost the same way for other dataset types.</p> <p>Note</p> <p>For every configuration and more detailed examples refer to Jupyter notebook <code>using_anomaly_handlers</code></p> <p>Relevant configuration values:</p> <ul> <li><code>handle_anomalies_with</code> - Defines the anomaly handler used to transform anomalies in the train set.</li> </ul>"},{"location":"using_anomaly_handlers/#anomaly-handlers","title":"Anomaly handlers","text":"<ul> <li>Anomaly handlers are implemented as class.<ul> <li>You can create your own or use built-in one.</li> </ul> </li> <li>Every time series in train set has its own anomaly handler instance.</li> <li>Anomaly handler must implement <code>fit</code> and <code>transform_anomalies</code>.</li> <li>To use anomaly handler, train set must be implemented.</li> <li>Anomaly handler will only be used on train set.</li> <li>You can change used anomaly handler later with <code>update_dataset_config_and_initialize</code> or <code>apply_anomaly_handler</code>.</li> </ul>"},{"location":"using_anomaly_handlers/#built-in","title":"Built-in","text":"<p>To see all built-in anomaly handlers refer to <code>Anomaly handlers</code>.</p> <pre><code>from cesnet_tszoo.utils.enums import AnomalyHandlerType\nfrom cesnet_tszoo.configs import TimeBasedConfig\n\nconfig = TimeBasedConfig(ts_ids=500, train_time_period=0.5, val_time_period=0.2, test_time_period=0.1, features_to_take=['n_flows', 'n_packets'],\n                         handle_anomalies_with=AnomalyHandlerType.Z_SCORE, nan_threshold=0.5, random_state=1500)                                                                           \n\n# Call on time-based dataset to use created config\ntime_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre> <p>Or later with:</p> <pre><code>time_based_dataset.update_dataset_config_and_initialize(handle_anomalies_with=AnomalyHandlerType.Z_SCORE, workers=0)\n# Or\ntime_based_dataset.apply_anomaly_handler(handle_anomalies_with=AnomalyHandlerType.Z_SCORE, workers=0)\n</code></pre>"},{"location":"using_anomaly_handlers/#custom","title":"Custom","text":"<p>You can create your own custom anomaly handler. It is recommended to derive from 'AnomalyHandler' base class. </p> <p>To check AnomalyHandler base class refer to <code>AnomalyHandler</code></p> <pre><code>import numpy as np\nfrom cesnet_tszoo.utils.anomaly_handler import AnomalyHandler\nfrom cesnet_tszoo.configs import TimeBasedConfig\n\nclass CustomAnomalyHandler(AnomalyHandler):\n    def __init__(self):\n        self.lower_bound = None\n        self.upper_bound = None\n        self.iqr = None\n\n    def fit(self, data: np.ndarray) -&gt; None:\n        q25, q75 = np.percentile(data, [25, 75], axis=0)\n        self.iqr = q75 - q25\n\n        self.lower_bound = q25 - 1.5 * self.iqr\n        self.upper_bound = q75 + 1.5 * self.iqr\n\n    def transform_anomalies(self, data: np.ndarray) -&gt; np.ndarray:\n        mask_lower_outliers = data &lt; self.lower_bound\n        mask_upper_outliers = data &gt; self.upper_bound\n\n        data[mask_lower_outliers] = np.take(self.lower_bound, np.where(mask_lower_outliers)[1])\n        data[mask_upper_outliers] = np.take(self.upper_bound, np.where(mask_upper_outliers)[1])              \n\nconfig = TimeBasedConfig(ts_ids=500, train_time_period=0.5, val_time_period=0.2, test_time_period=0.1, features_to_take=['n_flows', 'n_packets'],\n                           handle_anomalies_with=CustomAnomalyHandler, nan_threshold=0.5, random_state=1500)                                                                    \n\ntime_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre> <p>Or later with:</p> <pre><code>time_based_dataset.update_dataset_config_and_initialize(handle_anomalies_with=CustomAnomalyHandler, workers=0)\n# Or\ntime_based_dataset.apply_anomaly_handler(handle_anomalies_with=CustomAnomalyHandler, workers=0)\n</code></pre>"},{"location":"using_anomaly_handlers/#changing-when-is-anomaly-handler-applied","title":"Changing when is anomaly handler applied","text":"<ul> <li>You can change when is a anomaly handler applied with <code>preprocess_order</code> parameter</li> </ul> <pre><code>from cesnet_tszoo.utils.utils.enums import AnomalyHandlerType\nfrom cesnet_tszoo.configs import TimeBasedConfig\n\nconfig = TimeBasedConfig(ts_ids=500, train_time_period=0.5, val_time_period=0.2, test_time_period=0.1, features_to_take=['n_flows', 'n_packets'],\n                           handle_anomalies_with=AnomalyHandlerType.Z_SCORE, nan_threshold=0.5, random_state=1500, preprocess_order=[\"handling_anomalies\", \"filling_gaps\", \"transforming\"])\n\n# Call on dataset to use created config\ntime_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre> <p>Or later with:</p> <pre><code>time_based_dataset.update_dataset_config_and_initialize(preprocess_order=[\"filling_gaps\", \"handling_anomalies\", \"transforming\"], workers=0)\n# Or\ntime_based_dataset.set_preprocess_order(preprocess_order=[\"filling_gaps\", \"handling_anomalies\", \"transforming\"], workers=0)\n</code></pre>"},{"location":"using_custom_handlers/","title":"Using custom handlers","text":"<p>This tutorial will look at some configuration options for using custom handlers.</p> <p>Only time-based will be used, because all methods work almost the same way for other dataset types.</p> <p>Note</p> <p>For every configuration and more detailed examples refer to Jupyter notebook <code>using_custom_handlers</code></p> <p>Relevant configuration values:</p> <ul> <li><code>preprocess_order</code> - Mainly used for changing order of preprocesses. Is also used as a way of adding custom handlers by adding type their type between the preprocesses.</li> </ul>"},{"location":"using_custom_handlers/#custom-handlers","title":"Custom handlers","text":"<ul> <li>Custom handlers are implemented as a class<ul> <li>Their main purpose is to allow creation of custom preprocessing steps</li> </ul> </li> <li>There are three types of custom handlers: <code>AllSeriesCustomHandler</code>, <code>PerSeriesCustomHandler</code>, <code>NoFitCustomHandler</code></li> <li>Custom handlers can be used by adding their type to <code>preprocessing_order</code> -&gt; which will also define when they will be applied</li> <li>All custom handler types allow specifying to which set they can be applied</li> <li>You can change used custom handlers later with <code>update_dataset_config_and_initialize</code> or <code>set_preprocess_order</code>, by modifying <code>preprocessing_order</code> parameter.</li> <li>Take care that all custom handlers should be imported from other file when while using this library in Jupyter notebook. When not importing from other file/s use workers == 0.</li> </ul>"},{"location":"using_custom_handlers/#allseriescustomhandler","title":"AllSeriesCustomHandler","text":"<ul> <li>You can refer to <code>AllSeriesCustomHandler</code></li> <li>One instance is created for all time series</li> <li>Must always be fitted on train set before use</li> </ul> <pre><code>import numpy as np\nfrom cesnet_tszoo.utils.custom_handler import AllSeriesCustomHandler\nfrom cesnet_tszoo.configs import TimeBasedConfig\n\nclass AllFitTest(AllSeriesCustomHandler):\n\n    def __init__(self):\n        self.count = 0\n        super().__init__()\n\n    def partial_fit(self, data: np.ndarray) -&gt; None:\n        self.count += 1\n\n    def apply(self, data: np.ndarray) -&gt; np.ndarray:\n        data[:, :] = self.count\n        return data\n\n    @staticmethod\n    def get_target_sets():\n        return [\"train\"]\n\n\nconfig = TimeBasedConfig(ts_ids=500, train_time_period=0.5, val_time_period=0.2, test_time_period=0.1, features_to_take=['n_flows', 'n_packets'],\n                        nan_threshold=0.5, random_state=1500, preprocess_order=[\"handling_anomalies\", \"filling_gaps\", \"transforming\", AllFitTest])\n\ntime_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre> <p>Or later with:</p> <pre><code>time_based_dataset.update_dataset_config_and_initialize(preprocess_order=[\"handling_anomalies\", AllFitTest, \"filling_gaps\", \"transforming\"], workers=0)\n# Or\ntime_based_dataset.set_preprocess_order(preprocess_order=[\"handling_anomalies\", AllFitTest, \"filling_gaps\", \"transforming\"], workers=0)\n</code></pre>"},{"location":"using_custom_handlers/#perseriescustomhandler","title":"PerSeriesCustomHandler","text":"<ul> <li>You can refer to <code>PerSeriesCustomHandler</code></li> <li>One instance is created per time series</li> <li>Must always be fitted on train set before use</li> <li>Supported only for Time-Based dataset</li> </ul> <pre><code>import numpy as np\nfrom cesnet_tszoo.utils.custom_handler import PerSeriesCustomHandler\nfrom cesnet_tszoo.configs import TimeBasedConfig\n\nclass PerFitTest(PerSeriesCustomHandler):\n\n    def __init__(self):\n        self.count = 0\n        super().__init__()\n\n    def fit(self, data: np.ndarray) -&gt; None:\n        self.count += 1\n\n    def apply(self, data: np.ndarray) -&gt; np.ndarray:\n        data[:, :] = self.count\n        return data\n\n    @staticmethod\n    def get_target_sets():\n        return [\"val\"]\n\n\n\nconfig = TimeBasedConfig(ts_ids=500, train_time_period=0.5, val_time_period=0.2, test_time_period=0.1, features_to_take=['n_flows', 'n_packets'],\n                        nan_threshold=0.5, random_state=1500, preprocess_order=[\"handling_anomalies\", \"filling_gaps\", \"transforming\", PerFitTest])\n\ntime_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre> <p>Or later with:</p> <pre><code>time_based_dataset.update_dataset_config_and_initialize(preprocess_order=[\"handling_anomalies\", PerFitTest, \"filling_gaps\", \"transforming\"], workers=0)\n# Or\ntime_based_dataset.set_preprocess_order(preprocess_order=[\"handling_anomalies\", PerFitTest, \"filling_gaps\", \"transforming\"], workers=0)\n</code></pre>"},{"location":"using_custom_handlers/#nofitcustomhandler","title":"NoFitCustomHandler","text":"<ul> <li>You can refer to <code>NoFitCustomHandler</code></li> <li>One instance is created per time series</li> <li>Does not require nor supports fitting</li> </ul> <pre><code>import numpy as np\nfrom cesnet_tszoo.utils.custom_handler import NoFitCustomHandler\nfrom cesnet_tszoo.configs import TimeBasedConfig\n\nclass NoFitTest(NoFitCustomHandler):\n    def apply(self, data: np.ndarray) -&gt; np.ndarray:\n        data[:, :] = -1\n        return data\n\n    @staticmethod\n    def get_target_sets():\n        return [\"test\"]\n\n\n\nconfig = TimeBasedConfig(ts_ids=500, train_time_period=0.5, val_time_period=0.2, test_time_period=0.1, features_to_take=['n_flows', 'n_packets'],\n                        nan_threshold=0.5, random_state=1500, preprocess_order=[\"handling_anomalies\", \"filling_gaps\", \"transforming\", NoFitTest])\n\ntime_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre> <p>Or later with:</p> <pre><code>time_based_dataset.update_dataset_config_and_initialize(preprocess_order=[\"handling_anomalies\", NoFitTest, \"filling_gaps\", \"transforming\"], workers=0)\n# Or\ntime_based_dataset.set_preprocess_order(preprocess_order=[\"handling_anomalies\", NoFitTest, \"filling_gaps\", \"transforming\"], workers=0)\n</code></pre>"},{"location":"using_datasets/","title":"Using datasets","text":"<p>This tutorial will look at what you need to use dataset.  Trying to use dataset you do not have downloaded, will automatically download it.</p> <p>There currently two supported datasets:</p> <ul> <li>CESNET-TimeSeries24 - supports all approaches</li> <li>CESNET-AGG23 - supports only time-based</li> </ul>"},{"location":"using_datasets/#using-dataset-from-benchmark","title":"Using dataset from benchmark","text":"<p>You can refer to benchmarks for more detailed usage.</p> <pre><code>from cesnet_tszoo.benchmarks import load_benchmark                                                                       \n\n# Imports built-in benchmark\nbenchmark = load_benchmark(identifier=\"2e92831cb502\", data_root=\"/some_directory/\")\ndataset = benchmark.get_initialized_dataset(display_config_details=\"text\", check_errors=False, workers=\"config\")\n\n# Imports custom benchmark\nbenchmark = load_benchmark(identifier=\"test2\", data_root=\"/some_directory/\")\ndataset = benchmark.get_initialized_dataset(display_config_details=\"text\", check_errors=False, workers=\"config\")\n</code></pre>"},{"location":"using_datasets/#creating-dataset","title":"Creating dataset","text":"<p>You can refer to choosing_data for more detailed data selection via config.</p>"},{"location":"using_datasets/#using-dataset-from-cesnet_timeseries24","title":"Using dataset from CESNET_TimeSeries24","text":"<pre><code>from cesnet_tszoo.configs import TimeBasedConfig # For time-based dataset\nfrom cesnet_tszoo.configs import SeriesBasedConfig # For series-based dataset   \nfrom cesnet_tszoo.configs import DisjointTimeBasedConfig # For disjoint-time-based dataset\n\nfrom cesnet_tszoo.utils.enums import SourceType, AgreggationType, DatasetType # Used for specifying which dataset to use\nfrom cesnet_tszoo.datasets import CESNET_TimeSeries24\n\n# Time-based\ntime_based_dataset = CESNET_TimeSeries24.get_dataset(data_root=\"/some_directory/\", source_type=SourceType.INSTITUTIONS, aggregation=AgreggationType.AGG_1_DAY, dataset_type=DatasetType.TIME_BASED)\nconfig = TimeBasedConfig(ts_ids=50)\ntime_based_dataset.set_dataset_config_and_initialize(config)\n\n# Disjoint-time-based\ndisjoint_dataset = CESNET_TimeSeries24.get_dataset(data_root=\"/some_directory/\", source_type=SourceType.INSTITUTIONS, aggregation=AgreggationType.AGG_1_DAY, dataset_type=DatasetType.DISJOINT_TIME_BASED)\nconfig = DisjointTimeBasedConfig(train_ts=50, val_ts=None, test_ts=None, train_time_period=range(0, 200))\ndisjoint_dataset.set_dataset_config_and_initialize(config)\n\n# Series-based\nseries_based_dataset = CESNET_TimeSeries24.get_dataset(data_root=\"/some_directory/\", source_type=SourceType.INSTITUTIONS, aggregation=AgreggationType.AGG_1_DAY, dataset_type=DatasetType.SERIES_BASED)\nconfig = SeriesBasedDataset(time_period=range(0, 200))\nseries_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre>"},{"location":"using_datasets/#using-dataset-from-cesnet_agg23","title":"Using dataset from CESNET_AGG23","text":"<pre><code>from cesnet_tszoo.configs import TimeBasedConfig # For time-based dataset \n\nfrom cesnet_tszoo.datasets import CESNET_AGG23\n\n# Using dataset from CESNET_AGG23\n# Only time-based\ntime_based_dataset = CESNET_TimeSeries24.get_dataset(data_root=\"/some_directory/\")\nconfig = TimeBasedConfig(ts_ids=1)\ntime_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre>"},{"location":"using_transformers/","title":"Using transformers","text":"<p>This tutorial will look at some configuration options for using transformers.</p> <p>Each dataset type will have its own part because of multiple differences of available configuration values.</p>"},{"location":"using_transformers/#timebasedcesnetdataset-dataset","title":"<code>TimeBasedCesnetDataset</code> dataset","text":"<p>Note</p> <p>For every configuration and more detailed examples refer to Jupyter notebook <code>time_based_using_transformers</code></p> <p>Relevant configuration values:</p> <ul> <li><code>transform_with</code> - Defines the transformer used to transform the dataset.</li> <li><code>create_transformer_per_time_series</code> - If True, a separate transformer is created for each time series.</li> <li><code>partial_fit_initialized_transformers</code> - If True, partial fitting on train set is performed when using initiliazed transformers.</li> </ul>"},{"location":"using_transformers/#transformers","title":"Transformers","text":"<ul> <li>Transformers are implemented as class.<ul> <li>You can create your own or use built-in one.</li> </ul> </li> <li>Transformer must implement <code>transform</code>.</li> <li>Transformer can implement <code>inverse_transform</code>.</li> <li>To use transformers, train set must be implemented (unless transformers are already fitted and <code>partial_fit_initialized_transformers</code> is False).</li> <li><code>fit</code> method on transformer:<ul> <li>must be implemented when <code>create_transformer_per_time_series</code> is True and transformers are not already fitted.</li> </ul> </li> <li><code>partial_fit</code> method on transformer:<ul> <li>must be implemented when <code>create_transformer_per_time_series</code> is False or using already fitted transformers with <code>partial_fit_initialized_transformers</code> set to True.</li> </ul> </li> <li>You can change used transformer later with <code>update_dataset_config_and_initialize</code> or <code>apply_transformer</code>.</li> </ul>"},{"location":"using_transformers/#built-in","title":"Built-in","text":"<p>To see all built-in transformers refer to <code>Transformers</code>.</p> <pre><code>from cesnet_tszoo.utils.enums import TransformerType\nfrom cesnet_tszoo.configs import TimeBasedConfig\n\nconfig = TimeBasedConfig(ts_ids=[1367, 1368], train_time_period=0.5, val_time_period=0.2, test_time_period=0.1, features_to_take=['n_flows', 'n_packets'],\n                         transform_with=TransformerType.MIN_MAX_SCALER, create_transformer_per_time_series=True)                                                                              \n\n# Call on time-based dataset to use created config\ntime_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre> <p>Or later with:</p> <pre><code>time_based_dataset.update_dataset_config_and_initialize(transform_with=TransformerType.MIN_MAX_SCALER, create_transformer_per_time_series=True, partial_fit_initialized_transformers=\"config\", workers=0)\n# Or\ntime_based_dataset.apply_transformer(transform_with=TransformerType.MIN_MAX_SCALER, create_transformer_per_time_series=True, partial_fit_initialized_transformers=\"config\", workers=0)\n</code></pre>"},{"location":"using_transformers/#custom","title":"Custom","text":"<p>You can create your own custom transformer. It is recommended to derive from 'Transformer' base class. </p> <p>To check Transformer base class refer to <code>Transformer</code></p> <pre><code>from cesnet_tszoo.utils.transformer import Transformer\nfrom cesnet_tszoo.configs import TimeBasedConfig\n\nclass CustomTransformer(Transformer):\n    def __init__(self):\n        super().__init__()\n\n        self.max = None\n        self.min = None\n\n    def transform(self, data):\n        return (data - self.min) / (self.max - self.min)\n\n    def fit(self, data):\n        self.partial_fit(data)\n\n    def partial_fit(self, data):\n\n        if self.max is None and self.min is None:\n            self.max = np.max(data, axis=0)\n            self.min = np.min(data, axis=0)\n            return\n\n        temp_max = np.max(data, axis=0)\n        temp = np.vstack((self.max, temp_max)) \n        self.max = np.max(temp, axis=0)\n\n        temp_min = np.min(data, axis=0)\n        temp = np.vstack((self.min, temp_min)) \n        self.min = np.min(temp, axis=0)      \n\n    def inverse_transform(self, transformed_data):\n        return transformed_data * (self.max - self.min) + self.min           \n\nconfig = TimeBasedConfig(ts_ids=[1367, 1368], train_time_period=0.5, val_time_period=0.2, test_time_period=0.1, features_to_take=['n_flows', 'n_packets'],\n                         transform_with=CustomTransformer, create_transformer_per_time_series=True)                                                                        \n\ntime_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre> <p>Or later with:</p> <pre><code>time_based_dataset.update_dataset_config_and_initialize(transform_with=CustomTransformer, create_transformer_per_time_series=True, partial_fit_initialized_transformers=\"config\", workers=0)\n# Or\ntime_based_dataset.apply_transformer(transform_with=CustomTransformer, create_transformer_per_time_series=True, partial_fit_initialized_transformers=\"config\", workers=0)\n</code></pre>"},{"location":"using_transformers/#using-already-fitted-transformers","title":"Using already fitted transformers","text":"<pre><code>from cesnet_tszoo.configs import TimeBasedConfig         \n\nconfig = TimeBasedConfig(ts_ids=[103, 118], train_time_period=0.5, val_time_period=0.2, test_time_period=0.1, features_to_take=['n_flows', 'n_packets'],\n                         transform_with=list_of_fitted_transformers, create_transformer_per_time_series=True)    \n\n# Length of list_of_fitted_transformers must be equal to number of time series in ts_ids \n# All transformers in list_of_fitted_transformers must be of same type                                                            \n\nconfig = TimeBasedConfig(ts_ids=[103, 118], train_time_period=0.5, val_time_period=0.2, test_time_period=0.1, features_to_take=['n_flows', 'n_packets'],\n                         transform_with=one_prefitted_transformer, create_transformer_per_time_series=True)\n\n# one_prefitted_transformer must be just one transformer (not a list)                     \n\ntime_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre>"},{"location":"using_transformers/#getting-pre-transform-value","title":"Getting pre-transform value","text":"<ul> <li>You can use <code>inverse_transform</code> for transformers you can get via <code>get_transformers()</code> to get pre-transform value.</li> <li><code>inverse_transformer</code> expects input as numpy array of shape <code>(times, features)</code> where features do not contain ids.</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import TransformerType\nfrom cesnet_tszoo.configs import TimeBasedConfig         \n\nconfig = TimeBasedConfig(ts_ids=[1367, 1368], train_time_period=0.5, val_time_period=0.2, test_time_period=0.1, features_to_take=['n_flows', 'n_packets'],\n                         transform_with=TransformerType.MIN_MAX_SCALER, create_transformer_per_time_series=False)   \n\ntime_based_dataset.set_dataset_config_and_initialize(config)                                                                           \n\ntransformer = time_based_dataset.get_transformers()\n\ndata = None\nfor batch in time_based_dataset.get_train_dataloader():\n    data = batch[0, :, 2:]\n    break\n\ntransformer.inverse_transform(data)[:10]\n</code></pre>"},{"location":"using_transformers/#changing-when-are-transformers-applied","title":"Changing when are transformers applied","text":"<ul> <li>You can change when is a transformer applied with <code>preprocess_order</code> parameter</li> </ul> <pre><code>from cesnet_tszoo.utils.utils.enums import TransformerType\nfrom cesnet_tszoo.configs import TimeBasedConfig\n\nconfig = TimeBasedConfig(ts_ids=[1367, 1368], train_time_period=0.5, val_time_period=0.2, test_time_period=0.1, features_to_take=['n_flows', 'n_packets'],\n                         transform_with=TransformerType.MIN_MAX_SCALER, create_transformer_per_time_series=True, random_state=111, preprocess_order=[\"handling_anomalies\", \"filling_gaps\", \"transforming\"])\n\n# Call on time-based dataset to use created config\ntime_based_dataset.set_dataset_config_and_initialize(config)    \n</code></pre> <p>Or later with:</p> <pre><code>time_based_dataset.update_dataset_config_and_initialize(preprocess_order=[\"handling_anomalies\", \"transforming\", \"filling_gaps\"], workers=0)\n# Or\ntime_based_dataset.set_preprocess_order(preprocess_order=[\"handling_anomalies\", \"transforming\", \"filling_gaps\"], workers=0)\n</code></pre>"},{"location":"using_transformers/#disjointtimebasedcesnetdataset-dataset","title":"<code>DisjointTimeBasedCesnetDataset</code> dataset","text":"<p>Note</p> <p>For every configuration and more detailed examples refer to Jupyter notebook <code>disjoint_time_based_using_transformers</code></p> <p>Relevant configuration values:</p> <ul> <li><code>transform_with</code> - Defines the transformer used to transform the dataset.</li> <li><code>partial_fit_initialized_transformers</code> - If True, partial fitting on train set is performed when using initiliazed transformers.</li> </ul>"},{"location":"using_transformers/#transformers_1","title":"Transformers","text":"<ul> <li>Transformers are implemented as class.<ul> <li>You can create your own or use built-in one.</li> </ul> </li> <li>One transformer is used for all time series.</li> <li>Transformer must implement <code>transform</code>.</li> <li>Transformer can implement <code>inverse_transform</code>.</li> <li>Transformer must implement <code>partial_fit</code> (unless transformer is already fitted and <code>partial_fit_initialized_transformers</code> is False).</li> <li>To use transformer, train set must be implemented (unless transformer is already fitted and <code>partial_fit_initialized_transformers</code> is False).</li> <li>You can change used transformer later with <code>update_dataset_config_and_initialize</code> or <code>apply_transformer</code>.</li> </ul>"},{"location":"using_transformers/#built-in_1","title":"Built-in","text":"<p>To see all built-in transformers refer to <code>Transformers</code>.</p> <pre><code>from cesnet_tszoo.utils.enums import TransformerType\nfrom cesnet_tszoo.configs import DisjointTimeBasedConfig\n\nconfig = DisjointTimeBasedConfig(train_ts=500, val_ts=None, test_ts=None, train_time_period=0.5, features_to_take=[\"n_flows\", \"n_packets\"],\n                                 transform_with=TransformerType.MIN_MAX_SCALER, nan_threshold=0.5, random_state=1500)                                                                          \n\n# Call on disjoint-time-based dataset to use created config\ndisjoint_dataset.set_dataset_config_and_initialize(config)\n</code></pre> <p>Or later with:</p> <pre><code>disjoint_dataset.update_dataset_config_and_initialize(transform_with=TransformerType.MIN_MAX_SCALER, partial_fit_initialized_transformers=\"config\", workers=0)\n# Or\ndisjoint_dataset.apply_transformer(transform_with=TransformerType.MIN_MAX_SCALER, partial_fit_initialized_transformers=\"config\", workers=0)\n</code></pre>"},{"location":"using_transformers/#custom_1","title":"Custom","text":"<p>You can create your own custom transformer. It is recommended to derive from 'Transformer' base class. </p> <p>To check Transformer base class refer to <code>Transformer</code></p> <pre><code>from cesnet_tszoo.utils.transformer import Transformer\nfrom cesnet_tszoo.configs import DisjointTimeBasedConfig\n\nclass CustomTransformer(Transformer):\n    def __init__(self):\n        super().__init__()\n\n        self.max = None\n        self.min = None\n\n    def transform(self, data):\n        return (data - self.min) / (self.max - self.min)\n\n    def fit(self, data):\n        self.partial_fit(data)\n\n    def partial_fit(self, data):\n\n        if self.max is None and self.min is None:\n            self.max = np.max(data, axis=0)\n            self.min = np.min(data, axis=0)\n            return\n\n        temp_max = np.max(data, axis=0)\n        temp = np.vstack((self.max, temp_max)) \n        self.max = np.max(temp, axis=0)\n\n        temp_min = np.min(data, axis=0)\n        temp = np.vstack((self.min, temp_min)) \n        self.min = np.min(temp, axis=0)      \n\n    def inverse_transform(self, transformed_data):\n        return transformed_data * (self.max - self.min) + self.min           \n\nconfig = DisjointTimeBasedConfig(train_ts=500, val_ts=None, test_ts=None, train_time_period=0.5, features_to_take=[\"n_flows\", \"n_packets\"],\n                                 transform_with=CustomTransformer, nan_threshold=0.5, random_state=1500)                                                                      \n\ndisjoint_dataset.set_dataset_config_and_initialize(config)\n</code></pre> <p>Or later with:</p> <pre><code>disjoint_dataset.update_dataset_config_and_initialize(transform_with=CustomTransformer, partial_fit_initialized_transformers=\"config\", workers=0)\n# Or\ndisjoint_dataset.apply_transformer(transform_with=CustomTransformer, partial_fit_initialized_transformers=\"config\", workers=0)\n</code></pre>"},{"location":"using_transformers/#using-already-fitted-transformers_1","title":"Using already fitted transformers","text":"<pre><code>from cesnet_tszoo.configs import DisjointTimeBasedConfig                                                               \n\nconfig = DisjointTimeBasedConfig(train_ts=500, val_ts=500, test_ts=None, train_time_period=0.5, val_time_period=0.5, features_to_take=[\"n_flows\", \"n_packets\"],\n                                 transform_with=one_prefitted_transformer, nan_threshold=0.5, random_state=999)\n\n# one_prefitted_transformer must be just one transformer (not a list)                     \n\ntime_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre>"},{"location":"using_transformers/#getting-pre-transform-value_1","title":"Getting pre-transform value","text":"<ul> <li>You can use <code>inverse_transform</code> for transformers you can get via <code>get_transformers()</code> to get pre-transform value.</li> <li><code>inverse_transformer</code> expects input as numpy array of shape <code>(times, features)</code> where features do not contain ids.</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import TransformerType\nfrom cesnet_tszoo.configs import DisjointTimeBasedConfig\n\nconfig = DisjointTimeBasedConfig(train_ts=500, val_ts=None, test_ts=None, train_time_period=0.5, features_to_take=[\"n_flows\", \"n_packets\"],\n                                 transform_with=TransformerType.MIN_MAX_SCALER, nan_threshold=0.5, random_state=1500)                                                                          \n\n# Call on disjoint-time-based dataset to use created config\ndisjoint_dataset.set_dataset_config_and_initialize(config)                                                                           \n\ntransformer = disjoint_dataset.get_transformers()\n\ndata = None\nfor batch in disjoint_dataset.get_train_dataloader():\n    data = batch[0, :, 2:]\n    break\n\ntransformer.inverse_transform(data)[:10]\n</code></pre>"},{"location":"using_transformers/#changing-when-is-transformer-applied","title":"Changing when is transformer applied","text":"<ul> <li>You can change when is a transformer applied with <code>preprocess_order</code> parameter</li> </ul> <pre><code>from cesnet_tszoo.utils.utils.enums import TransformerType\nfrom cesnet_tszoo.configs import DisjointTimeBasedConfig\n\nconfig = DisjointTimeBasedConfig(train_ts=500, val_ts=None, test_ts=None, train_time_period=0.5, features_to_take=[\"n_flows\", \"n_packets\"],\n                           transform_with=TransformerType.MIN_MAX_SCALER, nan_threshold=0.5, random_state=1500, preprocess_order=[\"handling_anomalies\", \"filling_gaps\", \"transforming\"])\n\n# Call on disjoint-time-based dataset to use created config\ndisjoint_dataset.set_dataset_config_and_initialize(config)    \n</code></pre> <p>Or later with:</p> <pre><code>disjoint_dataset.update_dataset_config_and_initialize(preprocess_order=[\"handling_anomalies\", \"transforming\", \"filling_gaps\"], workers=0)\n# Or\ndisjoint_dataset.set_preprocess_order(preprocess_order=[\"handling_anomalies\", \"transforming\", \"filling_gaps\"], workers=0)\n</code></pre>"},{"location":"using_transformers/#seriesbasedcesnetdataset-dataset","title":"<code>SeriesBasedCesnetDataset</code> dataset","text":"<p>Note</p> <p>For every configuration and more detailed examples refer to Jupyter notebook <code>series_based_using_transformers</code></p> <p>Relevant configuration values:</p> <ul> <li><code>transform_with</code> - Defines the transformer used to transform the dataset.</li> <li><code>partial_fit_initialized_transformers</code> - If True, partial fitting on train set is performed when using initiliazed transformer.</li> </ul>"},{"location":"using_transformers/#transformers_2","title":"Transformers","text":"<ul> <li>Transformers are implemented as class.<ul> <li>You can create your own or use built-in one.</li> </ul> </li> <li>One transformer is used for all time series.</li> <li>Transformer must implement <code>transform</code>.</li> <li>Transformer can implement <code>inverse_transform</code>.</li> <li>Transformer must implement <code>partial_fit</code> (unless transformer is already fitted and <code>partial_fit_initialized_transformers</code> is False).</li> <li>To use transformer, train set must be implemented (unless transformer is already fitted and <code>partial_fit_initialized_transformers</code> is False).</li> <li>You can change used transformer later with <code>update_dataset_config_and_initialize</code> or <code>apply_transformer</code>.</li> </ul>"},{"location":"using_transformers/#built-in_2","title":"Built-in","text":"<p>To see all built-in transformers refer to <code>Transformers</code>.</p> <pre><code>from cesnet_tszoo.utils.enums import TransformerType\nfrom cesnet_tszoo.configs import SeriesBasedConfig\n\nconfig = SeriesBasedConfig(time_period=0.5, train_ts=500, features_to_take=[\"n_flows\", \"n_packets\"],\n                           transform_with=TransformerType.MIN_MAX_SCALER, nan_threshold=0.5, random_state=1500)                                                                          \n\n# Call on series-based dataset to use created config\nseries_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre> <p>Or later with:</p> <pre><code>series_based_dataset.update_dataset_config_and_initialize(transform_with=TransformerType.MIN_MAX_SCALER, partial_fit_initialized_transformers=\"config\", workers=0)\n# Or\nseries_based_dataset.apply_transformer(transform_with=TransformerType.MIN_MAX_SCALER, partial_fit_initialized_transformers=\"config\", workers=0)\n</code></pre>"},{"location":"using_transformers/#custom_2","title":"Custom","text":"<p>You can create your own custom transformer. It is recommended to derive from 'Transformer' base class. </p> <p>To check Transformer base class refer to <code>Transformer</code></p> <pre><code>from cesnet_tszoo.utils.transformer import Transformer\nfrom cesnet_tszoo.configs import SeriesBasedConfig\n\nclass CustomTransformer(Transformer):\n    def __init__(self):\n        super().__init__()\n\n        self.max = None\n        self.min = None\n\n    def transform(self, data):\n        return (data - self.min) / (self.max - self.min)\n\n    def fit(self, data):\n        self.partial_fit(data)\n\n    def partial_fit(self, data):\n\n        if self.max is None and self.min is None:\n            self.max = np.max(data, axis=0)\n            self.min = np.min(data, axis=0)\n            return\n\n        temp_max = np.max(data, axis=0)\n        temp = np.vstack((self.max, temp_max)) \n        self.max = np.max(temp, axis=0)\n\n        temp_min = np.min(data, axis=0)\n        temp = np.vstack((self.min, temp_min)) \n        self.min = np.min(temp, axis=0)      \n\n    def inverse_transform(self, transformed_data):\n        return transformed_data * (self.max - self.min) + self.min           \n\nconfig = SeriesBasedConfig(time_period=0.5, train_ts=500, features_to_take=[\"n_flows\", \"n_packets\"],\n                           transform_with=CustomTransformer, nan_threshold=0.5, random_state=1500)                                                                    \n\nseries_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre> <p>Or later with:</p> <pre><code>series_based_dataset.update_dataset_config_and_initialize(transform_with=CustomTransformer, partial_fit_initialized_transformers=\"config\", workers=0)\n# Or\nseries_based_dataset.apply_transformer(transform_with=CustomTransformer, partial_fit_initialized_transformers=\"config\", workers=0)\n</code></pre>"},{"location":"using_transformers/#using-already-fitted-transformers_2","title":"Using already fitted transformers","text":"<pre><code>from cesnet_tszoo.configs import SeriesBasedConfig         \n\nconfig = SeriesBasedConfig(time_period=0.5, val_ts=500, features_to_take=[\"n_flows\", \"n_packets\"],\n                           transform_with=fitted_transformer, nan_threshold=0.5, random_state=999)   \n\n# fitted_transformer must be just one transformer (not a list)                                                                     \n\nseries_based_dataset.set_dataset_config_and_initialize(config)\n</code></pre>"},{"location":"using_transformers/#getting-pre-transform-value_2","title":"Getting pre-transform value","text":"<ul> <li>You can use <code>inverse_transform</code> for transformers you can get via <code>get_transformers()</code> to get pre-transform value.</li> <li><code>inverse_transformer</code> expects input as numpy array of shape <code>(times, features)</code> where features do not contain ids.</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import TransformerType\nfrom cesnet_tszoo.configs import SeriesBasedConfig\n\nconfig = SeriesBasedConfig(time_period=0.5, train_ts=500, features_to_take=[\"n_flows\", \"n_packets\"],\n                           transform_with=TransformerType.MIN_MAX_SCALER, nan_threshold=0.5, random_state=1500)                                                                          \n\n# Call on series-based dataset to use created config\nseries_based_dataset.set_dataset_config_and_initialize(config)                                                                       \n\ntransformer = series_based_dataset.get_transformers()\n\ndata = None\nfor batch in series_based_dataset.get_train_dataloader():\n    data = batch[0, :, 2:]\n    break\n\ntransformer.inverse_transform(data)[:10]\n</code></pre>"},{"location":"using_transformers/#changing-when-is-transformer-applied_1","title":"Changing when is transformer applied","text":"<ul> <li>You can change when is a transformer applied with <code>preprocess_order</code> parameter</li> </ul> <pre><code>from cesnet_tszoo.utils.utils.enums import TransformerType\nfrom cesnet_tszoo.configs import SeriesBasedConfig\n\nconfig = SeriesBasedConfig(time_period=0.5, train_ts=500, features_to_take=[\"n_flows\", \"n_packets\"],\n                           transform_with=TransformerType.MIN_MAX_SCALER, nan_threshold=0.5, random_state=1500, preprocess_order=[\"handling_anomalies\", \"filling_gaps\", \"transforming\"])\n\n# Call on series-based dataset to use created config\nseries_based_dataset.set_dataset_config_and_initialize(config)    \n</code></pre> <p>Or later with:</p> <pre><code>series_based_dataset.update_dataset_config_and_initialize(preprocess_order=[\"handling_anomalies\", \"transforming\", \"filling_gaps\"], workers=0)\n# Or\nseries_based_dataset.set_preprocess_order(preprocess_order=[\"handling_anomalies\", \"transforming\", \"filling_gaps\"], workers=0)\n</code></pre>"},{"location":"utilities/","title":"Utilities","text":"<p>This tutorial will look at various utilities.</p> <p>Only time-based will be used, because all methods work almost the same way for other dataset types.</p> <p>Note</p> <p>For every option and more detailed examples refer to Jupyter notebook <code>utilities</code></p>"},{"location":"utilities/#setting-logger","title":"Setting logger","text":"<p>CESNET TS-Zoo uses logger, but without setting config below, it wont log anything.</p> <pre><code>import logging                                                                       \n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"[%(asctime)s][%(name)s][%(levelname)s] - %(message)s\")\n</code></pre>"},{"location":"utilities/#checking-errors","title":"Checking errors","text":"<ul> <li>Goes through all data in dataset to check whether everything is in correct state,</li> <li>Can be called when creating dataset or with method <code>check_errors</code> on already create dataset.</li> <li>Recommended to call at least once after download</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import SourceType, AgreggationType, DatasetType\nfrom cesnet_tszoo.datasets import CESNET_TimeSeries24                                                                \n\n# Can be called at dataset creation\ntime_based_dataset = CESNET_TimeSeries24.get_dataset(data_root=\"/some_directory/\", source_type=SourceType.IP_ADDRESSES_SAMPLE, aggregation=AgreggationType.AGG_1_DAY, dataset_type=DatasetType.TIME_BASED, check_errors=True)\n\n# Or after it\ntime_based_dataset.check_errors()\n</code></pre>"},{"location":"utilities/#dataset-details","title":"Dataset details","text":""},{"location":"utilities/#displaying-all-data-about-selected-dataset","title":"Displaying all data about selected dataset","text":"<p>Displays available times, time series, features with their default values, additional data provided by dataset.</p> <pre><code>dataset.display_dataset_details()\n</code></pre>"},{"location":"utilities/#get-list-of-available-features","title":"Get list of available features","text":"<pre><code>dataset.get_feature_names()\n</code></pre>"},{"location":"utilities/#get-numpy-array-of-available-dataset-time-series-indices","title":"Get numpy array of available dataset time series indices","text":"<pre><code>dataset.get_available_ts_indices()\n</code></pre>"},{"location":"utilities/#get-dictionary-of-related-set-data","title":"Get dictionary of related set data","text":"<p>Returns all data in dictionary related to set.</p> <pre><code>from cesnet_tszoo.configs import TimeBasedConfig   \n\nconfig = TimeBasedConfig(20, train_time_period=0.5)\ntime_based_dataset.set_dataset_config_and_initialize(config, workers=0, display_config_details=None)\n\ntime_based_dataset.get_data_about_set(about=SplitType.TRAIN)\n</code></pre>"},{"location":"utilities/#displaying-config-details","title":"Displaying config details","text":"<ul> <li>There are two ways to display preprocessing details, with text or diagram. </li> <li>Can be called when calling <code>set_dataset_config_and_initialize</code> or after it with <code>summary</code></li> </ul>"},{"location":"utilities/#displaying-as-a-text","title":"Displaying as a text","text":"<pre><code>from cesnet_tszoo.configs import TimeBasedConfig   \n\nconfig = TimeBasedConfig(20)\n\n# Can be called during initialization\ntime_based_dataset.set_dataset_config_and_initialize(config, workers=0, display_config_details=\"text\")\n\n# Or after it\ntime_based_dataset.summary(display_type=\"text\")\n</code></pre>"},{"location":"utilities/#displaying-as-a-html-interactive-diagram","title":"Displaying as a html interactive diagram","text":"<pre><code>from cesnet_tszoo.configs import TimeBasedConfig   \n\nconfig = TimeBasedConfig(20)\n\n# Can be called during initialization\ntime_based_dataset.set_dataset_config_and_initialize(config, workers=0, display_config_details=\"diagram\")\n\n# Or after it\ntime_based_dataset.summary(display_type=\"diagram\")\n</code></pre> <p>You can see example below:</p> <p></p> <p>You can also save the display diagram as html file</p> <pre><code>time_based_dataset.save_summary_diagram_as_html(path=\"/diagram.html\")\n</code></pre>"},{"location":"utilities/#plotting","title":"Plotting","text":"<ul> <li>Uses <code>Plotly</code> library.</li> <li>You can plot specific time series with method <code>plot</code></li> <li>You can set <code>ts_id</code> to any time series id used in config</li> <li>Config must be set before using</li> </ul> <pre><code># Features will be taken from config\ndataset.plot(ts_id=10, plot_type=\"line\", features=\"config\", feature_per_plot=True, time_format=\"datetime\")\n\n# Specifies features as list... features must be set in used config\ndataset.plot(ts_id=10, plot_type=\"line\", features=[\"n_flows\", \"n_packets\"], feature_per_plot=True, time_format=\"datetime\")\n\n# Can specify single feature... still must be set in used config\ndataset.plot(ts_id=10, plot_type=\"line\", features=\"n_flows\", feature_per_plot=True, time_format=\"datetime\")\n</code></pre>"},{"location":"utilities/#get-additional-data","title":"Get additional data","text":"<ul> <li>You can check whether dataset has additional data, with method <code>display_dataset_details</code>.</li> </ul> <pre><code>from cesnet_tszoo.utils.enums import SourceType, AgreggationType, DatasetType\nfrom cesnet_tszoo.datasets import CESNET_TimeSeries24                                                                \n\ntime_based_dataset = CESNET_TimeSeries24.get_dataset(data_root=\"/some_directory/\", source_type=SourceType.IP_ADDRESSES_SAMPLE, aggregation=AgreggationType.AGG_1_DAY, dataset_type=DatasetType.TIME_BASED, display_details=True)\n\n# Available additional data in CESNET_TimeSeries24 database\ntime_based_dataset.get_additional_data('ids_relationship')\ntime_based_dataset.get_additional_data('weekends_and_holidays')\n</code></pre>"},{"location":"utilities/#get-fitted-transformers","title":"Get fitted transformers","text":"<p>Returns used transformer/s that are used for transforming data.</p> <pre><code>dataset.get_transformers()\n</code></pre>"},{"location":"benchmarks/device_type_classification/69270dcc1819/","title":"Benchmark 69270dcc1819","text":"Parameter Value Benchmark hash 0d523e69c328 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source IP_ADDRESSES_FULL Type Disjoint-Time-Based Train time size 0.6 Val time size 0.2 Test time size 0.2 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train None Sliding window prediction None Sliding window step None Set shared size None All batch size 7* Train TS IDs 0.6 Val TS IDs 0.2 Test TS IDs 0.2 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Accuracy Precision Recall F1-score"},{"location":"benchmarks/device_type_classification/941261e8c367/","title":"Benchmark 941261e8c367","text":"Parameter Value Benchmark hash 0d523e69c328 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source IP_ADDRESSES_FULL Type Disjoint-Time-Based Train time size 0.6 Val time size 0.2 Test time size 0.2 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train None Sliding window prediction None Sliding window step None Set shared size None All batch size 7* Train TS IDs 0.6 Val TS IDs 0.2 Test TS IDs 0.2 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Accuracy Precision Recall F1-score"},{"location":"benchmarks/device_type_classification/bf0aec939afe/","title":"Benchamrk bf0aec939afe","text":"Parameter Value Benchmark hash 0d523e69c328 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source IP_ADDRESSES_FULL Type Disjoint-Time-Based Train time size 0.6 Val time size 0.2 Test time size 0.2 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train None Sliding window prediction None Sliding window step None Set shared size None All batch size 7* Train TS IDs 0.6 Val TS IDs 0.2 Test TS IDs 0.2 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Accuracy Precision Recall F1-score"},{"location":"benchmarks/multivariate_forecasting/generic_model/0197980a87c0/","title":"Benchmark 0197980a87c0","text":"Parameter Value Benchmark hash 0197980a87c0 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source IP_ADDRESSES_FULL Type Disjoint-Time-Based Train time size 0.6 Val time size 0.1 Test time size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 7* Sliding window prediction 1* Sliding window step 1* Set shared size 7* Train TS IDs 0.6 Val TS IDs 0.1 Test TS IDs 0.3 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/multivariate_forecasting/generic_model/16274e0b44af/","title":"Benchmark 16274e0b44af","text":"Parameter Value Benchmark hash 16274e0b44af Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source IP_ADDRESSES_SAMPLE Type Disjoint-Time-Based Train time size 0.6 Val time size 0.1 Test time size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 7* Sliding window prediction 1* Sliding window step 1* Set shared size 7* Train TS IDs 0.6 Val TS IDs 0.1 Test TS IDs 0.3 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/multivariate_forecasting/generic_model/4ae11863ee38/","title":"Benchmark 4ae11863ee38","text":"Parameter Value Benchmark hash 4ae11863ee38 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source IP_ADDRESSES_FULL Type Disjoint-Time-Based Train time size 0.6 Val time size 0.1 Test time size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 168* Sliding window prediction 1* Sliding window step 1* Set shared size 168* Train TS IDs 0.6 Val TS IDs 0.1 Test TS IDs 0.3 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/multivariate_forecasting/generic_model/50eb509e1e77/","title":"Benchmark 50eb509e1e77","text":"Parameter Value Benchmark hash 50eb509e1e77 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source IP_ADDRESSES_SAMPLE Type Disjoint-Time-Based Train time size 0.6 Val time size 0.1 Test time size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 1008* Sliding window prediction 1* Sliding window step 1* Set shared size 1008* Train TS IDs 0.6 Val TS IDs 0.1 Test TS IDs 0.3 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/multivariate_forecasting/generic_model/681a7fb90948/","title":"Benchmark 681a7fb90948","text":"Parameter Value Benchmark hash 681a7fb90948 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source IP_ADDRESSES_FULL Type Disjoint-Time-Based Train time size 0.6 Val time size 0.1 Test time size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 1008* Sliding window prediction 1* Sliding window step 1* Set shared size 1008* Train TS IDs 0.6 Val TS IDs 0.1 Test TS IDs 0.3 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/multivariate_forecasting/generic_model/7cd4e41b05ec/","title":"Benchmark 7cd4e41b05ec","text":"Parameter Value Benchmark hash 7cd4e41b05ec Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source INSTITUTION_SUBNETS Type Disjoint-Time-Based Train time size 0.6 Val time size 0.1 Test time size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 1008* Sliding window prediction 1* Sliding window step 1* Set shared size 1008* Train TS IDs 0.6 Val TS IDs 0.1 Test TS IDs 0.3 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/multivariate_forecasting/generic_model/88fd173619b2/","title":"Benchmark 88fd173619b2","text":"Parameter Value Benchmark hash 88fd173619b2 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source IP_ADDRESSES_SAMPLE Type Disjoint-Time-Based Train time size 0.6 Val time size 0.1 Test time size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 168* Sliding window prediction 1* Sliding window step 1* Set shared size 168* Train TS IDs 0.6 Val TS IDs 0.1 Test TS IDs 0.3 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/multivariate_forecasting/generic_model/9ac2b87c9a7c/","title":"Benchmark 9ac2b87c9a7c","text":"Parameter Value Benchmark hash 9ac2b87c9a7c Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source INSTITUTIONS Type Disjoint-Time-Based Train time size 0.6 Val time size 0.1 Test time size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 1008* Sliding window prediction 1* Sliding window step 1* Set shared size 1008* Train TS IDs 0.6 Val TS IDs 0.1 Test TS IDs 0.3 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/multivariate_forecasting/generic_model/ab8183ea80af/","title":"Benchmark ab8183ea80af","text":"Parameter Value Benchmark hash ab8183ea80af Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTIONS Type Disjoint-Time-Based Train time size 0.6 Val time size 0.1 Test time size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 168* Sliding window prediction 1* Sliding window step 1* Set shared size 168* Train TS IDs 0.6 Val TS IDs 0.1 Test TS IDs 0.3 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/multivariate_forecasting/generic_model/c95d66b0baf5/","title":"Benchmark c95d66b0baf5","text":"Parameter Value Benchmark hash c95d66b0baf5 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source INSTITUTION_SUBNETS Type Disjoint-Time-Based Train time size 0.6 Val time size 0.1 Test time size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 7* Sliding window prediction 1* Sliding window step 1* Set shared size 7* Train TS IDs 0.6 Val TS IDs 0.1 Test TS IDs 0.3 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/multivariate_forecasting/generic_model/cdb79dbf54ea/","title":"Benchmark cdb79dbf54ea","text":"Parameter Value Benchmark hash cdb79dbf54ea Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source INSTITUTIONS Type Disjoint-Time-Based Train time size 0.6 Val time size 0.1 Test time size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 7* Sliding window prediction 1* Sliding window step 1* Set shared size 7* Train TS IDs 0.6 Val TS IDs 0.1 Test TS IDs 0.3 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/multivariate_forecasting/generic_model/f9bd005c7efe/","title":"Benchmark f9bd005c7efe","text":"Parameter Value Benchmark hash f9bd005c7efe Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTION_SUBNETS Type Disjoint-Time-Based Train time size 0.6 Val time size 0.1 Test time size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 168* Sliding window prediction 1* Sliding window step 1* Set shared size 168* Train TS IDs 0.6 Val TS IDs 0.1 Test TS IDs 0.3 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/multivariate_forecasting/unique_model/084f368f4c82/","title":"Benchmark 084f368f4c82","text":"Parameter Value Benchmark hash 084f368f4c82 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source IP_ADDRESSES_FULL Type Time-Based Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 7* Sliding window prediction 1* Sliding window step 1* Set shared size 7* TS IDs 1.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/multivariate_forecasting/unique_model/15737f3fceec/","title":"Benchmark 15737f3fceec","text":"Parameter Value Benchmark hash 15737f3fceec Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source IP_ADDRESSES_SAMPLE Type Time-Based Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 7* Sliding window prediction 1* Sliding window step 1* Set shared size 7* TS IDs 1.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/multivariate_forecasting/unique_model/18d04cab63e4/","title":"Benchmark 18d04cab63e4","text":"Parameter Value Benchmark hash 18d04cab63e4 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source IP_ADDRESSES_SAMPLE Type Time-Based Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 168* Sliding window prediction 1* Sliding window step 1* Set shared size 168* TS IDs 1.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/multivariate_forecasting/unique_model/3687fb52c433/","title":"Benchmark 3687fb52c433","text":"Parameter Value Benchmark hash 3687fb52c433 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source IP_ADDRESSES_FULL Type Time-Based Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 1008* Sliding window prediction 1* Sliding window step 1* Set shared size 1008* TS IDs 1.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/multivariate_forecasting/unique_model/63e1f696e7c5/","title":"Benchmark 63e1f696e7c5","text":"Parameter Value Benchmark hash 63e1f696e7c5 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source INSTITUTIONS Type Time-Based Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 7* Sliding window prediction 1* Sliding window step 1* Set shared size 7* TS IDs 1.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/multivariate_forecasting/unique_model/71d17ad3550f/","title":"Benchmark 71d17ad3550f","text":"Parameter Value Benchmark hash 71d17ad3550f Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source INSTITUTION_SUBNETS Type Time-Based Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 7* Sliding window prediction 1* Sliding window step 1* Set shared size 7* TS IDs 1.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/multivariate_forecasting/unique_model/7495b16f5fe6/","title":"Benchmark 7495b16f5fe6","text":"Parameter Value Benchmark hash 7495b16f5fe6 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source IP_ADDRESSES_SAMPLE Type Time-Based Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 1008* Sliding window prediction 1* Sliding window step 1* Set shared size 1008* TS IDs 1.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/multivariate_forecasting/unique_model/930f0b401065/","title":"Benchmark 930f0b401065","text":"Parameter Value Benchmark hash 930f0b401065 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source INSTITUTIONS Type Time-Based Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 1008* Sliding window prediction 1* Sliding window step 1* Set shared size 1008* TS IDs 1.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/multivariate_forecasting/unique_model/a6e56f99ab8a/","title":"Benchmark a6e56f99ab8a","text":"Parameter Value Benchmark hash a6e56f99ab8a Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTIONS Type Time-Based Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 168* Sliding window prediction 1* Sliding window step 1* Set shared size 168* TS IDs 1.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/multivariate_forecasting/unique_model/b0ea46897cae/","title":"Benchmark b0ea46897cae","text":"Parameter Value Benchmark hash b0ea46897cae Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source IP_ADDRESSES_FULL Type Time-Based Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 168* Sliding window prediction 1* Sliding window step 1* Set shared size 168* TS IDs 1.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/multivariate_forecasting/unique_model/ca6999ea7e24/","title":"Benchmark ca6999ea7e24","text":"Parameter Value Benchmark hash ca6999ea7e24 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source INSTITUTION_SUBNETS Type Time-Based Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 1008* Sliding window prediction 1* Sliding window step 1* Set shared size 1008* TS IDs 1.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/multivariate_forecasting/unique_model/e44334732033/","title":"Benchmark e44334732033","text":"Parameter Value Benchmark hash e44334732033 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTION_SUBNETS Type Time-Based Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Multivariate Metrics all Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 168* Sliding window prediction 1* Sliding window step 1* Set shared size 168* TS IDs 1.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/univariate_forecasting/generic_model/09de83e89e42/","title":"Benchmark 09de83e89e42","text":"Parameter Value Benchmark hash 09de83e89e42 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTIONS Type Disjoint-Time-Based Train time size 0.6 Val time size 0.1 Test time size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 168* Sliding window prediction 1* Sliding window step 1* Set shared size 168* Train TS IDs 0.6 Val TS IDs 0.1 Test TS IDs 0.3 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/univariate_forecasting/generic_model/570b215d790d/","title":"Benchmark 570b215d790d","text":"Parameter Value Benchmark hash 570b215d790d Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source IP_ADDRESSES_FULL Type Disjoint-Time-Based Train time size 0.6 Val time size 0.1 Test time size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 7* Sliding window prediction 1* Sliding window step 1* Set shared size 7* Train TS IDs 0.6 Val TS IDs 0.1 Test TS IDs 0.3 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/univariate_forecasting/generic_model/6249383544ef/","title":"Benchmark 6249383544ef","text":"Parameter Value Benchmark hash 6249383544ef Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source IP_ADDRESSES_SAMPLE Type Disjoint-Time-Based Train time size 0.6 Val time size 0.1 Test time size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 168* Sliding window prediction 1* Sliding window step 1* Set shared size 168* Train TS IDs 0.6 Val TS IDs 0.1 Test TS IDs 0.3 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/univariate_forecasting/generic_model/73a9add2c4af/","title":"Benchmark 73a9add2c4af","text":"Parameter Value Benchmark hash 73a9add2c4af Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTION_SUBNETS Type Disjoint-Time-Based Train time size 0.6 Val time size 0.1 Test time size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 168* Sliding window prediction 1* Sliding window step 1* Set shared size 168* Train TS IDs 0.6 Val TS IDs 0.1 Test TS IDs 0.3 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/univariate_forecasting/generic_model/7706f1087922/","title":"Benchmark 7706f1087922","text":"Parameter Value Benchmark hash 7706f1087922 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source INSTITUTIONS Type Disjoint-Time-Based Train time size 0.6 Val time size 0.1 Test time size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 1008* Sliding window prediction 1* Sliding window step 1* Set shared size 1008* Train TS IDs 0.6 Val TS IDs 0.1 Test TS IDs 0.3 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/univariate_forecasting/generic_model/8b03d0d508ce/","title":"Benchmark 8b03d0d508ce","text":"Parameter Value Benchmark hash 8b03d0d508ce Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source IP_ADDRESSES_FULL Type Disjoint-Time-Based Train time size 0.6 Val time size 0.1 Test time size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 1008* Sliding window prediction 1* Sliding window step 1* Set shared size 1008* Train TS IDs 0.6 Val TS IDs 0.1 Test TS IDs 0.3 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/univariate_forecasting/generic_model/9f7047902d66/","title":"Benchmark 9f7047902d66","text":"Parameter Value Benchmark hash 9f7047902d66 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source IP_ADDRESSES_SAMPLE Type Disjoint-Time-Based Train time size 0.6 Val time size 0.1 Test time size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 7* Sliding window prediction 1* Sliding window step 1* Set shared size 7* Train TS IDs 0.6 Val TS IDs 0.1 Test TS IDs 0.3 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/univariate_forecasting/generic_model/a642915953ad/","title":"Benchmark a642915953ad","text":"Parameter Value Benchmark hash a642915953ad Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source INSTITUTION_SUBNETS Type Disjoint-Time-Based Train time size 0.6 Val time size 0.1 Test time size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 1008* Sliding window prediction 1* Sliding window step 1* Set shared size 1008* Train TS IDs 0.6 Val TS IDs 0.1 Test TS IDs 0.3 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/univariate_forecasting/generic_model/b8098753b97b/","title":"Benchmark b8098753b97b","text":"Parameter Value Benchmark hash b8098753b97b Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source IP_ADDRESSES_FULL Type Disjoint-Time-Based Train time size 0.6 Val time size 0.1 Test time size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 168* Sliding window prediction 1* Sliding window step 1* Set shared size 168* Train TS IDs 0.6 Val TS IDs 0.1 Test TS IDs 0.3 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/univariate_forecasting/generic_model/ce63551ffaab/","title":"Benchmark ce63551ffaab","text":"Parameter Value Benchmark hash ce63551ffaab Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source INSTITUTION_SUBNETS Type Disjoint-Time-Based Train time size 0.6 Val time size 0.1 Test time size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 7* Sliding window prediction 1* Sliding window step 1* Set shared size 7* Train TS IDs 0.6 Val TS IDs 0.1 Test TS IDs 0.3 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/univariate_forecasting/generic_model/e3de1fc0a44e/","title":"Benchmark e3de1fc0a44e","text":"Parameter Value Benchmark hash e3de1fc0a44e Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source IP_ADDRESSES_SAMPLE Type Disjoint-Time-Based Train time size 0.6 Val time size 0.1 Test time size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 1008* Sliding window prediction 1* Sliding window step 1* Set shared size 1008* Train TS IDs 0.6 Val TS IDs 0.1 Test TS IDs 0.3 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/univariate_forecasting/generic_model/ef632e70c252/","title":"Benchmark ef632e70c252","text":"Parameter Value Benchmark hash ef632e70c252 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source INSTITUTIONS Type Disjoint-Time-Based Train time size 0.6 Val time size 0.1 Test time size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 7* Sliding window prediction 1* Sliding window step 1* Set shared size 7* Train TS IDs 0.6 Val TS IDs 0.1 Test TS IDs 0.3 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work RMSE R2-score SMAPE"},{"location":"benchmarks/univariate_forecasting/unique_model/080582bcd519/","title":"Benchmark 080582bcd519","text":"Parameter Value Benchmark hash 080582bcd519 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTION_SUBNETS Type Time-Based Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 168* Sliding window prediction 1* Sliding window step 1* Set shared size 168* TS IDs 1.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/univariate_forecasting/unique_model/095f847ca755/","title":"Benchmark 095f847ca755","text":"Parameter Value Benchmark hash 095f847ca755 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source INSTITUTION_SUBNETS Type Time-Based Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 1008* Sliding window prediction 1* Sliding window step 1* Set shared size 1008* TS IDs 1.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/univariate_forecasting/unique_model/0d523e69c328/","title":"Benchmark 0d523e69c328","text":"Parameter Value Benchmark hash 0d523e69c328 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source INSTITUTIONS Type Time-Based Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 7* Sliding window prediction 1* Sliding window step 1* Set shared size 7* TS IDs 1.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/univariate_forecasting/unique_model/22c5a8e8ffd3/","title":"Benchmark 22c5a8e8ffd3","text":"Parameter Value Benchmark hash 22c5a8e8ffd3 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source INSTITUTIONS Type Time-Based Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 1008* Sliding window prediction 1* Sliding window step 1* Set shared size 1008* TS IDs 1.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/univariate_forecasting/unique_model/871f5972109e/","title":"Benchmark 871f5972109e","text":"Parameter Value Benchmark hash 871f5972109e Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTIONS Type Time-Based Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 168* Sliding window prediction 1* Sliding window step 1* Set shared size 168* TS IDs 1.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/univariate_forecasting/unique_model/8e2a07fb3177/","title":"Benchmark 8e2a07fb3177","text":"Parameter Value Benchmark hash 8e2a07fb3177 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source INSTITUTION_SUBNETS Type Time-Based Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 7* Sliding window prediction 1* Sliding window step 1* Set shared size 7* TS IDs 1.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/univariate_forecasting/unique_model/b5e5ea044b81/","title":"Benchmark b5e5ea044b81","text":"Parameter Value Benchmark hash b5e5ea044b81 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source IP_ADDRESSES_SAMPLE Type Time-Based Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 7* Sliding window prediction 1* Sliding window step 1* Set shared size 78* TS IDs 1.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/univariate_forecasting/unique_model/c2970e89d824/","title":"Benchmark c2970e89d824","text":"Parameter Value Benchmark hash c2970e89d824 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source IP_ADDRESSES_SAMPLE Type Time-Based Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 1008* Sliding window prediction 1* Sliding window step 1* Set shared size 1008* TS IDs 1.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/univariate_forecasting/unique_model/d19ba386743f/","title":"Benchmark d19ba386743f","text":"Parameter Value Benchmark hash d19ba386743f Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_DAY Source IP_ADDRESSES_FULL Type Time-Based Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 7* Sliding window prediction 1* Sliding window step 1* Set shared size 7* TS IDs 1.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/univariate_forecasting/unique_model/ddb1f02dae43/","title":"Benchmark ddb1f02dae43","text":"Parameter Value Benchmark hash ddb1f02dae43 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_10_MINUTES Source IP_ADDRESSES_FULL Type Time-Based Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 1008* Sliding window prediction 1* Sliding window step 1* Set shared size 1008* TS IDs 1.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/univariate_forecasting/unique_model/e268fa9957f2/","title":"Benchmark e268fa9957f2","text":"Parameter Value Benchmark hash e268fa9957f2 Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source IP_ADDRESSES_FULL Type Time-Based Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 168* Sliding window prediction 1* Sliding window step 1* Set shared size 168* TS IDs 1.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"benchmarks/univariate_forecasting/unique_model/f3fc14310e2e/","title":"Benchmark f3fc14310e2e","text":"Parameter Value Benchmark hash f3fc14310e2e Original paper None Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source IP_ADDRESSES_SAMPLE Type Time-Based Train size 0.6 Val size 0.1 Test size 0.3 Uni/Multi variate Univariate Metrics n_bytes Default value None* Filler None* Transformer None* Anomaly handler None* Sliding window train 168* Sliding window prediction 1* Sliding window step 1* Set shared size 168* TS IDs 1.0 <p>Note</p> <p>Values marked with the * can users change in the benchmark.</p> Related work Average RMSE Std RMSE Average R2-score Std R2-score Average SMAPE Std SMAPE"},{"location":"related_works/arXiv_2503.17410/","title":"Comparative Analysis of Deep Learning Models for Real-World ISP Network Traffic Forecasting","text":"<p>These benchmarks were used in the paper \"Koumar, J., Smole\u0148, T., Je\u0159\u00e1bek, K. and \u010cejka, T., 2025. Comparative Analysis of Deep Learning Models for Real-World ISP Network Traffic Forecasting. arXiv preprint arXiv:2503.17410.\".</p> Benchmark hash Dataset Aggregation Source 2439d12c2292 CESNET-TimeSeries24 1 HOUR INSTITUTIONS 63882fe052f8 CESNET-TimeSeries24 1 HOUR INSTITUTIONS 5a79f6cd3506 CESNET-TimeSeries24 1 HOUR INSTITUTIONS 5a7471b2c70b CESNET-TimeSeries24 1 HOUR INSTITUTIONS 0f4fbc0419ce CESNET-TimeSeries24 1 HOUR INSTITUTIONS d166d3b19a87 CESNET-TimeSeries24 1 HOUR INSTITUTION_SUBNETS 2112383abab7 CESNET-TimeSeries24 1 HOUR INSTITUTION_SUBNETS 5d9e6e63cbf0 CESNET-TimeSeries24 1 HOUR INSTITUTION_SUBNETS d82139cd671f CESNET-TimeSeries24 1 HOUR INSTITUTION_SUBNETS d03ba8db5892 CESNET-TimeSeries24 1 HOUR INSTITUTION_SUBNETS 2e92831cb502 CESNET-TimeSeries24 1 HOUR IP_ADDRESSES_SAMPLE 702e58166879 CESNET-TimeSeries24 1 HOUR IP_ADDRESSES_SAMPLE 394603854070 CESNET-TimeSeries24 1 HOUR IP_ADDRESSES_SAMPLE 420d4303f949 CESNET-TimeSeries24 1 HOUR IP_ADDRESSES_SAMPLE e2c2148a178c CESNET-TimeSeries24 1 HOUR IP_ADDRESSES_SAMPLE <p>Example of usage of this related works benchmarks:</p> <pre><code>from cesnet_tszoo.benchmarks import load_benchmark\nfrom cesnet_tszoo.utils.enums import FillerType, TransformerType\n\nbenchmark = load_benchmark(\"2439d12c2292\", \"../\")\ndataset = benchmark.get_initialized_dataset()\n\n# Get related results\nrelated_results = benchmark.get_related_results()\nprint(related_results)\n\n# Process with your own defined model\nresults = []\nfor ts_id in tqdm.tqdm(dataset.get_data_about_set(about='train')['ts_ids']):\n    model = SimpleLSTM().to(device)\n    model.fit(\n        dataset.get_train_dataloader(ts_id), \n        dataset.get_val_dataloader(ts_id), \n        n_epochs=5, \n        device=device,\n    )\n    y_pred, y_true = model.predict(\n        dataset.get_test_dataloader(ts_id), \n        device=device,\n    )\n\n    rmse = mean_squared_error(y_true, y_pred)\n    results.append(rmse)\n\n_mean = round(np.mean(results), 3)\n_std = round(np.std(results), 3)\nprint(f\"Mean RMSE: {_mean}\")\nprint(f\"Std RMSE: {_std}\") \n\n# Compare with related works results\nbetter_works = related_results[related_results['Avg. RMSE'] &lt; _mean]\nworse_works = related_results[related_results['Avg. RMSE'] &lt;= _mean]\nprint(better_works)\nprint(worse_works)\n</code></pre>"},{"location":"related_works/arXiv_2503.17410/0f4fbc0419ce/","title":"Benchmark 0f4fbc0419ce","text":"Parameter Value Benchmark hash 0f4fbc0419ce Original paper https://doi.org/10.48550/arXiv.2503.17410 Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTIONS Type Time-Based Train size 0.35 Val size 0.05 Test size 0.6 Uni/Multi variate Univariate Metrics n_bytes Default value 0 Filler None Transformer MinMaxScaler Anomaly handler None Sliding window train 744 Sliding window prediction 168 Sliding window step 168 Set shared size 744 TS IDs 1.0 Related work Model Average RMSE Std RMSE Average R2-score Std R2-score https://arxiv.org/abs/2503.17410 GRU 0.165 0.82 -0.53 1.3 https://arxiv.org/abs/2503.17410 GRU_FCN 0.165 0.82 -1.08 2.8 https://arxiv.org/abs/2503.17410 INCEPTIONTIME 0.519 0.78 -9.18 2.3 https://arxiv.org/abs/2503.17410 LSTM 0.165 0.82 -0.49 1.2 https://arxiv.org/abs/2503.17410 LSTM_FCN 0.164 0.82 -0.54 1.4 https://arxiv.org/abs/2503.17410 MEAN 0.152 0.76 -0.01 0.1 https://arxiv.org/abs/2503.17410 RCLSTM 0.193 0.83 -1.8 2.4 https://arxiv.org/abs/2503.17410 RESNET 0.173 0.82 -0.74 1.5"},{"location":"related_works/arXiv_2503.17410/2112383abab7/","title":"Benchmark 2112383abab7","text":"Parameter Value Benchmark hash 2112383abab7 Original paper https://doi.org/10.48550/arXiv.2503.17410 Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTION_SUBNETS Type Time-Based Train size 0.35 Val size 0.05 Test size 0.6 Uni/Multi variate Univariate Metrics n_bytes Default value 0 Filler None Transformer MinMaxScaler Anomaly handler None Sliding window train 168 Sliding window prediction 1 Sliding window step 1 Set shared size 168 TS IDs 1.0 Related work Model Average RMSE Std RMSE Average R2-score Std R2-score https://arxiv.org/abs/2503.17410 GRU 0.219 1.15 -0.1 1.2 https://arxiv.org/abs/2503.17410 GRU_FCN 0.219 1.15 0.05 0.9 https://arxiv.org/abs/2503.17410 INCEPTIONTIME 0.238 1.15 -1.57 3.0 https://arxiv.org/abs/2503.17410 LSTM 0.22 1.15 -0.07 1.2 https://arxiv.org/abs/2503.17410 LSTM_FCN 0.219 1.15 -0.01 1.2 https://arxiv.org/abs/2503.17410 MEAN 0.392 1.66 0.06 0.1 https://arxiv.org/abs/2503.17410 RCLSTM 0.277 1.36 0.09 1.0 https://arxiv.org/abs/2503.17410 RESNET 0.235 1.15 -0.68 2.1"},{"location":"related_works/arXiv_2503.17410/2439d12c2292/","title":"Benchmark 2439d12c2292","text":"Parameter Value Benchmark hash 2439d12c2292 Original paper https://doi.org/10.48550/arXiv.2503.17410 Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTIONS Type Time-Based Train size 0.35 Val size 0.05 Test size 0.6 Uni/Multi variate Univariate Metrics n_bytes Default value 0 Filler None Transformer MinMaxScaler Anomaly handler None Sliding window train 24 Sliding window prediction 1 Sliding window step 1 Set shared size 24 TS IDs 1.0 Related work Model Average RMSE Std RMSE Average R2-score Std R2-score https://arxiv.org/abs/2503.17410 GRU 0.104 0.53 0.08 0.8 https://arxiv.org/abs/2503.17410 GRU_FCN 0.102 0.55 0.15 1.0 https://arxiv.org/abs/2503.17410 INCEPTIONTIME 0.112 0.55 -0.77 2.5 https://arxiv.org/abs/2503.17410 LSTM 0.105 0.54 0.09 0.8 https://arxiv.org/abs/2503.17410 LSTM_FCN 0.102 0.54 0.19 0.7 https://arxiv.org/abs/2503.17410 MEAN 0.146 0.75 0.09 0.1 https://arxiv.org/abs/2503.17410 RCLSTM 0.14 0.8 0.2 0.9 https://arxiv.org/abs/2503.17410 RESNET 0.106 0.55 0.06 1.1"},{"location":"related_works/arXiv_2503.17410/2e92831cb502/","title":"Benchmark 2e92831cb502","text":"Parameter Value Benchmark hash 2e92831cb502 Original paper https://doi.org/10.48550/arXiv.2503.17410 Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source IP_ADDRESSES_SAMPLE Type Time-Based Train size 0.35 Val size 0.05 Test size 0.6 Uni/Multi variate Univariate Metrics n_bytes Default value 0 Filler None Transformer MinMaxScaler Anomaly handler None Sliding window train 24 Sliding window prediction 1 Sliding window step 1 Set shared size 24 TS IDs 1.0 Related work Model Average RMSE Std RMSE Average R2-score Std R2-score https://arxiv.org/abs/2503.17410 GRU 0.149 0.82 -0.46 1.9 https://arxiv.org/abs/2503.17410 GRU_FCN 0.15 0.82 -0.12 1.1 https://arxiv.org/abs/2503.17410 INCEPTIONTIME 0.165 0.82 -2.7 3.9 https://arxiv.org/abs/2503.17410 LSTM 0.15 0.82 -0.41 1.8 https://arxiv.org/abs/2503.17410 LSTM_FCN 0.151 0.82 -0.44 1.9 https://arxiv.org/abs/2503.17410 MEAN 1.01 2.86 0.0 0.1 https://arxiv.org/abs/2503.17410 RCLSTM 0.221 1.08 -0.09 1.0 https://arxiv.org/abs/2503.17410 RESNET 0.152 0.82 -0.81 2.4"},{"location":"related_works/arXiv_2503.17410/394603854070/","title":"Benchmark 394603854070","text":"Parameter Value Benchmark hash 394603854070 Original paper https://doi.org/10.48550/arXiv.2503.17410 Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source IP_ADDRESSES_SAMPLE Type Time-Based Train size 0.35 Val size 0.05 Test size 0.6 Uni/Multi variate Univariate Metrics n_bytes Default value 0 Filler None Transformer MinMaxScaler Anomaly handler None Sliding window train 168 Sliding window prediction 24 Sliding window step 24 Set shared size 168 TS IDs 1.0 Related work Model Average RMSE Std RMSE Average R2-score Std R2-score https://arxiv.org/abs/2503.17410 GRU 0.154 0.82 -0.31 1.5 https://arxiv.org/abs/2503.17410 GRU_FCN 0.154 0.82 -0.61 2.1 https://arxiv.org/abs/2503.17410 INCEPTIONTIME 0.573 0.77 -9.22 2.5 https://arxiv.org/abs/2503.17410 LSTM 0.154 0.82 -0.35 1.6 https://arxiv.org/abs/2503.17410 LSTM_FCN 0.158 0.82 -1.28 3.0 https://arxiv.org/abs/2503.17410 MEAN 1.011 2.86 0.01 0.1 https://arxiv.org/abs/2503.17410 RCLSTM 0.247 1.17 -0.99 2.6 https://arxiv.org/abs/2503.17410 RESNET 0.159 0.82 -0.92 2.6"},{"location":"related_works/arXiv_2503.17410/420d4303f949/","title":"Benchmark 420d4303f949","text":"Parameter Value Benchmark hash 420d4303f949 Original paper https://doi.org/10.48550/arXiv.2503.17410 Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source IP_ADDRESSES_SAMPLE Type Time-Based Train size 0.35 Val size 0.05 Test size 0.6 Uni/Multi variate Univariate Metrics n_bytes Default value 0 Filler None Transformer MinMaxScaler Anomaly handler None Sliding window train 744 Sliding window prediction 1 Sliding window step 1 Set shared size 744 TS IDs 1.0 Related work Model Average RMSE Std RMSE Average R2-score Std R2-score https://arxiv.org/abs/2503.17410 GRU 0.15 0.82 -0.38 1.7 https://arxiv.org/abs/2503.17410 GRU_FCN 0.153 0.82 -0.25 1.3 https://arxiv.org/abs/2503.17410 INCEPTIONTIME 0.183 0.82 -3.57 4.2 https://arxiv.org/abs/2503.17410 LSTM 0.151 0.82 -0.39 1.8 https://arxiv.org/abs/2503.17410 LSTM_FCN 0.154 0.82 -0.62 2.2 https://arxiv.org/abs/2503.17410 MEAN 1.013 2.86 -0.08 0.9 https://arxiv.org/abs/2503.17410 RCLSTM 0.166 0.89 -0.29 1.6 https://arxiv.org/abs/2503.17410 RESNET 0.163 0.82 -1.17 3.0"},{"location":"related_works/arXiv_2503.17410/5a7471b2c70b/","title":"Benchmark 5a7471b2c70b","text":"Parameter Value Benchmark hash 5a7471b2c70b Original paper https://doi.org/10.48550/arXiv.2503.17410 Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTIONS Type Time-Based Train size 0.35 Val size 0.05 Test size 0.6 Uni/Multi variate Univariate Metrics n_bytes Default value 0 Filler None Transformer MinMaxScaler Anomaly handler None Sliding window train 744 Sliding window prediction 1 Sliding window step 1 Set shared size 744 TS IDs 1.0 Related work Model Average RMSE Std RMSE Average R2-score Std R2-score https://arxiv.org/abs/2503.17410 GRU 0.106 0.53 0.06 0.7 https://arxiv.org/abs/2503.17410 GRU_FCN 0.111 0.55 -0.05 1.0 https://arxiv.org/abs/2503.17410 INCEPTIONTIME 0.136 0.55 -1.59 2.8 https://arxiv.org/abs/2503.17410 LSTM 0.106 0.53 0.07 0.8 https://arxiv.org/abs/2503.17410 LSTM_FCN 0.11 0.53 -0.09 1.2 https://arxiv.org/abs/2503.17410 MEAN 0.151 0.75 0.01 0.1 https://arxiv.org/abs/2503.17410 RCLSTM 0.112 0.58 0.23 0.7 https://arxiv.org/abs/2503.17410 RESNET 0.146 0.56 -0.92 2.4"},{"location":"related_works/arXiv_2503.17410/5a79f6cd3506/","title":"Benchmark 5a79f6cd3506","text":"Parameter Value Benchmark hash 5a79f6cd3506 Original paper https://doi.org/10.48550/arXiv.2503.17410 Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTIONS Type Time-Based Train size 0.35 Val size 0.05 Test size 0.6 Uni/Multi variate Univariate Metrics n_bytes Default value 0 Filler None Transformer MinMaxScaler Anomaly handler None Sliding window train 168 Sliding window prediction 24 Sliding window step 24 Set shared size 168 TS IDs 1.0 Related work Model Average RMSE Std RMSE Average R2-score Std R2-score https://arxiv.org/abs/2503.17410 GRU 0.123 0.55 -0.45 1.2 https://arxiv.org/abs/2503.17410 GRU_FCN 0.115 0.55 -0.18 1.1 https://arxiv.org/abs/2503.17410 INCEPTIONTIME 0.483 0.51 -9.22 2.2 https://arxiv.org/abs/2503.17410 LSTM 0.124 0.55 -0.45 1.3 https://arxiv.org/abs/2503.17410 LSTM_FCN 0.117 0.55 -0.29 1.3 https://arxiv.org/abs/2503.17410 MEAN 0.15 0.75 0.03 0.1 https://arxiv.org/abs/2503.17410 RCLSTM 0.163 0.81 -0.37 1.5 https://arxiv.org/abs/2503.17410 RESNET 0.131 0.55 -0.73 1.7"},{"location":"related_works/arXiv_2503.17410/5d9e6e63cbf0/","title":"Benchmark 5d9e6e63cbf0","text":"Parameter Value Benchmark hash 5d9e6e63cbf0 Original paper https://doi.org/10.48550/arXiv.2503.17410 Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTION_SUBNETS Type Time-Based Train size 0.35 Val size 0.05 Test size 0.6 Uni/Multi variate Univariate Metrics n_bytes Default value 0 Filler None Transformer MinMaxScaler Anomaly handler None Sliding window train 168 Sliding window prediction 24 Sliding window step 24 Set shared size 168 TS IDs 1.0 Related work Model Average RMSE Std RMSE Average R2-score Std R2-score https://arxiv.org/abs/2503.17410 GRU 0.237 1.15 -0.55 1.4 https://arxiv.org/abs/2503.17410 GRU_FCN 0.229 1.15 -0.27 1.1 https://arxiv.org/abs/2503.17410 INCEPTIONTIME 0.59 1.1 -8.9 2.7 https://arxiv.org/abs/2503.17410 LSTM 0.237 1.15 -0.53 1.3 https://arxiv.org/abs/2503.17410 LSTM_FCN 0.231 1.15 -0.55 1.8 https://arxiv.org/abs/2503.17410 MEAN 0.394 1.66 0.05 0.1 https://arxiv.org/abs/2503.17410 RCLSTM 0.334 1.48 -0.64 1.9 https://arxiv.org/abs/2503.17410 RESNET 0.244 1.15 -0.77 1.6"},{"location":"related_works/arXiv_2503.17410/63882fe052f8/","title":"Benchmark 63882fe052f8","text":"Parameter Value Benchmark hash 63882fe052f8 Original paper https://doi.org/10.48550/arXiv.2503.17410 Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTIONS Type Time-Based Train size 0.35 Val size 0.05 Test size 0.6 Uni/Multi variate Univariate Metrics n_bytes Default value 0 Filler None Transformer MinMaxScaler Anomaly handler None Sliding window train 168 Sliding window prediction 1 Sliding window step 1 Set shared size 168 TS IDs 1.0 Related work Model Average RMSE Std RMSE Average R2-score Std R2-score https://arxiv.org/abs/2503.17410 GRU 0.105 0.53 0.03 0.9 https://arxiv.org/abs/2503.17410 GRU_FCN 0.104 0.55 0.17 0.7 https://arxiv.org/abs/2503.17410 INCEPTIONTIME 0.124 0.55 -1.08 2.5 https://arxiv.org/abs/2503.17410 LSTM 0.106 0.54 0.1 0.7 https://arxiv.org/abs/2503.17410 LSTM_FCN 0.103 0.54 0.14 0.9 https://arxiv.org/abs/2503.17410 MEAN 0.149 0.75 0.04 0.1 https://arxiv.org/abs/2503.17410 RCLSTM 0.14 0.8 0.2 0.9 https://arxiv.org/abs/2503.17410 RESNET 0.127 0.55 -0.63 2.2"},{"location":"related_works/arXiv_2503.17410/702e58166879/","title":"Benchmark 702e58166879","text":"Parameter Value Benchmark hash 702e58166879 Original paper https://doi.org/10.48550/arXiv.2503.17410 Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source IP_ADDRESSES_SAMPLE Type Time-Based Train size 0.35 Val size 0.05 Test size 0.6 Uni/Multi variate Univariate Metrics n_bytes Default value 0 Filler None Transformer MinMaxScaler Anomaly handler None Sliding window train 168 Sliding window prediction 1 Sliding window step 1 Set shared size 168 TS IDs 1.0 Related work Model Average RMSE Std RMSE Average R2-score Std R2-score https://arxiv.org/abs/2503.17410 GRU 0.15 0.82 -0.41 1.8 https://arxiv.org/abs/2503.17410 GRU_FCN 0.152 0.82 -0.18 1.2 https://arxiv.org/abs/2503.17410 INCEPTIONTIME 0.168 0.82 -2.82 3.9 https://arxiv.org/abs/2503.17410 LSTM 0.151 0.82 -0.4 1.8 https://arxiv.org/abs/2503.17410 LSTM_FCN 0.152 0.82 -0.66 2.3 https://arxiv.org/abs/2503.17410 MEAN 1.011 2.86 0.01 0.1 https://arxiv.org/abs/2503.17410 RCLSTM 0.226 1.12 -0.12 1.1 https://arxiv.org/abs/2503.17410 RESNET 0.158 0.82 -1.03 2.8"},{"location":"related_works/arXiv_2503.17410/d03ba8db5892/","title":"Benchmark d03ba8db5892","text":"Parameter Value Benchmark hash d03ba8db5892 Original paper https://doi.org/10.48550/arXiv.2503.17410 Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTION_SUBNETS Type Time-Based Train size 0.35 Val size 0.05 Test size 0.6 Uni/Multi variate Univariate Metrics n_bytes Default value 0 Filler None Transformer MinMaxScaler Anomaly handler None Sliding window train 744 Sliding window prediction 168 Sliding window step 168 Set shared size 744 TS IDs 1.0 Related work Model Average RMSE Std RMSE Average R2-score Std R2-score https://arxiv.org/abs/2503.17410 GRU 0.265 1.23 -0.61 1.4 https://arxiv.org/abs/2503.17410 GRU_FCN 0.269 1.23 -1.67 3.3 https://arxiv.org/abs/2503.17410 INCEPTIONTIME 0.613 1.18 -8.85 2.8 https://arxiv.org/abs/2503.17410 LSTM 0.265 1.23 -0.55 1.3 https://arxiv.org/abs/2503.17410 LSTM_FCN 0.265 1.23 -0.94 2.2 https://arxiv.org/abs/2503.17410 MEAN 0.402 1.69 0.0 0.1 https://arxiv.org/abs/2503.17410 RCLSTM 0.326 1.37 -2.26 2.9 https://arxiv.org/abs/2503.17410 RESNET 0.275 1.23 -0.91 1.8"},{"location":"related_works/arXiv_2503.17410/d166d3b19a87/","title":"Benchmark d166d3b19a87","text":"Parameter Value Benchmark hash d166d3b19a87 Original paper https://doi.org/10.48550/arXiv.2503.17410 Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTION_SUBNETS Type Time-Based Train size 0.35 Val size 0.05 Test size 0.6 Uni/Multi variate Univariate Metrics n_bytes Default value 0 Filler None Transformer MinMaxScaler Anomaly handler None Sliding window train 24 Sliding window prediction 1 Sliding window step 1 Set shared size 24 TS IDs 1.0 Related work Model Average RMSE Std RMSE Average R2-score Std R2-score https://arxiv.org/abs/2503.17410 GRU 0.218 1.14 -0.1 1.3 https://arxiv.org/abs/2503.17410 GRU_FCN 0.217 1.15 0.06 0.8 https://arxiv.org/abs/2503.17410 INCEPTIONTIME 0.23 1.15 -1.29 3.1 https://arxiv.org/abs/2503.17410 LSTM 0.219 1.15 -0.05 1.1 https://arxiv.org/abs/2503.17410 LSTM_FCN 0.217 1.15 0.08 0.8 https://arxiv.org/abs/2503.17410 MEAN 0.383 1.63 0.12 0.2 https://arxiv.org/abs/2503.17410 RCLSTM 0.323 1.5 0.08 1.1 https://arxiv.org/abs/2503.17410 RESNET 0.22 1.15 -0.15 1.4"},{"location":"related_works/arXiv_2503.17410/d82139cd671f/","title":"Benchmark d82139cd671f","text":"Parameter Value Benchmark hash d82139cd671f Original paper https://doi.org/10.48550/arXiv.2503.17410 Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source INSTITUTION_SUBNETS Type Time-Based Train size 0.35 Val size 0.05 Test size 0.6 Uni/Multi variate Univariate Metrics n_bytes Default value 0 Filler None Transformer MinMaxScaler Anomaly handler None Sliding window train 744 Sliding window prediction 1 Sliding window step 1 Set shared size 744 TS IDs 1.0 Related work Model Average RMSE Std RMSE Average R2-score Std R2-score https://arxiv.org/abs/2503.17410 GRU 0.22 1.14 -0.1 1.2 https://arxiv.org/abs/2503.17410 GRU_FCN 0.228 1.15 -0.19 1.1 https://arxiv.org/abs/2503.17410 INCEPTIONTIME 0.253 1.15 -2.19 3.4 https://arxiv.org/abs/2503.17410 LSTM 0.22 1.15 -0.09 1.1 https://arxiv.org/abs/2503.17410 LSTM_FCN 0.228 1.15 -0.24 1.3 https://arxiv.org/abs/2503.17410 MEAN 0.4 1.68 0.02 0.1 https://arxiv.org/abs/2503.17410 RCLSTM 0.225 1.16 0.1 1.0 https://arxiv.org/abs/2503.17410 RESNET 0.255 1.15 -1.05 2.5"},{"location":"related_works/arXiv_2503.17410/e2c2148a178c/","title":"Benchmark e2c2148a178c","text":"Parameter Value Benchmark hash e2c2148a178c Original paper https://doi.org/10.48550/arXiv.2503.17410 Dataset CESNET-TimeSeries24 Aggregation AGG_1_HOUR Source IP_ADDRESSES_SAMPLE Type Time-Based Train size 0.35 Val size 0.05 Test size 0.6 Uni/Multi variate Univariate Metrics n_bytes Default value 0 Filler None Transformer MinMaxScaler Anomaly handler None Sliding window train 744 Sliding window prediction 168 Sliding window step 168 Set shared size 744 TS IDs 1.0 Related work Model Average RMSE Std RMSE Average R2-score Std R2-score https://arxiv.org/abs/2503.17410 GRU 0.179 0.93 -0.5 1.8 https://arxiv.org/abs/2503.17410 GRU_FCN 0.214 0.92 -3.97 4.2 https://arxiv.org/abs/2503.17410 INCEPTIONTIME 0.593 0.88 -9.17 2.5 https://arxiv.org/abs/2503.17410 LSTM 0.179 0.93 -0.34 1.5 https://arxiv.org/abs/2503.17410 LSTM_FCN 0.189 0.93 -1.52 3.2 https://arxiv.org/abs/2503.17410 MEAN 1.014 2.86 -0.09 0.9 https://arxiv.org/abs/2503.17410 RCLSTM 0.234 1.08 -2.77 3.8 https://arxiv.org/abs/2503.17410 RESNET 0.182 0.93 -0.52 1.9"}]}